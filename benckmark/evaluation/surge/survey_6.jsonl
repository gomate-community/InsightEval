{"id": "c7a70291-a1e9-476f-8254-e897ee781284", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "44777335-fb2f-4b5d-83e4-47fe65116410", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Introduction"]], "content": "\\label{sec1}\nIn a narrow definition, deepfakes (stemming from ``deep learning\" and ``fake\") are created by techniques that can superimpose face images of a target person onto a video of a source person to make a video of the target person doing or saying things the source person does. This constitutes a category of deepfakes, namely \\emph{face-swap}. In a broader definition, deepfakes are artificial intelligence-synthesized content that can also fall into two other categories, i.e., \\emph{lip-sync} and \\emph{puppet-master}. Lip-sync deepfakes refer to videos that are modified to make the mouth movements consistent with an audio recording. Puppet-master deepfakes include videos of a target person (puppet) who is animated following the facial expressions, eye and head movements of another person (master) sitting in front of a camera .\nWhile some deepfakes can be created by traditional visual effects or computer-graphics approaches, the recent common underlying mechanism for deepfake creation is deep learning models such as autoencoders and generative adversarial networks (GANs), which have been applied widely in the computer vision domain . These models are used to examine facial expressions and movements of a person and synthesize facial images of another person making analogous expressions and movements . Deepfake methods normally require a large amount of image and video data to train models to create photo-realistic images and videos. As public figures such as celebrities and politicians may have a large number of videos and images available online, they are initial targets of deepfakes. Deepfakes were used to swap faces of celebrities or politicians to bodies in porn images and videos. The first deepfake video emerged in 2017 where face of a celebrity was swapped to the face of a porn actor. It is threatening to world security when deepfake methods can be employed to create videos of world leaders with fake speeches for falsification purposes . Deepfakes therefore can be abused to cause political or religion tensions between countries, to fool public and affect results in election campaigns, or create chaos in financial markets by creating fake news . It can be even used to generate fake satellite images of the Earth to contain objects that do not really exist to confuse military analysts, e.g., creating a fake bridge across a river although there is no such a bridge in reality. This can mislead a troop who have been guided to cross the bridge in a battle .\nAs the democratization of creating realistic digital humans has positive implications, there is also positive use of deepfakes such as their applications in visual effects, digital avatars, snapchat filters, creating voices of those who have lost theirs or updating episodes of movies without reshooting them . Deepfakes can have creative or productive impacts in photography, video games, virtual reality, movie productions, and entertainment, e.g., realistic video dubbing of foreign films, education through the reanimation of historical figures, virtually trying on clothes while shopping, and so on . However, the number of malicious uses of deepfakes largely dominates that of the positive ones. The development of advanced deep neural networks and the availability of large amount of data have made the forged images and videos almost indistinguishable to humans and even to sophisticated computer algorithms. The process of creating those manipulated images and videos is also much simpler today as it needs as little as an identity photo or a short video of a target individual. Less and less effort is required to produce a stunningly convincing tempered footage. Recent advances can even create a deepfake with just a still image . Deepfakes therefore can be a threat affecting not only public figures but also ordinary people. For example, a voice deepfake was used to scam a CEO out of \\$243,000 . A recent release of a software called DeepNude shows more disturbing threats as it can transform a person to a non-consensual porn . Likewise, the Chinese app Zao has gone viral lately as less-skilled users can swap their faces onto bodies of movie stars and insert themselves into well-known movies and TV clips . These forms of falsification create a huge threat to violation of privacy and identity, and affect many aspects of human lives.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=0.85\\columnwidth]{Fig1.pdf}\n\\caption{Number of papers related to deepfakes in years from 2016 to 2021, obtained from https://app.dimensions.ai at the end of 2021 with the search keyword ``deepfake\" applied to full text of scholarly papers.}\n\\label{fig0} \n\\end{figure}\nFinding the truth in digital domain therefore has become increasingly critical. It is even more challenging when dealing with deepfakes as they are majorly used to serve malicious purposes and almost anyone can create deepfakes these days using existing deepfake tools. Thus far, there have been numerous methods proposed to detect deepfakes . Most of them are based on deep learning, and thus a battle between malicious and positive uses of deep learning methods has been arising. To address the threat of face-swapping technology or deepfakes, the United States Defense Advanced Research Projects Agency (DARPA) initiated a research scheme in media forensics (named Media Forensics or MediFor) to accelerate the development of fake digital visual media detection methods . Recently, Facebook Inc. teaming up with Microsoft Corp and the Partnership on AI coalition have launched the Deepfake Detection Challenge to catalyse more research and development in detecting and preventing deepfakes from being used to mislead viewers . Data obtained from https://app.dimensions.ai at the end of 2021 show that the number of deepfake papers has increased significantly in recent years (Fig. \\ref{fig0}). Although the obtained numbers of deepfake papers may be lower than actual numbers but the research trend of this topic is obviously increasing.\nThere have been existing survey papers about creating and detecting deepfakes, presented in . For example,  focused on reenactment approaches (i.e., to change a target’s expression, mouth, pose, gaze or body), and replacement approaches (i.e., to replace a target’s face by swap or transfer methods).  separated detection approaches into conventional methods (e.g., blind methods without using any external data for training, one-class sensor-based and model-based methods, and supervised methods with handcrafted features) and deep learning-based approaches (e.g., CNN models).  categorized both creation and detection methods based on the way deepfakes are created, including entire face synthesis, identity swap, attribute manipulation, and expression swap. On the other hand, we carry out the survey with a different perspective and taxonomy. We categorize the deepfake detection methods based on the data type, i.e., images or videos, as presented in Fig. \\ref{fig2}. With fake image detection methods, we focus on the features that are used, i.e., whether they are handcrafted features or deep features. With fake video detection methods, two main subcategories are identified based on whether the method uses temporal features across frames or visual artifacts within a video frame. We also discuss extensively the challenges, research trends and directions on deepfake detection and multimedia forensics problems.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{Fig2.pdf}\n\\caption{Categories of reviewed papers relevant to deepfake detection methods where we divide papers into two major groups, i.e., fake image detection and face video detection.}\n\\label{fig2} \n\\end{figure}", "cites": [5133, 5128, 5132, 5130, 5131, 5680, 5129, 1602, 986, 1256, 4939], "cite_extract_rate": 0.5, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to define deepfakes and their categories, and introduces the survey's unique taxonomy for detection methods. It provides a critical view by contrasting positive and malicious uses of deepfakes and highlighting detection challenges. However, the analysis remains somewhat at the surface level and lacks deeper evaluation of the cited works' strengths and weaknesses."}}
{"id": "b11ed0ba-b192-4a4a-b063-d32fbaab94d4", "title": "Deepfake Creation", "level": "section", "subsections": [], "parent_id": "44777335-fb2f-4b5d-83e4-47fe65116410", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Creation"]], "content": "\\label{sec2}\nDeepfakes have become popular due to the quality of tampered videos and also the easy-to-use ability of their applications to a wide range of users with various computer skills from professional to novice. These applications are mostly developed based on deep learning techniques. Deep learning is well known for its capability of representing complex and high-dimensional data. One variant of the deep networks with that capability is deep autoencoders, which have been widely applied for dimensionality reduction and image compression . The first attempt of deepfake creation was FakeApp, developed by a Reddit user using autoencoder-decoder pairing structure . In that method, the autoencoder extracts latent features of face images and the decoder is used to reconstruct the face images. To swap faces between source images and target images, there is a need of two encoder-decoder pairs where each pair is used to train on an image set, and the encoder's parameters are shared between two network pairs. In other words, two pairs have the same encoder network. This strategy enables the common encoder to find and learn the similarity between two sets of face images, which are relatively unchallenging because faces normally have similar features such as eyes, nose, mouth positions. Fig. \\ref{fig1} shows a deepfake creation process where the feature set of face A is connected with the decoder B to reconstruct face B from the original face A. This approach is applied in several works such as DeepFaceLab , DFaker , DeepFake\\_tf (tensorflow-based deepfakes) .\n\\begin{table*}\n\\centering\n\\begin{scriptsize}\n\\caption{Summary of notable deepfake tools}\n\\label{table1}\n\\begin{tabular}{p{0.10\\textwidth} p{0.29\\textwidth} p{0.54\\textwidth}}\n\\hline\n\\textbf{Tools} & \\textbf{Links} & \\textbf{Key Features}\\\\\n\\hline\nFaceswap\n&https://github.com/deepfakes/faceswap\n&- Using two encoder-decoder pairs.\\newline\n- Parameters of the encoder are shared.\\\\\n\\hline\nFaceswap-GAN\n&https://github.com/shaoanlu/faceswap-GAN\n&Adversarial loss and perceptual loss (VGGface) are added to an auto-encoder architecture.\\\\\n\\hline\nFew-Shot Face Translation\n& https://github.com/shaoanlu/fewshot-face-translation-GAN\n& - Use a pre-trained face recognition model to extract latent embeddings for GAN processing.\\newline\n- Incorporate semantic priors obtained by modules from FUNIT  and SPADE .\\\\\n\\hline\nDeepFaceLab\n&https://github.com/iperov/DeepFaceLab\n&- Expand from the Faceswap method with new models, e.g. H64, H128, LIAEF128, SAE .\\newline\n- Support multiple face extraction modes, e.g. S3FD, MTCNN, dlib, or manual .\\\\\n\\hline\nDFaker\n&https://github.com/dfaker/df\n&- DSSIM loss function  is used to reconstruct face.\\newline\n- Implemented based on Keras library.\\\\\n\\hline\nDeepFake\\_tf\n&https://github.com/StromWine/DeepFake\\_tf\n&Similar to DFaker but implemented based on tensorflow.\\\\\n\\hline\nAvatarMe\n& https://github.com/lattas/AvatarMe\n& -\tReconstruct 3D faces from arbitrary ``in-the-wild\" images.\\newline\n- Can reconstruct authentic 4K by 6K-resolution 3D faces from a single low-resolution image .\\\\\n\\hline\nMarioNETte\n& https://hyperconnect.github.io/MarioNETte\n& -\tA few-shot face reenactment framework that preserves the target identity.\\newline\n- No additional fine-tuning phase is needed for identity adaptation .\\\\\n\\hline\nDiscoFaceGAN\n& https://github.com/microsoft/DiscoFaceGAN\n& -\tGenerate face images of virtual people with independent latent variables of identity, expression, pose, and illumination.\\newline\n- Embed 3D priors into adversarial learning .\\\\\n\\hline\nStyleRig\n& https://gvv.mpi-inf.mpg.de/projects/StyleRig\n& -\tCreate portrait images of faces with a rig-like control over a pretrained and fixed StyleGAN via 3D morphable face models.\\newline\n- Self-supervised without manual annotations .\\\\\n\\hline\nFaceShifter\n& https://lingzhili.com/FaceShifterPage\n& -\tFace swapping in high-fidelity by exploiting and integrating the target attributes.\\newline\n- Can be applied to any new face pairs without requiring subject specific training .\\\\\n\\hline\nFSGAN\n& https://github.com/YuvalNirkin/fsgan\n& -\tA face swapping and reenactment model that can be applied to pairs of faces without requiring training on those faces.\\newline\n- Adjust to both pose and expression variations .\\\\\n\\hline\nStyleGAN\n& https://github.com/NVlabs/stylegan\n& -\tA new generator architecture for GANs is proposed based on style transfer literature.\\newline\n- The new architecture leads to automatic, unsupervised separation of high-level attributes and enables intuitive, scale-specific control of the synthesis of images .\\\\\n\\hline\nFace2Face & https://justusthies.github.io/posts/face2face/ & - Real-time facial reenactment of monocular target video sequence, e.g. Youtube video.\\newline\n- Animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion .\\\\\n\\hline\nNeural Textures & https://github.com/SSRSGJYD/NeuralTexture & \n- Feature maps that are learned as part of the scene capture process and stored as maps on top of {3D} mesh proxies.\\newline\n- Can coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates .\\\\\n\\hline\nTransformable Bottleneck Networks\n& https://github.com/kyleolsz/TB-Networks\n& -\tA method for fine-grained 3D manipulation of image content.\\newline\n- Apply spatial transformations in CNN models using a transformable bottleneck framework .\\\\\n\\hline\n``Do as I Do\" Motion \\newline Transfer\n& github.com/carolineec/EverybodyDanceNow\n& - Automatically transfer the motion from a source to a target person by learning a video-to-video translation.\\newline\n-  Can create a motion-synchronized dancing video with multiple subjects .\\\\\n\\hline\nNeural Voice Puppetry\n& https://justusthies.github.io/posts/neural-voice-puppetry\n& - A method for audio-driven facial video synthesis.\\newline\n- Synthesize videos of a talking head from an audio sequence of another person using 3D face representation. .\\\\\n\\hline\n\\end{tabular}\n\\end{scriptsize}\n\\end{table*}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.92\\columnwidth]{Fig3.pdf}\n\\caption{A deepfake creation model using two encoder-decoder pairs. Two networks use the same encoder but different decoders for training process (top). An image of face A is encoded with the common encoder and decoded with decoder B to create a deepfake (bottom). The reconstructed image (in the bottom) is the face B with the mouth shape of face A. Face B originally has the mouth of an upside-down heart while the reconstructed face B has the mouth of a conventional heart.}\n\\label{fig1} \n\\end{figure} \nBy adding adversarial loss and perceptual loss implemented in VGGFace  to the encoder-decoder architecture, an improved version of deepfakes based on the generative adversarial network , i.e., faceswap-GAN, was proposed in . The VGGFace perceptual loss is added to make eye movements to be more realistic and consistent with input faces and help to smooth out artifacts in segmentation mask, leading to higher quality output videos. This model facilitates the creation of outputs with 64x64, 128x128, and 256x256 resolutions. In addition, the multi-task convolutional neural network (CNN) from the FaceNet implementation  is used to make face detection more stable and face alignment more reliable. The CycleGAN  is utilized for generative network implementation in this model.\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.55\\columnwidth]{Fig4.pdf}\n\\caption{The GAN architecture consisting of a generator and a discriminator, and each can be implemented by a neural network. The entire system can be trained with backpropagation that allows both networks to improve their capabilities.}\n\\label{figGAN} \n\\end{figure} \nA conventional GAN model comprises two neural networks: a generator and a discriminator as depicted in Fig. \\ref{figGAN}. Given a dataset of real images $x$ having a distribution of $p_{data}$, the aim of the generator $G$ is to produce images $G(z)$ similar to real images $x$ with $z$ being noise signals having a distribution of $p_z$. The aim of the discriminator $G$ is to correctly classify images generated by $G$ and real images $x$. The discriminator $D$ is trained to improve its classification capability, i.e., to maximize $D(x)$, which represents the probability that $x$ is a real image rather than a fake image generated by $G$. On the other hand, $G$ is trained to minimize the probability that its outputs are classified by $D$ as synthetic images, i.e., to minimize $1-D(G(z))$. This is a minimax game between two players $D$ and $G$ that can be described by the following value function :\n\\begin{multline}\n    \\min_G \\max_D V(D,G)=\\EX_{x\\sim p_{data}(x)}[\\log D(x)] \\\\ + \\EX_{z\\sim p_z(z)}[\\log (1-D(G(z)))]\n\\end{multline}\nAfter sufficient training, both networks improve their capabilities, i.e., the generator $G$ is able to produce images that are really similar to real images while the discriminator $D$ is highly capable of distinguishing fake images from real ones.\nTable \\ref{table1} presents a summary of popular deepfake tools and their typical features. Among them, a prominent method for face synthesis based on a GAN model, namely StyleGAN, was introduced in . StyleGAN is motivated by style transfer  with a special generator network architecture that is able to create realistic face images. In a traditional GAN model, e.g., the progressive growing of GAN (PGGAN) , the signal noise (latent code) is fed to the input layer of a feedforward network that represents the generator. In StyleGAN, there are two networks constructed and linked together, a mapping network $f$ and a synthesis network $g$. The latent code $z \\in Z$ is first converted to $w \\in W$ (where $W$ is an intermediate latent space) through a non-linear function $f:Z\\rightarrow W$, which is characterized by a neural network (i.e., the mapping network) consisting of several fully connected layers. Using an affine tranformation, the intermediate representation $w$ is specialized to styles $y=(y_s,y_b)$ that will be fed to the adaptive instance normalization (AdaIN) operations, specified as: \n\\begin{equation}\n    \\mathrm{AdaIN}(x_i,y)=y_{s,i}\\frac{x_i-\\mu(x_i)}{\\sigma(x_i)}+y_{b,i}\n\\end{equation}\nwhere each feature map $x_i$ is normalized separately. The StyleGAN generator architecture allows controlling the image synthesis by modifying the styles via different scales. In addition, instead of using one random latent code during training, this method uses two latent codes to generate a given proportion of images. More specifically, two latent codes $z_1$ and $z_2$ are fed to the mapping network to create respectively $w_1$ and $w_2$ that control the styles by applying $w_1$ before and $w_2$ after the crossover point. Fig. \\ref{figMixing} demonstrates examples of images created by mixing two latent codes at three different scales where each subset of styles controls separate meaningful high-level attributes of the image. In other words, the generator architecture of StyleGAN is able to learn separation of high-level attributes (e.g., pose and identity when trained on human faces) and enables intuitive, scale-speciﬁc control of the face synthesis.\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{Fig5.pdf}\n\\caption{Examples of mixing styles using StyleGAN: the output images are generated by copying a specified subset of styles from source B and taking the rest from source A. a) Copying coarse styles from source B will generate images that have high-level aspects from source B and all colors and finer facial features from source A; b) if copying the styles of middle resolutions from B, the output images will have smaller scale facial features from B and preserve the pose, general face shape, and eyeglasses from A; c) if copying the fine styles from source B, the generated images will have the color scheme and microstructure of source B .}\n\\label{figMixing} \n\\end{figure}", "cites": [7893, 5134, 62, 7896, 5138, 5136, 7895, 96, 5137, 150, 157, 7894, 5135, 7892], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes individual deepfake creation tools and their features, with limited synthesis of overarching themes or integration of multiple papers into a coherent framework. It includes some connections (e.g., the use of GANs in several tools), but lacks critical evaluation of the methods or broader abstraction to identify general principles or trends in the field."}}
{"id": "7513ea5e-21ef-4144-b033-7ba34feb94da", "title": "Deepfake Detection", "level": "section", "subsections": ["f1d90192-c843-4231-8ac1-201458f556a0", "406b6759-531d-4af3-af75-8057a4f625fc"], "parent_id": "44777335-fb2f-4b5d-83e4-47fe65116410", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"]], "content": "\\label{sec:3}\nDeepfake detection is normally deemed a binary classification problem where classifiers are used to classify between authentic videos and tampered ones. This kind of methods requires a large database of real and fake videos to train classification models. The number of fake videos is increasingly available, but it is still limited in terms of setting a benchmark for validating various detection methods. To address this issue, Korshunov and Marcel  produced a notable deepfake dataset consisting of 620 videos based on the GAN model using the open source code Faceswap-GAN . Videos from the publicly available VidTIMIT database  were used to generate low and high quality deepfake videos, which can effectively mimic the facial expressions, mouth movements, and eye blinking. These videos were then used to test various deepfake detection methods. Test results show that the popular face recognition systems based on VGG  and Facenet  are unable to detect deepfakes effectively. Other methods such as lip-syncing approaches  and image quality metrics with support vector machine (SVM)  produce very high error rate when applied to detect deepfake videos from this newly produced dataset. This raises concerns about the critical need of future development of more robust methods that can detect deepfakes from genuine.\nThis section presents a survey of deepfake detection methods where we group them into two major categories: fake image detection methods and fake video detection ones (Fig. \\ref{fig2}). The latter is distinguished into two smaller groups: \\emph{visual artifacts} within single video frame-based methods and \\emph{temporal features} across frames-based ones. Whilst most of the methods based on temporal features use deep learning \\emph{recurrent} classification models, the methods use visual artifacts within video frame can be implemented by either deep or shallow classifiers.", "cites": [1244, 5139, 5134], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from cited papers by discussing their application in deepfake detection and categorizing approaches into visual artifacts and temporal features. It provides some critical analysis by pointing out the limitations of existing systems like FaceNet and SVM-based methods. However, the synthesis is somewhat limited in depth, and the abstraction remains at a moderate level without clearly articulating overarching theoretical or conceptual frameworks."}}
{"id": "f1d90192-c843-4231-8ac1-201458f556a0", "title": "Fake Image Detection", "level": "subsection", "subsections": ["5d2250d1-1cae-45db-a279-c36ed6840045", "87acc6e6-8b7c-49ef-9ce2-768ae70fea85"], "parent_id": "7513ea5e-21ef-4144-b033-7ba34feb94da", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Image Detection"]], "content": "Deepfakes are increasingly detrimental to privacy, society security and democracy . Methods for detecting deepfakes have been proposed as soon as this threat was introduced. Early attempts were based on handcrafted features obtained from artifacts and inconsistencies of the fake image synthesis process. Recent methods, e.g., , have commonly applied deep learning to automatically extract salient and discriminative features to detect deepfakes.", "cites": [5140], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited paper, merely mentioning the shift from handcrafted features to deep learning without contextualizing the specific contribution of the cited work. It lacks critical analysis of the methods discussed and does not abstract or generalize findings to highlight broader trends or principles in deepfake detection research."}}
{"id": "5d2250d1-1cae-45db-a279-c36ed6840045", "title": "Handcrafted Features-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "f1d90192-c843-4231-8ac1-201458f556a0", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Image Detection"], ["subsubsection", "Handcrafted Features-based Methods"]], "content": "Most works on detection of GAN generated images do not consider the generalization capability of the detection models although the development of GAN is ongoing, and many new extensions of GAN are frequently introduced. Xuan et al.  used an image preprocessing step, e.g., Gaussian blur and Gaussian noise, to remove low level high frequency clues of GAN images. This increases the pixel level statistical similarity between real images and fake images and allows the forensic classifier to learn more intrinsic and meaningful features, which has better generalization capability than previous image forensics methods  or image steganalysis networks .\nZhang et al.  used the bag of words method to extract a set of compact features and fed it into various classifiers such as SVM , random forest (RF)  and multi-layer perceptrons (MLP)  for discriminating swapped face images from the genuine. Among deep learning-generated images, those synthesised by GAN models are probably most difficult to detect as they are realistic and high-quality based on GAN's capability to learn distribution of the complex input data and generate new outputs with similar input distribution. \nOn the other hand, Agarwal and Varshney  cast the GAN-based deepfake detection as a hypothesis testing problem where a statistical framework was introduced using the information-theoretic study of authentication . The minimum distance between distributions of legitimate images and images generated by a particular GAN is defined, namely the oracle error. The analytic results show that this distance increases when the GAN is less accurate, and in this case, it is easier to detect deepfakes. In case of high-resolution image inputs, an extremely accurate GAN is required to generate fake images that are hard to detect by this method.", "cites": [7897, 5141], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from two papers, linking them to the broader issue of generalization in deepfake detection. It provides an analytical perspective by discussing the effectiveness of preprocessing, feature extraction, and hypothesis testing in GAN-based detection. However, it lacks deeper comparative or evaluative insights and does not fully abstract overarching principles."}}
{"id": "87acc6e6-8b7c-49ef-9ce2-768ae70fea85", "title": "Deep Features-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "f1d90192-c843-4231-8ac1-201458f556a0", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Image Detection"], ["subsubsection", "Deep Features-based Methods"]], "content": "Face swapping has a number of compelling applications in video compositing, transfiguration in portraits, and especially in identity protection as it can replace faces in photographs by ones from a collection of stock images. However, it is also one of the techniques that cyber attackers employ to penetrate identification or authentication systems to gain illegitimate access. The use of deep learning such as CNN and GAN has made swapped face images more challenging for forensics models as it can preserve pose, facial expression and lighting of the photographs .\nHsu et al.  introduced a two-phase deep learning method for detection of deepfake images. The first phase is a feature extractor based on the common fake feature network (CFFN) where the Siamese network architecture presented in  is used. The CFFN encompasses several dense units with each unit including different numbers of dense blocks  to improve the representative capability for the input images. \nDiscriminative features between the fake and real images are extracted through the CFFN learning process based on the use of pairwise information, which is the label of each pair of two input images. If the two images are of the same type, i.e., fake-fake or real-real, the pairwise label is $1$. In contrast, if they are of different types, i.e., fake-real, the pairwise label is $0$. The CFFN-based discriminative features are then fed to a neural network classifier to distinguish deceptive images from genuine. The proposed method is validated for both fake face and fake general image detection. On the one hand, the face dataset is obtained from CelebA , containing 10,177 identities and 202,599 aligned face images of various poses and background clutter. Five GAN variants are used to generate fake images with size of 64x64, including deep convolutional GAN (DCGAN) , Wasserstein GAN (WGAN) , WGAN with gradient penalty (WGAN-GP) , least squares GAN , and PGGAN . A total of 385,198 training images and 10,000 test images of both real and fake ones are obtained for validating the proposed method. On the other hand, the general dataset is extracted from the ILSVRC12 . The large scale GAN training model for high fidelity natural image synthesis (BIGGAN) , self-attention GAN  and spectral normalization GAN  are used to generate fake images with size of 128x128. The training set consists of 600,000 fake and real images whilst the test set includes 10,000 images of both types. Experimental results show the superior performance of the proposed method against its competing methods such as those introduced in .\nLikewise,  proposed a CNN model, namely SCnet, to detect deepfake images, which are generated by the Glow-based facial forgery tool . The fake images synthesized by the Glow model  have the facial expression maliciously tampered. These images are hyper-realistic with perfect visual qualities, but they still have subtle or noticeable manipulation traces, which are exploited by the SCnet. The SCnet is able to automatically learn high-level forensics features of image data thanks to a hierarchical feature extraction block, which is formed by stacking four convolutional layers. Each layer learns a new set of feature maps from the previous layer, with each convolutional operation is defined by:\n\\begin{equation}\n    f_j^{(n)} = \\sum_{i=1}^i f_i^{(n-1)}*\\omega_{ij}^{(n)} + b_j^{(n)}\n\\end{equation}\nwhere $f_j^{(n)}$ is the $j^{th}$ feature map of the $n^{th}$ layer, $\\omega_{ij}^{(n)}$ is the weight of the $i^{th}$ channel of the $j^{th}$ convolutional kernel in the $n^{th}$ layer, and $b_j^{(n)}$ is the bias term of the $j^{th}$ convolutional kernel in the $n^{th}$ layer.\nThe proposed approach is evaluated using a dataset consisting of 321,378 face images, which are created by applying the Glow model  to the CelebA face image dataset . Evaluation results show that the SCnet model obtains higher accuracy and better generalization than the Meso-4 model proposed in .\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=1.75\\columnwidth]{Fig6.pdf}\n\\caption{A two-step process for face manipulation detection where the preprocessing step aims to detect, crop and align faces on a sequence of frames and the second step distinguishes manipulated and authentic face images by combining convolutional neural network (CNN) and recurrent neural network (RNN) .}\n\\label{fig3} \n\\end{figure*}\nRecently,  proposed a method for deepfake detection using self-consistency of local source features, which are content-independent, spatially-local information of images. These features could come from either imaging pipelines, encoding methods or image synthesis approaches. The hypothesis is that a modified image would have different source features at different locations, while an original image will have the same source features across locations. These source features, represented in the form of down-sampled feature maps, are extracted by a CNN model using a special representation learning method called pairwise self-consistency learning. This learning method aims to penalize pairs of feature vectors that refer to locations from the same image for having a low cosine similarity score. At the same time, it also penalizes the pairs from different images for having a high similarity score. The learned feature maps are then fed to a classification method for deepfake detection. This proposed approach is evaluated on seven popular datasets, including FaceForensics++ , DeepfakeDetection , Celeb-DF-v1 \\& Celeb-DF-v2 , Deepfake Detection Challenge (DFDC) , DFDC Preview , and DeeperForensics-1.0 . Experimental results demonstrate that the proposed approach is superior to state-of-the-art methods. It however may have a limitation when dealing with fake images that are generated by methods that directly output the whole images whose source features are consistent across all positions within each image.", "cites": [1603, 895, 56, 62, 5142, 63, 5143, 7194, 8317, 3212, 1003, 485, 96, 1600, 78, 7898], "cite_extract_rate": 0.5517241379310345, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent narrative by integrating multiple deepfake detection methods, particularly those using CNN and GAN-based fake images. It connects ideas across sources and discusses methodological differences and performance comparisons. While it includes some critical evaluation (e.g., limitations of source consistency in certain fake generation methods), the critique is not as deep or nuanced as it could be. The section identifies patterns in deep feature extraction but lacks higher-level abstraction or synthesis into a novel framework."}}
{"id": "406b6759-531d-4af3-af75-8057a4f625fc", "title": "Fake Video Detection", "level": "subsection", "subsections": ["f76aa108-55fe-4a3b-9495-cbb64d02a169", "89bc57b3-3d0e-4686-9c89-9b9c741caaac"], "parent_id": "7513ea5e-21ef-4144-b033-7ba34feb94da", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Video Detection"]], "content": "Most image detection methods cannot be used for videos because of the strong degradation of the frame data after video compression . Furthermore, videos have temporal characteristics that are varied among sets of frames and they are thus challenging for methods designed to detect only still fake images. This subsection focuses on deepfake video detection methods and categorizes them into two smaller groups: methods that employ temporal features and those that explore visual artifacts within frames.", "cites": [1603], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief categorization of video detection methods but does not deeply synthesize information across multiple cited papers. It mentions one paper (MesoNet) in a general context without elaborating on how it fits into the broader classification or comparing it with other works. There is minimal critical analysis or abstraction to identify overarching patterns or principles in the field."}}
{"id": "f76aa108-55fe-4a3b-9495-cbb64d02a169", "title": "Temporal Features across Video Frames", "level": "subsubsection", "subsections": [], "parent_id": "406b6759-531d-4af3-af75-8057a4f625fc", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Video Detection"], ["subsubsection", "Temporal Features across Video Frames"]], "content": "Based on the observation that temporal coherence is not enforced effectively in the synthesis process of deepfakes, Sabir et al.  leveraged the use of spatio-temporal features of video streams to detect deepfakes. Video manipulation is carried out on a frame-by-frame basis so that low level artifacts produced by face manipulations are believed to further manifest themselves as temporal artifacts with inconsistencies across frames. A recurrent convolutional model (RCN) was proposed based on the integration of the convolutional network DenseNet  and the gated recurrent unit cells  to exploit temporal discrepancies across frames (see Fig. \\ref{fig3}). The proposed method is tested on the FaceForensics++ dataset, which includes 1,000 videos , and shows promising results. \nLikewise,  highlighted that deepfake videos contain intra-frame inconsistencies and temporal inconsistencies between frames. They then proposed the temporal-aware pipeline method that uses CNN and long short term memory (LSTM) to detect deepfake videos. CNN is employed to extract frame-level features, which are then fed into the LSTM to create a temporal sequence descriptor. A fully-connected network is finally used for classifying doctored videos from real ones based on the sequence descriptor as illustrated in Fig. \\ref{fig4}. An accuracy of greater than 97\\% was obtained using a dataset of 600 videos, including 300 deepfake videos collected from multiple video-hosting websites and 300 pristine videos randomly selected from the Hollywood human actions dataset in .\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=1\\columnwidth]{Fig7.pdf}\n\\caption{A deepfake detection method using convolutional neural network (CNN) and long short term memory (LSTM) to extract temporal features of a given video sequence, which are represented via the sequence descriptor. The detection network consisting of fully-connected layers is employed to take the sequence descriptor as input and calculate probabilities of the frame sequence belonging to either authentic or deepfake class .}\n\\label{fig4} \n\\end{figure}\nOn the other hand, the use of a physiological signal, eye blinking, to detect deepfakes was proposed in Li et al.  based on the observation that a person in deepfakes has a lot less frequent blinking than that in untampered videos. A healthy adult human would normally blink somewhere between 2 to 10 seconds, and each blink would take 0.1 and 0.4 seconds. Deepfake algorithms, however, often use face images available online for training, which normally show people with open eyes, i.e., very few images published on the internet show people with closed eyes. Thus, without having access to images of people blinking, deepfake algorithms do not have the capability to generate fake faces that can blink normally. In other words, blinking rates in deepfakes are much lower than those in normal videos. To discriminate real and fake videos, Li et al.  crop eye areas in the videos and distribute them into long-term recurrent convolutional networks (LRCN)  for dynamic state prediction. The LRCN consists of a feature extractor based on CNN, a sequence learning based on long short term memory (LSTM), and a state prediction based on a fully connected layer to predict probability of eye open and close state. The eye blinking shows strong temporal dependencies and thus the implementation of LSTM helps to capture these temporal patterns effectively.\nRecently,  proposed the use of optical flow to gauge the information along the temporal axis of a frame sequence for video deepfake detection. The optical flow is a vector field calculated on two temporal-distinct frames of a video that can describe the movement of objects in a scene. The optical flow fields are expected to be different between synthetically created frames and naturally generated ones . Unnatural movements of lips, eyes, or of the entire faces inserted into deepfake videos would introduce distinctive motion patterns when compared with pristine ones. Based on this assumption, features consisting of optical flow fields are fed into a CNN model for discriminating between deepfakes and original videos. More specifically, the ResNet50 architecture  is implemented as a CNN model for experiments. The results obtained using the FaceForensics++ dataset  show that this approach is comparable with state-of-the-art methods in terms of classification accuracy. A combination of this kind of feature with frame-based features is also experimented, which results in an improved deepfake detection performance. This demonstrates the usefulness of optical flow fields in capturing the inconsistencies on the temporal axis of video frames for deepfake detection.", "cites": [5142, 1600, 97, 243, 3758, 96], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches to detecting temporal inconsistencies in deepfake videos by connecting methods based on recurrent networks, optical flow, and eye blinking patterns. It provides a coherent narrative on how temporal features can be exploited for detection. Some critical analysis is present, such as pointing out limitations in deepfake generation of realistic blinking, but deeper comparative or evaluative insights are limited. The abstraction is moderate, identifying a recurring theme of temporal discrepancies in deepfake synthesis but not fully articulating a meta-level framework."}}
{"id": "c8afcbd9-52a4-4801-8c8d-0c5d0d477915", "title": "Deep classifiers", "level": "paragraph", "subsections": [], "parent_id": "89bc57b3-3d0e-4686-9c89-9b9c741caaac", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Video Detection"], ["subsubsection", "Visual Artifacts within Video Frame"], ["paragraph", "Deep classifiers"]], "content": "Deepfake videos are normally created with limited resolutions, which require an affine face warping approach (i.e., scaling, rotation and shearing) to match the configuration of the original ones. Because of the resolution inconsistency between the warped face area and the surrounding context, this process leaves artifacts that can be detected by CNN models such as VGG16 , ResNet50, ResNet101 and ResNet152 . A deep learning method to detect deepfakes based on the artifacts observed during the face warping step of the deepfake generation algorithms was proposed in . The proposed method is evaluated on two deepfake datasets, namely the UADFV and DeepfakeTIMIT. The UADFV dataset  contains 49 real videos and 49 fake videos with 32,752 frames in total. The DeepfakeTIMIT dataset  includes a set of low quality videos of 64 x 64 size and another set of high quality videos of 128 x 128 with totally 10,537 pristine images and 34,023 fabricated images extracted from 320 videos for each quality set. Performance of the proposed method is compared with other prevalent methods such as two deepfake detection MesoNet methods, i.e. Meso-4 and MesoInception-4 , HeadPose , and the face tampering detection method two-stream NN . Advantage of the proposed method is that it needs not to generate deepfake videos as negative examples before training the detection models. Instead, the negative examples are generated dynamically by extracting the face region of the original image and aligning it into multiple scales before applying Gaussian blur to a scaled image of random pick and warping back to the original image. This reduces a large amount of time and computational resources compared to other methods, which require deepfakes are generated in advance.\nNguyen et al.  proposed the use of capsule networks for detecting manipulated images and videos. The capsule network was initially introduced to address limitations of CNNs when applied to inverse graphics tasks, which aim to find physical processes used to produce images of the world . The recent development of capsule network based on dynamic routing algorithm  demonstrates its ability to describe the hierarchical pose relationships between object parts. This development is employed as a component in a pipeline for detecting fabricated images and videos as demonstrated in Fig. \\ref{fig5}. A dynamic routing algorithm is deployed to route the outputs of the three capsules to the output capsules through a number of iterations to separate between fake and real images. The method is evaluated through four datasets covering a wide range of forged image and video attacks. They include the well-known Idiap Research Institute replay-attack dataset , the deepfake face swapping dataset created by Afchar et al. , the facial reenactment FaceForensics dataset , produced by the Face2Face method , and the fully computer-generated image dataset generated by Rahmouni et al. . The proposed method yields the best performance compared to its competing methods in all of these datasets. This shows the potential of the capsule network in building a general detection system that can work effectively for various forged image and video attacks.\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=1\\columnwidth]{Fig8.pdf}\n\\caption{Capsule network takes features obtained from the VGG-19 network  to distinguish fake images or videos from the real ones (top). The pre-processing step detects face region and scales it to the size of 128x128 before VGG-19 is used to extract latent features for the capsule network, which comprises three primary capsules and two output capsules, one for real and one for fake images (bottom). The statistical pooling constitutes an important part of capsule network that deals with forgery detection .}\n\\label{fig5} \n\\end{figure}", "cites": [5144, 1603, 5145, 7899, 97, 514, 5146, 5147, 306], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple cited papers, particularly integrating the concept of visual artifacts in deepfake videos and the use of CNNs and capsule networks for detection. It provides a comparative analysis of different methods, such as the capsule network approach versus MesoNet and others. While it highlights the advantages of the proposed dynamic example generation and capsule networks, the critical evaluation remains somewhat surface-level without deeper limitations or trade-offs. The abstraction level is moderate, as it identifies general patterns in artifact-based detection but does not fully elevate the discussion to a meta-level framework."}}
{"id": "b02a12ee-cc0c-427d-b853-a4c928ab1f56", "title": "Shallow classifiers", "level": "paragraph", "subsections": [], "parent_id": "89bc57b3-3d0e-4686-9c89-9b9c741caaac", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Deepfake Detection"], ["subsection", "Fake Video Detection"], ["subsubsection", "Visual Artifacts within Video Frame"], ["paragraph", "Shallow classifiers"]], "content": "Deepfake detection methods mostly rely on the artifacts or inconsistency of intrinsic features between fake and real images or videos. Yang et al.  proposed a detection method by observing the differences between 3D head poses comprising head orientation and position, which are estimated based on 68 facial landmarks of the central face region. The 3D head poses are examined because there is a shortcoming in the deepfake face generation pipeline. The extracted features are fed into an SVM classifier to obtain the detection results. Experiments on two datasets show the great performance of the proposed approach against its competing methods. The first dataset, namely UADFV, consists of 49 deep fake videos and their respective real videos . The second dataset comprises 241 real images and 252 deep fake images, which is a subset of data used in the DARPA MediFor GAN Image/Video Challenge . Likewise, a method to exploit artifacts of deepfakes and face manipulations based on visual features of eyes, teeth and facial contours was studied in . The visual artifacts arise from lacking global consistency, wrong or imprecise estimation of the incident illumination, or imprecise estimation of the underlying geometry. For deepfakes detection, missing reflections and missing details in the eye and teeth areas are exploited as well as texture features extracted from the facial region based on facial landmarks. Accordingly, the eye feature vector, teeth feature vector and features extracted from the full-face crop are used. After extracting the features, two classifiers including logistic regression and small neural network are employed to classify the deepfakes from real videos. Experiments carried out on a video dataset downloaded from YouTube show the best result of 0.851 in terms of the area under the receiver operating characteristics curve. The proposed method however has a disadvantage that requires images meeting certain prerequisite such as open eyes or visual teeth.\nThe use of photo response non uniformity (PRNU) analysis was proposed in  to detect deepfakes from authentic ones. PRNU is a component of sensor pattern noise, which is attributed to the manufacturing imperfection of silicon wafers and the inconsistent sensitivity of pixels to light because of the variation of the physical characteristics of the silicon wafers. The PRNU analysis is widely used in image forensics  and advocated to use in  because the swapped face is supposed to alter the local PRNU pattern in the facial area of video frames. The videos are converted into frames, which are cropped to the questioned facial region. The cropped frames are then separated sequentially into eight groups where an average PRNU pattern is computed for each group. Normalised cross correlation scores are calculated for comparisons of PRNU patterns among these groups. A test dataset was created, consisting of 10 authentic videos and 16 manipulated videos, where the fake videos were produced from the genuine ones by the DeepFaceLab tool . The analysis shows a significant statistical difference in terms of mean normalised cross correlation scores between deepfakes and the genuine. This analysis therefore suggests that PRNU has a potential in deepfake detection although a larger dataset would need to be tested. \nWhen seeing a video or image with suspicion, users normally want to search for its origin. However, there is currently no feasibility for such a tool. Hasan and Salah  proposed the use of blockchain and smart contracts to help users detect deepfake videos based on the assumption that videos are only real when their sources are traceable. Each video is associated with a smart contract that links to its parent video and each parent video has a link to its child in a hierarchical structure. Through this chain, users can credibly trace back to the original smart contract associated with pristine video even if the video has been copied multiple times. An important attribute of the smart contract is the unique hashes of the interplanetary file system, which is used to store video and its metadata in a decentralized and content-addressable manner . The smart contract's key features and functionalities are tested against several common security challenges such as distributed denial of services, replay and man in the middle attacks to ensure the solution meeting security requirements. This approach is generic, and it can be extended to other types of digital content, e.g., images, audios and manuscripts.\n\\begin{table*}[h!]\n\\centering\n\\caption{Summary of prominent deepfake detection methods}\n\\label{table2}\n\\begin{scriptsize}\n\\begin{tabularx}{\\linewidth}{p{0.11\\textwidth} p{0.09\\textwidth} p{0.33\\textwidth} p{0.04\\textwidth} p{0.32\\textwidth}}\n\\toprule\n\\textbf{Methods} & \\textbf{Classifiers/\\newline Techniques} & \\textbf{Key Features} & \\textbf{Dealing with} & \\textbf{Datasets Used}\\\\\n\\midrule\nEye blinking \n&LRCN\n&- Use LRCN to learn the temporal patterns of eye blinking.\\newline\n- Based on the observation that blinking frequency of deepfakes is much smaller than normal.\n&Videos\n&Consist of 49 interview and presentation videos, and their corresponding generated deepfakes.\\\\\n\\hline\nIntra-frame and temporal inconsistencies \n&CNN and LSTM\n&CNN is employed to extract frame-level features, which are distributed to LSTM to construct sequence descriptor useful for classification.\t\n&Videos\n&A collection of 600 videos obtained from multiple websites.\\\\\n\\hline\nUsing face warping artifacts \n&VGG16 ,\\newline ResNet models \n&Artifacts are discovered using CNN models based on resolution inconsistency between the warped face area and the surrounding context.\n&Videos\n&- UADFV , containing 49 real videos and 49 fake videos with 32752 frames in total.\\newline\n- DeepfakeTIMIT \\\\\n\\hline\nMesoNet \n&CNN\n&- Two deep networks, i.e. Meso-4 and MesoInception-4 are introduced to examine deepfake videos at the mesoscopic analysis level.\\newline\n- Accuracy obtained on deepfake and FaceForensics datasets are 98\\% and 95\\% respectively.\n&Videos\n&Two datasets: deepfake one constituted from online videos and the FaceForensics one created by the Face2Face approach .\\\\\n\\hline\nEye, teach and facial texture \n&Logistic regression and neural network (NN)\n&- Exploit facial texture differences, and missing reflections and details in eye and teeth areas of deepfakes.\\newline\n- Logistic regression and NN are used for classifying.\t\n&Videos\n&A video dataset downloaded from YouTube.\\\\\n\\hline\nSpatio-temporal features with RCN \n&RCN\n&Temporal discrepancies across frames are explored using RCN that integrates convolutional network DenseNet  and the gated recurrent unit cells \n&Videos\n&FaceForensics++ dataset, including 1,000 videos .\\\\\n\\hline\nSpatio-temporal features with LSTM \n& Convolutional bidirectional recurrent LSTM network\n& - An XceptionNet CNN is used for facial feature extraction while audio embeddings are obtained by stacking multiple convolution modules.\\newline\n- Two loss functions, i.e. cross-entropy and Kullback-Leibler divergence, are used.\n& Videos\n& FaceForensics++  and Celeb-DF (5,639 deepfake videos)  datasets and the ASVSpoof 2019 Logical Access audio dataset .\\\\\n\\hline\nAnalysis of PRNU \n&PRNU\n&- Analysis of noise patterns of light sensitive sensors of digital cameras due to their factory defects.\\newline\n- Explore the differences of PRNU patterns between the authentic and deepfake videos because face swapping is believed to alter the local PRNU patterns.\n&Videos\t\n&Created by the authors, including 10 authentic and 16 deepfake videos using DeepFaceLab .\\\\\n\\hline\nPhoneme-viseme mismatches \n& CNN\n& - Exploit the mismatches between the dynamics of the mouth shape, i.e. visemes, with a spoken phoneme.\\newline\n- Focus on sounds associated with the M, B and P phonemes as they require complete mouth closure while deepfakes often incorrectly synthesize it.\n& Videos\n& Four in-the-wild lip-sync deepfakes from Instagram and YouTube (www.instagram.com/bill\\_posters\\_uk and youtu.be/VWMEDacz3L4) and others are created using synthesis techniques, i.e. Audio-to-Video (A2V)  and Text-to-Video (T2V) .\\\\\n\\hline\nUsing attribution-based confidence (ABC) metric \n& ResNet50 model , pre-trained on VGGFace2 \n& - The ABC metric  is used to detect deepfake videos without accessing to training data.\\newline\n- ABC values obtained for original videos are greater than 0.94 while those of deepfakes have low ABC values.\n& Videos\n& VidTIMIT and two other original datasets obtained from the COHFACE (https://www.idiap.ch/dataset/cohface) and from YouTube. datasets from COHFACE  and YouTube are used to generate two deepfake datasets by commercial website https://deepfakesweb.com and another deepfake dataset is DeepfakeTIMIT .\\\\\n\\hline\nUsing appearance and behaviour \n& Rules based on facial and behavioural features.\n& Temporal, behavioral biometric based on facial expressions and head movements are learned using ResNet-101  while static facial biometric is obtained using VGG .\n& Videos\n& The world leaders dataset , FaceForensics++ , Google/Jigsaw deepfake detection dataset , DFDC  and Celeb-DF .\\\\\n\\hline\nFakeCatcher \n& CNN\n& Extract biological signals in portrait videos and use them as an implicit descriptor of authenticity because they are not spatially and temporally well-preserved in deepfakes.\n& Videos\n& UADFV , FaceForensics , FaceForensics++ , Celeb-DF , and a new dataset of 142 videos, independent of the generative model, resolution, compression, content, and context.\\\\\n\\hline\nEmotion audio-visual affective cues \n& Siamese network \n& Modality and emotion embedding vectors for the face and speech are extracted for deepfake detection.\n& Videos\n& DeepfakeTIMIT  and DFDC .\\\\\n\\hline\nHead poses \n&SVM\n&- Features are extracted using 68 landmarks of the face region.\\newline\n- Use SVM to classify using the extracted features.\t\n&Videos/\\newline Images\n&- UADFV consists of 49 deep fake videos and their respective real videos.\\newline\n- 241 real images and 252 deep fake images from DARPA MediFor GAN Image/Video Challenge.\\\\\n\\hline\nCapsule-forensics \n&Capsule networks\n&- Latent features extracted by VGG-19 network  are fed into the capsule network for classification.\\newline\n- A dynamic routing algorithm  is used to route the outputs of three convolutional capsules to two output capsules, one for fake and another for real images, through a number of iterations.\t\n&Videos/\\newline Images\n&Four datasets: the Idiap Research Institute replay-attack , deepfake face swapping by , facial reenactment FaceForensics , and fully computer-generated image set using .\\\\\n\\hline\n\\end{tabularx}\n\\end{scriptsize}\n\\end{table*}\n\\begin{table*}[h!]\n\\centering\n\\begin{scriptsize}\n\\begin{tabularx}{\\linewidth}{p{0.1\\textwidth} p{0.1\\textwidth} p{0.33\\textwidth} p{0.04\\textwidth} p{0.32\\textwidth}}\n\\toprule\n\\textbf{Methods} & \\textbf{Classifiers/\\newline Techniques} & \\textbf{Key Features} & \\textbf{Dealing with} & \\textbf{Datasets Used}\\\\\n\\midrule\nPreprocessing combined with deep network \n&DCGAN,\nWGAN-GP and PGGAN.\n&- Enhance generalization ability of deep learning models to detect GAN generated images.\\newline\n- Remove low level features of fake images.\\newline\n- Force deep networks to focus more on pixel level similarity between fake and real images to improve generalization ability.\n&Images\n&- Real dataset: CelebA-HQ , including high quality face images of 1024x1024 resolution.\\newline\n- Fake datasets: generated by DCGAN , WGAN-GP  and PGGAN .\\\\\n\\hline\nAnalyzing convolutional traces \n& KNN, SVM, and linear discriminant analysis (LDA)\n& Using expectation-maximization algorithm to extract local features pertaining to convolutional generative process of GAN-based image deepfake generators.\n& Images\n& Authentic images from CelebA and corresponding deepfakes are created by five different GANs (group-wise deep whitening-and-coloring transformation GDWCT , StarGAN ,\nAttGAN , StyleGAN , StyleGAN2 ).\\\\\n\\hline\nBag of words and shallow classifiers \t\n&SVM, RF, MLP\n&Extract discriminant features using bag of words method and feed these features into SVM, RF and MLP for binary classification: innocent vs fabricated.\n&Images\n&The well-known LFW face database , containing 13,223 images with resolution of 250x250.\\\\\n\\hline\nPairwise learning \n&CNN concatenated to CFFN\n&Two-phase procedure: feature extraction using CFFN based on the Siamese network architecture  and classification using CNN.\n&Images\n&- Face images: real ones from CelebA , and fake ones generated by DCGAN , WGAN , WGAN-GP , least squares GAN , and PGGAN .\\newline\n- General images: real ones from ILSVRC12 , and fake ones generated by BIGGAN , self-attention GAN  and spectral normalization GAN .\\\\\n\\hline\nDefenses against adversarial perturbations in deepfakes \n& VGG  and ResNet \n& - Introduce adversarial perturbations to enhance deepfakes and fool deepfake detectors.\\newline\n- Improve accuracy of deepfake detectors using Lipschitz regularization and deep image prior techniques.\n& Images\n& 5,000 real images from CelebA  and 5,000 fake images created by the “Few-Shot Face Translation GAN” method .\\\\\n\\hline\nFace X-ray \n& CNN\n& - Try to locate the blending boundary between the target and original faces instead of capturing the synthesized artifacts of specific manipulations.\\newline\n- Can be trained without fake images.\n& Images\n& FaceForensics++ , DeepfakeDetection (DFD) , DFDC  and Celeb-DF .\\\\\n\\hline\nUsing common artifacts of CNN-generated images \n& ResNet-50 \npre-trained with ImageNet \n& Train the classifier using a large number of fake images generated by a high-performing unconditional GAN model, i.e., PGGAN  and evaluate how well the classifier generalizes to other CNN-synthesized images.\n& Images\n& A new dataset of CNN-generated images, namely ForenSynths, consisting of\nsynthesized images from 11 models such as StyleGAN , super-resolution methods  and FaceForensics++ .\\\\\n\\hline\nUsing convolutional traces on GAN-based images \n& KNN, SVM, and LDA\n& Training the expectation-maximization algorithm  to detect\nand extract discriminative features via a fingerprint that represents the convolutional traces left by GANs during image generation.\n& Images\n& A dataset of images generated by ten GAN models, including CycleGAN , StarGAN , AttGAN , GDWCT , StyleGAN , StyleGAN2 , PGGAN , FaceForensics++ , IMLE , and\nSPADE .\\\\\n\\hline\nUsing deep features extracted by CNN \n& A new CNN model, namely SCnet\n& The CNN-based SCnet is able to automatically learn high-level forensics features of image data thanks to a hierarchical feature extraction block, which is formed by stacking four convolutional layers.\n& Images\n& A dataset of 321,378 face images, created by applying the Glow model  to the CelebA face image dataset .\n\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{scriptsize}\n\\end{table*}", "cites": [1255, 56, 7899, 5148, 5149, 5151, 485, 5145, 895, 97, 514, 7900, 5134, 5153, 306, 7022, 96, 5152, 5144, 5156, 1600, 5155, 1216, 62, 5142, 1003, 150, 1603, 7897, 5150, 5147, 63, 7194, 8317, 3212, 59, 5154, 157, 78, 243], "cite_extract_rate": 0.5128205128205128, "origin_cites_number": 78, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section synthesizes multiple detection methods by highlighting shared principles such as visual artifacts and inconsistencies in deepfakes. It offers critical evaluation by pointing out limitations, such as the prerequisites for eye/teeth-based methods. While it identifies some overarching patterns (e.g., reliance on sensor noise and temporal cues), the abstraction remains focused on method-level insights rather than broader theoretical frameworks."}}
{"id": "af5e6522-a34d-4deb-8b41-532983a76f29", "title": "Discussions and Future Research Directions", "level": "section", "subsections": [], "parent_id": "44777335-fb2f-4b5d-83e4-47fe65116410", "prefix_titles": [["title", "Deep Learning for Deepfakes Creation and Detection: A Survey"], ["section", "Discussions and Future Research Directions"]], "content": "\\label{sec:4}\nWith the support of deep learning, deepfakes can be created easier than ever before. The spread of these fake contents is also quicker thanks to the development of social media platforms . Sometimes deepfakes do not need to be spread to massive audience to cause detrimental effects. People who create deepfakes with malicious purpose only need to deliver them to target audiences as part of their sabotage strategy without using social media. For example, this approach can be utilized by intelligence services trying to influence decisions made by important people such as politicians, leading to national and international security threats . Catching the deepfake alarming problem, research community has focused on developing deepfake detection algorithms and numerous results have been reported. This paper has reviewed the state-of-the-art methods and a summary of typical approaches is provided in Table \\ref{table2}. It is noticeable that a battle between those who use advanced machine learning to create deepfakes with those who make effort to detect deepfakes is growing.\nDeepfakes' quality has been increasing and the performance of detection methods needs to be improved accordingly. The inspiration is that what AI has broken can be fixed by AI as well . Detection methods are still in their early stage and various methods have been proposed and evaluated but using fragmented datasets. An approach to improve performance of detection methods is to create a growing updated benchmark dataset of deepfakes to validate the ongoing development of detection methods. This will facilitate the training process of detection models, especially those based on deep learning, which requires a large training set . \nImproving performance of deepfake detection methods is important, especially in cross-forgery and cross-dataset scenarios. Most detection models are designed and evaluated in the same-forgery and in-dataset experiments, which do not ensure their generalization capability. Some previous studies have addressed this issue, e.g., in , but more work needs to be done in this direction. A model trained on a specific forgery needs to be able to work against another unknown one because potential deepfake types are not normally known in the real-world scenarios. Likewise, current detection methods mostly focus on drawbacks of the deepfake generation pipelines, i.e., finding weakness of the competitors to attack them. This kind of information and knowledge is not always available in adversarial environments where attackers commonly attempt not to reveal such deepfake creation technologies. Recent works on adversarial perturbation attacks to fool DNN-based detectors make the deepfake detection task more difficult . These are real challenges for detection method development and a future study needs to focus on introducing more robust, scalable and generalizable methods.\nAnother research direction is to integrate detection methods into distribution platforms such as social media to increase its effectiveness in dealing with the widespread impact of deepfakes. The screening or filtering mechanism using effective detection methods can be implemented on these platforms to ease the deepfakes detection . Legal requirements can be made for tech companies who own these platforms to remove deepfakes quickly to reduce its impacts. In addition, watermarking tools can also be integrated into devices that people use to make digital contents to create immutable metadata for storing originality details such as time and location of multimedia contents as well as their untampered attestment . This integration is difficult to implement but a solution for this could be the use of the disruptive blockchain technology. The blockchain has been used effectively in many areas and there are very few studies so far addressing the deepfake detection problems based on this technology. As it can create a chain of unique unchangeable blocks of metadata, it is a great tool for digital provenance solution. The integration of blockchain technologies to this problem has demonstrated certain results  but this research direction is far from mature.\nUsing detection methods to spot deepfakes is crucial, but understanding the real intent of people publishing deepfakes is even more important. This requires the judgement of users based on social context in which deepfake is discovered, e.g. who distributed it and what they said about it . This is critical as deepfakes are getting more and more photorealistic and it is highly anticipated that detection software will be lagging behind deepfake creation technology. A study on social context of deepfakes to assist users in such judgement is thus worth performing. \nVideos and photographics have been widely used as evidences in police investigation and justice cases. They may be introduced as evidences in a court of law by digital media forensics experts who have background in computer or law enforcement and experience in collecting, examining and analysing digital information. The development of machine learning and AI technologies might have been used to modify these digital contents and thus the experts' opinions may not be enough to authenticate these evidences because even experts are unable to discern manipulated contents. This aspect needs to take into account in courtrooms nowadays when images and videos are used as evidences to convict perpetrators because of the existence of a wide range of digital manipulation methods . The digital media forensics results therefore must be proved to be valid and reliable before they can be used in courts. This requires careful documentation for each step of the forensics process and how the results are reached. Machine learning and AI algorithms can be used to support the determination of the authenticity of digital media and have obtained accurate and reliable results, e.g., , but most of these algorithms are unexplainable. This creates a huge hurdle for the applications of AI in forensics problems because not only the forensics experts oftentimes do not have expertise in computer algorithms, but the computer professionals also cannot explain the results properly as most of these algorithms are black box models . This is more critical as the most recent models with the most accurate results are based on deep learning methods consisting of many neural network parameters. Researchers have recently attempted to create white box and explainable detection methods. An example is the approach proposed by  in which they use discrete cosine transform statistics to detect so-called specific GAN frequencies to differentiate between real images and deepfakes. Through the analysis of particular frequency statistics, that method can be used to mathematically explain whether a multimedia content is a deepfake and why it is. More research must be conducted in this area and explainable AI in computer vision therefore is a research direction that is needed to promote and utilize the advances and advantages of AI and machine learning in digital media forensics.", "cites": [7901, 5160, 5157, 5158, 5156, 5161, 5162, 5159], "cite_extract_rate": 0.4, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple cited papers to highlight common challenges in deepfake detection, such as adversarial attacks, lack of generalization, and the need for explainability. It offers critical analysis by pointing out limitations in current methods, such as reliance on known forgery types and black-box models. The section abstracts beyond individual studies to discuss broader implications like legal and social contexts, and proposes future research directions with meta-level insights."}}
