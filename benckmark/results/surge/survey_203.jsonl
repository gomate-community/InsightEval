{"id": "881ada37-c6d5-459f-9418-184e97419bb4", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{C}{onvolutional} Neural Networks (CNNs)~ have significantly impacted the field of medical imaging\ndue to their ability to learn highly complex representations in a data-driven manner. Since their renaissance, CNNs have \ndemonstrated remarkable improvements on numerous medical imaging modalities, including Radiography , Endoscopy , Computed Tomography (CT) , Mammography Images (MG) , Ultrasound Images , Magnetic Resonance Imaging (MRI) , and Positron Emission Tomography (PET) , to name a few.\nThe workhorse in CNNs is the \\textit{convolution} operator, which operates locally and provides translational equivariance. While these properties help in developing efficient and generalizable medical imaging solutions, the local receptive field in convolution operation limits capturing long-range pixel relationships. Furthermore, the convolutional filters have stationary weights that are not adapted for the given input image content at inference time. \nMeanwhile, significant research effort has been made by the vision community to integrate the attention mechanisms~ in CNN-inspired architectures . \nThese attention-based `Transformer' models have become an attractive solution \ndue to their ability to encode long-range dependencies and learn highly effective feature representations . \nRecent works have shown that these Transformer modules can fully replace the standard convolutions in deep neural networks by operating on a sequence of image patches, giving rise to Vision Transformers (ViTs)~.\nSince their inception, ViT models have been shown to push the state-of-the-art in numerous vision tasks, including image classification~, object detection~, semantic segmentation~, image colorization~, low-level vision~, and video understanding~ to name a few. Furthermore, recent research indicate that the prediction errors of ViTs are more consistent with those of humans than CNNs . These desirable properties of ViTs have sparked great interest in the medical community to adapt them for medical imaging applications, thereby mitigating the inherent inductive biases of CNNs .\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[trim={0.45cm 7.6cm 0.8cm 7cm},clip,width = \\textwidth]{piechart_updated.pdf} \n\\caption{(Left) The pie-charts show statistics of the papers included in this survey according to medical imaging problem settings and data modalities. The rightmost figure shows consistent growth in the recent literature (for year 2021). Seg: segmentation, Class: classification, Recons: reconstruction, Reg: registration, Synth: synthesis, Det: detection, Rep: report generation, US: ultrasound.}\n\\label{fig:appvsnumbers}\n\\end{figure*}\n\\textbf{Motivation and Contributions:} Recently, medical imaging community has witnessed an exponential growth in the number of Transformer based techniques, especially after the inception of ViTs (see Fig. \\ref{fig:appvsnumbers}).\nThe topic is now dominant in prestigious medical imaging conferences and journals, and it is getting increasingly difficult to keep pace with the recent progress due to the rapid influx of papers. \nAs such, a survey of the existing relevant works is timely \nto provide a comprehensive account of new methods \nin this emerging field. To this end, we provide a holistic overview of the applications of Transformer models in medical imaging. We hope this work can provide a roadmap for the researchers to explore the field further. Our major contributions include:\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.45\\textwidth]{applications_taxonomy_.pdf}\n\\caption{A diverse set of application areas of Transformers in medical imaging covered in this survey.}\n\\label{fig:applications_transformers}\n\\end{figure}\n\\begin{itemize}\n\\item This is the first survey paper that comprehensively covers applications of Transformers in the medical imaging domain, thereby bridging the gap between the vision and medical imaging community in this rapidly evolving area. Specifically, we present a comprehensive overview of more than 125 relevant papers to cover the recent progress.\n\\item  We  provide a detailed coverage of the field by categorizing the papers based on their applications in medical imaging as depicted in Fig. \\ref{fig:applications_transformers}. \nFor each of these applications, we develop a taxonomy, highlight task-specific challenges, and provide insights about solving them based on the literature reviewed. \n\\item Finally, we provide a critical discussion of the field’s current state as a whole, including identifying key challenges, highlighting open problems, and outlining promising future  directions.  \n\\item Although the main focus of this survey is on Vision Transformers, we are also the first since the inception of the \\text{original} Transformer, about half a decade ago, to extensively cover its language modeling capabilities in the clinical report generation task (see Sec.~\\ref{sec:cli}).\n\\end{itemize}\n\\textit{\\textbf{Paper Organization.}} The rest of the paper is organized as follows. In Sec. \\ref{sec:background}, we provide background of the field with a focus on salient concepts underlying Transformers. From Sec. \\ref{sec:seg} to Sec. \\ref{sec:other_app}, we comprehensively cover applications of Transformers in several medical imaging tasks as shown in Fig. \\ref{fig:applications_transformers}. In particular, for each of these tasks, we develop a taxonomy and identify task-specific challenges. \nSec. \\ref{sec:openproblems} presents open problems and future directions about the field as a whole. Finally, in Sec. \\ref{sec:conc}, we give recommendations to cope with the rapid development of the field and conclude the paper.\n\\begin{figure*}[]\n\\centering\n\\includegraphics[width = \\textwidth]{collage_landscape_.pdf} \n\\caption{Applications of ViTs in various medical imaging problems along with the baseline CNN-based approaches. ViT-based approaches give superior\nperformance as compared to CNN-based methods due to their ability to model the global context. Figure sources: (a) , (b) , (c) , (d) , (e) , (f) . }\n\\label{fig:applications}\n\\end{figure*}\n\\iffalse\n\\begin{figure*}[h]\n\\centering\n\\includegraphics[scale=0.3]{taxonomy_full.pdf}\n\\caption{Taxonomy of applications of ViTs in clinical report generation.}\n\\end{figure*}\n\\fi", "cites": [6570, 6574, 6572, 9052, 6573, 707, 8839, 6569, 6576, 7070, 1470, 38, 4738, 7, 7373, 6571, 5770, 728, 5807, 9053, 6575, 5802, 5764, 732, 166, 6577, 9136], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 39, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers, connecting the limitations of CNNs with the strengths of Transformers, especially in capturing global context. It provides a critical perspective by pointing out the shortcomings of CNNs and the challenges of ViTs in medical imaging. It also abstracts by identifying broader patterns such as the shift from CNNs to attention-based models and the increasing relevance of Transformers in diverse medical imaging tasks."}}
{"id": "ded43306-befb-4fd7-91fd-6c8c3deef957", "title": "Hand-Crafted Approaches", "level": "subsection", "subsections": [], "parent_id": "c08cd057-2b11-4f38-8953-7db77d0dc19f", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Background"], ["subsection", "Hand-Crafted Approaches"]], "content": "Conventional algorithms to solve medical imaging tasks are based on hand-crafted mathematical models designed by field experts using domain knowledge.\nThe development of these hand-crafted models with a focus on refining discriminative features and efficient optimization algorithms for a range of medical imaging problems has been the central research topic in the past . Successful hand-crafted models in medical imaging include total-variation , non-local self-similarity , sparsity/structured sparsity , Markov-tree models on wavelet coefficients , and untrained neural networks .\nThese models have been extensively leveraged in medical domain for image segmentation , reconstruction , disease classification , enhancement , and anomaly detection  due to their interpretability with solid mathematical foundations and theoretical supports on the robustness, recovery, and complexity . Further, unlike deep learning-based approaches, they do not require large annotated medical imaging datasets for training. This reduced reliance on labeled datasets \nis crucial to the medical research community as collecting voluminous, reliable, and labeled data in the medical domain is difficult due to the lack of expert annotators, high time consumption, ethical considerations, and financial costs. \nHowever, due to inadequacy to leverage the expressive power of large medical imaging data sets, these hand-crafted models often suffer from poor discriminative capability . Consequently, these models often fail to represent nuances of high-dimensional complex medical imaging data that can hamper the performance of the medical imaging diagnosis systems . To circumvent the poor discriminability and generalization issue, learned hand-crafted models have been proposed to exploit data better.\nThe representative approaches include \noptimal directions , K-SVD , data-driven tight frame~, low-rank models , and piece-wise smooth image model. Next, we explain the popular data-driven approaches explored in the literature. \n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = \\textwidth]{ViT_figure_2.pdf} \n\\caption{Architecture of the Vision Transformer (\\textit{on the left}) and details of the Vision Transformer encoder block (\\textit{on the right}). Vision transformer first\nsplits the input image into patches and projects them (after flattening) into a feature space where a transformer encoder processes them to produce the final classification output.}\n\\label{fig:vit}\n\\end{figure*}", "cites": [6580, 9137, 6578, 524, 6579], "cite_extract_rate": 0.20833333333333334, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of hand-crafted approaches in medical imaging by synthesizing multiple papers into a coherent narrative that highlights their evolution and limitations. It critically discusses the drawbacks of traditional models in capturing high-dimensional data nuances and introduces data-driven alternatives. However, the abstraction is limited to general categories rather than offering a deeper theoretical or conceptual framework."}}
{"id": "e588f662-a475-49ff-8a31-412b8b55d114", "title": "CNN-based methods", "level": "subsection", "subsections": [], "parent_id": "c08cd057-2b11-4f38-8953-7db77d0dc19f", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Background"], ["subsection", "CNN-based methods"]], "content": "CNNs are effective at learning discriminative features and extracting generalizable priors from large-scale medical datasets, thus providing excellent performance on medical imaging tasks, \nmaking them an integral component of modern AI-based medical imaging systems.\nThe advancements in CNNs have been mainly fueled by novel architectural designs, better optimization procedures, availability of special hardware (\\textit{e.g.,} GPUs) and purpose-built open source software libraries . \nWe refer interested readers to comprehensive survey papers related to CNNs applications in medical imaging . Despite considerable performance gains, the reliance of CNNs on large labeled datasets limits their applicability over the full spectrum of medical imaging tasks. Furthermore, CNNs-based approaches are generally more challenging to interpret and often act as black box solutions. \nTherefore, there has been an increasing effort in the medical imaging community to amalgamate the strengths of hand-crafted and CNNs based methods resulting in the prior information-guided CNNs models . These hybrid methods contain special domain-specific layers, and include unrolled optimization , generative models , and learned denoiser-based approaches . \nDespite these architectural and algorithmic advancements, the decisive factor behind CNNs success has been primarily attributed to their image-specific inductive bias in dealing with scale invariance and modeling local visual structures. While this intrinsic locality (limited receptive field) brings efficiency to CNNs, it impairs their ability to capture long-range spatial dependencies in an input image, thereby stagnating performance~ (see Fig.~\\ref{fig:applications}).\nThis demands an alternative architectural design capable of modeling long-range pixel relationships for better representation learning.", "cites": [6583, 6582, 6584, 6571, 6581], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key aspects of CNN-based methods in medical imaging by integrating themes from multiple papers, such as architectural advancements, interpretability issues, and hybrid approaches. It critically addresses limitations like the need for large labeled data and the inability to model long-range dependencies, while also pointing to broader trends in the field. The abstraction is strong, identifying the core strengths and weaknesses of CNNs and their implications for architectural evolution."}}
{"id": "c78fd902-da13-4bbc-a760-35c016e4100a", "title": "Transformers", "level": "subsection", "subsections": ["98bd83cb-5fed-439c-bee1-c27ac9860001", "5ef566a4-1ff1-4e27-8124-3453557b091a"], "parent_id": "c08cd057-2b11-4f38-8953-7db77d0dc19f", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Background"], ["subsection", "Transformers"]], "content": "Transformers were introduced by Vaswani \\textit{et al.}  as a new attention-driven building block for machine translation. Specifically, these attention blocks are neural network layers that aggregate information from the entire input sequence . Since their inception, these models have demonstrated state-of-the-art performance on several Natural Language Processing (NLP) tasks, thereby becoming the default choice over recurrent models.\nIn this section, we will focus on Vision Transformers (ViTs)  that are built on vanilla Transformer model~ by cascading multiple transformer layers to capture the global context of an input image. Specifically, Dosovitskiy \\textit{et al.}  interpret an image as a sequence of patches and process it by a standard transformer encoder as used in NLP. These ViT models continue the long-lasting trend of removing hand-crafted visual features and inductive biases from models in an effort to leverage the availability of larger datasets coupled with increased computational capacity. ViTs have garnered immense interest in the medical imaging community, and a number of recent approaches have been proposed which build upon ViTs. We highlight the working principle of ViT in a step-by-step manner in Algorithm \\ref{alg:vit} for medical image classification.\n\\iffalse\n\\begin{itemize}\n    \\item Split image into patches of fixed sizes.\n    \\item Vectorize image patches after flattening them.\n    \\item Create lower-dimensional linear embedding from vectorized patches via trainable linear layer.\n    \\item Add positional encoding to lower dimensional linear embeddings.\n    \\item Feed the sequence to ViT encoder as shown in Figure \\ref{fig:vit}.\n    \\item Pre-train the ViT model on large-scale image dataset.\n    \\item Fine tune on down stream classification task.\n\\end{itemize}\n\\fi\n\\makeatletter\n\\def\\BState{\\State\\hskip-\\ALG@thistlm}\n\\makeatother\n\\algnewcommand\\algorithmicinput{\\textbf{Input:}}\n\\algnewcommand\\algorithmicoutput{\\textbf{Output:}}\n\\algnewcommand\\Input{\\item[\\algorithmicinput]}\n\\algnewcommand\\Output{\\item[\\algorithmicoutput]}\n\\begin{algorithm}[t]\n    \\caption{ViT Working Principle}\\label{alg:vit}\n    \\begin{algorithmic}[1]\n    \\item Split a medical image into patches of fixed sizes \n    \\item Vectorize image patches via flattening operation\n    \\item Create lower-dimensional linear embedding from vectorized patches via trainable linear layer\n    \\item Add positional encoding to lower dimensional linear embeddings\n    \\item Feed the sequence to ViT encoder as shown in Figure \\ref{fig:vit}\n    \\item Pre-train the ViT model on a large-scale image dataset\n    \\item Fine-tune on the down stream medical image classification task\n    \\end{algorithmic}\n\\end{algorithm}\nBelow, we briefly describe the core components behind the success of ViTs that are \\textit{self-attention} and \\textit{multi-head self-attention}. For a more in-depth analysis of numerous ViT architectures and applications, we refer interesting readers to the recent relevant survey papers .\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[trim = {5cm 14.5cm 4cm 4cm},clip,width = \\textwidth]{segmentation_taxonomy_port.pdf}\n\\caption{Taxonomy of ViT-based medical image segmentation approaches.}\n\\label{fig:seg_taxonomy}\n\\end{figure*}", "cites": [1458, 7078, 7040, 7302, 732, 38, 168, 728], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of Transformers and ViTs, following a step-by-step explanation of their working principle. It cites multiple relevant papers but does not synthesize their insights into a broader narrative or framework. There is minimal critical evaluation or abstraction beyond the specific components of ViTs."}}
{"id": "98bd83cb-5fed-439c-bee1-c27ac9860001", "title": "Self-Attention", "level": "subsubsection", "subsections": [], "parent_id": "c78fd902-da13-4bbc-a760-35c016e4100a", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Background"], ["subsection", "Transformers"], ["subsubsection", "Self-Attention"]], "content": "}The success of the Transformer models has been widely attributed to the self-attention (SA) mechanism due to its ability to model long-range dependencies. The key idea behind the SA mechanism is to learn self-alignment, that is, to determine the relative importance of a single token (patch embedding) with respect to all other tokens in the sequence .\nFor 2D images, we first reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N\\times(P^2·C)}$, where $H$ and $W$ denotes height and width of the original image respectively, $C$ is the number of channels, $P \\times P$ is the resolution of each image patch, and $N = HW/P^2$ is the resulting number of patches. These flattened patches are projected to $D$ dimension via trainable linear projection layer and can be represented in matrix form as $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$. The goal of self-attention is to capture the interaction amongst all these $N$ embeddings, that is done by defining three learnable weight matrices to transform input $\\mathbf{X}$ into Queries (via $\\mathbf{W}^Q \\in \\mathbb{R}^{D \\times D_q}$), Keys (via $\\mathbf{W}^K \\in \\mathbb{R}^{D \\times D_k}$) and Values (via $\\mathbf{W}^V \\in \\mathbb{R}^{D \\times D_v}$), where $D_q = D_k$. The input sequence $\\mathbf{X}$ is first projected onto these weight matrices to get $\\mathbf{Q}=\\mathbf{X}\\mathbf{W}^Q$, $\\mathbf{K}=\\mathbf{X}\\mathbf{W}^K$ and $\\mathbf{V}=\\mathbf{X}\\mathbf{W}^V$.\nThe corresponding attention matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ can be written as,\n$$\\mathbf{A} = \\mathbf{softmax}\\left (\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D_q}}\\right ).$$\nThe output $\\mathbf{Z}\\in\\mathbb{R}^{N \\times D_v}$ of the SA layer is then given by,\n$$ \\mathbf{Z} = \\text{SA}(\\mathbf{X}) =  \\mathbf{A}\\mathbf{V}.$$", "cites": [168], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of the self-attention mechanism, including mathematical formulations. However, it lacks synthesis of ideas beyond the single cited paper and does not compare or critique different approaches. There is minimal abstraction, focusing instead on the mechanics of self-attention in the context of 2D images."}}
{"id": "5ef566a4-1ff1-4e27-8124-3453557b091a", "title": "Multi-Head Self-Attention", "level": "subsubsection", "subsections": [], "parent_id": "c78fd902-da13-4bbc-a760-35c016e4100a", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Background"], ["subsection", "Transformers"], ["subsubsection", "Multi-Head Self-Attention"]], "content": "} Multi-Head Self Attention (MHSA) consists of multiple SA blocks (heads) concatenated together channel-wise to model complex dependencies between different elements in the input sequence. Each head has its own learnable weight matrices denoted by  $\\{\\mathbf{W}^{Q_i},\\mathbf{W}^{K_i},\\mathbf{W}^{V_i} \\}$, where $i=0 \\cdots (h{-}1)$ and $h$ denotes total number of heads in MHSA block. Specifically, we can write,\n$$\\text{MHSA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) = [\\mathbf{Z}_0,\\mathbf{Z}_1,...,\\mathbf{Z}_{h-1}]\\mathbf{W}^{O},$$\nwhereas $\\mathbf{W}^{O} \\in \\mathbb{R}^{h.D_v \\times N}$ computes linear transformation of heads and $\\mathbf{Z}_i$ can be written as,\n$$ \\mathbf{Z}_i = \\mathbf{softmax}\\left (\\frac{\\mathbf{QW}^{Q_i}(\\mathbf{KW}^{K_i})^{T}}{\\sqrt{D_q/h}}\\right ) \\mathbf{VW}^{V_i}.$$\nNote that the complexity of computing the softmax for SA block is quadratic with respect to the length of the input sequence that can limit its applicability to high-resolution medical images. Recently, numerous efforts have been made to reduce complexity, including sparse attention , linearization attention , low-rank attention , memory compression based approaches , and improved MHSA . We will discuss the efficient SA in the context of medical imaging in the relevant sections.\nFurther, we find it important to clarify that several alternate attention approaches  have been explored in the literature based on convolutional architectures. In this survey, we focus on the specific attention used in transformer blocks (MHSA) which has recently gained significant research attention in medical image analysis. Next, we outline these methods categorized according to specific application domains.", "cites": [8384, 787, 798, 6585, 8558, 1460, 6586], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of Multi-Head Self-Attention and its relevance to medical imaging, synthesizing several papers that propose modifications or improvements to attention mechanisms. While it connects ideas (e.g., linearization, sparsification), the integration is not fully novel but does form a coherent narrative. The critical analysis is moderate, as it highlights computational limitations of MHSA and briefly mentions alternatives without deep comparative evaluation. Some abstraction is present in categorizing these methods and noting the importance of attention in vision tasks."}}
{"id": "9eff7477-d20d-4419-96b9-bdb14b8f8e92", "title": "2D Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "f8c7fc05-befe-461b-8606-058d18def53d", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Organ-Specific Segmentation"], ["subsubsection", "2D Segmentation"]], "content": "} Here, we describe the organ-specific ViT-based segmentation approaches for 2D medical scans.\n {{\\textbf{Skin Lesion Segmentation.}}}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{lesion_seg_res_.PNG}\n\\caption{Comparison of different skin lesion segmentation approaches. From left to right: Input image, CNN based UNet++ , ViT-based TransUNet , Boundary aware transformer (BAT) , and the ground truth (GT) image. Red circles highlight small regions with an ambiguous boundary where BAT can perform well due to the use of boundary-wise prior knowledge. Image taken from .}\n\\label{fig:lesion}\n\\end{figure}\nAccurate skin lesion segmentation for identifying melanoma (cancer cells) is crucial for cancer diagnosis and subsequent treatment planning. However, it remains a challenging task due to significant variations in color, size, occlusions, and contrast of skin lesion areas, resulting in ambiguous boundaries~ and consequently deterioration in segmentation performance. \nTo address the issue of ambiguous boundaries, Wang \\textit{et al.}~ propose a novel Boundary-Aware Transformer (BAT).\nSpecifically, they design a boundary-wise attention gate in Transformer architecture to exploit the prior knowledge about boundaries. The auxiliary supervision of the boundary-wise attention gate provides feedback to train BAT effectively. \nExtensive experiments on ISIC 2016+PH2  and ISIC 2018  validate the efficacy of their boundary-wise prior, as shown in Fig. \\ref{fig:lesion}. \nSimilarly, Wu \\textit{et al.}~ propose a dual encoder-based feature adaptive transformer network (FAT-Net) that consists of CNN and transformer branches in the encoder. To effectively fuse the features from these two branches, a memory-efficient decoder and feature adaptation module have been designed. Experiments on ISIC 2016-2018~, and PH2~ datasets demonstrate the effectiveness of FAT-Net fusion modules.\n {{\\textbf{Tooth Root Segmentation.}}}\nTooth root segmentation is one of the critical steps in\nroot canal therapy to treat periodontitis (gum infection) . However, it is challenging due to blurry boundaries and overexposed and underexposed images. To address these challenges, Li \\textit{et al.}~ propose Group Transformer U-Net (GT U-Net) that consists of transformer and convolutional layers to encode global and local context, respectively. A shape-sensitive Fourier Descriptor loss function  has been proposed to deal with the fuzzy tooth boundaries. Furthermore, grouping and bottleneck structure has been introduced in the GT U-Net to significantly reduce the computational cost. Experiments on their in-house Tooth Root segmentation dataset with six evaluation metrics demonstrate the effectiveness of GT U-Net architectural components and Fourier-based loss function. In another work, Li \\textit{et al.}~ propose anatomy-guided multibranch Transformer (AGMB-Transformer) to incorporate the strengths of group convolutions~ and progressive Transformer network. Experiments on their self-collected dataset of 245 tooth root X-ray images show the effectiveness of AGMB-Transformer.\n {{\\textbf{Cardiac Image Segmentation.}}}\nDespite their impressive performance in medical image segmentation, Transformers are computationally demanding to train and come with a high parameter budget. To handle these challenges for cardiac image segmentation task, Deng \\textit{et al.} propose TransBridge, a lightweight parameter-efficient hybrid model.\nTransBridge consists of Transformers and CNNs based encoder-decoder structure for left ventricle segmentation in echocardiography. Specifically, the patch embedding layer of the Transformer has been re-designed using the shuffling layer  and group convolutions to significantly reduce the number of parameters. \nExtensive experiments on large-scale left ventricle segmentation dataset, echo-cardiographs~ demonstrate the benefit of TransBridge over CNNs and Transformer-based baseline approaches~.\n\\iffalse\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{transbridge.PNG}\n\\caption{Shuffling layer and group convolution layer architecture of Trans-Bridge architecture  for left ventricle segmentation. Specifically, with shuffling layer and group convolution Deng \\textit{et al.}  redesign patch embedding of ViT that significantly reduce the number of parameters and token numbers. Specifically, a group convolution layer has been applied to the grouped tokens to shorten the token sequence of length $M_1 + M_2 + M_3$ to $N$. Image taken from .}\n\\label{fig:transbridge}\n\\end{figure}\n\\fi\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 1\\textwidth]{spectr.pdf}\n\\caption{Segmentation results for hyperspectral pathology dataset using Spectral Transformer (SpecTr). From left to right: Input image, Ground truth label, HIS Hyper (CNN-based)~, UNet (CNN-based)~, Attn UNet (CNN-based)~, UNet++ (CNN-based)~, and SpecTr (ViT-based)~. Image adapted from .}\n\\label{fig:corneal}\n\\end{figure*}\n {{\\textbf{Kidney Tumor Segmentation.}}}\nAccurate segmentation of kidney tumors via computer diagnosis systems can reduce the effort of radiologists and is a critical step in related surgical procedures. However, it is challenging due to varying kidney tumor sizes and the contrast between tumors and their anatomical surroundings. To address these challenges, Shen \\textit{et al.}~ propose a hybrid encoder-decoder architecture, COTR-Net, that consists of convolution and transformer layers for end-to-end kidney, kidney cyst, and kidney tumor segmentation. Specifically, the encoder of COTR-Net consists of several convolution-transformer blocks, and the decoder comprises several up-sampling layers with skip connections from the encoder. The encoder weights have been initialized using a pre-trained ResNet~ architecture to accelerate convergence, and deep supervision has been exploited in the decoder layers to boost segmentation performance. Furthermore, the segmentation masks are refined using morphological operations as a post-processing step. Extensive experiments on the Kidney Tumor Segmentation dataset (KiTS21)~ demonstrate the effectiveness COTR-Net.\n\\begin{table}[]\n\\centering\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{V{3}l|l|lV{3}}\n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{CNN-based Models}         & \\textbf{40x Magnification} & \\textbf{20x Magnification} \\\\ \\hline\nPSPNet                    & 0.58$\\pm$0.33         & 0.49$\\pm$0.27         \\\\ \\hline\nU-Net                     & 0.65$\\pm$0.24         & 0.60$\\pm$0.31         \\\\ \\hline\nDeepLabV3               & 0.63$\\pm$0.28         & 0.67$\\pm$0.24         \\\\ \\hline\nFPN                      & 0.64$\\pm$0.20         & 0.72$\\pm$0.22         \\\\ \\hline\nPAN                      & 0.63$\\pm$0.24         & 0.69$\\pm$0.23         \\\\ \\hline\nLinkNet                  & 0.35$\\pm$0.33         & 0.54$\\pm$0.25         \\\\ \\hline \n\\rowcolor{mygray} \\textbf{Transformer-based Models} & \\textbf{40x Magnification} & \\textbf{20x Magnification} \\\\ \\hline\nTransUNet                & 0.77$\\pm$0.12         & 0.77$\\pm$0.13         \\\\ \\hline\nSwin-UNet                 & 0.53$\\pm$0.23         & 0.42+0.23         \\\\ \\hline\nSwin Transformer (Base)   & 0.79$\\pm$0.14         & 0.71$\\pm$0.26         \\\\ \\hline\nSegmenter                & 0.80$\\pm$0.14         & 0.82$\\pm$0.11         \\\\ \\hline\nMedical Transformer      & 0.71$\\pm$0.14         & 0.62$\\pm$0.17         \\\\ \\hline\nBEiT                     & 0.72$\\pm$0.21         & 0.66$\\pm$0.28         \\\\ \\hlineB{3}\n\\end{tabular}\n}\n\\caption{Evaluation of Transformer-based semantic segmentation methods for pathological image segmentation in terms of average jaccard index on PAIP liver histopathological dataset . It can be seen that transformer-based models tend to outperform CNNs with the exception of Swin-UNet. Results are from , which is one of the first study to systematically evaluate the performance of transformers on pathological image segmentation task. }\n\\label{tab:histopathology}\n\\end{table}\n {{\\textbf{Cell Segmentation.}}}\nInspired from the Detection Transformers (DETR)~, Zhang \\textit{et al.} proposed Cell-DETR~, a Transformer-based framework for instance segmentation of biological cells. Specifically, they integrate a dedicated attention branch to the DETR framework to obtain instance-wise segmentation masks in addition to box predictions. During training, focal loss  and Sorenson dice loss  are used for the segmentation branch. To enhance performance, they integrate three residual decoder blocks  in Cell-DETR to generate accurate instance masks. Experiments on their in-house yeast cells dataset demonstrate the effectiveness of Cell-DETR relative to U-Net based baselines . \nSimilarly, existing medical imaging segmentation approaches generally struggle for Corneal endothelial cells due to blurry edges caused by the subject's movement . This demands preserving more local details and making full use of the global context. Considering these attributes, Zhang \\textit{et al.}  propose a Multi-Branch hybrid Transformer Network (MBT-Net) consisting of convolutional and transformer layers. Specifically, they propose a body-edge branch that provides precise edge location information and promotes local consistency. Extensive ablation studies  on their self-collected TM-EM3000 and public Alisarine dataset  of Corneal Endothelial Cells show the effectiveness of MBT-Net architectural components.\n \\textbf{Histopathology.}\nHistopathology refers to the diagnosis and study of the diseases of tissues under a microscope and is the\ngold standard for cancer recognition. Therefore accurate automatic segmentation of histopathology images can substantially alleviate the workload of pathologists. Recently, Nguyen \\textit{et al.} \nsystematically evaluate the performance of six latest ViTs, and CNNs-based approaches on whole slide images of the PAIP liver histopathological dataset . Their results (shown in Table \\ref{tab:histopathology})  demonstrate that almost all Transformer-based models indeed exhibit superior performance as compared to CNN-based approaches due to their ability to encode the global context.", "cites": [6587, 799, 1746, 508, 6589, 9055, 505, 7360, 3769, 6594, 6592, 2578, 6593, 4771, 97, 4812, 9054, 6591, 4811, 1501, 825, 9149, 6596, 6588, 6590, 1743, 6595], "cite_extract_rate": 0.627906976744186, "origin_cites_number": 43, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of 2D segmentation approaches using Transformers in specific organs, including skin, tooth root, cardiac, and kidney. While it does integrate some ideas (e.g., boundary-aware attention for skin lesion segmentation), the synthesis is limited to individual techniques rather than connecting broader themes or trends. The analysis remains largely superficial, with minimal critique or comparison of methods beyond stating their effectiveness on specific datasets."}}
{"id": "b2c2a236-1c44-487f-a734-adbc25e3f937", "title": "3D Medical Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "f8c7fc05-befe-461b-8606-058d18def53d", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Organ-Specific Segmentation"], ["subsubsection", "3D Medical Segmentation"]], "content": "} Here, we describe ViT-based segmentation approaches for volumetric medical data.\n {{\\textbf{Brain Tumor Segmentation.}}}\nAn automatic and accurate brain tumor segmentation approach can lead to the timely diagnosis of neurological disorders such as Alzheimer’s disease. Recently, ViT-based models have been proposed to segment brain tumors effectively. Wang \\textit{et al.}~ have made the first attempt to leverage Transformers for 3D multimodal brain tumor segmentation by effectively modeling local and global features in both spatial and depth dimensions. Specifically, their encoder-decoder architecture, TransBTS, employs a 3D CNN to extract local 3D volumetric spatial features and Transformers to encode global features. Progressive upsampling in the 3D CNN-based decoder has been used to predict the final segmentation map.\nTo further boost the performance, they make use of test-time augmentation. Extensive experimentation on BraTS 2019\\footnote{https://www.med.upenn.edu/cbica/brats2019} and BraTS 2020\\footnote{https://www.med.upenn.edu/cbica/brats2020} datasets show the effectiveness of their proposed approach compared to CNN-based methods. \nUnlike most of the ViT-based image segmentation approaches, TransBTS does not require pre-training on large datasets and has been trained from scratch. \n\\iffalse\n\\begin{figure}[t]\n\\includegraphics[width = 0.5\\textwidth]{bts_iguana.PNG}\n\\caption{Architecture of the TransBTS for multi-modal brain tumor segmentation . The encoder consists of a 3D CNN to generate compact feature maps, and then leverages the Transformer encoder to model global context. After that, there is a repeated stack of upsampling and convolutional layers to gradually generate a high-resolution segmentation result. Figure modified from .}\n\\label{fig:transbts}\n\\end{figure}\n\\fi\nIn another work, inspired from the architectural design of TransBTS , Jia \\textit{et al.}  propose Bi-Transformer U-Net (BiTr-UNet) that performs relatively better on BraTS 2021~ segmentation challenge. Different from TransBTS , BiTr-UNet consists of an attention module to refine encoder and decoder features and has two ViT layers (instead of one as in TransBTS).\nFurthermore, BiTr-UNet adopts a post-processing strategy to eliminate a volume of predicted segmentation if the volume is smaller than a threshold  followed by model ensemble via majority voting .\nSimilarly, Peiris \\textit{et al.}~ propose a light-weight UNet shaped volumetric transformer, VT-UNet, to segment 3D medical image modalities in a hierarchical manner. Specifically, two self-attention layers have been introduced in the encoder of VT-UNet to capture both global and local contexts. Furthermore, the introduction of window-based self-attention and cross-attention modules and Fourier positional encoding in the decoder significantly improve the accuracy and efficiency of VT-UNet. Experiments on BraTs 2021~ show that VT-UNet is robust to data artifacts and exhibits strong generalization ability.\n\\begin{table}\n\\centering\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{V{3}lcccV{3}} \n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{Method} & \\textbf{\\#params}      & \\textbf{Flops}        & \\textbf{Dice Score (Avg.)}  \\\\ \n\\hline\nTransBTS~                                 & 33 M           & 333 G         & 84.99              \\\\\nBiTr-UNet~                                 & -           & -         &    86.20           \\\\\nUNETR~                                    & 102.5 M        & 193.5 G       & 84.51              \\\\\nnnFormer~                                 & 39.7 M         & 110.7 G       & 86.56              \\\\\nSwin UNETR~                               & 61.98 M        & ~394.84 G     & \\textbf{88.97}     \\\\\nVT-UNET-T~                                & \\textbf{5.4 M} & \\textbf{52 G} & 86.82              \\\\\nVT-UNET-S~                                & 11.8 M         & 100.8 G       & 87.00              \\\\\nVT-UNET-B~                                & 20.8 M         & 165 G         & 88.07              \\\\\n\\hlineB{3}\n\\end{tabular}\n}\n\\caption{Segmentation results and parameters of various Transformer-based models on 3D Multimodal Brain Tumor BraTS 2021 dataset~.}\n\\label{tab:brats21}\n\\end{table}\nIn another similar work, Hatamizadeh \\textit{et al.}~ propose Swin UNet based architecture, Swin UNETR, that consists of Swin transformer as the encoder and a CNN-based decoder. Specifically,  Swin UNETR computes self-attention in an efficient shifted window partitioning scheme and is a top-performing model on BraTs 2021~ validation set. In Table \\ref{tab:brats21}, we provide dice score and other parameters of various Transformer based models for the 3D multimodal BraTs 2021 dataset~.\n {{\\textbf{Histopathology.}}}\nBoxiang \\textit{et. al}  propose Spectral Transformer (SpecTr) for hyperspectral pathology image segmentation, which employs transformers to learn the contextual feature across the spectral dimension. To discard the irrelevant spectral bands, they introduce a sparsity-based scheme . Furthermore, they employ separate group normalization for each band to eliminate the interference caused by distribution mismatch among spectral images. Extensive experimentation on the hyperspectral pathology dataset, Cholangiocarcinoma~, shows the effectiveness of SpecTr as also shown in Fig. \\ref{fig:corneal}.\n {{\\textbf{Breast Tumor Segmentation.}}} Detection of breast cancer in the early stages can reduce the fatality rate by more than 40$\\%$~. Therefore, automatic breast tumor detection is of immense importance to doctors. Recently, Zhu \\textit{et al.}~ propose a region aware transformer network (RAT-Net) to effectively fuse the Breast tumor region information into multiple scales to obtain precise segmentation. Extensive experiments on a large ultrasound breast tumor segmentation dataset show that RAT-Net outperforms CNN and transformer-based baselines.\nSimilarly, Liu \\textit{et al.}~ also propose a hybrid architecture consisting of transformer layers in the decoder part of 3D UNet~ to effectively segment tumors from volumetric breast data.", "cites": [6600, 6601, 6570, 6587, 4731, 6598, 6597, 8255, 6599], "cite_extract_rate": 0.5625, "origin_cites_number": 16, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several ViT-based 3D segmentation approaches, particularly for brain tumor and breast tumor segmentation, and provides some connections through architectural similarities (e.g., TransBTS, BiTr-UNet, VT-UNet). It includes a comparative table that helps in assessing model performance and efficiency. However, it lacks deeper critical analysis of the limitations or trade-offs of these methods, and generalization is limited to a few observed trends rather than broader principles."}}
{"id": "b5cf7111-bd3a-45d6-aacb-a926e25f1ffa", "title": "Multi-organ Segmentation", "level": "subsection", "subsections": ["a71687ce-a4f7-4d36-ab50-f9bb432f9e73", "949db42a-f097-48a8-bf3c-838b1691d462"], "parent_id": "7529a8d0-3662-4f9d-9c34-1867e0f6edab", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Multi-organ Segmentation"]], "content": "Multi-organ segmentation aims to segment several organs simultaneously and is challenging due to inter-class imbalance and varying sizes, shapes, and contrast of different organs. ViT models are particularly suitable for the multi-organ segmentation due to their ability to effectively model global relations and differentiate multiple organs. We have categorized multi-organ segmentation approaches based on the architectural design, as these approaches do not consider any organ-specific aspect and generally focus on boosting performance by designing effective and efficient architectural modules~. We categorize multi-organ segmentation approaches into \\textit{Pure Transformer} (only ViT layers) and \\textit{Hybrid Architectures} (both CNNs and ViTs layers).\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = \\columnwidth]{transunet_iguana.PNG}\n\\caption{Overview of TransUNet architecture  proposed for multi-organ segmentation. It is one of the first transformer-based architecture proposed for medical image segmentation and merits both transformer and UNet. It employs a hybrid CNN-Transformer architecture for encoder, followed by multiple upsampling layers in decoder to output final segmentation mask. Image adapted from .}\n\\label{fig:transunet}\n\\end{figure}", "cites": [6602, 6594], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the concept of multi-organ segmentation and briefly mentions the suitability of ViT models for this task, but it primarily summarizes the architectural design categories without a deeper synthesis of insights from the cited papers. It lacks critical evaluation of the approaches and only slightly generalizes by noting the ability of Transformers to model global relations. The figure from TransUNet is included descriptively, without analysis of its strengths or limitations."}}
{"id": "a71687ce-a4f7-4d36-ab50-f9bb432f9e73", "title": "Pure Transformers", "level": "subsubsection", "subsections": [], "parent_id": "b5cf7111-bd3a-45d6-aacb-a926e25f1ffa", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Multi-organ Segmentation"], ["subsubsection", "Pure Transformers"]], "content": "} \nPure Transformer based architectures consist of only ViT layers and have seen fewer applications in medical image segmentation compared to hybrid architectures as both global and local information is crucial for dense prediction tasks like segmentation~.\nRecently, Karimi \\textit{et. al}   propose a pure Transformer-based model for 3D medical image segmentation by leveraging self-attention  between neighboring linear embedding of 3D medical image patches. They also propose a method to effectively pre-train their model when only a few labeled images are available. Extensive experiments show the effectiveness of their convolution-free network on three benchmark 3D medical imaging datasets related to brain cortical plate~, pancreas, and hippocampus. \nOne of the drawbacks of using Pure Transformer-based models in segmentation is the quadratic complexity of self-attention with respect to the input image dimensions. This can hinder the ViTs applicability in the segmentation of high-resolution medical images. To mitigate this issue, Cao \\textit{et al.}  propose Swin-UNet that, like Swin Transformer~, computes self-attention within a local window and has linear computational complexity with respect to the input image. \nSwin-UNet also contains a patch expanding layer for upsampling decoder's feature maps and shows superior performance in recovering fine details compared to bilinear upsampling.\nExperiments on Synapse and ACDC~ dataset demonstrate the effectiveness of the Swin-UNet architectural design.", "cites": [6603, 4771, 6604, 6594, 1501], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes the cited papers by connecting the concept of pure Transformer models with their specific applications in multi-organ medical image segmentation. It highlights a key limitation (quadratic complexity) and introduces Swin-UNet as a solution, showing analytical depth. The section abstracts from individual methods to discuss broader trends like the trade-off between global attention and computational efficiency in medical imaging."}}
{"id": "5c373539-ad63-4584-8708-c446010c9e3c", "title": "Single-Scale Architectures", "level": "paragraph", "subsections": [], "parent_id": "949db42a-f097-48a8-bf3c-838b1691d462", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Multi-organ Segmentation"], ["subsubsection", "Hybrid Architectures"], ["paragraph", "Single-Scale Architectures"]], "content": "}}\nThese methods process the input image information at one scale only and have seen widespread applications in medical image segmentation due to their low computational complexity compared to multi-scale architectures. \nWe can sub-categorized single-scale architectures based on the position of the Transformer layers in the model. These sub-categories include \\textit{Transformer in Encoder}, \\textit{Transformer between Encoder and Decoder}, \\textit{Transformer in Encoder and Decoder}, and \\textit{Transformer in Decoder}.\n{{\\textbf{Transformer in Encoder.}}} Most initially developed Transformer-based medical image segmentation approaches have Transformer layers in the model's encoder.\nThe first work in this category is TransUNet~ that consists of 12 Transformer layers in the encoder as shown in Figure \\ref{fig:transunet}. These Transformer layers encode the tokenized image patches from the CNN layers.\nThe resulting encoded features are upsampled via up-sampling layers in the decoder to output the final segmentation map. With skip-connection incorporated, TransUnet sets new records (at the time of publication) on synapse multi-organ segmentation dataset~ and automated cardiac diagnosis challenge (ACDC)~.\nIn other work, Zhang \\textit{et al.} propose TransFuse~ to effectively fuse features from the Transformer and CNN layers via BiFusion module.\nThe BiFusion module leverages the self-attention and multi-modal fusion mechanism to selectively fuse the features.\nExtensive evaluation of TransFuse on multiple modalities (2D and 3D), including Polyp segmentation, skin lesion segmentation, Hip segmentation, and prostate segmentation, demonstrate its efficacy. \nBoth TransUNet~ and TransFuse~ require pre-training on ImageNet dataset~ to effectively learn the positional encoding of the images. To learn this positional bias without any pre-training, Valanarasu \\textit{et al.}  propose a modified gated axial attention layer~ that works well on small medical image segmentation datasets. Furthermore, to boost segmentation performance, they propose a Local-Global training scheme to focus on the fine details of input images.\nExtensive experimentation on brain anatomy segmentation , gland segmentation , and MoNuSeg (microscopy)  demonstrate the effectiveness of their proposed gated axial attention module.\nIn another work, Tang \\textit{et al.}~ introduce Swin UNETR, a novel self-supervised learning framework with proxy tasks to pre-train Transformer encoder on 5,050 images of CT dataset. They validate the effectiveness of pre-training by fine-tuning the Transformer encoder with a CNN-based decoder on the downstream task of MSD and BTCV segmentation datasets.\nSimilarly, Sobirov \\textit{et al.}~ show that transformer-based models can achieve comparable results to state-of-the-art CNN-based approaches on the task of head and neck tumor segmentation. \nFew works have also investigated the effectiveness of Transformer layers by integrating them into the encoder of UNet-based architectures in a plug-and-play manner. For instance, Cheng \\textit{et al.}  propose TransClaw UNet by integrating Transformer layers in the encoding part of the Claw UNet~ to exploit multi-scale information.\nTransClaw-UNet achieves an absolute gain of 0.6 in dice score compared to Claw-UNet on Synapse multi-organ segmentation dataset and shows excellent generalization.\nSimilarly, inspired from the LeViT , Xu \\textit{et al.}~ propose LeViT-UNet which aims to optimize the trade-off between accuracy and efficiency. \nLeViT-UNet is a multi-stage architecture \nthat demonstrates good performance and generalization ability on Synapse and ACDC \nbenchmarks.\n{{\\textbf{Transformer between Encoder and Decoder.}}} In this category, Transformer layers are between the encoder and decoder of a U-Shape architecture. These architectures are more suitable to avoid the loss of details during down-sampling in the encoder layers. The first work in this category is TransAttUNet~\nthat leverages guided attention and multi-scale skip connection to enhance the flexibility of traditional UNet. Specifically, a robust self-aware attention module has been embedded between the encoder and decoder of UNet to concurrently exploit the expressive abilities of global spatial attention and transformer self-attention. Extensive experiments on five benchmark medical imaging segmentation datasets demonstrate the effectiveness of TransAttUNet architecture. Similarly, Yan \\textit{et al.}~ propose Axial Fusion Transformer UNet (AFTer-UNet) that contains a computationally efficient axial fusion layer between encoder and decoder to effectively fuse inter and intra-slice \ninformation for 3D medical image segmentation. Experimentation on BCV~, Thorax-85~, and SegTHOR~ datasets demonstrate the effectiveness of their proposed fusion layer.\n{{\\textbf{Transformer in Encoder and Decoder.}}} Few works integrate Transformer layers in both encoder and decoder of a U-shape architecture to better exploit the global context for medical image segmentation. The first work in this category is UTNet that efficiently reduces the complexity of the self-attention mechanism from quadratic to linear~.\nFurthermore, to model the image content effectively, UTNet exploits the two-dimensional relative position encoding~. Experiments show strong generalization ability of UTNet on multi-label and multi-vendor cardiac MRI challenge dataset cohort~.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{nninterleave.pdf} \n\\caption{Overview of the interleaved encoder not-another transFormer (nnFormer)  for volumetric medical image segmentation. Note that  convolution and transformer layers are interleaved to give full play to their strengths. Image taken from .}\n\\label{fig:nnformer}\n\\end{figure}\nSimilarly, to optimally combine convolution and transformer layers for medical image segmentation, Zhou \\textit{et al.}~ propose nnFormer, an interleave encoder-decoder based architecture, where convolution layer encodes precise spatial information and Transformer layer encodes global context as shown in Fig. \\ref{fig:nnformer}. Like Swin Transformers , the self-attention in nnFormer has been computed within a local window to reduce the computational complexity. Moreover, deep supervision in the decoder layers has been employed to enhance performance.\nExperiments on ACDC and Synapse datasets show that\nnnFormer surpass Swin-UNet~ (transformer-based medical segmentation approach) by over 7$\\%$ (dice score) on Synapse dataset.\nIn other work, Lin \\textit{et al.} propose  Dual Swin Transformer UNet (DS-TransUNet)  to incorporate the advantages of Swin Transformer in U-shaped architecture for medical image segmentation. They split the input image into non-overlapping patches at two scales and feed them into the two Swin Transformer-based branches of the encoder. A novel Transformer Interactive Fusion module has been proposed to build long-range dependencies between different scale features in encoder. DS-TransUNet outperforms CNN-based methods on four standard datasets related to Polyp segmentation, ISIC 2018, GLAS, and Datascience bowl 2018. \n{{\\textbf{Transformer in Decoder.}}} Li \\textit{et al.}~ investigate the use of Transformer as an upsampling block in the decoder of the UNet for medical image segmentation. Specifically, they adopt a window-based self-attention mechanism to better complement the upsampled feature maps while maintaining \n\\begin{table*}[]\n\\rotatebox{90}{\n\\centering\n\\begin{adjustbox}{max width=1.2\\textwidth}\n\\begin{tabular}{V{3}c|c|p{2.2cm}|c|p{3.5cm}|p{3cm}|c|c|p{11.5cm}V{3}}\n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{Method} & \\textbf{Organ} & \\textbf{Modality}        & \\textbf{Type} & \\textbf{Datasets}                                                                  & \\textbf{Metrics}                                             & \\textbf{Arch.} & \\textbf{P.T.} & {\\textbf{Highlights}}                                                                                                                                                                                     \\\\ \\hline\nTransUNet       &     Multi-organ           & CT, MRI                & 2D            & Synapse~ , ACDC~                                              &                     Dice, Hausdorff distance                                         &         Hybrid             &  Yes    &  Encodes strong global\ncontext by treating the image features as sequences but also well utilizes the low-level CNN features via a u-shaped hybrid architectural design.                                                                                                                   \\\\ \\hline\nTransFuse       &      Multi-organ          & Colonoscopy                        & 2D, 3D        &                                        KVASIR~, Clinic DB~, Colon DB~, EndoScene~, ETIS~, ISIC 2017~, MSD~                                 &         Dice, Jaccard index                                                      &         Hybrid             &   Yes     & Leverages the inductive bias of CNNs on modeling spatial correlation and the powerful capability of Transformers on modelling global relationship. Novel Bi-Fusion module to fuse CNN and  Transformers features for segmentation.                                                                                                                                          \\\\ \\hline\nMedT            &     Multi-organ           & Ultrasound, Microscopy   & 2D            & Brain US (Private), GLAS, MoNuSeg~                                                     & F1                                                             &        Hybrid               &    No  & Gated axial attention layer for positional encoding. Local global training for training on both full resolution images as well in patches.                                                                        \\\\ \\hline\nConv. Free      &      Multi-organ          & MRI, CT                  & 3D            & Brain crotial, (private)                                               &                                         Dice, Hausdorff Distance, Average Symmetric Surface Distance                  &          Pure             &   Yes   & Convolutional free medical segmentation model. Based on self-attention between neighboring 3D patches.                                                                                                                                                          \\\\ \\hline\nCoTr            &      Multi-organ          & CT                       & 3D            & BCV~                                                                       &                                                  Dice           &          Hybrid            &   Yes    &  Deformable self-attention mechanism to reduce the computational and spatial complexities of modelling the long range dependency.                                                                                                                                                                    \\\\ \\hline\nSpecTr          &      Bile-duct          & Hyperspectral                      & 3D            & Choledock~ &                                              Dice, IoU, Hausdorff distance                &           Hybrid            &   No   & First application to hyperspectral and learn contextual feature across spectral dimension.                                                                                                              \\\\ \\hline\nTransBTS        &        Brain        & MRI                      & 3D            & BraTS 19~, BraTS 20~                                                          &        Dice, Hausdorff distance                                                      &           Hybrid            &   Yes   & 3D CNN for capturing local volumetric features and transformers for encoding global features.                                                                                                                    \\\\ \\hline\nU-Transformer   &      Multi-organ          & CT                       & 2D            & TCIA~, Private multi-organ &                                      Dice                        &           Hybrid            &    No  & Propose self and cross-attention modules\nto model long-range interactions and spatial dependencies.                                                                                       \\\\ \\hline\nUNETR           &      Brain, Spleen          & MRI, CT                  & 3D            & BTCV, MSD~  &    Dice, Hausdorff distance  &            Hybrid           & No     &  Transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information                                                                                                        \\\\ \\hline\nPMTrans         &       Multi-organ         & Microscopy, CT, PET      & 2D            & GLAS~, MoNuSeg~, HECKTOR~                                                &      Dice                                                        &            Hybrid           &   No   & Integrate multi-scale attention and CNN feature extraction using a pyramidal network architecture. An adaptive partitioning scheme was implemented to retain informative relations and to access different receptive fields efficiently.                                                                                                                               \\\\ \\hline\nSwin-UNet       &       Multi-organ         & CT                       & 2D            & Synapse~, ACDC~                                                                      &       Dice                                                       &       Pure                &   Yes   & Swin transformer based pure transformer architecture to segmentation with patch expanding layer design in decoder.                                                                                      \\\\ \\hline\nSegtran         & Multi-organ    & Fundus, Colonoscopy, MRI & 2D, 3D        & REFUGE 20~, BraTS 19~, CVC~, KVASIR~                                               & Dice                                                         & Hybrid                & Yes  & Squeeze-and-Expansion transformer where squeeze block regularize the self-attention module and expansion block learns diversified represetations.                                                                           \\\\ \\hline\nMBT-Net         & Eye            & Pathology                & 2D            & TM-EM3000 (private), Alisarine~                                                               & Dice, F1, Sensitivity, Specificity                           & Hybrid                & No   & Body edge branch for precise edge location information for corneal endothelial cells segmentation.                                                                                                     \\\\ \\hline\nDS-TransUNet    & Multi-organ    & Colonoscopy, Histology   & 2D            & Kvasir~, Colon DB~ and Clinic DB~, EndoScene~, ETIS~, ISIC 18~, GLAS~, Data Science Bowl 18~ & Mean Dice, Mean IoU, Precision, Recall                       & Hybrid                & Yes  & Dual-branch Swin Transformer in encoder and decoder to extract multiscale representation. Transformer Interactive Fusion module to build long-range  dependencies between features of different scales \\\\ \\hline\nMCTrans         & Multi-organ    & Colonoscopy, Pathology   & 2D            & Pannuke dataset, Colon DB~, Clinic DB~, ETIS~, KVASIR~, ISIC 2018~                     & Dice                                                         & Hybrid                & No   & Transformer self-attention for cross-scale contextual dependencies Transformer cross attention layer for semantic correspondence.                                                                      \\\\ \\hline\nLi \\textit{et al.}       & Multi-organ    & MRI, CT                  & 2D            & Synapse~, MSD Brain~                                                                 & Dice, Hausdorff distance                                     & Hybrid                & No   & Investigate the use of transformer decoder for medical image segmentation and its usage in upsampling.                                                                                                 \\\\ \\hline\nUTNet           & Heart          & MRI                      & 2D            & MRI Challenge Cohort~                                                               & Dice, Hausdorff distance                                     & Hybrid                & No   & Self-attention modules in encoder and decoder. Design relative position encoding to reduce the complexity of self-attention from quadratic to linear.                                                  \\\\ \\hline\nTransClaw UNet  & Multi-organ    & CT                       & 2D            & Synapse~                                                                            & Dice, Hausdorff distance                                     & Hybrid                & No   & Integrated transformer layer in the encoder path of Claw-UNet to extract shallow spatial features.                                                                                                     \\\\ \\hline\nTransAttUNet    & Multi-organ    & Xray, CT                 & 2D            & ISIC 2018~, JSRT~, Montgomery~, NIH~, Clean-CC-CCII~, Data Science Bowl 18~, GLAS~                         & Dice, F1                                                     & Hybrid                & No   & Multi-level guided attention and multi-scale skip connections to mitigate information recession problem.                                                                                               \\\\ \\hline\nLeViT-UNet      & Multi-organ    & CT, MRI                  & 2D            & Synapse~, ACDC~                                                                       & Dice, Hausdorff distance                                     & Hybrid                & Yes  & Integrate multiscale LeViT architecture as the a encoder in UNet.                                                                                                                                      \\\\ \\hline\nPolyp-PVT       & Multi-organ    & Colonoscopy              & 2D            & KVASIR~, Clinic DB~, Colon DB~, Endoscene~, ETIS~                                   & Dice, IoU, MAE, Weighted F-measure, S-measure, E-measure     & Hybrid                & No   & Pyramid vision transformer backbone as encoder to extract robust features. Proposed architectural components to handle noise, occlusions, and capturing global semantic cues.                          \\\\ \\hline\nCOTRNet         & Kidney         & CT                       & 2D            & KITS21 Challenge~                                                                   & Dice, Surface Dice                                           & Hybrid                & Yes  & CNN and transformer based interleaved encoder-decoder. Supervision of decoder's hidden layers.                                                                                                          \\\\ \\hline\nnnFormer        & Multi-organ    & CT, MRI                  & 3D            & Synapse~, ACDC~                                                                       & Dice                                                         & Hybrid                & Yes  & Interleaved convolution and self-attention based encoder-decoder architecture.                                                                                                                          \\\\ \\hline\nMISSFormer      & Multi-organ    & CT, MRI                  & 2D            & Synapse~, ACDC~                                                                       & Dice, Hausdorff distance                                     & Hybrid                & No   & Hierarchical encoder-decoder network with enhanced transformer block to mitigate the problem of feature inconsistency                                                                                  \\\\ \\hline\nTransBridge     & Heart          & Echocardiography        & 2D            & EchoNet-Dynamic~                                                                     & Dice, Hausdorff distance                                     & Hybrid                & No   & Shuffling layer and group convolution for patch embedding to significantly reduce the number of parameters.                                                                                            \\\\ \\hline\nBiTr-UNet       & Brain          & MRI                      & 3D            & BraTS 21~                                                                         & Dice, Hausdorff distance                                     & Hybrid                & No   & Refined version of TransBTS with two sets of ViT layers instead of one.                                                                                                                                \\\\ \\hline\nGT UNet         & Tooth          & X-ray                    & 2D            & Tooth root dataset (private)                                                           & Dice, Accuracy, Sensitivity, Specificity, Jaccard similarity & Hybrid                & No   & Group transformer layers to reduce computational cost. Fourier descriptor based loss function to integrate shape prior.                                                                                \\\\ \\hline\nBAT             & --             & Dermoscopy               & 2D            & ISIC 2016+PH2~, ISIC 2018~                                                           & Dice, IoU                                                    & Hybrid                & Yes  & Boundary-wise attention gate is added at the end of each transformer encoder layer to tackle challenging cases with ambiguous boundaries.                                                              \\\\ \\hline\nAFTer-UNet      & Multi-organ    & CT                       & 3D            & BCV~, Thorax-85~, SegTHOR~                                                            & Dice                                                         & Hybrid                & No   & Axial fusion mechanism to fuse intra-slice and inter-slice contextual information to guide segmentation.                                                                                              \\\\ \\hline\nVT-UNet      & Multi-organ & CT, MRI                       & 3D            & BraTS 21~, MSD~                                                            & Dice, Hausdorff distance                                                         & Hybrid                & Yes   & U-shaped encoder-decoder design. Encoder has two consecutive self-attention layers to encode local and global cues, and our decoder has novel parallel shifted window based self and cross attention blocks to capture fine details.  \\\\ \\hline\nSwin UNETR      & Brain & MRI                       & 3D            & BraTS 21~                                                            & Dice, Hausdorff distance                                                         & Hybrid                & Yes   & Swin UNet based architecture that consists of Swin transformer as the encoder and a CNN-based decoder. Computes self-attention in an efficient shifted window partitioning scheme.                                                         \\\\ \\hlineB{3}\n\\end{tabular}\n\\end{adjustbox}\n}\n\\caption{An overview of ViT-based approaches for medical image segmentation. P.T: pretraining.}\n\\label{tab:seg_taxonomy}\n\\end{table*}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.9\\textwidth]{unetr_nvidia.PNG}\n\\caption{Qualitative results of brain tumor segmentation task using transformer. From left to right: Ground truth image, UNETR~ (ViT-based), TransBTS~ (ViT-based), CoTr~ (ViT-based), and UNet~ (CNN based). Note that transformer-based approaches demonstrate better performance in capturing the fine-grained details of brain tumors as compared to CNN-based method. Image courtesy .}\n\\label{fig:unetr}\n\\end{figure*}\n\\begin{SCfigure*}[][t]\n\\centering\n\\caption{Overview of CNN and a Transformer (CoTr) architecture  proposed for 3D medical image segmentation. It consists of CNN encoder (left) to extract multi-scale features from the input, followed by  DeTrans-encoder (yellow blocks) to process the flattened multi-scale feature maps. Output features from encoder are fed to the CNN decoder (right) for segmentation mask prediction. Image courtesy .}\n\\includegraphics[height = 0.3\\textwidth,width = 0.70\\textwidth]{cotr.PNG}\n\\label{fig:cotr}\n\\end{SCfigure*}\n\\hspace{-1.5em}efficiency. Experiments on MSD Brain and Synapse datasets demonstrate the superiority of their architecture compared to bilinear upsampling.\nIn another work, \nLi \\textit{et. al}  propose SegTran, a Squeeze-and-Expansion Transformer for 2D and 3D medical image segmentation. Specifically, the squeeze block regularizes the attention matrix, and the expansion block learns diversified representations. Furthermore, a learnable sinusoidal positional encoding has been proposed that helps the model to encode spatial relationships. Extensive experiments on Polyp, BraTS19, and REFUGE20 (fundus images) segmentation challenges demonstrate the strong generalization ability of Segtran. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{seg_challenge_plt_.pdf} \n\\caption{Dice results of BTCV challenge on multi-organ segmentation for various transformer based methods. It can be seen that Swin-UNETR is able to achieve on average 13$\\%$ improvement in dice coefficient score compare to SETR method, indicating rapid pace of research in the field. Transformer based approaches used for the comparison include SETR NUP~, SETR PUP~, SETR MLA~, TransUNet~, CoTr*~ (small CNN encoder compared to CoTr), CoTr~, UNETR~, and Swin UNETR~.\nNote: Avg: Average results (over 12 organs), AG: left and right adrenal glands, Pan: pancreas, Sto: stomach, Spl: spleen, Liv: liver, Gall: gallbladder.}\n\\label{fig:seg_challenge}\n\\end{figure}", "cites": [6572, 6619, 6573, 6618, 6613, 8839, 6594, 2581, 6610, 5770, 6590, 6603, 6611, 9055, 6607, 6593, 7333, 6620, 825, 6616, 6600, 6598, 6588, 6599, 6612, 9056, 6609, 6615, 6601, 7998, 6608, 6592, 6605, 1501, 6229, 6587, 6570, 8257, 2578, 6606, 8256, 4771, 6614, 6617], "cite_extract_rate": 0.5789473684210527, "origin_cites_number": 76, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers into a structured framework based on the placement of Transformer layers within segmentation architectures. It abstracts key design patterns (e.g., encoder-only, interleaved, decoder integration) and identifies trends in performance and pre-training strategies. Critical analysis is evident in the discussion of limitations, such as the need for pre-training or handling computational complexity, and comparisons of effectiveness across different designs."}}
{"id": "371641ea-efef-4ac7-b4c8-cd59c4e9eb93", "title": "Multi-Scale Architectures", "level": "paragraph", "subsections": [], "parent_id": "949db42a-f097-48a8-bf3c-838b1691d462", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "Multi-organ Segmentation"], ["subsubsection", "Hybrid Architectures"], ["paragraph", "Multi-Scale Architectures"]], "content": "}}\nThese architectures process input at multiple scales to effectively segment organs having irregular shapes and different sizes. Here, we highlight various attempts to integrate the multi-scale architectures for medical image segmentation. We further group these approaches into 2D and 3D segmentation categories based on the input image type.\n{{\\textbf{2D Segmentation}.}}\nMost ViT-based multi-organ segmentation approaches struggle to capture information at multiple scales as they partition the input image into fixed-size patches, thereby losing useful information. To address this issue, Zhang \\textit{et. al.}  propose a pyramid medical transformer, PMTrans, that leverage multi-resolution attention to capture correlation at different image scales using a pyramidal architecture~. PMTrans works on multi-resolution images via an adaptive partitioning scheme of patches to access different receptive fields without changing the overall complexity of self-attention computation. Extensive experiments on three medical imaging datasets of GLAS , MoNuSeg~, and HECKTOR  show the effectiveness of exploiting multi-scale information.\nIn other work, Ji \\textit{et al.}  propose a Multi-Compound transformer (MCTrans) that learns not only feature consistency of the same semantic categories but also capture correlation among different semantic categories for accurate segmentation~. Specifically, MCTrans captures cross-scale contextual dependencies via the Transformer self-attention module and learned semantic correspondence among different categories via Transformer Cross-Attention module. An auxiliary loss has also been introduced to improve feature correlation of the same semantic category. Extensive experiments on six benchmark segmentation datasets demonstrate the effectiveness of the architectural components of MCTrans.\n\\textbf{3D Segmentation.}\nThe majority of multi-scale architectures have been proposed for 2D medical image segmentation. To directly handle volumetric data, Hatamizadeh \\textit{et. al.}  propose a ViT-based architecture (UNETR) for 3D medical image segmentation. UNETR consists of a pure transformer as the encoder to learn sequence representations of the input volume. The encoder is connected to a CNN-based decoder via skip connections to compute the final segmentation output.\nUNETR achieves impressive performance on BTCV  and MSD~ segmentation datasets as shown in Fig. \\ref{fig:unetr}.\nOne of the drawbacks of UNETR is its large computational complexity in processing large 3D input volumes. To mitigate this issue, Xie \\textit{et. al}  propose a computationally efficient \ndeformable self-attention module   that casts attention only to a small set using multi-scale features, as shown in Figure \\ref{fig:cotr}, to reduce the computational and spatial complexities. Experiments on BTCV~ demonstrate the effectiveness of their deformable self-attention module for 3D multi-organ segmentation. \n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6621, 6570, 6612, 6588, 6573, 6622, 6620, 722], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section synthesizes information across multiple papers to highlight how multi-scale architectures are used to improve medical image segmentation, particularly in capturing cross-scale and cross-category dependencies. It offers critical analysis by pointing out the computational limitations of some approaches (e.g., UNETR) and how they are addressed (e.g., with deformable self-attention). While it identifies patterns such as the use of hybrid CNN-Transformer designs, it stops short of offering deeper, meta-level abstraction or a novel theoretical framework."}}
{"id": "eb9820b0-a942-4bbb-a4df-a6b8bf8c9ec3", "title": "\\underline{Discussion", "level": "subsection", "subsections": [], "parent_id": "7529a8d0-3662-4f9d-9c34-1867e0f6edab", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Segmentation"], ["subsection", "\\underline{Discussion"]], "content": "}}\n\\hspace{0.75em}\\textit{From the extensive literature reviewed in this section, we note that the medical image segmentation area is heavily impacted by transformer-based models, with more than 50 publications within one year since the inception of the first ViT model~. We believe such interest is due to the availability of large medical segmentation datasets and challenge competitions associated with them in top conferences compared to other medical imaging applications. As shown in Fig.~\\ref{fig:seg_challenge}, a recent transformer-based hybrid architecture is able to achieve 13$\\%$ performance gain in terms of dice score compared to the baseline transformer model, indicating rapid progression of the field. In short, ViT-based architectures have achieved impressive results over benchmark medical datasets, competing and most of the time improving over CNN-based segmentation approaches (see Table \\ref{tab:seg_taxonomy} for details). Below, we briefly describe some of the challenges associated with ViTs based medical segmentation methods and give possible solutions based on insights from the relevant papers discussed.}\n\\\\\n\\vspace{0.01em}\n\\textit{As mentioned before, the high computational cost associated with extracting features at multiple levels hinders the applicability of multi-scale architectures in medical segmentation tasks. These multi-scale architectures exploit processing input image information at multiple levels and achieve superior performance than single-scale architectures. Therefore, designing efficient transformer architectures for multi-scale processing requires more attention.\n}\n\\\\\n\\vspace{0.01em}\n\\textit{ Most of the proposed ViT-based models are pre-trained on the ImageNet dataset for the downstream task of medical image segmentation. This approach is sub-optimal due to the large domain gap between natural and medical image modalities. Recently, few attempts have been made to investigate the impact of self-supervised pre-training on medical imaging datasets on the ViTs segmentation performance. However, these works have shown that ViT pre-trained on one modality (CT) gives unsatisfactory performance when applied directly to other medical imaging modalities (MRI) due to the large domain gap making it an exciting avenue to explore. We defer detailed discussion related to pre-training ViTs for downstream medical imaging tasks to Sec.~\\ref{sec:pretraining}.\n} \\\\\n\\vspace{0.01em}\n\\textit{ Moreover, recent ViT-based approaches mainly focus on 2D medical image segmentation. Designing customized architectural components by incorporating temporal information for efficient high-resolution and high-dimensional segmentation of volumetric images has not been extensively explored. Recently, few efforts have been made, e.g.,  UNETR~ uses Swin Transformer~ based architectures to avoid quadratic computing complexity; however, it requires further attention from the community. }\n\\\\\n\\vspace{0.01em}\n\\textit{In addition to focusing on the scale of datasets,  \nwith the advent of ViTs, we note there is a need to collect more diverse and challenging medical imaging datasets. Although diverse and challenging  datasets are also crucial to gauge the performance of ViTs in other medical imaging applications, they are particularly relevant for medical image segmentation due to a major influx of ViT-based models in this area. We believe these datasets will play a decisive role in exploring the limits of ViTs for medical image segmentation.}\n\\end{tcolorbox}", "cites": [732, 1501, 6570], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key trends and challenges from the literature, connecting ideas across papers to form a coherent narrative on the current state of ViT-based medical segmentation. It critically examines issues such as computational costs, domain gaps, and limitations in 3D segmentation, while also abstracting these into broader research directions. Although it refers to a separate section for deeper pre-training analysis, it provides valuable insights on the impact of data diversity and architectural design."}}
{"id": "6b091b98-1144-4177-ab5e-bad7c86d0f66", "title": "Black-Box Models", "level": "subsubsection", "subsections": [], "parent_id": "9df4d58a-7dca-49d8-9676-95f0034930a0", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Classification"], ["subsection", "COVID-19 Diagnosis"], ["subsubsection", "Black-Box Models"]], "content": "}\nViT-based Black-box models for COVID-19 imaging classification generally focus on improving accuracy by designing novel and efficient ViT architectures. However, these models are not easily interpretable, making it challenging to gain user-trust.   We have further sub-categorized black-box models into 2D and 3D categories, depending on the input image type. Below, we briefly describe these approaches: \n {\\underline{\\textbf{2D:}}} The High computational cost of ViTs hinders their deployment on portable devices, thereby limiting their applicability in real-time COVID-19 diagnosis. Perera \\textit{et al.} propose a lightweight \\textbf{Point-of-Care Transformer (POCFormer)} to diagnose COVID-19 from lungs images captured via portable devices. Specifically, POCFormer leverages Linformer  to reduce the space and time complexity of self-attention from quadratic to linear. POCFormer has two million parameters that are about half of MobileNetv2~, thus making it suitable for real-time diagnosis. Experiments on COVID-19 lungs POCUS dataset  demonstrate the effectiveness of their proposed architecture with above 90$\\%$ classification accuracy. \nIn other work, Liu \\textit{et al.}  proposed ViT-based model for COVID-19 diagnosis by exploiting a new attention mechanism named Vision Outlooker (VOLO) . VOLO is effective for \\textbf{encoding fine-level features} into ViT token representation, thereby improving classification performance.\nFurther, they leverage the transfer learning approach to handle the issue of insufficient and generally unbalanced COVID-19 datasets. Experiments on two publicly available COVID-19 CXR datasets  demonstrate the effectiveness of their architecture. \nSimilarly, Jiang \\textit{et al.}~ leverage \\textbf{Swin Transformer~ and Transformer-in-Transformer } to classify COVID-19 images from Pneumonia and normal images. To further boost the accuracy, they employ model ensembling using a weighted average. Research progress in ViT-based COVID-19 diagnosis approaches is heavily impeded due to the requirement of a large amount of labeled COVID-19 data, thereby demanding collaborations among hospitals. This collaboration is difficult due to limited consent by patients, privacy concerns, and ethical data usage . To mitigate this issue, Park \\textit{et al.}  proposed a \\textbf{Federated Split Task-Agnostic (FESTA) framework that leveraged the merits of Federated and Split Learning}~ in utilizing ViT to simultaneously process multiple chest X-ray tasks, including the diagnosis in COVID-19 Chest X-ray images on a massive decentralized dataset. Specifically, they split ViT into the shared transformer body and task-specific heads and demonstrate the suitability of ViT body to be shared across relevant tasks by leveraging multitask-learning (MTL)  strategy as shown in Fig. \\ref{fig:festa}. They affirm the suitability of ViTs for collaborative learning in medical imaging applications via extensive experiments on the CXR dataset. \n {\\underline{\\textbf{3D:}}}\nMost of the ViT-based approaches for COVID-19 classification operate on 2D information only. However, as suggested by Kwee \\textit{et al.} , the symptoms of COVID-19 might be present at different depths (slices) for different patients. To exploit both 2D and 3D information, Hsu \\textit{et al.}  propose a hybrid network consisting of transformers and CNNs. Specifically, they determine the importance of slices based on significant symptoms in the CT scan via Wilcoxon signed-rank test  with Swin Transformer~ as backbone network. To further exploit the intrinsic features in the spatial and temporal dimensions, they propose a Convolutional CT Scan Aware Transformer module to fully capture the context of the 3D scans. Extensive experiments on the COVID-19-CT dataset show the effectiveness of their proposed architectural components. \nSimilarly, Zhang \\textit{et al.}  also proposed Swin Transformer based two-stage framework for the diagnosis of COVID-19 in the 3D CT scan dataset . Specifically, their framework consists of UNet based lung segmentation model followed by the image classification with Swin Transformer~ backbone. \n\\iffalse\n\\begin{table}[]\n\\centering\n\\caption{Description of COVID-19 datasets used by transformers based models to evaluate results.}\n\\resizebox{0.4\\textwidth}{!}{\n\\begin{tabular}{ccc}\n\\hline\n\\multicolumn{1}{|c|}{}                     & \\multicolumn{1}{c|}{No of images} & \\multicolumn{1}{c|}{} \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{3}{*}{CXR}} & \\multicolumn{1}{c|}{}             & \\multicolumn{1}{c|}{} \\\\ \\cline{2-3} \n\\multicolumn{1}{|c|}{}                     & \\multicolumn{1}{c|}{}             & \\multicolumn{1}{c|}{} \\\\ \\cline{2-3} \n\\multicolumn{1}{|c|}{}                     & \\multicolumn{1}{c|}{}             & \\multicolumn{1}{c|}{} \\\\ \\hline\n\\multicolumn{1}{|c|}{Ultrasound}           & \\multicolumn{1}{c|}{}             & \\multicolumn{1}{c|}{} \\\\ \\hline\n\\multicolumn{1}{|c|}{CT}                   & \\multicolumn{1}{c|}{}             & \\multicolumn{1}{c|}{} \\\\ \\hline\n\\multicolumn{1}{l}{}                       & \\multicolumn{1}{l}{}              & \\multicolumn{1}{l}{} \n\\end{tabular}\n}\n\\end{table}\n\\fi\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.8\\textwidth]{figures/fed_neurips.png}\n\\caption{Implementation details of Federated Split Task-Agnostic (FESTA) framework  equipped with a Transformer to simultaneously process multiple chest X-ray tasks including diagnosis of COVID-19. (a) Experimental setting for multi-task learning of classification, segmentation, and detection of chest X-ray images. The clients only train the head ($\\mathcal{H}$) and tail ($\\mathcal{T}$) parts of the network, whereas the transformer body ($\\mathcal{B}$) is shared across multiple clients. In the second step, the embedded features in the head are utilized by transformers for processing of individual tasks. (b) shows training scheme for single task. (c) shows training scheme for multi-task learning. Image courtesy .}\n\\label{fig:festa}\n\\end{figure*}", "cites": [7367, 4788, 6628, 6623, 7333, 671, 1501, 842, 8258, 6624, 6627, 6000, 6625, 6626], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers into a coherent narrative on 2D and 3D ViT-based black-box models for COVID-19 classification, highlighting architectural innovations and the challenges of data privacy. It shows some critical analysis by pointing out issues like the high computational cost of ViTs and the need for labeled data. It identifies broader patterns, such as the use of attention mechanisms and the need for collaborative learning frameworks, but does not reach the level of abstract, meta-level insights."}}
{"id": "8cc2805e-6814-457b-8fae-e19db603a95f", "title": "Interpretable Models", "level": "subsubsection", "subsections": [], "parent_id": "9df4d58a-7dca-49d8-9676-95f0034930a0", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Classification"], ["subsection", "COVID-19 Diagnosis"], ["subsubsection", "Interpretable Models"]], "content": "}\nInterpretable models aim to show the features that influence the decision of a model the most, generally via visualization techniques like saliency-based methods, Grad-CAM, etc. Due to their interpretable nature, these models are well suited to gain the trust of physicians and patients and therefore have paved their way for clinical deployment. We have further divided interpretable models into saliency-based~ and Grad-CAM~ based visualization approaches.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width = 0.49\\textwidth]{interpretable.png}\n\\caption{ CT scans (a) and X-ray (b) images along with\ntheir ground truth labels (left) and saliency maps (right). For figure (a),  xViTCOS-CT localized suspicious lesion regions exhibiting ground glass opacities, consolidation, reticulations in bilateral postero basal lung. xViTCOS-CT~ is able predict these regions correctly. For figure (b), radiologist’s interpretation is: \\textit{thick walled cavity in right middle zone with surrounding consolidation}. As shown in last column, xViTCOS-CXR~ is able predict it correctly. Figure courtesy .}\n\\label{fig:saliency}\n\\end{figure}\n {{\\textbf{Saliency Based Visualization.}}} Park \\textit{et al.} , propose a ViT-based method for COVID-19 diagnosis by exploiting the low-level CXR features extracted from the pre-trained backbone network. The backbone network has been trained in a self-supervised manner (using contrastive-learning based SimCLR  method) to extract abnormal CXR features embeddings from large and well-curated CXR dataset of CheXpert . These feature embeddings have been leveraged by ViT model for high-level diagnosis of COVID-19 images. Extensive experiments on three CXR test datasets acquired from  different hospitals demonstrate the superiority of their approach compared to CNN-based models. They also validated the generalization ability of their proposed approach and adopted saliency map visualizations  to provide interpretable results.\nSimilarly, Gao \\textit{et al.}  propose COVID-ViT  to classify COVID from non-COVID images as part of the MIA-COVID19 challenge . Their experiments on 3D CT lungs images demonstrated the superiority of ViT-based approach over DenseNet  baseline in terms of F1 score.\nIn another work, Mondal \\textit{et al.}  introduce xViTCOS for COVID-19 screening from lungs CT and X-ray images. Specifically, they pre-train xViTCOS on ImageNet to learn generic image representations and fine-tune the pre-trained model on a large chest radiographic dataset. Further, xViTCOS leverage the explainability-driven saliency-based approach  with clinically interpretable visualizations to highlight the role of critical factors in the resulting predictions, as shown in Figure \\ref{fig:saliency}. Experiments on COVID CT-2A  and their privately collected Chest X-ray dataset demonstrate the effectiveness of xViTCOS.\n {{\\textbf{Grad-CAM Based Visualization.}}} Shome \\textit{et al.}  propose a ViT-based model to diagnose COVID-19 infection at scale. They combine several open-source COVID-19 CXR datasets to form a large-scale multi-class and binary classification dataset. For better visual representation and model interpretability, they further create Grad-CAM based visualization~.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.9\\textwidth]{transmil.PNG}\n\\caption{Overview of Transformer based Multiple Instance Learning (TransMIL) architecture  for whole slide brain tumor classification. Patches of WSI are embedded in the feature space of ResNet-50. The sequence of embedded features are then processed by their proposed pipeline that include: squaring of sequence, Correlation modelling of the sequence, conditional position encoding (via Pyramid Position Encoding Generator (PPEG) module)\nand local information fusion, feature aggregation, and mapping from transformer space to label space. Image taken from .}\n\\label{fig:transmil}\n\\end{figure*}", "cites": [96, 6627, 6233, 4849, 5069, 6630, 7000, 6629], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of papers focusing on interpretable Transformer models for COVID-19 diagnosis, grouping them by visualization techniques. However, it primarily describes individual works with limited critical evaluation or comparison of their methodologies and results. There is minimal abstraction or identification of broader trends or principles in the field of interpretable models."}}
{"id": "089a60c3-f545-4c9b-8511-97df2eed76cd", "title": "Tumor Classification", "level": "subsection", "subsections": [], "parent_id": "082afea7-6a1c-4020-8543-28cc0f715e1b", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Classification"], ["subsection", "Tumor Classification"]], "content": "}\nA tumor is an abnormal growth of body tissues and can be cancerous (malignant) or noncancerous (benign). Early-stage malignant tumor diagnosis is crucial for subsequent treatment planning and can greatly improve the patient's survival rate. In this section, we review\nViT-based models for tumor classification. These models can be mainly categorized into \\textit{Black-box models} and \\textit{Interpretable models}. We highlight the relevant anatomies in bold.\n {{\\textbf{Black-Box Models.}}}\nTransMed~ is the first work that leverages ViTs for medical image classification. It is a hybrid CNN and transformer-based architecture that is capable of classifying \\textbf{parotid} tumors in the multi-modal MRI medical images. \nTransMed also employs a novel image fusion strategy to effectively capture mutual information from images of different modalities, thereby achieving competitive results on their privately collected parotid tumor classification dataset. \nLater, Lu \\textit{et al.}~ propose a two-stage framework that first performs contrastive pre-training on glioma sub-type classification in the \\textbf{brain} followed by the feature aggregation via proposed transformer-based sparse attention module. Ablation studies on TCGA-NSCLC~ dataset show the effectiveness of their two-stage framework.\nFor the task of \\textbf{breat} cancer classification, Gheflati \\textit{et al.}~ systematically evaluate the performance of pure and hybrid pre-trained ViT models. Experiments on two breast ultrasound datasets provided by Al-Dhabyani \\textit{et al.}  and Yap \\textit{et al.}  shows that Vit-based models provide better results than those of the CNNs for classifying images into benign, malignant, and normal categories.\nSimilarly, other works employ hybrid Transformer-CNN architectures to solve medical classification problem for different organs. For instance,  Khan \\textit{et al.}~ propose Gene-Transformer to predict the \\textbf{lung} cancer subtypes. Experiments on TCGA-NSCLC~ dataset demonstrates the superiority of Gene Transformer over CNN baselines. Chen \\textit{et al.}~ present a multi-scale GasHis-Transformer to diagnose gastric cancer in the \\textbf{stomach}. \nJiang \\textit{et al.}~ propose a hybrid model \nto diagnose acute lymphocytic leukemia by using symmetric cross-entropy loss function.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[trim={0cm 0cm 2cm 0.1cm},clip,width = 0.49\\textwidth]{interpretable_.png}\n\\caption{ Left: The area within the blue region is the cancer region. Middle: Attention scores from TransMIL are visualised as a heatmap (red for tumor and blue for normal) to interpret the important morphology used for diagnosis. Right: Zoomed-in view of the black square in the middle figure. Figure courtesy .}\n\\label{fig:saliency_trans}\n\\end{figure}\n {{\\textbf{Interpretable Models.}}} Since the annotation procedure is expensive and laborious, one label is assigned to a set of instances (bag) in whole slide imaging (WSI) based pathology diagnosis. This type of weakly supervised learning is known as Multiple Instance Learning~, where a bag is labeled positive if at least one instance is positive or labeled negative when all instances in a bag are negative. Most of the current MIL methods assume that the instances in each bag are independent and identically distributed, thereby neglecting the correlation among different instances. Shao \\textit{et al.}~ present TransMIL to explore both morphological and spatial information in weakly supervised WSI classification. Specifically, TransMIL aggregates morphological information with two transformer-based modules and a position encoding layer as shown in Fig. \\ref{fig:transmil}. To encode spatial information, a pyramid position encoding generator is proposed. Further, the attention scores from the TransMIL have been visualized to demonstrate interpretability, as shown in Fig. \\ref{fig:saliency_trans}. TransMIL shows state-of-the-art performance on three different computational pathology datasets CAMELYON16 \\textbf{(breast)}~, TCGA-NSCLC \\textbf{(lung)}~, and TCGA-R \\textbf{(kidney)}~. \nTo diagnose \\textbf{lung} tumors, Zheng \\textit{et al.}~ propose graph transformer network (GTN) to leverage the graph-based representation of WSI. GTN consists of a graph convolutional layer~, a transformer layer, and a pooling layer. GTN further employs GraphCAM~ to identify regions that are highly associated with the class label. Extensive evaluations on TCGA dataset~ show the effectiveness of GTN.\n\\begin{table*}[]\n\\centering\n\\resizebox{0.95\\textwidth}{!}{\n\\begin{tabular}{V{3}l|l|p{2cm}|l|p{2.5cm}|p{2cm}|l|p{7cm}V{3}}\n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{Method}                                           & \\textbf{Organ} & \\textbf{Modality}                                                & \\textbf{Type}             & \\textbf{Datasets}                                                                        & \\textbf{Metrics}                                                                                            & \\textbf{Arch.} & \\textbf{Highlights}                                                                                                                                                                                                 \\\\ \\hlineB{2}\nTransMed                                                   & Ear            & MRI (T1,T2)                                                      & 3D                       &  MRI (private)                                                                          &  Accuracy \\newline Precision                                 & Pure           & First ViT-based multi-modal medical image classification approach with novel multi-modal fusion strategy                                                                                            \\\\ \\hline\n {\\color{teal}TransMIL}~                                                  & Multi-organ    & Pathology                                                        & 2D                        & Camelyon16~ \\newline TCGA-NSCLC~ \\newline TCGA-RCC~         & Accuracy \\newline AuC                  & Hybrid         &  Transformer based architecture to explore morphological and spatial information for Whole Slide Image  classification.                                        \\\\ \\hline\nMatsokus \\textit{et al.} & Multi-organ    & Mammograms \\newline Dermoscopy & 2D                        & APTOS-2019~ \\newline ISIC-2019~ \\newline CBIS-DDSM~         & Recall \\newline AuC                                               & Pure           & Systemetic study of whether one should replace CNNs with ViTs for medical image classification. \\\\ \\hline\nGheflati \\textit{et al.} & Breast         & Ultrasound        & 2D                        &  BUSY~ \\newline Yap \\textit{et al.}~                             & Accuracy \\newline AuC                                                 & Pure           & First application of ViTs to ultrasound images classification.    \n\\\\ \\hline\n{\\color{teal}GTN}~ & Lung         & Microscopy        & 2D                        &  TCGA dataset~                             & Accuracy \\newline Precision \\newline Sensitivity \\newline Specificity \\newline Recall                                                 & Hybrid          & Consists of a graph convolutional layer, a transformer module, and a pooling layer for accurate classification of WSI images.    \n\\\\ \\hline\nMIL-ViT~ & Eye         & Fundus        & 2D                        &  APTOS-2019~ \\newline  RFMiD2020~                             & Accuracy \\newline AuC, F1 \\newline Precision \\newline Recall                                                 & Pure           &  First pretrained on a large fundus image dataset and later fine-tuned\non the downstream task of the retinal disease classification.   \\\\ \\hline\nLAT~ & Eye         & Fundus        & 2D                        &  Messidor-1 ~~ \\newline  Messidor-2~ \\newline EyePACS~                             & AuC \\newline Kappa                                                & Hybrid           &  Formulate lesion discovery as a weakly supervised lesion localization problem via\na transformer decoder. Jointly solve diabetic retinopathy grading and lesion discovery. \n\\\\ \\hlineB{2}\n \\rowcolor{mygray} \\multicolumn{8}{V{3}cV{3}}{\\textbf{COVID-19}}  \\\\ \\hlineB{2} \n{\\color{teal}Park \\textit{et al.}}~                                               & Chest         & X-ray                                                            & 2D                        & CheXpert~                      &   AuC                                & Hybrid         & Leveraging a backbone network trained to find low-level abnormal CXR findings in pre-built large-scale dataset to embed feature  corpus suitable for high-level disease classification.     \\\\ \\hline\nPOCFormer~  & Chest          & Ultrasound        & 2D                        & POCUS~                                                                             & Recall, F1 \\newline Specificity \\newline Sensitivity \\newline Accuracy & Pure          & Proposed light weight transformer architecture that demonstrates the efficiency and performance improvements.\\\\ \\hline\nFESTA~  & Chest          & X-ray        & 2D                        & CheXpert~ \\newline SIIM-ACR~ \\newline  RSNA~                                                                              & Recall, F1 \\newline Specificity \\newline Sensitivity \\newline AuC & Pure          &   Utilize ViT  to  simultaneously  process  multiple  chest  X-ray  tasks, including the diagnosis in COVID-19 Chest X-ray images on a massive decentralized dataset.\\\\ \\hline\nLiu \\textit{et al.}~                                                & Chest          & X-ray                                                            & \\multicolumn{1}{l|}{2D}                        & COVID-19-1~ \\newline COVID-19-2~                      &  Accuracy                                                                                                 & Pure           & Explore VOLO tailored with transfer learning technique to effectively encodes fine-level features into the token representations.                                                            \\\\ \\hline\nCOVID-VIT~                                                & Chest          & CT                                                               & 3D & MIA-COV19~                                                                              & Accuracy, F1                                            & Pure           & Propose ViT based architecture to classifiy COVID-19 CT images in MIA-COV19 competition.                                                                                                     \\\\ \\hline\n{\\color{teal}xViTCOS}~                                                   & Chest          & X-ray, CT                                                        &  2D                        & xViTCOS-CT~ \\newline xViTCOS-CXR~                     & Precision \\newline Recall, F1 \\newline Specificity,  NPV        & Pure           & Propose ViT based multi-stage transfer learning technique to address the issue of COVID-19 data scarcity. The approach is clinically interpretable.                                         \\\\ \\hline\nHsu \\textit{et al.}~                                                & Chest          & CT                                                              & 3D                        & COV19 CT DB~                      & Accuracy \\newline Recall,F1 \\newline Precision                 & Hybrid        & Importance of slices are determined in CT scan via Wilcoxon signed-rank test. Then, spatial and temporal features are exploited via proposed convolutional CT scan Aware Transformer module. \\\\ \\hline\nZhang \\textit{et al.}                                              & Chest          & CT           & 3D                        & MIA-COV19~                                              & F1 score                                                 & Hybrid         & Swin Transformer based two stage framework for diagnosis of COVID-19 in 3D CT scans.                                                                                                          \\\\ \\hline\nCOViT-GAN~     & Chest          & CT          & 2D                          & COVID-CT~ \\newline Sars-CoV-2~                                & Accuracy \\newline Sensitivity \\newline Precision, F1                                                                                                             & Hybrid               &  Generate synthetic images using a self-attention generative adversarial network and use it as a data augmentation method to alleviate the problem of limited data and improve performance.\\\\ \\hline\n{\\color{teal}COVID-Trans.}        & Chest          & X-ray     & 2D                        & El-Shafai \\textit{et al.}~ \\newline Sait \\textit{et al.}~ \\newline Qi \\textit{et al.}~ & Accuracy \\newline Precision \\newline AuC, Recall, F1             & Pure           & Propose a ViT model to diagnose COVID-19 at scale. Combines several open source COVID-19 chest X-ray datasets to form large dataset  for binary and multi-class classification.        \\\\ \\hlineB{3}\n\\end{tabular}\n}\n\\caption{An overview of ViT-based approaches for medical image classification. {\\color{teal} Teal color} indicates that the model is interpretable.}\n\\label{tab:class_taxonomy}\n\\end{table*}\n\\begin{table}[ht!]\n\\centering\n{\\renewcommand{\\arraystretch}{1.5}\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{V{3}llccV{3}}\n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{Initialization} & \\textbf{Model} & \\textbf{APTOS2019}, $\\kappa \\uparrow$ & \\textbf{ISIC2019}, Recall $\\uparrow$ \n\\\\ \\hline\n & ResNet50 & 0.849 $\\pm$ 0.022 & 0.662 $\\pm$ 0.018 \n\\\\ \n\\multirow{-2}{*}{Random} & DeiT-S   & 0.687 $\\pm$ 0.017 & 0.579 $\\pm$ 0.028 \n\\\\ \\hline\n & ResNet50 & 0.893 $\\pm$ 0.004 & 0.810 $\\pm$ 0.008 \n\\\\ \n\\multirow{-2}{*}{ImageNet (supervised)} & DeiT-S   & 0.896 $\\pm$ 0.005 & 0.844 $\\pm$ 0.021 \n\\\\ \\hline\n& ResNet50 & 0.894 $\\pm$ 0.008 & 0.833 $\\pm$ 0.007 \n\\\\\n\\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}ImageNet (supervised) + \\\\ Self-supervised with DINO \\end{tabular}} & DeiT-S   & 0.896 $\\pm$ 0.010 & 0.853 $\\pm$ 0.009 \n\\\\ \\hlineB{3}\n\\end{tabular}\n}}\n\\caption{Comparison of vanilla CNNs vs.~ViTs with different initialization strategies on \\textit{medical imaging} classification tasks. For APTOS 2019~ and ISIC 2019~ datasets quadratic Cohen Kappa and recall score have been reported.\n\\textit{First row}: For randomly initialized networks, CNNs outperform ViTs. \\textit{Second row}: ViTs appear to benefit significantly from pre-training on ImageNet dataset. \\textit{Third row}: Both ViTs and CNNs perform better with self-supervised pretraining. Table taken from .}\n\\label{tab:finetuned}\n\\end{table}", "cites": [2514, 8259, 6628, 4849, 6571, 6632, 6629, 8258, 6624, 6627, 6631, 6625, 6626, 6630, 5069], "cite_extract_rate": 0.29411764705882354, "origin_cites_number": 51, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of ViT-based models for tumor classification, organizing them into black-box and interpretable categories. It integrates some common themes, such as hybrid CNN-Transformer architectures and the use of attention mechanisms, but lacks deeper comparative analysis or evaluation of limitations. The generalization is limited, focusing primarily on individual system descriptions without elevating the discussion to a meta-level."}}
{"id": "352a9232-7b10-42ec-a295-f6e42ea01ff5", "title": "Retinal Disease Classification", "level": "subsection", "subsections": [], "parent_id": "082afea7-6a1c-4020-8543-28cc0f715e1b", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Classification"], ["subsection", "Retinal Disease Classification"]], "content": "}\nYu \\textit{et al}~ propose MIL-ViT model which is first pre-trained on a large fundus image dataset and later fine-tuned on the downstream task of the retinal disease classification. MIL-ViT architecture uses MIL-based head that can be used with ViT in a plug-and-play manner. Evaluation performed on APTOS2019~ and RFMiD2020~ datasets shows that MIL-ViT is achieving more favorable performance than CNN-based baselines.\nMost data-driven approaches treat diabetic retinopathy (DR) grading and lesion discovery as two separate tasks, which may be sub-optimal as the error may propagate from one stage to the other. To jointly handle both these tasks, Sun \\textit{et al.}~ propose lesion aware transformer (LAT) that consists of a pixel relation based encoder and a lesion-aware transformer decoder. In particular, they leverage transformer decoder to formulate lesion discovery as a weakly supervised lesion localization problem. LAT model sets state-of-the-art on Messidor-1~, Messidor-2~, and EyePACS~ datasets.\nYang \\textit{et al.}~ propose a hybrid architecture consisting of convolutional and Transformer layers for fundus disease classification on OIA dataset~. Similarly, Wu \\textit{et al.} and Aldahou \\textit{et al.} ~ also verify that ViT models are more accurate in DR grading than their CNNs counterparts. \n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6632], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes individual papers and their contributions without showing strong synthesis or critical evaluation. It mentions the models and datasets used but does not connect the ideas across works or analyze their broader implications. The content remains at a factual level with minimal abstraction or comparative insight."}}
{"id": "1843af91-8170-49d7-9972-4dada814bb66", "title": "\\underline{Discussion", "level": "subsection", "subsections": [], "parent_id": "082afea7-6a1c-4020-8543-28cc0f715e1b", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Classification"], ["subsection", "\\underline{Discussion"]], "content": "}}\n\\hspace{0.75em}\\textit{ In this section, we provide a comprehensive overview of about 25 papers related to applications of ViTs in medical image classification. In particular, we see a surge of Transformer-based architectures for diagnosing COVID-19, compelling us to develop taxonomy accordingly. Below, we briefly highlight some of the challenges associated with this area, identify recent trends, and provide future directions worthy of further exploration.}\n \\\\\n\\vspace{0.01em}\n\\textit{ \nThe lack of large COVID-19 datasets hindered the applicability of ViT models to diagnose COVID-19. A recent work by Shome \\textit{et al.}~ attempts to mitigate this issue by combining three open-source COVID-19 datasets to create a large dataset comprising 30,000 images. Still, creating diverse and large COVID-19 datasets is challenging and requires significant effort from the medical community.}\n\\\\\n\\vspace{0.01em}\n\\textit{ \nMore attention must be given to design \\textbf{interpretabile} (to gain end-users trust) and \\textbf{efficient} (for point-of-care testing) ViT models for COVID-19 diagnosis to make them a viable alternative of RT-PCR testing in the future. \n} \n\\\\\n\\vspace{0.01em}\n\\textit{ We notice that most works have used the original ViT model~ as a plug-and-play manner to boost the medical image classification performance. In this regard, we believe that integrating domain-specific context and accordingly designing architectural components and loss functions can enhance performance and provide more insights in designing effective ViT-based classification models in the future.}\n\\\\\n\\vspace{0.01em}\n\\textit{ Finally, let us highlight the exciting work of Matsoukas \\textit{et al.}~\nthat, for the first time, demonstrates that ViTs pre-trained on ImageNet perform comparably to CNNs for the medical image classification task as shown in Table \\ref{tab:finetuned}. This also raises an interesting question ``\\textbf{Can ViT models pre-trained on medical imaging datasets perform better than ViT models pre-trained on ImageNet for medical image classification?}.\"\nA recent work by Xie \\textit{et al.}~ attempts to answer this by pre-training the ViT on large-scale 2D and 3D medical images. On the medical image classification problem, their model obtains substantial performance gain over the ViT model pre-trained on ImageNet, indicating that this area is worth exploring further. A brief overview of ViT-based medical image classification approaches has been provided in Table \\ref{tab:class_taxonomy}. }\n\\end{tcolorbox}", "cites": [732, 6571], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key findings from multiple papers to highlight trends and challenges in ViT-based medical image classification, particularly for COVID-19. It critically evaluates issues like dataset limitations and model efficiency, and abstracts these into broader implications such as the potential of domain-specific pre-training. The narrative is cohesive and goes beyond mere description to offer meaningful insights and future directions."}}
{"id": "7d3be9ac-6c8d-479d-a13a-267c578b0321", "title": "Medical Image Detection", "level": "section", "subsections": ["13d2627e-6eb0-4ce1-9861-4f4ea6f1654a"], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Detection"]], "content": "\\label{sec:det}\nIn medical image analysis, object detection refers to localization and identification of a region of interest (ROIs) such as lung nodules from X-ray images and is typically an essential aspect of diagnosis. However, it is one of the most time-consuming tasks for clinicians, thereby demanding the accurate computer-aided diagnosis (CAD) system to act as a second observer that may accelerate the process. Following the success of CNNs in medical image detection~, recently few attempts have been made to improve performance further using Transformer models. These approaches are mainly based on the detection transformer (DETR) framework~.\nShen \\textit{et al.}  propose the first hybrid framework COTR, consisting of convolutional and transformer layers for end-to-end polyp detection. Specifically, the encoder of COTR contains six hybrid convolution-in-transformer layers to encode features. Whereas, the decoder consists of six transformer layers for object querying followed by a feed-forward network for object detection. COTR performs better than DETR on two different datasets ETIS-LARIB and CVC-ColonDB. The DETR model~ is also adapted in other works~ for the end-to-end polyp detection~, and detecting lymph nodes in T2 MRI scans for the assessment of lymphoproliferative diseases~. \n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6633, 7373, 6618, 6634, 6635], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of Transformer-based medical image detection with some mention of specific papers, but it lacks deeper synthesis of ideas or trends. There is minimal comparison or evaluation of the cited approaches, and no clear abstraction to broader principles or frameworks in the field."}}
{"id": "13d2627e-6eb0-4ce1-9861-4f4ea6f1654a", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "7d3be9ac-6c8d-479d-a13a-267c578b0321", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Detection"], ["subsection", "Discussion"]], "content": "}\n \\hspace{0.75em}\\textit{Overall, the frequency of new Transformer-based approaches for the problem of medical image detection is lesser than those for the segmentation and classification. This is in contrast to the early years of CNN-based designs that were rapidly developed for the medical image detection, as indicated in Fig. \\ref{fig:vit_vs_cnn}. A recent work~ shows that generic class-agnostic detection mechanism of multi-modal ViTs (like MDETR~) pre-trained on natural images-text pairs performs poorly on medical datasets. Therefore, investigating the performance of multi-modal ViTs by pre-training them on modality-specific medical imaging datasets is a promising future direction to explore. Furthermore, since the recent ViT-based methods yield competitive results on medical image detection problems, we expect to see more contributions in the near future.\n}\n\\end{tcolorbox}", "cites": [8000], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides some analytical insight by noting a trend in the slower development of Transformer-based methods for medical image detection compared to segmentation and classification. It critically mentions the underperformance of multi-modal ViTs like MDETR on medical datasets and suggests future directions, showing a moderate level of synthesis and abstraction by connecting the cited work to broader challenges and opportunities in the field."}}
{"id": "c16d32da-40ac-41e8-983d-8938919a9332", "title": "LDCT Enhancement", "level": "subsubsection", "subsections": [], "parent_id": "7be668a9-7353-4deb-9897-1447df4426dd", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Reconstruction"], ["subsection", "Medical Image Enhancement"], ["subsubsection", "LDCT Enhancement"]], "content": "}\nZhang \\textit{et al.}~ propose \nan hybrid architecture TransCT that leverages the internal similarity of the LDCT images to effectively enhance them.\nTransCT first decomposes the LDCT image into high-frequency (HF) (containing noise) and low-frequency (LF) parts. Next it removes the noise from the HF part with the assistance of latent textures. To reconstruct the final high-quality LDCT images, TransCT further integrates features from the LF part to the output of the transformer decoder. Experiments on Mayo LDCT dataset  demonstrate the effectiveness of TransCT over CNN-based approaches.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.95\\textwidth]{mri_zeroshot.PNG}\n\\caption{Unsupervised under-sampled MRI reconstruction results of CNN based and transformer based approaches. From left to right (top row): Fourier method (ZF) , GAN$_{\\text{DIP}}$ (unsupervised CNN), GAN$_{\\text{Prior}}$  (unsupervised CNN with pre-training), Self-Attention GAN$_{\\text{DIP}}$  (unsupervised CNN), Self-Attention GAN  (unsupervised CNN with pre-training), SLATER$_{\\text{DIP}}$  (unsupervised transformer), SLATER  (unsupervised transformer with pre-training), and reference image. Bottom row shows corresponding error maps. It can be seen that SLATTER outperforms all other approaches in term of quality of reconstruction. Results taken from . }\n\\label{fig:slater}\n\\end{figure*}\nTo perform LDCT image enhancement, Wang \\textit{et al.}~ propose a convolution-free ViT-based encoder-decoder architecture TED-Net. It employs Token-to-token block  to enrich the image tokenization via a cascaded process. To refine contextual information, TED-Net introduces dilation and cyclic-shift blocks~ in tokenization. TED-Net shows favorable performance on the Mayo Clinic LDCT dataset ~.\nIn another work, Luthra \\textit{et al.}~ propose Eformer which is Transformer-based residual learning architecture for LDCT images denoising. To focus on edges, Eformer uses the power of Sobel-Feldman operator  in the proposed edge enhancement block to boost denoising performance. Moreover, to handle the over-smoothness issue, the multi-scale perceptual loss is used. Eformer achieves impressive image quality gains in terms PSNR, SSIM, and RMSE on the AAPM-Mayo Clinic dataset~.", "cites": [4771, 524, 9052, 9136, 4782, 6637, 6636], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes three different Transformer-based architectures for LDCT enhancement without critically analyzing their strengths, weaknesses, or comparing them in depth. It integrates the cited papers minimally by listing their approaches and results, but lacks synthesis of ideas or abstraction to broader trends in the field."}}
{"id": "b6e9c019-b050-49bc-bdca-b28f31e07ae2", "title": "Undersampled MRI Reconstruction", "level": "subsubsection", "subsections": [], "parent_id": "74513791-5660-485f-9a1e-906e3e35c2f6", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Reconstruction"], ["subsection", "Medical Image Restoration"], ["subsubsection", "Undersampled MRI Reconstruction"]], "content": "} Reducing the number of MRI measurements can result in faster scan times and a reduction in artifacts due to patients movement at the expense of aliasing artifacts in the image~. \n {\\textbf{High-Data Regime Approaches.}} Approaches in this category assume the availability of large MRI training datasets to train the ViT model.\nFeng \\textit{et al.}~ propose Transformer-based architecture, MTrans, for accelerated multi-modal MR imaging.\nThe main component of MTrans is the cross attention module that extracts and fuses complementary features from the auxiliary modality to the target modality. Experiments on fastMRI and uiMRI datasets for reconstruction and super-resolution tasks show that MTrans achieve good performance gains over previous methods.\nHowever, MTrans requires separate training for MR reconstruction and super-resolution tasks. To jointly reconstruct and super-resolve MRI images, Feng \\textit{et al.}~ propose Task-Transformer that leverages the power of multi-task learning to fuse complementary information between the reconstruction branch and the super-resolution branch. Experiments are performed on the public IXI and private MRI brain datasets.\nSimilarly, Mahapatra \\textit{et al.}~ propose a hybrid architecture to super-resolve MRI images by exploiting the complementary advantages of both CNNs and ViTs. They also propose novel loss functions~ to preserve semantic and structural information\nin the super-resolved images.\n {\\textbf{Low-Data Regime Approaches.}}\nOne drawback of the aforementioned approaches is the requirement of a massive paired dataset of undersampled and corresponding fully sampled MRI acquisitions to train ViT models. To alleviate the data requirement issue, Korkmaz \\textit{et al.}  propose a zero-shot framework, SLATER, that leverages prior induced by randomly initialized neural networks  for unsupervised MR image reconstruction. Specifically, during inference, SLATER inverts its transformer-based generative model via iterative optimization over-network weights to minimize the error between the network output and the under-sampled multi-coil MRI acquisitions while satisfying the MRI forward model constraints. SLATER yields quality improvements on single and multi-coil MRI brain datasets over other unsupervised learning-based approaches as shown in Fig.~\\ref{fig:slater}.\nSimilarly, Lin \\textit{et al.}~ show that a ViT model pre-trained on ImageNet, when fine-tuned on only 100 fastMRI images, not only yields sharp reconstructions but is also more robust towards anatomy shifts compared to CNNs as shown in Fig. \\ref{fig:recon_100}. Furthermore, their experiments indicate that ViT benefits from higher throughput and less memory consumption than the U-Net baseline.\n\\begin{table*}[t]\n\\centering\n\\resizebox{0.98\\textwidth}{!}{\n\\begin{tabular}{V{3}l|p{8cm}|c|c|p{2.2cm}|cV{3}}\n\\hlineB{3}\n\\rowcolor{mygray}{\\textbf{Method}} & \\hspace{12em}\\textbf{Highlights}                                                                                                                              & \\textbf{Modality} & \\textbf{Input Type} & \\textbf{Datasets}                                         & \\textbf{Metric}                                                  \\\\ \\hline\nTransCT~                               & Transformer for LDCT enhancement with high and low frequency decomposition.                                 & CT                & 2D                  & NIH-AAPM~                                                & RMSE, SSIM, VIF~  \\\\ \\hline\nSLATER~                                & Transformer based approach for zero shot MRI  image reconstruction.                                           & MRI               & 3D                  & IXI~ \\newline fastMRI~ &  PSNR, SSIM          \\\\ \\hline\nTED-Net~                               & Pure transformer based encoder decoder dilation architecture for LDCT denoising.                            & CT                & 2D                  & NIH-AAPM~                                                 & RMSE, SSIM         \\\\ \\hline\nEformer~                               & Transformers based residual image denoising. Incorporate learnable Sobel filters for edge enhancement. & CT                & 2D                  & NIH-AAPM~                                                 & PSNR, SSIM, RMSE \\\\ \\hline\nTransformer-GAN                              & End-to-end \nGAN-based method integrated with Transformers to enhance LDPET images.                           & PET                & 3D                  & Private                                                & PSNR, SSIM, MSE         \\\\ \\hline\nMTrans~                               & Leverage cross-attention module to fuse complementary features\nfrom the auxiliary modality to the target modality for fast multi-modal MRI image reconstruction. & MRI                & 2D                  & fastMRI~ \\newline uiMRI (private)                                                & PSNR, SSIM, NMSE \\\\ \\hline\nTask-Transformer    & Simultaneously, reconstruct and super-resolve MRI images via multi-task learning.                                           & MRI               & 2D                  & IXI~ &  PSNR, SSIM, NMSE          \\\\ \\hline\nMahapatra \\textit{et al.}~                              & Hybrid architecture to super-resolve MRI\nimages by exploiting the complementary advantages of CNNs and ViTs.                         & MRI                & 2D                  & fastMRI~ \\newline IXI~                                               & PSNR, SSIM, NMSE         \\\\ \\hline\nLin \\textit{et al.}~                               & ViT pretrained on ImageNet, when fine-tuned on only 100 fastMRI images, yields sharp reconstructions and is robust towards anatomy shifts. & MRI                & 2D                  & fastMRI~                                                & SSIM \\\\ \\hline\nDuDoTrans~           &  A hybrid CNN-Transformer architecture\nthat consider the global nature of sinogram’s sampling\nprocess to restore high-quality CT images from sparse views.                             & CT                & 2D                  & NIH-AAPM~                                                & PSNR, SSIM         \\\\ \\hline\nMIST-Net~                               & Swin-transformer based projection and image domain two-stage framework to reconstruct high-quality CT images from sparse views.   & CT                & 2D                  & NIH-AAPM~                                                & PSNR, SSIM, RMSE \\\\ \\hline\nE-DSSR~                               & Leverage lightweight stereo Transformer module to\nestimate depth images with high confidence and a segmentor network to accurately predict the surgical tool’s mask.   & Endo.                & 2D                  & Hamlyn~ \\newline DaVinci (private)                                                & PSNR, SSIM \\\\ \\hline\nTranSMS~                               & ViT-based data consistancy module to super-resolve magnetic particle imaging (MPI) system matrices for accelerated calibration.   & MPI                & 2D, 3D                  & Open MPI~ \\newline  Private datasets                                                & RMSE \\\\ \\hlineB{3}\n\\end{tabular}\n}\n\\caption{An overview of ViT-based approaches for medical image reconstruction.}\n\\label{tab:recon_taxonomy}\n\\end{table*}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{recon_discussion.pdf}\n\\caption{\\textbf{Top row}: Example reconstructions of models trained on 100 images (low-data regime) of fastMRI dataset. It can be seen that ViT model pre-trained on ImageNet produce sharp results as compared to recent CNN-based model~ and randomly initialized ViT model. \\textbf{Bottom row}: Example reconstructions of Brain images by models pre-trained on ImageNet and fine-tuned on Knee MRI dataset. Results show that pre-trained ViT models are more robust to anatomical shifts. Figures adapted from .}\n\\label{fig:recon_100}\n\\end{figure}", "cites": [9052, 6637, 6641, 9057, 6643, 6639, 524, 6640, 6642, 6638, 9136], "cite_extract_rate": 0.44, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key approaches for undersampled MRI reconstruction using Transformers, connecting methods across high- and low-data regimes. It includes critical evaluation by highlighting limitations such as the need for large paired datasets and proposing solutions like zero-shot learning and pretraining. While it identifies trends like the use of cross-attention and pretraining, it stops short of offering a novel meta-framework or deep theoretical critique."}}
{"id": "a3dfad8b-d07f-4191-9f75-07d652afc0bc", "title": "Sparse-View CT Reconstruction", "level": "subsubsection", "subsections": [], "parent_id": "74513791-5660-485f-9a1e-906e3e35c2f6", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Reconstruction"], ["subsection", "Medical Image Restoration"], ["subsubsection", "Sparse-View CT Reconstruction"]], "content": "}\nSparse-view CT~ can effectively reduce the effective radiation dose by acquiring fewer projections. However, a decrease in the number of projections demands sophisticated image processing algorithms to achieve high-quality image reconstruction . Wang \\textit{et al.}  present a hybrid CNN-Transformer, named Dual-Domain Transformer (DuDoTrans), by considering the global nature of sinogram's sampling process to better restore high-quality images. In the first step, DuDoTrans reconstructs low-quality reconstructions of sinogram via filtered back projection step and learnable DuDo consistency layer.  In the second step, a residual image reconstruction module performs enhancement to yield high-quality images. Experiments are performed on the NIH-AAPM dataset~ to show generalizability, and robustness (against noise and artifacts) of DuDoTrans.", "cites": [6644], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the DuDoTrans model and its application to sparse-view CT reconstruction but lacks synthesis of ideas from the cited work. It does not compare or contrast DuDoTrans with other methods like the U-Net-based approach mentioned in the cited paper. Additionally, it offers minimal abstraction or critical analysis, merely summarizing the method and results without deeper insights or identifying broader trends."}}
{"id": "4fd246d1-0b6d-448e-b217-1f18a455a269", "title": "Endoscopic Video Reconstruction", "level": "subsubsection", "subsections": [], "parent_id": "74513791-5660-485f-9a1e-906e3e35c2f6", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Reconstruction"], ["subsection", "Medical Image Restoration"], ["subsubsection", "Endoscopic Video Reconstruction"]], "content": "}\nReconstructing surgical scenes from a stereoscopic video is challenging due to surgical tool occlusion and camera viewpoint changes. Long \\textit{et al.}~ propose E-DSSR to reconstruct surgical scenes from stereo endoscopic videos. Specifically, E-DSSR contains a lightweight stereo Transformer module to estimate depth images with high confidence and a segmentor network to accurately predict the surgical tool's mask. Extensive experiments on Hamlyn Centre Endoscopic Video Dataset~ and privately collected DaVinci robotic surgery dataset demonstrate the robustness of E-DSSR against abrupt camera movements and tissue deformations in real-time.\n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6643], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the E-DSSR method for endoscopic video reconstruction, referencing one cited paper. It lacks synthesis of ideas from multiple sources, critical evaluation of the method's strengths or weaknesses, and broader abstraction to general principles or trends in the field. The content remains focused on factual summary without deeper insight."}}
{"id": "d7884157-7ba3-42c0-a931-3be5dee05ea1", "title": " \\underline{Discussion", "level": "subsection", "subsections": [], "parent_id": "4d546787-3062-4b4d-b865-da32deeacc31", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Reconstruction"], ["subsection", " \\underline{Discussion"]], "content": "}}\n\\textit{ In this section, we have reviewed about a dozen papers related to the applications of ViT models in medical image reconstruction, as shown in Table \\ref{tab:recon_taxonomy}. Below, we highlight a few challenging problems and provide recent trends in the field.}\n \\\\\n\\vspace{0.01em}\n\\textit{ Recently, an interesting work~ has investigated the impact of pre-training ViT on the task of MRI image reconstruction. Their results indicate that pre-trained ViT yields sharp reconstructions and is robust towards anatomy shifts (see Fig.~\\ref{fig:recon_100}). The robustness of ViTs can be of particular relevance to the pathology image reconstruction as the range of pathology can vary significantly in the anatomy being imaged. Further, it raises an interesting question ``\\textbf{Are ViTs pre-trained on medical image dataset able to provide any advantages in terms of reconstruction performance and robustness against anatomy shifts compared to their counterparts pre-trained on ImageNet\"}? Extensive and systematic experiments are required to answer this question. Another promising future direction is to investigate the impact on the performance of ViT pre-trained on one image modality (like CT) and fine-tuned on another modality (like MRI) for image reconstruction tasks.  }\n \\\\\n\\vspace{0.01em}\n\\textit{ We notice that most of the Transformer-based approach focus on MRI and CT image reconstruction tasks, and their applicability to other modalities are yet to be explored.\nIn addition, proposed architectures are mostly generic and have not fully exploited the application-specific aspects. We believe that designing architectural components and formulating loss functions according to the task at hand can significantly boost performance. \n}\n \\\\\n\\vspace{0.01em}\n\\textit{ We want to highlight one particular work that uses the Transformer-layer architecture to regularize the challenging problem of MRI image reconstruction from under-sampled measurements~. This work is inspired by the strong prior induced by the structure of untrained neural networks~. These untrained network priors have recently garnered much attention from the medical image community as they do not need labeled training data. Considering advances in the untrained neural network area, we believe this direction requires further attention from medical imaging researchers in the context of Transformers.}\n \\\\\n\\vspace{0.01em}\n\\textit{ We also observe that compared to the early years of CNNs (one paper from 2012 to 2015), Transformers have rapidly gained widespread attention in the medical image reconstruction community (more than a dozen papers in 2021), potentially due to the recent advancement in image-to-image translation frameworks.}\n\\end{tcolorbox}", "cites": [9052, 524], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the reviewed papers to highlight trends such as the use of pre-trained ViTs and the focus on MRI/CT reconstruction. It identifies limitations, such as the lack of modality-specific architectural design and the under-explored potential of untrained neural network priors in Transformers. While it offers some general insights and raises relevant research questions, it stops short of offering a novel framework or deep critique of the field."}}
{"id": "2889c9d9-4bb7-458d-89d6-9d5c0fc93f9f", "title": "Medical Image Synthesis", "level": "section", "subsections": ["aee518bf-775f-44ab-9278-04ff666b9fbe", "6d04dce6-641e-45f8-96b4-00b38af6e2c8", "aaf99df9-591f-4831-aadd-d9a6eff556c9"], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Synthesis"]], "content": "\\label{sec:syn}\nIn this section, we provide an overview of the applications of ViTs in medical image synthesis. Most of these approaches incorporate adversarial loss to synthesize realistic and high-quality medical images, albeit at the expense of training instability . We have further classified these approaches into \\textit{intra-modality synthesis} and \\textit{inter-modality synthesis} due to a different set of challenges in both categories, as shown in Fig. \\ref{fig:syn_tax}.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{synthesis_taxonomy.pdf}\n\\caption{Taxonomy of ViT-based medical image synthesis approaches.}\n\\label{fig:syn_tax}\n\\end{figure}", "cites": [9058], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces ViT-based medical image synthesis and mentions the use of adversarial loss, citing one paper to explain training instability. However, it lacks deeper synthesis of ideas, critical evaluation of methods, or abstraction to broader trends. The content appears to be a minimal descriptive overview with limited analytical depth."}}
{"id": "2f0d3ec8-ac02-4338-9827-458cbba7ccaf", "title": "Supervised Methods", "level": "subsubsection", "subsections": [], "parent_id": "aee518bf-775f-44ab-9278-04ff666b9fbe", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Synthesis"], ["subsection", "Intra-Modality Approaches"], ["subsubsection", "Supervised Methods"]], "content": "}} Supervised image synthesis methods require paired source and target images to train ViT-based models. Paired data is difficult to obtain due to annotation cost and time constraints, thereby generally hindering the applicability of these models in medical imaging applications. Zhang \\textit{et al.}  focus on synthesizing infant brain structural MRIs (T1w and T2w scans) using both transformer and performer (simplified self-attention) layers . Specifically, they design a novel multi-resolution pyramid-like U-Net framework, PTNet, utilizing performer encoder, performer decoder, and transformer bottleneck to synthesize high-quality infant MRI. They demonstrate the superiority of PTNet both qualitatively and quantitatively compared to pix2pix , and pix2pixHD  on large-scale infant MRI dataset . Furthermore, in addition to better synthesis quality, PTNet has a reasonable execution time of around 30 slices per second.", "cites": [8384, 8715, 6645, 896], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual summary of PTNet, a supervised method for infant MRI synthesis, and compares its performance to pix2pix and pix2pixHD. It integrates the role of performer and transformer layers from related papers but does not deeply connect the broader implications or trends across the cited works. The critical evaluation is limited to performance and speed, without deeper analysis of methodological limitations or trade-offs."}}
{"id": "37b4b112-c8ee-4eae-9197-99f18888f3a8", "title": "Semi-Supervised Methods", "level": "subsubsection", "subsections": [], "parent_id": "aee518bf-775f-44ab-9278-04ff666b9fbe", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Synthesis"], ["subsection", "Intra-Modality Approaches"], ["subsubsection", "Semi-Supervised Methods"]], "content": "}} Semi-supervised approaches typically require small amounts of labeled data along with large unlabelled data to train models effectively. Kamran \\textit{et al.}  propose a  multi-scale conditional generative adversarial network (GAN)  using ViT as a discriminator. They train their proposed model in a semi-supervised way to simultaneously synthesize Fluorescein Angiography (FA) images from fundus photographs and predict retinal degeneration. They use softmax activation after MLP head output and a categorical CE loss for classification. Besides adversarial loss, they also use MSE and perceptual losses to train their network. For ViT discriminator, they use embedding feature loss calculated using positional and patch features from the transformer encoder layers by successfully inserting the real and synthesized FA images. Their quantitative results in terms of Frechet inception Distance  and Kernel Inception Distance  demonstrate the superiority of their approach over baseline methods on diabetic retinopathy dataset provided by Hajeb \\textit{et al.} .\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.8\\textwidth]{cytran_lguana.PNG}\n\\caption{Hybrid convolutional-transformer network for CT image generation as proposed in . It consists of down-sampling convolutional layers to extract features from input images,  a convolutional-transformer block comprising a multi-head self-attention mechanism, and an upsampling block to generate output images.}\n\\label{fig:cytran}\n\\end{figure*}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width = 0.24\\textwidth]{syn_MRI.png} \\hspace{-1em}\n\\includegraphics[trim={0cm 0cm 0cm 0.1cm},clip,width = 0.24\\textwidth]{syn_CT.png} \n\\caption{A pair of MRI (left) and CT (right) images of the same subject showing the significant appearance gap between\nthe two modalities making medical image synthesis from MRI to CT a challenging task. Image is from .}\n\\label{fig:mri_ct}\n\\end{figure}", "cites": [6648, 60, 6646, 6647, 896], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes a single paper (doc_id: 6647) in detail, focusing on its methodology and results. While it references other GAN-related papers (doc_id: 6648, 60, 6646, 896), it does not synthesize their contributions or connect them in a broader framework. The analysis remains at a surface level, lacking critical evaluation or abstraction of common themes or limitations in semi-supervised medical image synthesis methods."}}
{"id": "6d04dce6-641e-45f8-96b4-00b38af6e2c8", "title": "Inter-Modality Approaches", "level": "subsection", "subsections": [], "parent_id": "2889c9d9-4bb7-458d-89d6-9d5c0fc93f9f", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Synthesis"], ["subsection", "Inter-Modality Approaches"]], "content": "The inter-modality approaches aim to synthesize targets to capture the useful structural information in the source images of different modalities. Examples include CT to MRI translation or vice-versa. Due to challenges associated with inter-modal translation, only supervised approaches have been explored.\nDalmaz \\textit{et al.}  introduce a novel synthesis approach, ResViT, for the multi-modal imaging based on a conditional deep adversarial network with ViT-based generator. Specifically, ResViT, employs convolutional and transformer branches within a residual bottleneck to preserve both local precision and contextual sensitivity along with the realism of adversarial learning. The bottleneck comprises novel aggregated residual transformer blocks to synergistically preserve local and global context, with a weight-sharing strategy to minimize model complexity. The effectiveness of ResViT model is demonstrated on two multi-contrast brain MRI datasets, BraTS ), and a multi-modal pelvic MRI-CT dataset .\n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [9059], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the ResViT model, summarizing its architecture and application to multi-modal medical image synthesis. It lacks meaningful synthesis with other works, as only one paper is cited, and no comparative or broader contextual analysis is offered. The abstraction level is minimal, focusing on specific components of the cited paper without generalizing to broader trends or principles in the field."}}
{"id": "aaf99df9-591f-4831-aadd-d9a6eff556c9", "title": " \\underline{Discussion", "level": "subsection", "subsections": [], "parent_id": "2889c9d9-4bb7-458d-89d6-9d5c0fc93f9f", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Synthesis"], ["subsection", " \\underline{Discussion"]], "content": "}}\n \\hspace{0.75em}\\textit{ In this section, we have reviewed the applications of ViT models in medical image synthesis. Realistic synthesis of medical images is particularly important as, in general, more than one imaging modality is involved in accurate clinical decision-making due to their complementary strengths. Therefore, in many practical applications, a\ncertain modality is desired but infeasible to acquire due to cost and privacy issues. Recent transformer-based approaches can effectively circumvent these issues due to their ability to generate more realistic images than GAN-based methods.\n}\n\\\\\n\\vspace{0.01em}\n\\textit{ Furthermore, most Transformer-based medical image synthesis approaches use the adversarial loss to generate realistic images. The adversarial loss can cause mode-collapse, and effective strategies must be employed to mitigate this issue~.}\n\\\\\n\\vspace{0.01em}\n\\textit{ Lastly, to the best of our knowledge, no work has been done using transformer-based models for inter-modality image synthesis approaches in an unsupervised setting. This can be due to the highly challenging nature of the problem (like CT and MRI images of the same subject have significantly different appearances, as shown in Fig. \\ref{fig:mri_ct}), thereby making it a promising direction to explore.}\n\\end{tcolorbox}\n\\iffalse\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width = 0.8\\textwidth]{trans_reg.pdf}\n\\caption{Architecture of Dual Transform Network (DTN) proposed for Diffeomorphic registration . Features of moving image are extracted via 3D U-net encoder $\\phi_s$ while features of concatenated moving and fixed image are extracted via another 3D U-Net encoder $\\phi_c$. These features are collapsed into sequences and passed to two transformers networks, $T_s$ and $T_c$, to handle the cross-image global relevance learning on separate single channel images and the image concatenation. The resulting features are concatenated together and passed to the CNN decoder ($\\phi_r$) to infer the velocity field ($v$), and subsequently registration field ($\\psi$).}\n\\label{fig:diff_reg}\n\\end{figure*}\n\\fi", "cites": [5906], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Transformer-based approaches in medical image synthesis, highlighting their advantages over GANs and noting the use of adversarial loss. It integrates the broader context from the cited survey on GANs and points out a specific research gap in unsupervised inter-modality synthesis. While it offers some critical evaluation and identifies limitations, the synthesis remains limited to a few high-level observations without deeper cross-paper integration or abstraction into broader principles."}}
{"id": "86c6ba50-a1d9-47f4-b7ba-6ac3b736504e", "title": "Medical Image Registration", "level": "section", "subsections": ["bac81f01-75b4-4c95-ae21-ffb0b5b5a18d"], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Medical Image Registration"]], "content": "\\label{sec:reg}\nMedical image registration aims to find dense per-voxel displacement and establish alignment between a pair of fixed and moving images. In medical imaging, registration may be necessary when analyzing a pair of images acquired at different times, from different viewpoints, or using different modalities (like MRI and CT) . Accurate medical image registration is a challenging task due to difficulties in extracting discriminative features from multimodal medical images, complex motion, and lack of robust outlier rejection approaches . In this section, we briefly highlight the applications of ViTs in medical image registration.\nThe first study to investigate the usage of transformers for self-supervised medical volumetric image registration has been proposed by Chen \\textit{et al.} . Their model, ViT-V-Net, consists of a hybrid architecture composed of convolutional and transformer layers. Specifically, ViTs are applied to the high-level features of fixed and moving images extracted via a series of convolutional and max-pooling layers. The output from ViT is then reshaped and decoded using a V-Net style decoder . To efficiently propagate the information, ViT-V-Net uses long skip connections between the encoder and decoder. The output of the ViT-V-Net decoder is a dense displacement field, which is fed to the spatial transformer network for warping. Experiments on in-house MRI dataset show superiority of ViT-V-Net over other competing approaches in terms of Dice score. \nChen \\textit{et al.}~ further extends ViT-V-Net  and propose TransMorph model for volumetric medical image registration. Particularly, TransMorph makes use of Swin Transformer in the encoder to capture the semantic correspondence between input fixed and moving images, followed by long skip connections-based convolutional decoder to predict dense displacement field. For uncertainty estimation, they also introduce Bayesian deep learning by applying variational inference on the parameters of the encoder in TransMorph. Extensive evaluation is performed to compare TransMorph with other approaches for the medical image registration task. Specifically, experiments on inter-patient brain MRI registration provided by John-Hopkin university and XCAT-to-CT registration demonstrate the superiority of TransMorph against twelve different hand-crafted, CNN-based, and transformer-based approaches.\nSimilarly, Zhang \\textit{et al.}  present a novel dual transformer architecture (DTN) for volumetric diffeomorphic registration by effectively establishing correspondences between anatomical structures in an unsupervised manner.\nThe DTN consists of two CNN-based 3D U-Net encoders to extract embeddings of separate and concatenated volumetric MRI images. To further refine and enhance the embeddings, they propose encoder-decoder-based dual transformers to encode the cross-volume dependencies. Given the enhanced embeddings, the CNN decoder infers the deformation fields. Qualitative and quantitative results in terms of Dice similarity coefficient and negative Jacobian determinant on OASIS dataset~ of MRI scans demonstrate the effectiveness of their proposed architecture.\n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6649, 6650, 6651], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of selected Transformer-based models for medical image registration, but lacks deeper synthesis or comparison across the cited works. While it notes the use of hybrid architectures and unsupervised learning, it does not critically evaluate their strengths and weaknesses or abstract broader trends in the application of Transformers to this task."}}
{"id": "86a8effc-aef0-41d7-9ccf-62ad9c931650", "title": "Clinical report generation", "level": "section", "subsections": ["f271df31-b377-4bb3-ab88-5b1018fcf9aa", "435152e5-cc0f-45dd-b33c-65fb58aa9509", "be6cd508-90f9-474b-849a-5125cebbe095"], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"]], "content": "\\label{sec:cli}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[trim={1cm 5cm 5.5cm 1.5cm},clip,width = 0.49\\textwidth]{report_taxonomy.pdf}\n\\caption{Taxonomy of applications of ViTs in clinical report generation.}\n\\label{fig:report_tax}\n\\end{figure}\nRecently, immense progress has been made to automatically generate clinical reports from medical images using deep learning~. This automatic report generation process can help clinicians in accurate decision-making. However, generating reports (or captions) from the medical imaging data is challenging due to diversity in the reports of different radiologists, long sequence length (unlike natural image captions), and dataset bias (more normal data compared to abnormal). Moreover, an effective medical report generation model is expected to process two key attributes: (1) \\textit{language fluency} for human readability and (2) \\textit{clinical accuracy} to correctly identify the disease along with related symptoms.\nIn this section, we briefly describe how transformer models help achieve these desired goals and effectively mitigate the aforementioned challenges associated with medical report generation. Specifically, these transformer-based approaches have achieved state-of-the-art performance both in terms of Natural Language Generation (NLG) and Clinical Efficacy (CE) metrics.\nAlso note that, unlike previous sections that mainly discuss ViTs, in this section, the focus is on the transformers as powerful language models to exploit the long-range dependencies for sentence generation. We have broadly categorized transformer-based clinical report generation approaches into reinforcement learning (RL) based and supervised/unsupervised learning methods, as shown in Fig.~\\ref{fig:report_tax}, due to differences in their underlying training mechanism.", "cites": [6653, 6652], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a general description of the challenges in clinical report generation and introduces the use of transformers for this task, citing two survey papers. While it attempts to synthesize the role of transformers and categorize methods, the integration is basic and lacks a deeper, novel framework. The analysis remains mostly descriptive with little comparison or critique of specific approaches, and the abstraction is limited to surface-level categorization rather than identifying broader principles or trends."}}
{"id": "f271df31-b377-4bb3-ab88-5b1018fcf9aa", "title": "Reinforcement Learning Based Approaches", "level": "subsection", "subsections": [], "parent_id": "86a8effc-aef0-41d7-9ccf-62ad9c931650", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "Reinforcement Learning Based Approaches"]], "content": "RL-based medical report generation approaches can directly use the evaluation metrics of interest (like human evaluation, relevant medical terminologies, etc.) as rewards and update the model parameters via policy gradient. All approaches covered in this section use the self-critical RL~ approach to train models, which is more suitable for the report generation task compared to the conventional RL.\nOne of the first attempts to integrate transformer in clinical report generation has been made by Xiong \\textit{et al.}~. They propose Reinforced-Transformer for Medical Image Captioning (RTMIC) that consists of a pre-trained DenseNet~ to identify the region of interest from the input medical image, followed by a transformer-based encoder to extract visual features.\nThese features are given as input to the captioning decoder to generate sentences. All these modules are updated via self-critical reinforcement learning method during training on IU Chest X-ray dataset~. \nSimilarly, Miura \\textit{et al.}~ show that the high accuracy of automatic radiology reports as measured by natural language generation metrics such as BLEU~ and CIDer~ are often incomplete and inconsistent.\nTo address these challenges,  Miura \\textit{et al.}~ propose a transformer-based model that directly optimizes the two newly proposed reward functions using self-critical RL. The first reward promotes the coverage of radiology domain entities with corresponding reference reports, and the second reward promotes the consistency of the generated reports with their descriptions in the reference reports. Further, they combine these reward functions with the semantic equivalence metric of BERTScore~ that results in generated reports with better performance in terms of clinical metrics. \n\\textbf{{Surgical Instructions Generation.}}\nInspired by the success of transformers in medical report generation, Zhang \\textit{et al.}~ propose a transformer model to generate instructions from the surgical scenes. Lack of a predefined template, as in the case of medical report generation, makes generation of surgical instructions a challenging task. To handle this challenge, Zhang \\textit{et al.}~ have proposed an encoder-decoder based architecture back-boned by a transformer model. Specifically, their proposed architecture, optimized via self-critical reinforcement learning~, effectively models the dependencies for visual features, textual features, and visual-textural relational features to accurately generate surgical reports on the DAISI dataset~.", "cites": [96, 6654, 6655, 6656, 4227, 7099], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers, particularly emphasizing the use of self-critical reinforcement learning for report generation and its adaptation to surgical instruction generation. It provides a critical view by pointing out the limitations of traditional metrics like BLEU and CIDEr in assessing the quality of medical reports. However, while it identifies patterns such as the use of transformers and RL, it lacks a deeper abstraction or a novel framework that unifies these insights."}}
{"id": "714dffdc-f08a-40ae-8c01-c85012a53bed", "title": "Dataset Bias", "level": "subsubsection", "subsections": [], "parent_id": "435152e5-cc0f-45dd-b33c-65fb58aa9509", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "Supervised and Unsupervised Approaches"], ["subsubsection", "Dataset Bias"]], "content": "}}} Dataset bias is a common problem in medical report generation as there are far more sentences describing normalities than abnormalities. \nTo mitigate this bias,   Srinivasan~ propose a hierarchical classification approach using a transformer as a decoder. Specifically, the transformer decoder leverage attention between and across features obtained from reports, images, and tags for effective report generation. The architecture consists of \\textit{Abnormality Detection Network} to classify normal and abnormal images, \\textit{Tag Classification Net} to generate tags against images, and \\textit{Report Generation Net} that takes image features and tags as inputs to generate final reports. Experiments on IU Chest X-ray dataset~ demonstrate the effectiveness of the proposed architectural components. \nSimilarly, Liu \\textit{et al.}~ try to imitate the work of radiologists by distilling posterior and prior knowledge to generate accurate radiology reports. Their proposed architecture consists of three modules of Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE), and Multidomain Knowledge Distiller (MKD). Specifically, PoKE identifies the abnormal area in the input images (mitigate image data bias), PrKE explores relevant prior information from the radiological reports and medical knowledge graph (mitigate textual data bias), and MKD (based on transformer decoder) distills the posterior and prior knowledge to generate radiology report.   \nIn another work, You \\textit{et al.}  propose AlignTransformer to generate a medical report from X-ray images. Specifically, AlignTransformer consists of two modules: align hierarchial attention and multi-grained transformer. Align hierarchial attention module helps to better locate the abnormal region in the input medical images. \nOn the other hand, multi-grained transformer leverages multi-grained visual features using adaptive exploiting attention~ to accurately generate long medical reports. AlignTransformer achieves favorable performance on IU-Xray~ and MIMIC-CXR~ datasets. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width = 0.48\\textwidth]{rep_xray.PNG}\n\\caption{A chest X-ray image and its accompanying report, which includes findings and impressions, with aligned visual and textual components highlighted in different colors. Figure taken from .}\n\\label{fig:rep_xray}\n\\end{figure}\n\\iffalse\n\\begin{table}\n\\centering\n\\begin{tabular}{|l|ccc|ccc|} \n\\hline\n\\multicolumn{1}{|c|}{\\multirow{2}{*}{\\textbf{Dataset}}} & \\multicolumn{3}{c|}{\\textbf{IU X-RAY~}} & \\multicolumn{3}{c|}{\\textbf{MIMIC-CXR~}}  \\\\%[0.2ex]\n\\cline{2-7}\n\\multicolumn{1}{|c|}{}                                  & Train & Val   & Test                   & Train    & Val   & Test              \\\\%[0.2ex]\n\\toprule\\toprule\nImages                                                  & 5,226 & 748   & 1,496                  & 3,68,960 & 291   & 5,159                 \\\\\nReports                                                 & 2,770 & 395   & 790                    & 222,758  & 1,808 & 3,269                 \\\\\nPatients                                                & 2,770 & 395   & 790                    & 64,586   & 500   & 293                   \\\\\nAvg. Len.                                               & 37.56 & 36.78 & 33.62                  & 53.00    & 53.05 & 66.40                 \\\\\n\\toprule\n\\end{tabular}\n\\caption{Statistics of the benchmark medical report generation datasets used by transformer-based models for evaluation. Avg. Len. is average word-based length of reports.}\n\\end{table}\n\\fi\n\\begin{figure*}[h]\n\\centering\n\\includegraphics[width = 0.85\\textwidth]{rep_exp.png}\n\\caption{The depiction of lesion-image attention mapping areas and ground truth between CNN+Transformer and Faster-RCNN+Transformer samples, where green boxes represent the annotated region for each lesion word and red boxes represent the lesion-image attention mapping regions. Image taken from .}\n\\label{fig:rep_exp}\n\\end{figure*}\n\\begin{table}[]\n\\setlength{\\arrayrulewidth}{0.35mm}\n\\centering\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{|l|ccc|ccc|}\n\\hline\n\\rowcolor{mygray} & \\multicolumn{3}{c|}{\\textbf{Image}}                                                    & \\multicolumn{3}{c|}{\\textbf{Report}}                                            \\\\ \n\\rowcolor{mygray}  \\multirow{-2}{*}{\\textbf{Dataset}}                                 & \\multicolumn{1}{l}{Number} & \\multicolumn{1}{l}{Modality} & \\multicolumn{1}{l|}{View*} & \\multicolumn{1}{l}{Length*} & \\multicolumn{1}{l}{Language} & \\multicolumn{1}{l|}{Cases} \\\\ \\hline\nIU X-ray~                         & 7,470                      & X-Ray                        & 2                          & 32.5                        & Eng.                         & 2,955                      \\\\\nMIMIC-CXR~                         & 377,110                    & X-Ray                        & 1                          & 53.2                        & Eng.                         & 276,778                    \\\\\nPadChest~                          & 160,868                    & X-Ray                        & 2                          & -                           & Es                           & 22,710                     \\\\\nCX-CHR~                         & 45,598                     & X-Ray                        & 2                          & 66.9                        & Zh                           & 40,410           \n                \\\\\nDIARETDB1~                         & 89                         & CFP                          & 1                          & -                           & Eng                          & 89                         \\\\\nMESSIDOR~                          & 1,200                      & CFP                          & 2                          & -                           & Fr                           & 587                        \\\\\nFFA-IR                            & 1,048,584                  & FFA                          & 87                         & 91.2                        & Eng/Zh                       & 10,790     \n\\\\\nCOV-CTR~                           & 728                        & CT-Scans                     & 1                          & 77.3                        & Eng/Zh                       & 728                        \\\\\nDEN~                               & 15,709                     & CFP+FFA                      & 1                          & 7                           & Eng                          & -                          \\\\\nSTARE~                             & 397                        & CFP+FFA                      & 5                          & -                           & Eng                          & 397                        \\\\ \\hline\n\\end{tabular}\n}\n\\caption{Statistics of existing medical report generation datasets where * means average number and - for datasets that do not provide relevant information. Most of the transformers based models for medical report generation use Open-IU and MIMIC-CXR for evaluating results. Table adapted from .}\n\\label{tab:rep_datasets}\n\\end{table}", "cites": [6661, 7056, 6657, 6659, 6660, 6662, 6663, 6658], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes several transformer-based approaches to address dataset bias in medical report generation but primarily focuses on summarizing individual papers without deep integration or comparison. It lacks critical evaluation of the methods' strengths and weaknesses and offers limited abstraction beyond specific systems."}}
{"id": "3375675d-b1d5-43a4-981f-88a469831eaf", "title": "Feature Alignment", "level": "subsubsection", "subsections": [], "parent_id": "435152e5-cc0f-45dd-b33c-65fb58aa9509", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "Supervised and Unsupervised Approaches"], ["subsubsection", "Feature Alignment"]], "content": "}}} Feature alignment based approaches mainly focus on the accurate alignment of encoded representation of the medical images and corresponding text, which is crucial for the interaction and generation across modalities (images and text here) and subsequently for accurate report generation, as indicated in Fig.~\\ref{fig:rep_xray}.\nTo align better, Chen \\textit{et al.}~ propose a cross-modal memory network to augment the transformer-based encoder-decoder model for radiology report generation. They design a shared memory to facilitate the alignment between the features of medical images and texts. Experiments on IU-Xray~ and MIMIC-CXR~ datasets demonstrate that the proposed model can better align image and text features as compared to baseline methods. \nSimilarly, building on the shared-memory work of Chen \\textit{et al.}~, Yan \\textit{et al.}~ introduce a weakly supervised contrastive objective to favor reports that are semantically close to the target, thereby producing more clinically accurate outputs. \nSimilarly, Amjoud \\textit{et al.}~ investigate the impact on the report generation performance by modifying different architectural components of the model proposed by Chen \\textit{et al.}~  including replacing visual extractor and changing the number of layers in transformer-based decoder. \n\\iffalse\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.4\\textwidth]{report_gen.PNG}\n\\caption{}\n\\end{figure}\n\\fi", "cites": [6661, 6664, 6657], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by connecting the cited works through the common theme of feature alignment in clinical report generation. However, it remains largely descriptive, summarizing methods rather than offering a deep comparative or critical analysis. There is minimal abstraction or identification of broader principles beyond the specific architectural modifications discussed."}}
{"id": "107c1ce8-8c1c-4ab0-9552-b3cfcf278ddb", "title": "Explainable Models", "level": "subsubsection", "subsections": [], "parent_id": "435152e5-cc0f-45dd-b33c-65fb58aa9509", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "Supervised and Unsupervised Approaches"], ["subsubsection", "Explainable Models"]], "content": "}}}\nExplainability in medical report generation is crucial to improve trustworthiness for deploying models in clinical settings and a mean for extracting bounding boxes for lesion localization.\nFor model explainability, Hou \\textit{et al.}~  employ attention to identify regions of interest in the input image and demonstrate where the model is focusing for the resulting text. This attention mechanism increases the explainability of black-box models used in clinical settings and provides a method for extracting bounding boxes for disease localization. \nSpecifically, they propose RATCHET transformer model to generate reports by using DenseNet-101~ as an image feature extractor. RATCHET consists of a transformer-based RNN-Decoder for generating chest radiograph reports. \nThey assess the model's natural language skills and the medical correctness of generated reports. \nSimilarly, despite the immense interest of AI and clinical medicine researchers in the automatic report generation area, benchmark datasets are scarce, and the field lacks reliable evaluation metrics. To address these challenges, Li \\textit{et al.}~ introduce a large-scale Fundus fluorescence in Angiography images and reports dataset containing 10,790 reports describing 1,048,584 images with explainable annotations as shown in Fig. \\ref{fig:rep_exp}. The dataset comes with annotated Chinese reports and corresponding translated English reports. Further, they introduce nine reliable metrics based on human evaluation criteria. \n\\begin{table*}\n\\setlength{\\arrayrulewidth}{0.4mm}\n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|l|l|llllll|lll|} \n\\hline\n\\rowcolor{mygray}        &  &  & \\multicolumn{6}{c|}{\\textbf{\\textsc{NLG Metrics}}}                                                                                                                       & \\multicolumn{3}{c|}{\\textbf{\\textsc{CE Metrics}}}                                 \\\\\n \\rowcolor{mygray}   \\multirow{-2}{*}{\\textbf{\\textsc{Dataset}}}                                    & \\multicolumn{1}{c|}{\\multirow{-2}{*}{\\textbf{\\textsc{Model}}}}                               &            \\multirow{-2}{*}{\\textbf{\\textsc{Year}}}                    & \\multicolumn{1}{c}{\\textsc{BL-1}} & \\multicolumn{1}{c}{\\textsc{BL-2}} & \\multicolumn{1}{c}{\\textsc{BL-3}} & \\multicolumn{1}{c}{\\textsc{BL-4}} & \\multicolumn{1}{c}{\\textsc{MTR}} & \\multicolumn{1}{c|}{\\textsc{RG-L}} & \\multicolumn{1}{c}{\\textsc{P}} & \\multicolumn{1}{c}{\\textsc{R}} & \\multicolumn{1}{c|}{\\textsc{F1}}  \\\\ \n\\hline\n\\multirow{10}{*}{\\textbf{\\textsc{IU X-Ray~ }}}   & \\textsc{RTMIC~}                                                & 2019                           & 0.350                    & 0.234                    & 0.143                    & 0.096                    & -                       & -                         & -                     & -                     & -                        \\\\\n                                      & \\textsc{HRG Transformer}                                      & 2019                           & 0.464                    & 0.301                    & 0.212                    & 0.158                    & -                       & -                         & -                     & -                     & -                        \\\\\n                                      & \\textsc{KERP~}                                          & 2019                           & 0.482                    & 0.325                    & 0.226                    & 0.162                    & -                       & 0.339                     & -                     & -                     & -                        \\\\\n                                    & \\textsc{Hierarchical Transformer~}                                    & 2020                           & 0.464                    & 0.301                    & 0.212                    & 0.158                   & -                   & -                     & -                     & -                     & -                        \\\\ \n                                      & \\textsc{Memory Transformer~}                                   & 2020                           & 0.470                    & 0.304                    & 0.219                    & 0.165                    & 0.187                   & 0.371                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ TR.~ }                                                & 2021                           & 0.475                    & 0.301                    & 0.228                    & 0.180                    & 0.169                   & 0.373                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ TR. Prog.~}                                          & 2021                           & 0.486                    & 0.317                    & 0.232                    & 0.173                    & 0.192                   & 0.390                     & -                     & -                     & -                        \\\\\n                                    & \\textsc{Wang et al.~}                                          & 2021                           & 0.481                    & 0.309                    & 0.223                    & 0.169                    & 0.193                   & 0.365                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{Nguyen et al~}                                         & 2021                           & 0.515                    & 0.378                    & 0.293                    & 0.235                    & 0.219                   & 0.436                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{PPKED~}                                         & 2021                           & 0.483                    & 0.315                    & 0.224                    & 0.168                    & 0.190                   & 0.376                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{Align Transformer~}                                    & 2021                           & 0.484                    & 0.313                    & 0.225                    & 0.173                    & 0.204                   & 0.379                     & -                     & -                     & -                        \\\\ \n                        & \\textsc{KGAE Unsupervised~}                                    & 2021                           & 0.417                    & 0.263                    & 0.181                    & 0.126                    & 0.149                   & 0.318                     & -                     & -                     & -                        \\\\ \n                                  & \\textsc{KGAE Semi-supervised~}                                    & 2021                           & 0.497                    & 0.320                    & 0.232                    & 0.171                    & 0.189                   & 0.379                     & -                     & -                     & -                        \\\\ \n                                  & \\textsc{KGAE Supervised~}                                    & 2021                           & 0.512                    & 0.327                    & 0.240                    & 0.179                    & 0.195                   & 0.383                     & -                     & -                     & -                        \\\\   \n\\hline\n\\multirow{12}{*}{\\textbf{\\textsc{MIMIC-CXR~} }} & \\textsc{Transformers~}                                         & 2017                           & 0.409                    & 0.268                    & 0.191                    & 0.144                    & 0.157                   & 0.318                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{Memory Transformer~}                                   & 2020                           & 0.353                    & 0.218                    & 0.145                    & 0.103                    & 0.142                   & 0.277                     & 0.333                 & 0.273                 & 0.276                    \\\\\n                                      & \\textsc{Clinical Transformer~}                                 & 2020                           & 0.415                    & 0.272                    & 0.193                    & 0.146                    & 0.159                   & 0.318                     & 0.411                 & 0.475                 & 0.361                    \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ TR.~}                                                 & 2021                           & 0.361                    & 0.221                    & 0.146                    & 0.101                    & 0.139                   & 0.266                     & 0.324                 & 0.241                 & 0.276                    \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ TR. Prog.~}                                            & 2021                           & 0.378                    & 0.232                    & 0.154                    & 0.107                    & 0.145                   & 0.272                     & 0.240                 & 0.428                 & 0.308                    \\\\\n                                      & \\textsc{PPKED~}                                         & 2021                           & 0.360                    & 0.224                    & 0.149                    & 0.106                    & 0.149                   & 0.284                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{Align Transformer~}                                    & 2021                           & 0.378                    & 0.235                    & 0.156                    & 0.112                    & 0.158                   & 0.283                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{Ngyuen et al~}                                         & 2021                           & 0.495                    & 0.360                    & 0.278                    & 0.224                    & 0.222                   & 0.390                     & -                     & -                     & -                        \\\\\n                                      & \\textsc{MDT+WCL~}                                              & 2021                           & 0.373                    & -                        & -                        & 0.107                    & 0.144                   & 0.274                     & 0.384                 & 0.274                 & 0.294                    \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ Trans (CE)~}                                        & 2021                           & -                        & -                        & -                        & 0.111                    & -                       & -                         & 0.463                 & 0.732                 & 0.567                    \\\\\n                                      & \\textsc{$\\mathcal{M}^2$ Trans (EN)~ }                                       & 2021                           & -                        & -                        & -                        & 0.114                    & -                       & -                         & 0.503                 & 0.651                 & 0.567                    \\\\\n                                            & \\textsc{KGAE Unsupervised~}                                    & 2021                           & 0.221                    & 0.144                    & 0.096                    & 0.062                    & 0.097                   & 0.208                    & 0.214                     & 0.158                     & 0.156                        \\\\ \n                                  & \\textsc{KGAE Semi-supervised~}                                    & 2021                           & 0.352                    & 0.219                    & 0.149                    & 0.108                    & 0.147                   & 0.290                     & 0.360                     & 0.302                     & 0.307                        \\\\ \n                                  & \\textsc{KGAE Supervised~}                                    & 2021                           & 0.369                    & 0.231                    & 0.156                   & 0.118                   & 0.153                   & 0.295                     & 0.389                     & 0.362                    & 0.355                       \\\\  \n\\hline\n\\end{tabular}\n}\n\\caption{Quantitative comparison of transformer models for the task of clinical report generation in terms of Natural Language Generation (NLG) and Clinical Efficacy (CE) on two benchmark datasets. The NLG metrics include BLEU (BL)~, METEOR (MTR)~ and ROUGE-L (RG-L)~ and CE metrics include precision, recall and F1 score.}\n\\label{tab:report_gen}\n\\end{table*}", "cites": [96, 6655, 6660, 6665, 38, 6661, 6664, 6667, 6666, 6659, 6668], "cite_extract_rate": 0.5, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of explainable models in clinical report generation, citing several relevant papers and listing model performance metrics. While it touches on the importance of attention mechanisms and the need for benchmark datasets, it lacks deeper synthesis of ideas, critical evaluation of approaches, and abstraction to broader principles. The narrative remains largely centered on summarizing specific methods and results."}}
{"id": "71320c3e-aa95-4e05-9e11-001866664872", "title": "Miscellaneous", "level": "subsubsection", "subsections": [], "parent_id": "435152e5-cc0f-45dd-b33c-65fb58aa9509", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "Supervised and Unsupervised Approaches"], ["subsubsection", "Miscellaneous"]], "content": "}}}\nIn this section, we highlight several approaches that try to improve different aspects of clinical report generation from medical images. Examples include a memory-driven transformer to capture similar patterns in reports, uncertainty quantification for reliable report generation, a curriculum learning-based method, and an unsupervised approach to avoid paired training datasets.\nChen~\\textit{et al.}~ propose a \\textbf{memory-driven transformer to exploit similar patterns} in the radiology image reports. Specifically, they add a module to each layer of transformer-based decoder by optimizing the original layer normalization with a novel memory-driven conditional layer normalization. Extensive experiments on IU Chest X-ray~ and MIMIC-CXR~ datasets demonstrate the superiority of their approach both in terms of Natural Language Generation (NLG) and Clinical Efficacy (CE) metrics. \nSimilarly, Lovelace \\textit{et al.}~ also leverage the  transformer-based encoder and decoder for accurate medical report generation on  MIMIC-CXR dataset~. To \\textbf{emphasis on clinically relevant report generation}, they design a method to differentiate clinical information from generated reports, which they use to refine the model for clinical coherence. \nIn another work, Alfarghaly \\textit{et al.}~ present a pre-trained transformer-based model to generate a medical report from images. Specifically, the encoder consists of a pre-retrained CheXNet model that can generate semantic features from the input medical images. These semantic features are used to \\textbf{condition GPT2 decoder}~ to generate accurate medical reports. \nSimilarly, to judge the reliability of the automatic medical report generating model, uncertainty quantification is the key indicator. To incorporate this measure, Wang \\textit{et al.}~ propose \\textbf{transformer-based confidence guided framework} to quantify both visual and textual uncertainty. These uncertainties are subsequently used to construct an uncertainty-weighted loss to reduce misjudgment risk and improve the overall performance of the generated report. \nIn other work, Nguyen \\textit{et al.}~ propose differentiable end-to-end framework that consists of \\textbf{transformer as generator for report generation}.\nSpecifically, their proposed framework has three complementary modules: a \\textit{classifier} to learn the representation of disease features, a transformer-based \\textit{generator} model to generate the medical report, and \\textit{interpreter} to make the generated report consistent with the classifier output. They demonstrate the effectiveness of proposed components on IU-Xray~ and MIMIC-CXR~ datasets. \nInspired by \\textbf{curriculum learning}~, Nooralahzadeh \\textit{et al.}~ present a two-stage transformer architecture to progressively generate medical reports. Their progressive approach shows better performance over single-stage baselines in generating full-radiology reports. \nIn another study, Pahwa \\textit{et al.}~ investigate the \\textbf{impact of visual feature extractor} model on the performance of medical report generation. Based on insights, they propose a modified HRNet~, MedSkip, to extract visual features for the subsequent processing by the transformer-based decoder to generate an accurate medical report. \nSimilarly, Park \\textit{et al.}~ \\textbf{investigate the expressiveness of features} to discriminate between normal and abnormal images. They demonstrate the superiority of transformer-based decoder without global average pooling over hierarchical LSTM baseline.\nExisting transformer-based report generation models are mostly supervised and use paired image-report data during training. The paired data is difficult to obtain due to privacy and cost in the medical domain. To mitigate this issue, Liu \\textit{et al.}~ propose a knowledge graph auto-encoder that works in the share latent domain of images and reports to extract useful information in an \\textbf{unsupervised way}. Specifically, they use attention in the encoder to extract the knowledge representation from the knowledge graph and use a three-layer transformer in the decoder to generate reports. Their proposed framework can also be used in a semi-supervised or supervised manner in addition to the unsupervised mode.\nQuantitative and qualitative results, as well as evaluation by radiologists, corroborate the effectiveness of their approach.  \n\\begin{tcolorbox}[top=0.5pt,left=0pt,size=minimal,boxrule = 0pt,breakable, enhanced]", "cites": [6661, 6669, 6667, 3885, 6668, 6665], "cite_extract_rate": 0.4, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple papers by grouping them under common themes such as memory-driven models, uncertainty quantification, and unsupervised approaches, showing some synthesis. While it briefly explains how these approaches differ or aim to solve specific issues (e.g., privacy concerns, coherence), it lacks deeper comparative or critical analysis of their effectiveness or limitations. Some abstraction is visible through the categorization of methods, but overarching principles or trends in clinical report generation are not clearly identified."}}
{"id": "be6cd508-90f9-474b-849a-5125cebbe095", "title": "\\underline{Discussion", "level": "subsection", "subsections": [], "parent_id": "86a8effc-aef0-41d7-9ccf-62ad9c931650", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Clinical report generation"], ["subsection", "\\underline{Discussion"]], "content": "}}\n\\hspace{0.75em}\\textit{In this section, we have provided a comprehensive overview of the transformer's applications for clinical report generation from X-ray images. In contrast to previous sections that discuss applications of ViTs, this section focuses on transformers as powerful language models. It is also pertinent to note that even though multiple surveys exist covering the applications of deep learning in clinical report generation~, to the best of our knowledge, \\textbf{none of these have covered the applications of transformer models} in the area despite having transformers' phenomenal impact since their inception back in 2017.\nIn this regard, we hope this section will serve as a valuable resource to the research community. Below, we briefly highlight a few challenges associated with transformer-based models for report generation and outline promising future directions to explore.}\n\\\\\n\\vspace{0.01em}\n\\textit{ As we have seen, transformer-based report generation models {mostly rely on natural language generation (NLG) evaluation metrics} such as CIDEr and BLEU to assess performance. These NLG metrics often fail to represent clinical efficacy. One recent work by Miura \\textit{et al.}~ addresses this issue by proposing two new reward functions for the transformer model in reinforcement learning framework to better capture disease and anatomical knowledge in the generated reports. Another work by Li \\textit{et al.}~ introduces nine reliable human evaluation criteria to validate the generated reports. Despite these works, we believe that more attention from the research community is required to design reliable clinical evaluation metrics to facilitate the adoption of transformer-based medical report generation models in clinical settings.}\n\\\\\n\\vspace{0.01em}\n\\textit{ All transformer-based approaches covered in this section use the X-ray modality for automatic report generation. {Generating reports from other modalities like MRI or PET} have their own challenges associated with them due to their specific nature and distinct characteristics. Further, few medical datasets like ROCO~, PEIR Gross~, and ImageCLEF~ are available that consist of multiple modalities, different body parts, and corresponding captions. These datasets have the potential to become worthy benchmarks to gauge the performance of future multimodal (or unimodal like MRI, PET) transformer-based models for medical report generation. \nWe believe that transformer-based models tailored to specific modalities to generate reports must be explored in the future with a focus on creating diverse and challenging datasets of other modalities. Details of a few existing medical report generation datasets are given in Table \\ref{tab:rep_datasets}.\n}\n\\\\\n\\vspace{0.01em}\n\\textit{ Further, we would like to point out the interested researchers toward the recently explored {surgical instructions generation work using transformers}~ that could have a huge impact on surgical robotics, a market that is expected to reach USD 22.27 Billion by 2028. However, only one dataset, DAISI~, is available to evaluate models in this emerging area, demanding attention from the medical community to create diverse and more challenging datasets. }\n\\\\\n\\vspace{0.01em}\n\\textit{ Moreover, datasets for medical report generation like IU X-Ray~ {does not contain any standard train-test split} and most of the transformer-based approaches evaluate the performance on different tests data. In this regard, the results in Table \\ref{tab:report_gen} are not directly comparable, but they can provide an overall indication about the performance of the models.\nWe think what seems to be missing is a set of standardized procedures for creating challenging and diverse clinical report generation datasets.} \n\\end{tcolorbox}", "cites": [6653, 6654, 6655, 6656, 6670, 6652], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of the cited works by highlighting the use of transformers in clinical report generation, the limitations of current evaluation metrics, and the need for modality-specific models. It critically evaluates the reliance on NLG metrics and identifies a lack of standardization in datasets and evaluation procedures. The discussion abstracts from individual papers to broader issues such as clinical efficacy and the development of benchmarks, suggesting a need for future work in these areas."}}
{"id": "6baae926-76b2-496f-8613-adfc60b31052", "title": "Other applications", "level": "section", "subsections": [], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Other applications"]], "content": "\\label{sec:other_app}\nIn this section, we briefly highlight applications of Transformers in other medical imaging areas, including survival outcome prediction, visual question answering, and medical point cloud analysis. \n\\textbf{Survival outcome prediction} is a challenging regression task that seeks to predict the relative risk of cancer death. Recently, transformer models have shown impressive success in predicting survival rates. Chen \\textit{et al.}~ propose a Multimodal Co-Attention Transformer (MCAT) for the survival outcome prediction from whole-slide imaging (WSI) in pathology. MCAT learns a co-attention mapping between genomics and WSIs features to discover how histology features attend to genes while predicting patient survival outcomes. Extensive experiments on five cancer datasets demonstrate the superiority of MCAT compared to state-of-the-art CNN-based approaches. Similarly, Kipkogei \\textit{et al.}~ propose a Transformer-based architecture, Clinical Transformer, to model the relation between clinical and molecular features to predict survival outcomes from cancerous lung dataset~.\nIn another work, Eslami~  propose \\textbf{PubMedCLIP, a fine-tuned version of Contrastive Language-Image Pre-Training (CLIP)}~ for the medical domain by training it on the image–caption pairs from PubMed articles. Extensive experiments show that PubMedCLIP outperforms the previous state-of-the-art by nearly 3$\\%$.\nRecently, Liu \\textit{et al.}~ propose \\textbf{3D Medical Point Transformer} (3DMPT) to analyse 3D medical data. 3DMPT is tested on 3D medical classification and part segmentation based tasks.\nSimilarly, Malkiel \\textit{et al.}~ propose a \\textbf{Transformer-based architecture to analyse fMRI data}. They pre-train the model on 4D fMRI data in a self-supervised manner and fine-tune it on various downstream tasks, including age and gender prediction, as well as diagnosing Schizophrenia.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width =1\\textwidth]{pretrain_fig.pdf}\n\\caption{Impact of pre-training ViT on domain specific medical imaging dataset. \\textit{First column:} By only using 10$\\%$ labelled data, Swin UNET pre-trained on CT images ({\\color{blue} blue}) is able to achieve 10$\\%$ improvement in dice score over Swin UNET trained from scratch ({\\color{orange} orange}). \\textit{Middle column:} Dice scores of recently proposed transformer models on the spleen segmentation task of MSD dataset. Swin UNETR achieves state-of-the-art performance due to pre-training on the domain specific (CT) medical dataset. \\textit{Last column:} Qualitative visualizations of Swin UNETR pre-trained on CT images on BTCV multi-organ segmentation challenge. It can be seen that Swin UNETR predictions are closer to the groundtruth compared to baseline UNETR approach. First and last columns are adapted from .}\n\\label{fig:pre_train}\n\\end{figure*}", "cites": [1639, 6613], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lists several applications of Transformers in medical imaging beyond segmentation, detection, and classification, such as survival prediction and fMRI analysis. However, it lacks synthesis by not connecting the underlying mechanisms or themes across these applications. There is minimal critical analysis or abstraction, as the section primarily describes the papers and their results without deeper evaluation or generalization."}}
{"id": "5f4c64f6-331d-4fe7-ae24-d4e29e4be26c", "title": "Pre-training", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "Pre-training"]], "content": "\\label{sec:pretraining}\nDue to a lack of intrinsic inductive biases in modeling local visual features, ViTs need to figure out the image-specific concepts on their own via pre-training from large-scale training datasets . This may be a barrier to their widespread application in medical imaging, where typically datasets are orders of magnitude smaller compared to natural image datasets due to cost, privacy concerns, and the rarity of certain diseases, thereby making ViTs difficult to train efficiently in the medical domain. {Existing learning-based medical imaging approaches commonly rely on transferring learning via ImageNet\npretraining, which may be sub-optimal due to drastically different image characteristics between medical and natural images.}\nRecently, Matsoukas \\textit{et al.}~ has studied the impact of pre-training on ViTs performance for image classification and segmentation via a careful set of extensive experiments on several medical imaging datasets. Below, we briefly highlight major findings of their work. \n\\begin{itemize}\n\\item CNNs outperform ViTs for the medical image classification task when initialized with random weights.\n\\item CNNs and ViTs benefit significantly from ImageNet initialization for medical image classification. ViTs appear to benefit more from transfer learning, as they make up for the gap observed using random initialization, performing on par with their CNN counterparts.\n\\item CNNs and ViTs perform better with self-supervised pre-training approaches like DINO  and BYOL . ViTs appear to outperform CNNs in this setting for medical image classification by a small margin.\n\\end{itemize}\nIn short, although recent ViT-based data-efficient approaches like DeiT , Token-to-Token , transformer in transformer~, etc., report encouraging results in the generic vision applications, the task of learning these transformer models tailored to domain-specific medical imaging applications in a data-efficient manner is challenging. \nRecently, Tang \\textit{et al.}~ has made an attempt to handle this issue by investigating the effectiveness of self-supervised learning as a pre-training strategy on domain-specific medical images. Specifically, they propose 3D transformer-based hierarchical encoder, Swin UNETR, and after pre-training on 5,050 CT images, demonstrates its effectiveness via fine-tuning on the downstream task of medical image segmentation. \nThe pre-training on the medical imaging dataset also reduces the annotation effort compared to training Swin UNETR from scratch with random initialization. This is shown in Fig.~\\ref{fig:pre_train}, where it can be seen that pre-trained Swin UNETR can achieve the same performance by using only 60$\\%$ of data as achieved by random initialized Swin UNETR using 100$\\%$ of labeled data. This results in 40$\\%$reduction of manual annotation effort. Furthermore, as shown in Fig.~\\ref{fig:pre_train}, fine-tuning pre-trained Swin UNETR on the downstream medical image segmentation achieves better quantitative and qualitative results as compared to randomly initialized UNETR. Despite these efforts, there still remain several open challenges like Swin UNETR pre-trained on CT dataset gives unsatisfactory performance when applied directly to other medical imaging modalities like MRI due to large domain gap between CT and MRI images. \nFurthermore, the effectiveness of Swin UNETR on other downstream medical imaging tasks like classification and detection requires further investigation. \nMoreover, recent works for CNNs have shown that self-supervised pre-training on both ImageNet and medical datasets can improve the generalization performance (for classification) of the model on distribution shifted medical dataset~ as compared to pre-training on ImageNet only. We believe such studies for ViT-based models, along with multi-instance contrastive learning to leverage patient meta data~, will provide further insights to the community. Similarly, combining the self-supervised and semi-supervised pre-training in the context of ViTs for medical imaging applications is also an interesting avenue to explore~.\n\\iffalse\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{pretrain_barplot.pdf} \n\\caption{}\n\\end{figure}\n\\fi", "cites": [7367, 732, 2514, 4782, 6671, 6571, 6613, 7590, 2517, 865], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to discuss pre-training strategies in medical imaging, integrating findings on ViT performance, transfer learning, and self-supervised approaches. It provides critical analysis by identifying limitations such as the domain gap between CT and MRI and the need for further investigation in other tasks. The discussion abstracts beyond individual papers by highlighting broader trends and open challenges in adapting pre-training methods for medical imaging."}}
{"id": "1538f7e7-2516-4595-ac97-2de0b6c47a4e", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "Interpretability"]], "content": "\\label{sec:interpretability}\nAlthough the success of transformers has been empirically established in an impressive number of medical imaging applications, it has so far eluded a satisfactory interpretation.\nIn most medical imaging applications, ViT models have been deployed as block-boxes, thereby failing to provide insights and explain their learning behavior for making predictions. This black-box nature of ViTs has hindered their deployment in clinical practice since, in areas such as medical applications, it is imperative to identify the limitations and potential failure cases of designed systems, where interpretability plays a fundamental role~. \nAlthough several explainable AI-based medical imaging systems have been developed to gain deeper insights into the working of CNNs models for clinical applications~, however, the work is still in its infancy for ViT-based medical imaging applications. It is despite the inherent suitability of the self-attention mechanism to interpretability due to its ability to explicitly model interactions between every region in the image a shown in Fig.~\\ref{fig:gradcam} . Recent efforts for interpretable ViT-based medical imaging models leverage saliency-based approaches  and Grad-CAM based visualizations . Despite these efforts, the development of interpretable and explainable ViT-based approaches, specifically tailored for life-critical medical imaging applications, is a challenging and open research problem. Furthermore, formalisms, challenges, definitions, and evaluation protocols regarding interpretable ViTs based medical imaging systems must also be addressed. We believe that progress in this direction would not only help physicians to decide whether they should follow and trust automatic ViT-based model decisions but could also facilitate the deployment of such systems from a legal perspective.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width = 0.48\\textwidth]{gradcam.pdf} \n\\caption{Saliency maps comparison for a CNN-based ResNet-50~ (second row) and ViT-based DEIT-S~ (third row) on five medical imaging datasets for classification. Each column contains the original image (first row), a visualization of the ResNet-50 Grad-CAM saliency (second row), and a visualization of the DEIT-S’s attention map (third row), respectively. Note that the ViTs provide a clear, localized picture of attention compared to ResNet-50, thus giving insight into how the model makes decisions. Image taken from .}\n\\label{fig:gradcam}\n\\end{figure}", "cites": [6672, 6673, 97, 4849, 7590, 6571, 728], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.2}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers to highlight the current limitations and potential of ViT interpretability in medical imaging. It critically addresses the black-box nature of ViTs compared to CNNs and discusses the infancy of interpretability research in the context of medical applications. While it identifies broader themes such as the importance of interpretability for clinical trust and legal compliance, it does not propose a new framework or offer a deep abstraction beyond existing literature."}}
{"id": "a27d5516-a63a-4b60-8d9b-a31392a1fc85", "title": "Adversarial Robustness", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "Adversarial Robustness"]], "content": "\\label{sec:adversarialrobustness}\n\\begin{figure}[h]\n\\centering\n\\includegraphics[trim={6cm 2.5cm 8cm 1cm},clip,width = 0.48\\textwidth]{adversarial_attack.pdf} \n\\caption{Examples of adversarial attacks to fool learning-based models trained on medical image datasets. Left: normal images, Middle: adversarial perturbations, Right: adversarial images. The bottom tag is the predicted class, and green/red indicates correct/wrong predictions.}\n\\label{fig:adversarial}\n\\end{figure}\n\\begin{table}[t]\n\\centering\n\\caption{Description of datasets generally used in medical adversarial deep learning.}\n\\label{tab:adversarial_datasets}\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{V{3}c|c|cV{3}}\n\\hlineB{3}\n\\rowcolor{mygray} \\textbf{Dataset Name} & \\textbf{Dataset Size} & \\textbf{Modality} \\\\ \\hline\nRSNA~                 & 29,700                & X-ray             \\\\ \\hline\nJSRT~                 & 247                   & X-ray             \\\\ \\hline\nBraTS 2018~            & 1689                  & MRI               \\\\ \\hline\nBraTS 2019~           & 1675                  & MRI               \\\\ \\hline\nOASIS~              & 373-2168              & MRI               \\\\ \\hline\nHAM10000~              & 10,000                & Dermatoscopic     \\\\ \\hline\nISIC 18~              & 3594                  & Dermatoscopic     \\\\ \\hline\nLUNA 16~              & 888                   & CT-Scans          \\\\ \\hline\nNIH Chest X-ray~       & 112,000               & X-ray             \\\\ \\hline\nAPTOS~               & 5590                  & Fundoscopy        \\\\ \\hline\nChest X-ray~           & 5856                  & X-ray             \\\\ \\hline\nNSLT~                  & 75,000                & CT-Scans          \\\\ \\hline\nDiabetic Retinopathy~  & 35,000                & Fundoscopy        \\\\ \\hlineB{3}\n\\end{tabular}\n}\n\\end{table}\nAdvances in adversarial attacks have revealed the vulnerability of existing learning-based medical imaging systems against imperceptible perturbation in the input images . Considering the vast amount of money that underpins the medical imaging sector, this inevitably poses a risk whereby potential attackers may seek to profit from manipulation against the healthcare system, as shown in Fig.~\\ref{fig:adversarial}. For example, an attacker might try to manipulate the examination reports of patients for insurance fraud or a false medical reimbursement claim, thereby raising safety concerns.\nTherefore, ensuring the robustness of ViTs against adversarial attacks in life-critical medical imaging applications is of paramount importance. Although rich literature exists related to the robustness of CNNs in the medical imaging domain, to the best of our knowledge, no such study exists for ViTs, making it an exciting as well challenging direction to explore. Recently, few attempts have been made to evaluate the robustness of ViTs to adversarial attacks for natural images . The main conclusions of these attempts, ignoring their nuance difference, can be summarized as \\textit{ViTs are more robust to adversarial attacks than CNNs}. However, these robust ViT models cannot be directly deployed for medical imaging applications as the variety and type of patterns and textures in medical images differ significantly from the natural domain. Therefore, a principled approach to evaluate the robustness of ViTs against adversarial attacks in the medical imaging context, which builds the groundwork for resilience, could serve as a critical model to deploy these models in clinical settings. Furthermore, theoretical understanding to provide guarantees about the performance and robustness of ViTs, like CNNs , can be of significant interest to medical imaging researchers. In Table \\ref{tab:adversarial_datasets}, we provide a description of datasets used in adversarial medical learning to evaluate the robustness of CNNs for interesting researchers to benchmark the robustness of ViT-based models.\n\\begin{table*}[t]\n\\centering\n\\caption{Description of some of the open-source federated learning and privacy-preserving frameworks, including some specifically developed for medical imaging applications.}\n\\resizebox{0.9\\textwidth}{!}{\n\\begin{tabular}{V{3}c|c|p{8cm}V{3}}\n\\hlineB{3}\n\\hline\n\\rowcolor{mygray} \\textbf{Name}        & \\textbf{Framework}         & \\hspace{11em}\\textbf{Description}                                                                  \\\\ \\hline\nTensorFlow Fed & TensorFlow                 & Open-source framework for for machine learning and other computations on decentralized data.                           \\\\ \\hline\nCrypTen~             & PyTorch                    & Framework to facilitate research in secure and privacy-preserving machine learning.                                                                        \\\\ \\hline\nOpenMined~           & TensorFlow, PyTorch, Keras & Open-source decentralised privacy preserving framework. Includes specialized tools like PySyft, PyGrid, and SyferText. \\\\ \\hline\nOpacus~           & PyTorch                    & Enables training PyTorch models with differential privacy. Allows the client to online track the privacy budget.                                                          \\\\ \\hline\nDeepee~             & PyTorch                    &  Library for differentially private deep learning for medical imaging in PyTorch.                \\\\ \\hline\nPriMIA~             & PyTorch                    & Framework for end-to-end privacy-preserving decentralized deep learning for medical images.                 \\\\ \\hlineB{3}\n\\end{tabular}\n\\label{tab:federated_frameworks}\n}\n\\end{table*}", "cites": [6678, 6680, 6674, 6676, 8260, 6675, 6681, 6679, 5249, 8844, 5772, 6677], "cite_extract_rate": 0.375, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key findings from multiple papers to establish that ViTs are more robust than CNNs in the context of natural images, but highlights the lack of direct evaluation in medical imaging. It critically notes the domain-specific challenges in medical imaging that limit the direct application of these findings. The discussion abstracts the problem to the broader need for robustness evaluation frameworks tailored to medical imaging, though it stops short of offering a comprehensive meta-level insight."}}
{"id": "35dd36ed-4de3-4aa5-8cfc-f61e2c1357bd", "title": "ViTs for Medical Imaging on Edge Devices", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "ViTs for Medical Imaging on Edge Devices"]], "content": "\\label{sec:edgedevices}\nDespite the tremendous success of ViTs in numerous medical imaging applications, the intensive requirements for memory and computation hamper their deployment on resource constraint edge devices .\nDue to recent advancements in edge computing, healthcare providers can process, store and analyze complex medical imaging data on-premises, speeding diagnosis, improving clinician workflows, enhancing patient privacy, and saving time—and potentially lives. These edge devices provide extremely fast and highly accurate processing of large amounts of medical imaging data, therefore demanding efficient hardware design to make ViT-based models suitable for edge computing-based medical imaging hardware. Recently few efforts have been made to compress transformer-based models by leveraging enhanced block-circulant matrix-based representation~\nand neural architecture search strategies~. \nDue to the exceptional performance of ViTs, we believe that there is a dire need for their domain-optimized architectural designs tailored for edge devices. It can have a tremendous impact on medical imaging-based health care applications where on-demand insights help teams make crucial and urgent decisions about patients.", "cites": [8003, 4784, 9060, 4747], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the main theme of ViT inefficiency on edge devices and briefly references strategies like block-circulant matrix representation and neural architecture search. While it provides a coherent narrative on the challenge, the analysis remains relatively high-level without detailed evaluation or contrast between the cited approaches. It does generalize to highlight the importance of domain-specific optimization but lacks deeper abstraction or meta-level insights."}}
{"id": "3f19700d-0c58-4fe6-8571-32630c9262ce", "title": "Decentralized Medical Imaging Solutions using ViTs", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "Decentralized Medical Imaging Solutions using ViTs"]], "content": "\\label{sec:decentralized}\nBuilding robust deep learning-based medical imaging models highly depends on the amount and diversity of the training data. The training data required to train a reliable and robust model may not be available in a single institution due to strict privacy regulations, the low incidence rate of some pathologies,  data-ownership concerns, and limited numbers of patients. Federated Learning (FL) has been proposed to facilitate multi-hospital collaboration while obviating data transfer. Specifically, in FL, a shared model is built using distributed data from multiple devices where each device trains the model using its local data and then shares the model parameters with the central model without sharing its actual data. Although a plethora of approaches exists that address FL for CNNs based medical imaging applications, the work is still in its infancy for ViTs and requires further attention. Recently few research efforts have been made to exploit the inherent structure of ViT in distributed medical imaging applications. Park \\textit{et al.}  propose a Federated Split Task-Agnostic (FESTA) framework that integrates the power of Federated and Split Learning  in utilizing ViT to simultaneously process multiple chest X-ray tasks, including diagnosing COVID-19 CXR images on a large corpus of decentralized data. Specifically, they split ViT into shared body and task-specific heads and demonstrate that ViT body with sufficient capacity can be shared across relevant tasks by leveraging the multitask-learning (MTL)  strategy. \nHowever, FESTA is just a proof-of-concept study, and its applicability in clinical trials requires further experimentation. Furthermore, challenges like privacy attacks and robustness against communication bottlenecks for ViT-based FL medical imaging systems require in-depth investigation. An interesting future direction is to explore recent privacy enhancement approaches like differential privacy~ to prevent gradient inversion attacks~ on FL-based medical imaging systems in the context of ViTs.\nIn short, we believe that the successful implementation of distributed machine learning frameworks coupled with the strengths of ViTs could hold significant potential for enabling precision medicine at a large scale. This can lead to ViT models that yield unbiased decisions and are sensitive to rare diseases while respecting governance and privacy concerns. In Table~\\ref{tab:federated_frameworks}, we highlight various tools and libraries that have been developed to implement distributed and secure deep learning. This can provide useful information for researchers who wish to rapidly prototype their ViT-based models for medical imaging in distributed settings.", "cites": [6628, 7608, 6000, 9061, 671], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to discuss the potential and challenges of ViT-based decentralized medical imaging solutions, particularly in the context of Federated and Split Learning. It critically evaluates the limitations of existing approaches like FESTA and points to areas needing further research, such as privacy attacks and communication bottlenecks. The abstraction level is moderate, as it connects specific papers to broader themes like privacy, robustness, and scalability in distributed learning for medical imaging."}}
{"id": "2ff87e63-9a44-4c4c-a86f-b76f3b8cd7c3", "title": "Domain Adaptation and Out-of-Distribution Detection", "level": "subsection", "subsections": [], "parent_id": "12bec7ee-7be6-4eff-aae7-4998e3e3c559", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Open Challenges and Future Directions"], ["subsection", "Domain Adaptation and Out-of-Distribution Detection"]], "content": "\\label{sec:domainadaptation}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.32\\linewidth]{figures/section_resnet.pdf}\n\\includegraphics[width=0.32\\linewidth]{figures/section_vit_pretrain_only.pdf}\n\\includegraphics[width=0.32\\linewidth]{figures/section_vit_finetuned.pdf}\n\\caption{A 2D PCA projection of the space of embedding vectors for three models, having two in-distribution and one out-of-distribution class. The points are projections of embeddings of the categories of in-distribution classes ({\\color{yellow}yellow} and black) and out-of-distribution classes (red points). The color-coding shows the Mahalanobis outlier score . ResNet-20 plot (left panel) leads to overlapping clusters indicating that classes are not well separated. ViT pre-trained on ImageNet-21k (middle panel) can distinguish classes from each other well but does not lead to well-separated outlier scores. ViT fine-tuned on the in-distribution dataset (right panel) is excellent at clustering embeddings based on classes and assigning high Mahalanobis distance to out-of-distribution inputs ({\\color{red}red}). Image courtesy .\n}\n\\label{fig:ood}\n\\end{figure*}\n \\begin{figure}[t]\n\\centering\n\\includegraphics[trim={0cm 0cm 1.5cm 0cm},clip,width = 0.49\\textwidth]{vit_cnn_piechart.pdf}\n\\caption{ViT-based papers in medical imaging in 2021 (dark shade) vs pure CNN-based papers in medical imaging from 2012 to 2015 (light shade). It can be seen that ViTs have rapidly pervaded into almost all areas of medical imaging in a single year, with segmentation and classification being the most impacted areas. Statistics of CNNs-based papers are taken from .}\n\\label{fig:vit_vs_cnn}\n\\end{figure}\nRecent efforts for ViT-based medical imaging systems have primarily focused on improving the accuracy and generally lacking a principled mechanism to evaluate their generalization ability under  \ndifferent distribution/domain shifts.\nRecent studies have shown that test error\ngenerally increases in proportion to the distribution difference between training and test datasets, thereby making it a crucial issue to investigate in the context of ViTs.\nIn medical imaging applications, these distribution shifts in data arise due to several factors that include: images acquired with a different device model at a different hospital, images of some unseen disease not in the training dataset, images that are incorrectly prepared, e.g., poor contrast, blurry images, etc. Extensive research exists on CNN-based out-of-distribution detection approaches in medical imaging~. \nRecently, few attempts have been made to show that large-scale pre-trained ViTs, due to their high-quality representations, can significantly improve the state-of-the-art on a range of out-of-distribution tasks across different data modalities  . However, investigation in these works has been mostly carried out on toy datasets such as CIFAR-10 and CIFAR-100, therefore not necessarily reflecting out-of-distribution detection performance on medical images with complex textures and patterns, high variance in feature scale (like in X-ray images), and local specific features. This demands further research to design ViT-based medical imaging systems that should be accurate for classes seen during training while providing calibrated estimates of uncertainty for abnormalities and unseen classes. We believe that research in this direction using techniques from transfer learning and domain adaptation will be of interest to the practitioners working in medical imaging based life-critical applications to envision potential practical deployment. In Fig.~\\ref{fig:ood}, we highlight the performance gain of ViTs as compared to CNNs for out of distribution detection to inspire medical imaging researchers who wish to explore this area.\nAnother possible direction is to explore the recent advancements in continual learning~ to effectively mitigate the issue of domains shift using ViTs. Few preliminary efforts have been made to explore this direction~; however, the work is still in its infancy and requires further attention from the community.\nFurther, standardized and rigorous evaluation protocols also need to be established for domain adaptation in the medical imaging applications, similar to \\textsc{DomainBed}~ framework in the natural image domain. Such a framework will also help in advocating models reproducibility.\n\\iffalse", "cites": [6682, 6683, 7494, 1639, 2788, 3267, 885, 7611, 8261, 1624, 6684], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to provide a coherent narrative on domain adaptation and out-of-distribution detection for ViTs in medical imaging, highlighting trends such as the rapid adoption of ViTs and their limitations under real-world distribution shifts. It critically evaluates the current state of the field, pointing out that many ViT-based methods rely on toy datasets and lack robust evaluation on medical data. It also abstracts the issue to broader concerns such as model reliability in life-critical applications and proposes directions like continual learning and standardized frameworks."}}
{"id": "422b1a3b-2b1e-4d0b-ab28-ed31577c6e26", "title": "Supplementary", "level": "section", "subsections": [], "parent_id": "f84ad0a8-5dbd-4805-ad0e-c5f32eebc262", "prefix_titles": [["title", "Transformers in Medical Imaging: A Survey"], ["section", "Supplementary"]], "content": "\\begin{table*}[]\n\\centering\n\\caption{List of Acronyms.}\n\\resizebox{0.35\\textwidth}{!}{\n\\begin{tabular}{ll}\n\\hline\n\\multicolumn{1}{|l|}{\\textbf{Acronym}} & \\multicolumn{1}{l|}{\\textbf{Expanded Form}}       \\\\ \\hline\n\\multicolumn{1}{|l|}{CNN}              & \\multicolumn{1}{l|}{Convolutional Neural Network} \\\\ \\hline\n\\multicolumn{1}{|l|}{CT}               & \\multicolumn{1}{l|}{Computed Tomography}          \\\\ \\hline\n\\multicolumn{1}{|l|}{DeTr}             & \\multicolumn{1}{l|}{Detection Transformers}       \\\\ \\hline\n\\multicolumn{1}{|l|}{DL}               & \\multicolumn{1}{l|}{Deep Learning}                \\\\ \\hline\n\\multicolumn{1}{|l|}{DNN}              & \\multicolumn{1}{l|}{Deep Neural Network}          \\\\ \\hline\n\\multicolumn{1}{|l|}{FL}               & \\multicolumn{1}{l|}{Federated Learning}           \\\\ \\hline\n\\multicolumn{1}{|l|}{GPU}              & \\multicolumn{1}{l|}{Graphical Processing Unit}    \\\\ \\hline\n\\multicolumn{1}{|l|}{MedT}             & \\multicolumn{1}{l|}{Medical Transformer}          \\\\ \\hline\n\\multicolumn{1}{|l|}{MG}               & \\multicolumn{1}{l|}{Mammography}                  \\\\ \\hline\n\\multicolumn{1}{|l|}{MHA}              & \\multicolumn{1}{l|}{Multi-Head Attention}         \\\\ \\hline\n\\multicolumn{1}{|l|}{MIA}              & \\multicolumn{1}{l|}{Medical Image Analysis}       \\\\ \\hline\n\\multicolumn{1}{|l|}{ML}               & \\multicolumn{1}{l|}{Machine Learning}             \\\\ \\hline\n\\multicolumn{1}{|l|}{MRI}              & \\multicolumn{1}{l|}{Magnetic Resonance Imaging}   \\\\ \\hline\n\\multicolumn{1}{|l|}{PET}              & \\multicolumn{1}{l|}{Positron Emission Tomography} \\\\ \\hline\n\\multicolumn{1}{|l|}{PMTrans}          & \\multicolumn{1}{l|}{Pyramid Medical Transformer}  \\\\ \\hline\n\\multicolumn{1}{|l|}{SA}               & \\multicolumn{1}{l|}{Self-Attention}               \\\\ \\hline\n\\multicolumn{1}{|l|}{SOTA}             & \\multicolumn{1}{l|}{State-of-the-Art}             \\\\ \\hline\n\\multicolumn{1}{|l|}{SVD}              & \\multicolumn{1}{l|}{Singular Value Decomposition} \\\\ \\hline\n\\multicolumn{1}{|l|}{Trans-UNet}       & \\multicolumn{1}{l|}{Transformer UNet}             \\\\ \\hline\n\\multicolumn{1}{|l|}{ViT}              & \\multicolumn{1}{l|}{Vision Tranformer}            \\\\ \\hline\n                                       &                                                  \n\\end{tabular}\n}\n\\end{table*}\n\\begin{table*}\n\\centering\n\\begin{tabular}{|l|ccc|c|}\n\\hline\n       & Training  & Validation    & Test      & Total\\\\ \n\\hline\n\\hline\n\\#scans     & 70        & 15            & 15        & 100 \\\\ \n\\hline\n\\#images    & 25,311     & 5,937        & 6,042      & 37,290 \\\\ \n\\hline\n\\end{tabular}\n\\caption{The number of triphasic CT scans and individual slices from the first public Coltea-Lung-CT-100W data set .}\n\\label{tab_dataset}\n\\end{table*}\n\\begin{table}[h]\n\t\\centering\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{2pt}{\n\t\t\\begin{threeparttable}[b]\n\t\t\t\\caption{List of representative efficient transformer-based models. The data of the Table is from .}\n\t\t\t\\footnotesize\n\t\t\t\\begin{tabular}{c|c|c|c|c}\n\t\t\t\t\\hline\n\t\t\t\tModels & Compress Type & $\\#$Layer &Params & Speed Up \\\\\t\t\n\t\t\t\t\\hline\n\t\t\t\tBERT$_{BASE}$ & Baseline & 12 & 110M & $\\times$1\\\\\n\t\t\t\t\\hline\n\t\t\t\tALBERT  & Decomposition & 12 & 12M & $\\times$5.6 \\\\\n\t\t\t\t\\hline\n\t\t\t\tBERT-& Architecture & \\multirow{2}*{6} & \\multirow{2}*{66M} & \\multirow{2}*{$\\times$1.94}\\\\\n\t\t\t\tof-Theseus & design &  &  & \\\\\n\t\t\t\t\\hline\n\t\t\t\tQ-BERT  & \\multirow{2}*{Quantization} & 12 & \\multirow{2}*{-} & \\multirow{2}*{-} \\\\\n\t\t\t\tQ8BERT  & &12 & & \\\\\n\t\t\t\t\\hline\n\t\t\t\tTinyBERT  & \\multirow{5}*{Distillation} & 4 & 14.5M & $\\times$9.4 \\\\\n\t\t\t\tDistilBERT~ & & 6 & 6.6m & $\\times$1.63 \\\\\n\t\t\t\tBERT-PKD~ & & 3$\\sim$6 & 45.7$\\sim$67M & $\\times$3.73$\\sim$1.64 \\\\\n\t\t\t\tMobileBERT~ & &24 &25.3M & $\\times$4.0 \\\\\n\t\t\t\tPD~ & & 6 &67.5M &$\\times$2.0\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\t\\label{tab:compressed_tf}\n\t\\end{threeparttable}}\n\\end{table}\n\\fi\n\\end{document}", "cites": [2481, 856, 1150, 7, 4512, 2488, 1445, 7580, 854], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive listing of acronyms and a table of compressed transformer-based models, primarily drawing from NLP-focused papers rather than those directly relevant to medical imaging. It lacks synthesis of ideas across sources, critical evaluation of the methods, or abstraction to broader patterns. The content is factual but does not contribute to a deeper understanding of the field or the role of Transformers in medical imaging."}}
