{"id": "de0ae1bb-52ae-40d0-ad46-45f8c0caf390", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "b86b59ab-ef52-4a50-8a85-9083dd7cc7d5", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Introduction"]], "content": "Knowledge graph(KG) has received a lot of traction in the recent past and led to much research in this area. Most of the research is focused on the generation of Knowledge graphs and consumption of the information enshrined in the Knowledge Graph. Some of the earlier works to create KG are YAGO, Freebase, DBpedia and WikiData. The evolution of the Knowledge Graph starts with the seminal paper from Berners-Lee . The knowledge graph has evolved in three phases. In the first phase, the Knowledge representation was brought to the level of Web standard. The core focus shifted to Data management, linked data, and its application in the second phase. In the third phase, the focus shifted on the real-world application . The real-world application ranges from Semantic parsing, recommender system, question answering, named entity disambiguation, information extraction  etc. The knowledge graph is a representation of structured relational information in the form of Entities and relations between them. It is a multi-relational graph where nodes are entities and edges are relations. Entities are real-world objects or abstract information. The representation entities and relations between them are represented as triple. For e.g. \\textit{(New Delhi, IsCapitalOf, India)} is an example of a triple. \\textit{New Delhi} and \\textit{Delhi} are entities and \\textit{IsCapitalOf} is relation. Though the representation looks scientific but consuming these in the real-world application is not an easy task. The consumption of information enshrined in the knowledge graph will be very easy if it can be converted to numerical representation. \\textit{Knowledge graph embedding} is a solution to incorporate the knowledge from the knowledge graph in a real-world application. \\\\\nThe motivation behind Knowledge graph embedding  is to preserve the structural information, i.e., the relation between entities, and represent it in some vector space. This makes it easier to manipulate the information. Most of the work in \\textit{Knowledge graph embedding \\textbf{(KGE)}} is focused on generating a continuous vector representation for entities and relations and apply a relationship reasoning on the embedding. The relationship reasoning is supposed to optimize some scoring function to learn the embedding. Researchers have used different approaches to learn the embedding Path-based learning, Entity-based learning, textual-based learning, etc. A lot of work was focused were translation model  and semantic-based model. The representation of the triple results in a lot of information lost because it fails to take \"textual information\" into account. With the proposal of Graph attention network  the representation of the entities has become more contextualized. In recent years, the proposal of multi-modal graph has extended the spectrum to a new level. In multi-modal knowledge graph, Knowledge graph can have multi-modal information like image and text etc. . \\\\\nPrevious work on survey has focused on the KG Embedding , KG Embedding and application , KG embedding with textual data , KG embedding based on the deep-learning . This work shall focus on the KG embedding from translation-based model, semantic-based model, embedding with enriched representation from textual data and multi-modal data and their application. In section 2, we shall provide the details of KGE; in section 3, we shall present the application area. In the summary section, we shall try to put emerging areas of research in KGE.", "cites": [1165, 180], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of knowledge graph (KG) embedding, citing a survey and a graph attention network paper, but integration is limited to general mentions without deeper synthesis. It lacks critical evaluation of methods and instead focuses on surface-level descriptions. Some abstraction is attempted by outlining phases of KG evolution and types of embedding models, but the analysis remains shallow and does not present novel frameworks or meta-level insights."}}
{"id": "eb60db2c-06a9-47fd-93e9-9721cef7561d", "title": "TransR \\cite{10.5555/2886521.2886624", "level": "subsubsection", "subsections": [], "parent_id": "79c73227-24d3-43b6-b789-8702fd91e78e", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Translation Models"], ["subsubsection", "TransR \\cite{10.5555/2886521.2886624"]], "content": "}\nIt proposes that an entity may have multiple attributes and various relations. Each relation may focus on different attributes of entities. TransR models entities  and relation in different embedding space. It means that two different spaces: entity space and relation space are modelled. Each entity is mapped into relation space. The translation construct is applied on the projected representation in the relationship space. The Figure-\\ref{fig:my_label_3} presents the intuition behind TransR model.\\\\ \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{TransR.png}\n    \\caption{TransR model. Image taken from }\n    \\label{fig:my_label_3}\n\\end{figure}\nTo further refine the representation, a new model TransD was proposed by . TransR captures the possibility of relations and their embedding from the relation space. But the TransD, extends it to the entity space as well. Here Entity-relation pair is considered as the first-class object.", "cites": [5946], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the TransR model and its approach to embedding entities and relations in separate spaces. It mentions TransD as an extension but does not elaborate on the differences or advantages. There is little synthesis or critical evaluation, and no abstraction or identification of broader trends in the field."}}
{"id": "735f8d94-1514-439f-a270-0d20a5c2bdd2", "title": "RotatE \\cite{DBLP:journals/corr/abs-1902-10197", "level": "subsubsection", "subsections": [], "parent_id": "79c73227-24d3-43b6-b789-8702fd91e78e", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Translation Models"], ["subsubsection", "RotatE \\cite{DBLP:journals/corr/abs-1902-10197"]], "content": "}\nIn knowledge graphs, often, the relation is symmetric/anti-symmetric, inversion and composition. e.g., \"Marriage\" is a symmetric relation, \"My niece is my sister's daughter\" is a composition, etc. The models discussed above are not capable of predicting these relations. The model proposed here is based on the intuition that the relation from head to tail is modeled as rotation in a complex plane. It is motivated by Euler's identity. \n    \\begin{equation}\n        e^{\\iota\\theta} = \\sin \\theta + \\iota \\cos \\theta\n    \\end{equation}\n\\noindent\nFor a triplet \\textit{(h,r,t)}, the relation among them can be represented as \\textit{t = h $\\circ$ r}. Where \\textit{h, r, t} is the k-dimension embedding of the head, relation and tail, restricting the $|{r_i}|$ = 1. It means we are in the unit circle and $\\circ$ represents element wise product. For each dimension $t_i = r_i h_i$ subjected to constraint $|{r_i}|$ = 1.\nUnder these condition the relationship is symmetric $\\iff$  for all the values of i, $e^{0/\\iota\\theta} = \\pm 1$, The relationship is inverse $\\iff$ $r_j = \\bar{r}_i$, i.e. both are the complex conjugate. Two relations are composite $\\iff$ $r_j = r_i \\circ r_k$. It means the relation $r_j$ can be obtained by a combined rotation of $r_i$ and $r_k$, $\\theta_j = \\theta_i + \\theta_k$. The scoring function measures the angular distance.\n\\begin{equation}\n    d_r(h,t) = \\left\\| h \\circ r - t \\right\\|\n\\end{equation}\nThe Figure-\\ref{fig:my_label_4} shows the comparison of RotaE and TransE. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{rotaE.png}\n    \\caption{Representation of RotaE in comparison to TransE. In RotatE, the relationship between \\textit{h} and \\textit{t} is represented as angle of rotation. This image depicts the relationship in 1-dimension space.  }\n    \\label{fig:my_label_4}\n\\end{figure}", "cites": [5946], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of RotatE, capturing its core idea of modeling relations as rotations in a complex space and connecting it to properties like symmetry, inversion, and composition. It integrates the model's key components from the cited paper and abstracts them into general principles. However, it lacks deeper critical evaluation of the model's limitations or comparisons with alternative approaches beyond a brief mention of TransE."}}
{"id": "a384ab01-c8a5-449e-8f0b-9c000bc84415", "title": "RESCAL \\cite{nickel2011three", "level": "subsubsection", "subsections": [], "parent_id": "35efce91-9f70-4bb9-9152-d6a3a9e9d407", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Semantic Matching Models"], ["subsubsection", "RESCAL \\cite{nickel2011three"]], "content": "}\nRESCAL follows Statistical Relational Learning Approach which is based on a Tensor Factorization model that takes the inherent structure of relational data into account. A Tensor is a multidimensional array. More formally we can say that a first order Tensor is a vector, second order Tensor is a matrix and, Tensor with more than two order is called as higher order Tensor. Tensor Factorization is expressing a Tensor as a sequence of elementary operations acting on other, often a simpler Tensors.  Statistical Relational Learning inherits from Probability Theory and Statistics to address uncertainty and complexity of relational structures. \\\\\n models the Knowledge Graph triplet of the form (head, relation, tail) into three-way tensor, $\\mathcal{X}$ as shown in  Figure 5.\n\\begin{figure}[htb]\n    \\includegraphics[width=.9\\linewidth]{Fig_1.png}\n    \\caption{Tensor model for relational data. $E_{1} ... E_{n}$ denote the entities and $R_{1} ... R_{m}$ denote the relation in the domain }\n    \\label{fig: Menu}\n\\end{figure}\n\\noindent\nIn  $\\mathcal{X}$, two modes holds the concatenated entities (head and tail), and the third mode holds relation (relation). A Tensor entity $\\mathcal{X}_{ijk}$ = 1  denotes that there exist a relation and if $\\mathcal{X}_{ijk}$ = 0 denotes that there is unknown relation. It is assumed that data is given as n * n * m Tensor. Where n is the number of entities and m is the number of relations. RESCAL \"explains triples via pairwise interaction of latent features\". It performs the rank-r factorization on each slice of $\\mathcal{X}$ (relational data) and the score of a fact (head, relation, tail ) is given by the following bi-linear function.\n\\[f_{r}(h,t) = \\textbf{h}^T\\textbf{M}_r\\textbf{t} = \\sum_{i=0}^{d-1}\\sum_{j=0}^{d-1} [\\textbf{M}_r]_{ij} .[\\textbf{h}]_i . [\\textbf{t}]_j\\]\nwhere h,t $\\in$ $\\mathbb{R}^d$ are vector representation of entities, and $M_{r}$ $\\in$ $\\mathbb{R}^{d*d}$ is a matrix representation of $r^{th}$ relation. Thus from this equation we are able to calculating the score of the triple using the weighted sum of all the pairwise interactions between the latent features of the entities $h$ and $t$ as shown in the Figure 6.\n\\begin{figure}[htb]\n    \\includegraphics[width=.9\\linewidth]{Fig_2.png}\n    \\caption{ RESCAL Representation. Here number of latent features for entities are 3 and number of latent features for relations are 3.   }\n    \\label{fig: Graph}\n\\end{figure}\n\\noindent\nThis method require $O(d^{2})$ parameters per relation and the space complexity of $O(nd + md^{2})$ where n is the number of entities and m is the number of relations.", "cites": [7233], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a factual description of the RESCAL model, explaining its tensor factorization approach and mathematical formulation. However, it does not synthesize insights from multiple sources or connect RESCAL to broader trends in knowledge graph embedding. There is minimal critical evaluation or comparison with other methods, and while it introduces some abstract concepts like tensors and pairwise interactions, it lacks deeper meta-level insights."}}
{"id": "5dfa59d1-b784-426d-8ac5-ab1690c10b77", "title": "TATEC \\cite{10.1007/978-3-662-44848-9_28", "level": "subsubsection", "subsections": [], "parent_id": "35efce91-9f70-4bb9-9152-d6a3a9e9d407", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Semantic Matching Models"], ["subsubsection", "TATEC \\cite{10.1007/978-3-662-44848-9_28"]], "content": "}\nTATEC stands for \\textit{Two And Three-way Embeddings Combination}. The main disadvantage of RESCAL is that it is a Three-way model which performs fairly good for relationships which occur frequently but it performs poor for the rare relationships and leads to major over-fitting. The issue of major over-fitting for rare relationships can be controlled by regularizing or reducing the expressivity of the model and former method is not feasible. The second method of reducing the expressivity is Two-way interaction which is implemented in TransE and SME. Two-way interaction approaches overperform the Three-way approaches on many datasets from which we can conclude that Two-way interactions are more efficient for the datasets and specially for those datasets which have more rare relationships. But the problem with the two-way interaction is that they are limited and are not able to represent all kind of relations with entities.\\\\\nTATEC is a latent factor model which is capable of incorporating the high capacity Three-way model with well-controlled two-way interactions and take  the advantage of both of them. Since two-way and three-way models do not use the the same kind of data pattern and do not encode the same kind of information in the embedding. So, in TATEC during first stage they used two different embeddings and then combined and fine-tuned them in the later stage. The scoring function of TATEC is given by $f_{r}(h,t) = f_{r}^1(h,t) + f_{r}^2(h,t) $ which is a linear combination of bi-gram and tri-gram terms, where $f_{r}^1(h,t)$ is a two-way interaction score and $f_{r}^2(h,t)$ is a three-way interaction score. These can be calculates as follows:\\\\\n1) Two-way interactions terms  can be given by:\n\\[  f_{r}^1(h,t) = \\textbf{h}^T\\textbf{r} + \\textbf{t}^T\\textbf{r} + \\textbf{h}^T\\textbf{D}\\textbf{t} \\]\n\\noindent\nwhere D is the diagonal matrix shared across all the different relations and does not depend on input triple and r $\\in$ $\\mathbb{R}^d$ is a vector  that depends on relationships.\\\\\n2) Three-way interactions terms  can be given by:\n\\[f_{r}^2(h,t) = \\textbf{h}^T\\textbf{M}_r\\textbf{t} \\]\n\\noindent\nThe final scoring function of TATEC is given by \n\\[f_{r}(h,t) = \\textbf{h}^T\\textbf{M}_r\\textbf{t} + \\textbf{h}^T\\textbf{r} + \\textbf{t}^T\\textbf{r} + \\textbf{h}^T\\textbf{D}\\textbf{t}\\]\n\\begin{figure}[htb]\n    \\includegraphics[width=.9\\linewidth]{Fig_3.png}\n    \\caption{ Link prediction results  }\n    \\label{fig: Graph}\n\\end{figure}\n\\noindent\nAuthors of  compared this model to the other existing models such as RESCAL , TransE , LFM, SE and, SME for link prediction on FB15k dataset and as a result TATEC performs better than all other available models as shown in  Figure 7.\n\\noindent\nTime complexity and the space complexity of TATEC is same as RESCAL as TATEC extends RESCAL. The time complexity of TATEC is $O(d^{2})$ parameters per relation and the space complexity of $O(nd + md^{2})$ where n is the number of entities and m is the number of relations.", "cites": [5947], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of TATEC by synthesizing the strengths and weaknesses of both two-way and three-way embedding models, referencing prior work like RESCAL, TransE, and SME. It critically evaluates overfitting issues and the limitations of each interaction type, and abstracts the problem into a general modeling framework. However, the synthesis could be deeper by explicitly linking TATEC to the broader trends in hybrid embedding approaches mentioned in the survey abstract."}}
{"id": "e7ff6710-501c-4f0c-9d12-414078581997", "title": "HolE \\cite{nickel2016holographic", "level": "subsubsection", "subsections": [], "parent_id": "35efce91-9f70-4bb9-9152-d6a3a9e9d407", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Semantic Matching Models"], ["subsubsection", "HolE \\cite{nickel2016holographic"]], "content": "}\nHolE stands for Holographic Embedding. HolE tried to overcome the problem of Tensor Product used in RESCAL by using circular correlation. Tensor product uses pairwise multiplicative interactions between feature vectors which results in increase in dimensionality of the representation i.e., $\\mathbb{R}^{d^2}$ thus increase the computational demand.\n\\[ \\textbf{h} \\circ \\textbf{t} = \\textbf{h} \\otimes \\textbf{t} \\in \\mathbb{R}^{d^2} \\]\nWhere \\textbf{a},\\textbf{b} $\\in \\mathbb{R}^d$ are entity embeddings. Tensor products are very rich in capturing the interactions but are computational intensive. On the other hand HolE  use Circular Correlation which can be seen as compression of the Tensor Product. The main advantage of Circular Correlation over Tensor Product is that it won't increase the dimensionality of the representation.\n\\[ \\textbf{h} \\circ \\textbf{t} = \\textbf{h} * \\textbf{t} \\in \\mathbb{R}^d \\]\nWhere * : $ \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ denotes the circular correlation. \n\\[ [\\textbf{h} * \\textbf{t}]_{i} = \\sum_{k=0}^{d-1} [\\textbf{h}_{k}] . \\textbf{t}_{(k+i)mod(d)}\\]\nThe final score of the fact in  HolE is given by matching the compositional vector ($\\textbf{h} * \\textbf{t}$) with the relational representation,i.e.,\n\\[ f_{r}(h,t) = \\textbf{r}^T  (\\textbf{h} * \\textbf{t} ) = \\sum_{i=0}^{d-1} [\\textbf{r}]_{i} \\sum_{k=0}^{d-1}[\\textbf{h}]_k . [\\textbf{t}]_{(k+i)mod(d)} \\]\n\\begin{figure}[htb]\n    \\includegraphics[width=.9\\linewidth]{HolE.png}\n    \\caption{ HolE simple Illustration (It requires only d components)  }\n    \\label{fig: Graph}\n\\end{figure}\n\\noindent\nSo HolE is more efficient as compared to RESCAL or TransE. HolE take $O(d log d)$ parameters per relation and the space complexity of $O(nd + md)$ where n is the number of entities and m is the number of relations. Another advantage of HolE is that Circular Correlation is not commutative (  $\\textbf{h} * \\textbf{t} \\neq \\textbf{t} * \\textbf{h}$ ) thus HolE is able to model asymmetric relations (directed graphs) with compositional representations which is not possible in RESCAL.", "cites": [1929], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the HolE model, synthesizing its core idea from the cited paper and contrasting it with RESCAL and TransE by highlighting efficiency and non-commutativity. It includes a mathematical formulation and evaluates its advantages, indicating some critical and abstract thinking. However, it remains focused on the specific method without broader synthesis or deeper critique of the field."}}
{"id": "1baba919-d3ea-4de4-a06f-3996688565ef", "title": "Enrichment based embedding", "level": "subsection", "subsections": [], "parent_id": "38e76dcb-024f-4fa6-87d0-61d365f976c4", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Knowledge Graph embedding"], ["subsection", "Enrichment based embedding"]], "content": "In the recent times, new emerging research areas are focusing on contextualized embedding. Under this, the entity under the consideration is enriched information from the neighbourhood information. A few notable approaches are Graph attention network (GAT)  based information enrichment. Two methods based on the KGAT , MMGAT  has proposed models to embed contextual information for an entity. Under MMGAT, they proposed model to combine the embedding from multi-modal data with an attention framework, as adopted from GAT's attention framework. Both the frameworks, were using translation model to learn the representation after the enrichment. The new emerging research areas are try to learn the structural information as well as path based information, multi-modal data. There are other research areas in the embedding are: \\textit{Text-enhanced embedding}, \\textit{Logic-enhanced embedding}, \\textit{Image-enhanced embedding}  etc.", "cites": [8963, 180], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions a few approaches like GAT, KGAT, and MMGAT, but does not synthesize or integrate the concepts from the cited papers in a meaningful way. It lacks critical evaluation or comparison of these methods and merely describes the presence of different types of embeddings without identifying broader trends or principles."}}
{"id": "4292cb2e-004c-490b-b3b9-016dd55947d1", "title": "Link Prediction", "level": "subsection", "subsections": [], "parent_id": "fa3f79d6-bce6-4b30-8e32-f70d39a73f98", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Applications of Knowledge graph embedding"], ["subsection", "Link Prediction"]], "content": "The set of edges in a knowledge graph is a subset of \\textit{Entities$\\times$Relations$\\times$Entities}. The link prediction task focuses on finding an entity that can be represented as a fact (edge) together with a given relation and entity i.e., \\textit{(entity, relation, ?)} or \\textit{(?, relation, entity)} where ? refers to the missing entity. For e.g. \\textit{(New Delhi, isCapitalOf, ?)} or \\textit{(?, isCapitalOf, India)}. Link prediction is a way of Knowledge graph augmentation . It deduces missing information from the knowledge graph itself. \n\\noindent\nThe datasets for LP are constructed by sampling from the original knowledge graph. Then, the links removed can be used in validation set or the test set . The structure of such graphs play a vital role \nfor improving the results, multiple source entities making learning effective and multiple destination entities making learning difficult .\n\\noindent\nThe LP models assigns a score to the triplet corresponding to each possible entity to fill the question mark (?). The triplets are then ranked by a function and entity corresponding to the lowest rank is predicted. If the predicted facts in the ranked predictions are already present in the Knowledge graph, they may or may not be excluded while calculating the ranks called raw and filtered rankings respectively . For e.g. if the training knowledge graph contains the fact that \\textit{(Arjuna, isSonOf, Kunti)}, and the test query is \\textit{(?, isSonOf, Kunti)}. The target answer is \\textit{(Yudhishtra, isSonOf, Kunti)} and the system ranks  \\textit{(Arjuna, isSonOf, Kunti)} and then \\textit{(Yudhishtra, isSonOf, Kunti)}. The raw ranking of the triplet \\textit{(Yudhishtra, isSonOf, Kunti)} will be two and filtered ranking will be one. \n\\noindent\nThere are several tie breaking policies that are used by the ranking system. Assigning the \\textit{minimum} or the \\textit{\nmaximum} rank, or a \\textit{random} or the \\textit{average} rank to the targeted entity .\n\\noindent\nThe ranks obtained are used to compute metrics such as \\textit{Mean Rank} (average of all the ranks), \\textit{Mean Reciprocal Rank} (average of the inverse of ranks), or \\textit{Hits@M} (proportion of ranks \\textit{$\\leq$ M}).", "cites": [1167], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of the link prediction task, including its definition, dataset construction, scoring mechanism, and ranking metrics. It includes a brief example and mentions the concept of raw vs. filtered rankings. However, it does not synthesize or integrate ideas from multiple cited papers, nor does it offer critical evaluation or comparison of methods. The abstraction level is minimal, as it does not move beyond describing the general process."}}
{"id": "a756255c-e7ce-41d4-8385-7e19856a9b21", "title": "Recommender Systems", "level": "subsection", "subsections": [], "parent_id": "fa3f79d6-bce6-4b30-8e32-f70d39a73f98", "prefix_titles": [["title", "A Survey of Knowledge Graph Embedding and Their Applications"], ["section", "Applications of Knowledge graph embedding"], ["subsection", "Recommender Systems"]], "content": "Recommender system (RS) assists the user in an environment where multiple options are available by providing a certain ordering of choices that the recommendation algorithm infers. This inference can be based on the similarity of the choices and behaviour pattern of different users. This type of recommendation methods falls into the domain of collaborative filtering methods . \n\\noindent\nThe CF methods suffer from problems of Data sparsity and cold start. Data sparsity arises from the fact that only a small proportion of items are rated by the users and most options have only limited feedback from the users. Cold start problem is the problem of having no historical data about the new users and options. To deal with these problems different types of side information about a user and item are utilized by the RS .\n\\noindent\nKG is utilised for side information in CF. It acts as a heterogeneous graph that represent entities as nodes and relation as edges. The KG connects various entities via latent relationships and also provide explainability in recommendations .\n\\noindent\nThe KG embedding based methods for RS use two modules - Graph embedding and Recommendation Module. The way that these modules are coupled lead to categorization of embedding based methods in a). two stage learning methods, b). joint learning method and c). multi task learning method. \n\\noindent\nTwo stage learning methods first uses graph embedding module to obtain the embeddings using various KG algorithms and then use recommendation module to infer. The advantages of this method lies in its simplicity and scalability but since the two modules are loosely coupled the embeddings might not be suitable for recommendation tasks.\n\\noindent\nJoint learning methods train both the modules in an end to end fashion. Thus, recommendation module guides the training in graph embedding layer. \n\\noindent\nMulti task learning method train the recommendation module with the guidance of KG related task such as KG completion. The primary intuition behind this method is that the bipartite graph of user and item in recommendation task share structures with the corresponding KG entities.", "cites": [5947, 1443], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent overview of how KG embedding is applied in recommender systems by integrating the concepts of side information, heterogeneous graphs, and module coupling strategies (two stage, joint, multi-task learning). It connects ideas from both cited papers to form a structured narrative. However, it lacks deeper critical evaluation of the methods and does not offer a highly original or meta-level synthesis of the field."}}
