{"id": "71ae5584-b677-4f23-8d21-ff23fc783d4f", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "cc73abb4-e745-4f3e-9227-7cfc6839570d", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Introduction"]], "content": "Graph-structured networks arise naturally in several real-world complex systems, including social networks, biological networks, knowledge graphs, and finance, whose interactions between nodes allow the understanding of the structural information of these domains~. Therefore, graph-aware learning tasks play a key role in machine learning and network science literature, and scalable approaches to deal with these high-dimensional non-Euclidean data have been explored to address the computational challenges associated with graph data-driven analytics and inference. Embedding graphs in low-dimensional vector spaces have been applied to extract features from networks and encoding topological and semantic information, and many researchers have been using these network representation learning approaches for several applications, including node classification and clustering, link prediction, and network visualization~.\nPrevious work on network representation learning has focused on static graphs, either representing existing connections at a fixed moment (i.e., a graph snapshot), or node interactions over time that are aggregated into a single static graph~. However, several real networks display dynamic behavior, including nodes and edges being added or removed from the system~, labels and other properties changing over time~, and diffusion in the network~. The network temporal correlations are lost during the aggregating process and, in this sense, approaches to develop embedding methods for dynamic networks have been proposed over the past few years. These efforts have improved tasks such as link prediction~ and node classification~ over time, while enabling novel applications, including event prediction~, anomaly detection~ and diffusion prediction~.\nSeveral challenges arise when developing an approach to embed dynamic graphs, as (i) how to model the time domain, i.e. discrete-time or continuous-time, (ii) which dynamic behaviors are desired to be captured, and (iii) which temporal granularity will be represented in the vector space, i.e. the same granularity as the dataset, or a coarser granularity summarizing dynamics in a finer timescale. Considering the increasing number of studies proposing dynamic graph embedding techniques, these discussions are becoming more important to advance the comprehension of dynamic network representation learning. Therefore, in this survey, we overview the problem of embedding dynamic graphs, discussing its fundamental aspects and the recent advances that have been made so far. We introduce the formal definition of dynamic graph embedding, discussing different dynamic network models whose representations may be extracted, and introducing a novel taxonomy for the problem settings, i.e. embeddings input and output. Moreover, we explore and classify different dynamic behaviors that may be captured by embeddings, describe existing methods, discuss their similarities and differences, and propose a detailed taxonomy based on algorithmic approaches.\nTo the best of our knowledge, a few attempts have been made so far to survey dynamic graph embeddings. Kazemi et al.~ focus on recent representation learning techniques for dynamic graphs by using an encoder-decoder framework. Xie et al.~ propose a taxonomy based on algorithmic approaches to encode graphs. Additionally, Skarding et al.~ survey how the dynamic network topology can be modeled using dynamic graph neural networks. Our work is different from the aforementioned surveys since we discuss different dynamic network models that have been or may be used for embedding, in addition to detecting temporal behaviors in networks that can be captured. Moreover, we extend dynamic graph embedding techniques taxonomy, encompassing methods based on graph kernels, temporal point processes, and agnostic methods. In this sense, this survey has the following contributions:\n\\begin{compactitem}\n    \\item A taxonomy of dynamic graph embedding based on problem settings, extending graph embedding input and output to handle temporal heterogeneity (i.e. timestamps with labels, classifying network behavior over time) and temporal embeddings (i.e. different temporal granularities to represent in the low-dimensional vector space);\n    \\item A discussion about different dynamic behaviors in networks that embedding models may capture, including topological evolution (concerning both node and edge addition or removal), feature evolution (regarding changes of nodes/edges features or labels over time) and processes on networks (diffusion and global role of nodes and its evolution). Furthermore, we also bring some perspectives about temporal point processes on networks.\n    \\item A detailed analysis of embedding techniques for dynamic graphs, focused on a classification concerning their algorithmic approaches, comparing different methods proposed in the literature and discussing their similarities, differences and other particularities;\n    \\item The categorization, according to the topological structure, of several dynamic graph embedding applications, focused on: node related tasks, edge related tasks, node, and edge related tasks, and graph-related tasks;\n    \\item A discussion of future research directions in the area in terms of problem settings, solution techniques, and modeling, in addition to applications and representation learning on generalized graphs~(i.e. hypergraphs and higher-order graphs).\n\\end{compactitem}\nThe remainder of this survey is organized as follows. In Section~\\ref{sec:fundamentals}, we introduce the fundamentals behind the embedding of dynamic graphs, reviewing static graph embedding, defining the problem of dynamic graph embedding, presenting different dynamic graph models explored in the embedding scenario, along with other problem settings, including the dynamic graph embedding input and output as well as the dynamic behaviors that may be captured. In Section~\\ref{sec:tech}, we categorize the literature based on the embedding techniques, unraveling insights behind each paradigm and we provide a detailed comparison of different techniques. After that, we present in Section~\\ref{sec:apps} some concrete examples of applications enabled by dynamic graph embedding methods discussed in Section \\ref{sec:tech}, allowing the reader to better grasp the practical utility of these methods. Finally, our conclusions are presented in Section~\\ref{sec:conc}, alongside some discussions on potential future research directions in the field of dynamic graph embedding.", "cites": [7007, 7231, 219, 215, 7232], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The introduction section demonstrates strong synthesis by connecting multiple cited papers to define the scope and importance of dynamic graph embedding. It also presents a novel taxonomy and highlights key challenges, showing abstraction beyond individual papers. While it provides some critical distinctions between existing surveys, it could delve deeper into evaluating limitations of the cited works for a more comprehensive analysis."}}
{"id": "89438e2f-496d-4586-9af5-e51692d39394", "title": "Graphs and Static Graph Embedding", "level": "subsection", "subsections": [], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Graphs and Static Graph Embedding"]], "content": "\\label{subsection:staticgraphembedding}\nA \\textbf{graph} $G = (V,E)$ is a mathematical structure, where $V = \\{v_{1}, ..., v_{N}\\}$ is a finite set of $N$ nodes (vertices), and $E \\subseteq \\{(v_{i},v_{j})\\, \\vert \\,(v_{i},v_{j}) \\in V\\times V\\}$ is a finite set of unordered pairs of vertices, whose elements $e_{ij} = (v_{i},v_{j})$ are called edges (links). The \\textbf{adjacency matrix} $A$ of a graph is an $N \\times N$ matrix whose element $A_{ij} = 1$ if edge $e_{ij} \\in E$, or $A_{ij} = 0$ otherwise. A \\textbf{directed graph} is a graph in which an edge $e_{ij} \\in E$ is an ordered pair, i.e. the edge $e_{ij}$ is oriented. Otherwise, the graph is \\textbf{undirected}. A \\textbf{weighted graph} is a graph in which a weight function $w: E \\rightarrow \\mathbb{R}$ is assigned to it. Each edge has a weight associated with it, and it is possible to define a \\textbf{weight matrix} $W$ such that $W_{ij} = w(e_{ij})$. Otherwise, the graph is \\textbf{unweighted}. A \\textbf{homogeneous graph} is a graph in which the number of node types $\\mathcal{L}^{n}$ and the number of edge types $\\mathcal{L}^{e}$ is 1, i.e. $\\vert\\mathcal{L}^{v}\\vert = \\vert\\mathcal{L}^{e}\\vert = 1$, and every node in $G$ belongs to a single node category and every edge belongs to a single edge category. A \\textbf{heterogeneous graph} is a graph in which $\\vert\\mathcal{L}^{v}\\vert > 1$ or $\\vert\\mathcal{L}^{e}\\vert > 1$.\nDifferent ways to define \\textbf{proximity} or \\textbf{similarity} between nodes in a graph may be conceived~. The \\textbf{first-order proximity} $S_{ij}^{(1)}$ between nodes $v_{i}$ and $v_{j}$ is the weight of the edge $e_{ij}$, i.e., $W_{ij}$. The \\textbf{second-order proximity} $S_{ij}^{(2)}$ between nodes $v_{i}$ and $v_{j}$ is a similarity between $v_{i}$'s neighbourhood $S_{i}^{(1)}$ and $v_{j}$'s neighbourhood $S_{j}^{(1)}$ given by some defined metric, where $S_{i}^{(1)} = [S_{i1}^{(1)}, ..., S_{iN}^{(1)}]$ and $S_{j}^{(1)} = [S_{j1}^{(1)}, ..., S_{jN}^{(1)}]$. \\textbf{Higher-order proximities} can be defined as well, including the Katz centrality~, which is a weighted summation over the paths between two vertices in the graph whose weight is an exponential decay function of its length, and the Adamic/Adar Index~, which counts the number of vertices connecting two nodes taking into account a weight depending on the reciprocal of the neighbor's degree.\nThe central idea behind graph embedding lies in learning a mapping that embeds nodes, edges, subgraphs, or even entire graphs, in a low-dimensional vector space, where the embedding dimension is expected to be much lower than the total number of nodes in the network. More specifically, given a graph $G = (V, E)$, and a predefined embedding dimension $d$, such that $d \\ll |V|$, the problem of graph embedding is to map $G$ into a $d$-dimensional space, in which graph properties are preserved as much as possible, i.e. topology and similarity measures~. Based on the output of the graph embedding, four categories may be defined: (i) node embedding, where vector embeddings are learned for each node; (ii) edge embedding, where edges are mapped into the embedding space; (iii) substructure embedding, in which subgraphs (i.e. clusters, communities, graphlets, ...) are represented in the vector space; and (iv) whole-graph embedding, i.e. an entire graph is mapped into a single vector.~ (see Fig.~\\ref{fig:staticgraphembedding}).\n\\begin{figure}[tp]\n    \\centering\n    \\begin{minipage}[t]{0.3\\textwidth}\n    \t\\begin{subfigure}[b]{\\textwidth}\n         \\centering\n         \\includegraphics[width=0.7\\linewidth]{figs/FigGraphEmbedding.pdf}\n         \\caption{Graph $G$}\n         \\vspace*{-3cm}\n         \\label{fig:graph}\n     \\end{subfigure}\n     \\hfill\n    \\end{minipage}\n    \\begin{minipage}[t]{0.65\\textwidth}\n    \t\\begin{subfigure}[b]{0.35\\textwidth}\n        \\centering\n         \\includegraphics[width=\\linewidth]{figs/FigNodeEmbedding.pdf}\n         \\caption{Node Embedding}\n         \\label{fig:nodeembedding}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.35\\textwidth}\n         \\centering\n         \\includegraphics[width=\\linewidth]{figs/FigEdgeEmbedding.pdf}\n         \\caption{Edge Embedding}\n         \\label{fig:edgeembedding}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.35\\textwidth}\n         \\centering\n         \\includegraphics[width=\\linewidth]{figs/FigSubstructureEmbedding.pdf}\n         \\caption{Substructure Embedding}\n         \\label{fig:substructureembedding}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.35\\textwidth}\n         \\centering\n         \\includegraphics[width=\\linewidth]{figs/FigWholeGraphEmbedding.pdf}\n         \\caption{Whole-Graph Embedding}\n         \\label{fig:wholegraphembedding}\n     \\end{subfigure}\n    \\end{minipage}\n     \\caption{A toy example of embedding a graph into 2D space taking into account different granularities. (a) Sample network used as a reference for graph embedding, where the different node colors depict different substructures, and different edge colors depict intra-substructure and inter-substructure connections. (b-e)~Different static graph embedding outputs, as described by Cai et al.~, where $d = 2$. Note that the colors refer to the substructures and connections displayed in~(a).}\n     \\label{fig:staticgraphembedding}\n\\end{figure}\nStatic graphs embedding taxonomies have been proposed in the past few years~. Graph embedding based on \\textbf{matrix factorization} represents some graph similarity in the form of a matrix and factorizes this matrix to obtain a node embedding. The problem of graph embedding is treated as a structure-preserving dimensionality reduction problem, which assumes the input data lie in a low dimensional manifold. Approaches based on \\textbf{deep learning} apply deep neural architectures on graphs, including autoencoders (AEs), convolutional neural networks (CNNs), and variational autoencoders (VAEs). \\textbf{Random walk approaches} generate node sequences from a graph to create contexts for each node, then applying techniques from natural language processing for learning embeddings. They try to preserve higher-order proximity between nodes by maximizing the probability of occurrence of subsequent nodes in fixed-length random walks, using neural language models, such as SkipGram. Cai et al.~ further suggest other paradigms, such as \\textbf{edge reconstruction based optimization}, which learns representations that directly optimize either edge reconstruction probability or edge reconstruction loss; \\textbf{graph kernel-based methods}, which decompose the graph into atomic substructures~(as graphlets and subtrees) and build a vector using these features; and \\textbf{generative models}, which specify the joint distribution of the input features and the class labels conditioned on a set of parameters.\nSeveral further discussions about static graph embeddings were presented in other surveys and reviews~, along with some works concerning knowledge graph embedding, specifying their analysis to tasks including knowledge graph completion and relation extraction~. Moreover, Goyal developed GEM~, an open-source Python library that provides a framework for graph embedding implementation, and Grattarola and Alippi have presented Spektral~, another open-source Python library for building graph neural networks with Keras API and TensorFlow 2, handling tasks such as node classification, link prediction, and graph generation.", "cites": [7007, 476, 219, 215, 7233, 217, 212], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual overview of graphs and static graph embedding, incorporating definitions and methods from cited surveys. While it organizes the information coherently, it lacks deeper analysis or comparison of the approaches. Some abstraction is attempted through categorization of embedding types, but the discussion remains largely descriptive with minimal critical evaluation."}}
{"id": "0b71ddce-a020-4f6b-8c3f-423c2354c133", "title": "Dynamic Graphs", "level": "subsection", "subsections": [], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graphs"]], "content": "\\label{subsection:dynamicgraphs}\nThere are different mathematical formulations to describe interactions between nodes over the lifetime of a system~. A possible definition is to describe a dynamic graph by a mathematical structure $\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathcal{T})$, where $\\mathcal{V} = \\{V(t)\\}_{t \\in \\mathcal{T}}\\,$ is a collection of node sets over time, $\\mathcal{E} = \\{E(t)\\}_{t \\in \\mathcal{T}}\\,$ is a collection of edge sets over time, and $\\mathcal{T}$ is the time span. For each $t \\in \\mathcal{T}$, it is possible to define a graph snapshot $G(t) = (V(t),E(t))$, i.e. a static graph representing a fixed timestamp $t$ of the dynamic graph. Adjacency matrix $A(t)$, weight matrix $W(t)$ and similarity matrix $S(t)$ are now time-dependent, and can be calculated for each snapshot $G(t)$, as well as node types $\\mathcal{L}^{n}(t)$ and edge types $\\mathcal{L}^{e}(t)$.\nCasteigts et al.~ alternatively defined a dynamic graph as $\\mathcal{G} = (V,E,\\mathcal{T},\\rho_{v},\\rho_{e})$, where $V$ is a node set containing every node that is present in the network at any given time $t \\in \\mathcal{T}$, $E$ is an edge set defined similarly, and further defining a node presence function $\\rho_{v}: V \\times \\mathcal{T} \\rightarrow \\{0,1\\}$, indicating whether a given node $v \\in V$ is available at a given time $t \\in \\mathcal{T}$, and an edge presence function $\\rho_{e}: E \\times \\mathcal{T} \\rightarrow \\{0,1\\}$, specifying if a given edge $e \\in E$ exists at a timestamp $t \\in \\mathcal{T}$.\nFigure~\\ref{fig:tvg} depicts a dynamic network as a time-varying graph, containing 9 nodes along lifetime $\\mathcal{T} = [0,7)$. It is noteworthy that, at the beginning of the network lifetime, only nodes A, B, C, and D are present, as well as links (A,B), (A,C), (B,C), and (B,D). As time passes, new nodes and edges arrive, even as nodes and edges are removed from the system. In the end, we have nodes B, E, F, G, H, and I, and links (B,E), (B,F), (E,G), and (H,I).\nThe formulations we described above are sufficient for understanding the dynamic graph embedding methods we review in this paper. It is important to mention that dynamic graphs can present even more complex temporal patterns, such as latency~ (i.e. nodes/edges not arising instantaneously in the network, instead of taking a finite time interval to be established) and spatial-temporal edges~ (i.e. a node at a given timestamp connected to another node at another timestamp). In Sec.~\\ref{sec:conc}, we propose future directions for embedding dynamic graphs concerning these properties.\n\\begin{figure}[tbh]\n    \\centering\n    \\includegraphics[scale=0.45]{figs/FigTVG.pdf}\n    \\caption{A representation of a small dynamic network, showing edge presence (continuous intervals above edges) and node presence (bold continuous intervals next to nodes) intervals.}\n    \\label{fig:tvg}\n\\end{figure}", "cites": [477], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear, descriptive overview of dynamic graph formulations, integrating one cited paper (Casteigts et al.) to present alternative definitions. However, synthesis is limited to contrasting two formulations without deeper integration of broader ideas. There is minimal critical analysis or evaluation of the approaches, and abstraction is modest, focusing on the structural definitions rather than overarching principles or patterns across works."}}
{"id": "a4018741-5881-499c-bbf7-4eec27b0c468", "title": "Dynamic Graph Modeling", "level": "subsection", "subsections": ["ea760a39-ef3c-4a5b-a264-8163ea5030dd", "dc64abc3-d8c3-4b7e-bd32-8eca82eb0efe", "107b73d9-ba66-497a-9196-afe266d65893"], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Modeling"]], "content": "\\label{subsection:dynamicgraphmodels}\nOne of the first aspects to be considered when modeling a dynamic network is to define its life span $\\mathcal{T}$. Two different approaches may be adopted to model the system's time domain: \\textbf{discrete-time approaches}, where $\\mathcal{T}$ is a discrete set, hence the evolution of a dynamic graph can be described by a sequence of static graphs, with a fixed timestamp; and \\textbf{continuous-time approach}, where $\\mathcal{T}$ is a continuous set, therefore the evolution is modeled at a finer temporal granularity to encompass different events in real time~. Computationally, dynamic graph models assuming discrete-time domain are easier to manipulate. Most of the existing embedding methods are based on this approach~. However, some authors have proposed to model more sophisticated phenomena, such as stochastic events, to leverage applications such as event time prediction (i.e. predict when an edge or a node is created or removed from a network)~. Therefore, these approaches must rely on continuous-time lifespan to capture temporal evolution at an appropriate granularity. We briefly discuss some dynamic graph models covered by embedding methods presented in this survey.", "cites": [7232], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core concepts of dynamic graph modeling by introducing a distinction between discrete-time and continuous-time approaches and linking them to computational and application considerations. It provides some abstraction by framing the problem in terms of time-domain modeling and temporal granularity, but the critical analysis is limitedâ€”there are no explicit evaluations or comparisons of the cited methods. The discussion is insightful but remains at a moderate level of depth."}}
{"id": "ea760a39-ef3c-4a5b-a264-8163ea5030dd", "title": "Graph Snapshots", "level": "subsubsection", "subsections": [], "parent_id": "a4018741-5881-499c-bbf7-4eec27b0c468", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Modeling"], ["subsubsection", "Graph Snapshots"]], "content": "This model represents a dynamic graph as a list of static graphs, i.e. $\\mathcal{G} = \\{{G}(t_{0}), ..., G(t_{N_{S}-1})\\}$, where $G(t_{k}) = (V(t_{k}), E(t_{k}))$ is a static graph with timestamp $t_{k}$ ($k \\in \\{0,...,N_{S}-1\\})$, $N_{S}$ is the number of snapshots, $V(t_{k})$ is the node set at timestamp $t_{k}$ and $E(t_{k})$ is the edge set including all edges within the period $[t_{k},t_{k+1})$~. Most of the methods for embedding dynamic graphs manage this model as the input, either by adopting directly a sequence of successive state sub-graphs that represent the network in a discrete way as time passes~, or splitting the time domain into non-overlapping windows of fixed duration, establishing a static graph for each window~.", "cites": [7232, 7234], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the graph snapshot model for dynamic graphs, referencing two papers but without synthesizing their contributions or contrasting them meaningfully. It lacks critical evaluation of the cited methods and offers only minimal abstraction by briefly mentioning common input strategies without deeper generalization."}}
{"id": "dc64abc3-d8c3-4b7e-bd32-8eca82eb0efe", "title": "Difference Network Models", "level": "subsubsection", "subsections": [], "parent_id": "a4018741-5881-499c-bbf7-4eec27b0c468", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Modeling"], ["subsubsection", "Difference Network Models"]], "content": "In many real problems, the number of edges inserted or removed at any given time is much smaller than the total number of edges. i.e. the topological evolution is sparse~. These models representing these network changes take as input an initial graph $G_{t_{0}}$, and a list of adjacency matrix changes $\\Delta \\mathcal{A} = \\{\\Delta A(t_{1}), ..., \\Delta A(t_{N_{R}-1})\\}$, where $\\Delta A(t_{k}) = A(t_{k}) - A(t_{k-1})$, and $N_{R}$ is the total number of recorded timestamps. This definition may be extended for other similarity matrices~, and difference networks may be divided into a link formation network (concerning positive values of adjacency matrix change) and a link dissolution network (regarding negative values of adjacency matrix change)~. Note that these models do not handle nodes being added or removed from the network, as they rely on matrices with a fixed dimensionality.", "cites": [7231, 7235], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of difference network models and their assumptions, such as sparsity of topological evolution and fixed node sets. It cites two papers but does not integrate or synthesize their contributions into a broader framework. There is minimal critical analysis or abstraction beyond specific examples."}}
{"id": "107b73d9-ba66-497a-9196-afe266d65893", "title": "Continuous-Time Network Models", "level": "subsubsection", "subsections": [], "parent_id": "a4018741-5881-499c-bbf7-4eec27b0c468", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Modeling"], ["subsubsection", "Continuous-Time Network Models"]], "content": "Continuous-time approaches may include timestamped edges (edges with the information about the time they were created, or the time intervals concerning their existence in the network)~ and link streams (a list of node interactions over time)~. Events in the network, such as the creation and removal of nodes and edges, may occur in any time $t \\in \\mathcal{T}$, and maybe instantaneous (i.e. much faster than the typical temporal granularity of the system) or may be assigned with a latency~. Several dynamic graph embedding methods rely on continuous-time networks, either by modeling timestamped edges as stochastic point processes~ or leveraging link streams~. Furthermore, node arrival and removal may be included by these network models, as proposed in the literature by stream graphs~ and appeared in some embedding techniques~.", "cites": [477], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of continuous-time network models by listing key ideas such as timestamped edges, link streams, and stream graphs, and cites relevant papers. However, it lacks synthesis of these ideas into a broader framework and offers no critical evaluation or comparison of the approaches. The abstraction is minimal, focusing primarily on definitions rather than overarching principles."}}
{"id": "df987880-8b2e-4919-a9d2-19f1a6d24539", "title": "Temporal Point Processes on Graphs", "level": "subsection", "subsections": [], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Temporal Point Processes on Graphs"]], "content": "\\label{subsec:temporalpointprocess}\nAs discussed above, nodes and edges network churn may be modeled by stochastic events in a continuous-time domain, normally as stochastic point processes, random processes whose realization is comprised of discrete events in a continuous time. A \\textbf{temporal point process} is a point process that can be represented as a counting process $N(t)$, recording the number of events up to time $t$, thus being useful for modeling sequential asynchronous discrete events occurring in continuous time~. The \\textbf{conditional intensity function} $\\lambda(t)$ characterizes a temporal point process such that $\\lambda(t)\\,\\Delta t$ is the conditional probability of observing an event in the tiny window $[t,t+\\Delta t)$ given the network history, i.e., all events before $t$, and only one event can happen in this tiny interval $\\Delta t$. Similarly, a \\textbf{survival function} $S(t)$ determines the conditional probability that no event happens during a time window $[t,t+\\Delta t)$ given the network history, and the \\textbf{conditional density} $f(t) = \\lambda(t) S(t)$ for an event that occurs at time $t$ is further defined as well.\nThe functional form of the intensity $\\lambda(t)$ is often designed to capture the phenomena of interests, some of them include Poisson Process, Hawkes Process, Self-Correcting Process, Power Law, or Rayleigh Process~. Many dynamic graph embedding techniques consider that interactions between nodes are stochastic processes whose probabilities depend on the topological structure of the network, and node features (if applicable) at each timestamp~.", "cites": [7237, 7236, 7238], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear synthesis of temporal point processes as a method for modeling dynamic graph behavior, integrating concepts from the cited papers into a coherent explanation. It abstracts the role of point processes in dynamic graph embedding by introducing general definitions and functional forms. However, it lacks critical evaluation of the methods or limitations, merely highlighting their relevance without deeper comparative or evaluative analysis."}}
{"id": "bda1a2d5-2fcf-48fa-b44d-73315575b72a", "title": "Dynamic Graph Embedding Input", "level": "subsection", "subsections": ["f4317874-3ea0-4d28-9969-4853957212bc", "f38f18d5-9f93-4e14-aaf2-e6b5009022f2", "a6a830ab-bba1-4081-a49c-966816cc0019", "73d84c74-5e33-4f4c-a285-97c1978517b8"], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Embedding Input"]], "content": "In addition to the dynamic network modeling, discussed in Sec.~\\ref{subsection:dynamicgraphmodels}, dynamic graphs can be (i) homogeneous, in which only topological information over time is available, (ii) heterogeneous, in which either nodes, edges (topological heterogeneity) or timestamps (temporal heterogeneity) are assigned with labels, (iii) attributed (or with additional information), where nodes and edges may hold several different features, and (iv) constructed from non-relational data (see Fig.~\\ref{fig:input}). This proposed taxonomy extends Cai et al.~, who encompassed static graph embedding input considering static networks, without leveraging dynamic aspects neither handling the difference between topological and temporal heterogeneity. In the following, we discuss each dynamic graph embedding input shown in Figure~\\ref{fig:input}.\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.5\\linewidth]{figs/FigDynGraphEmbInput.pdf}\n    \\caption{The proposed taxonomy for dynamic graph embedding input, an extension of Cai et al.~ to encompass dynamic networks and to consider topological heterogeneity (similar to static networks) and temporal heterogeneity (i.e. timestamps having labels).}\n    \\label{fig:input}\n\\end{figure}", "cites": [215], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a synthesis by extending the taxonomy from Cai et al. to include dynamic aspects, which shows some integration of prior work with new ideas. It offers limited critical analysis, as it mainly describes the extension without evaluating the strengths or limitations of the original framework. The abstraction is moderate, as it identifies broader input categories (homogeneous, heterogeneous, attributed, non-relational) that apply to dynamic graphs, though deeper meta-level insights are not elaborated."}}
{"id": "f4317874-3ea0-4d28-9969-4853957212bc", "title": "Dynamic Homogeneous Graph", "level": "subsubsection", "subsections": [], "parent_id": "bda1a2d5-2fcf-48fa-b44d-73315575b72a", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Embedding Input"], ["subsubsection", "Dynamic Homogeneous Graph"]], "content": "Undirected and unweighted homogeneous graphs are widely used as dynamic graph embedding inputs due to their simplicity and to handle only basic structural information over time~. Several embedding methods, however, are proposed to handle weighted~ and directed dynamic graphs~.", "cites": [1000, 7232], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by briefly mentioning two papers that deal with weighted and directed dynamic graphs, but it does not integrate or connect their ideas meaningfully. There is little critical analysis of the approaches or their limitations, and the abstraction is limited to the surface-level description of input types without identifying broader patterns or principles."}}
{"id": "f38f18d5-9f93-4e14-aaf2-e6b5009022f2", "title": "Dynamic Heterogeneous Graph", "level": "subsubsection", "subsections": [], "parent_id": "bda1a2d5-2fcf-48fa-b44d-73315575b72a", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Embedding Input"], ["subsubsection", "Dynamic Heterogeneous Graph"]], "content": "Embedding methods to handle topological heterogeneity, i.e. nodes or edges having labels, are usually concerned with node and edge classification~. Nevertheless, graph snapshots may have labels, characterizing different global behavior~ and describing another type of heterogeneity, which we have named \\textbf{temporal heterogeneity}.", "cites": [7238], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the concept of temporal heterogeneity and mentions a cited paper, but it lacks synthesis, critical evaluation, or abstraction. It does not integrate multiple sources, analyze their contributions, or generalize to broader principles, instead providing only a minimal description of the topic."}}
{"id": "a6a830ab-bba1-4081-a49c-966816cc0019", "title": "Dynamic Graph with Additional Information", "level": "subsubsection", "subsections": [], "parent_id": "bda1a2d5-2fcf-48fa-b44d-73315575b72a", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Embedding Input"], ["subsubsection", "Dynamic Graph with Additional Information"]], "content": "Additional attributes may be assigned to nodes, such as a set of numerical or categorical features. It is possible to define a time-dependent node feature matrix $F(t) \\in \\mathbb{R}^{N \\times f}$, where $f$ is the number of additional node features, and learn representations leveraging these features in addition to their topological structure~. Although an edge feature matrix would be defined as well, its usage is much less common.", "cites": [7231], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the concept of dynamic graphs with additional node features but fails to synthesize or connect ideas from the cited paper in a meaningful way. It lacks critical evaluation of the cited work and does not abstract or generalize beyond the specific mention of node features."}}
{"id": "73d84c74-5e33-4f4c-a285-97c1978517b8", "title": "Dynamic Graph Constructed from Non-Relational Data", "level": "subsubsection", "subsections": [], "parent_id": "bda1a2d5-2fcf-48fa-b44d-73315575b72a", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Graph Embedding Input"], ["subsubsection", "Dynamic Graph Constructed from Non-Relational Data"]], "content": "Non-relational time series data can be transformed into a dynamic graph by defining a similarity measure between two data instances, and constructing a similarity matrix $S(t)$ afterward. Several papers use this step as an intermediate to learn vector representations from this constructed graph to support some task-driven application, such as traffic forecasting~, predicting bike-sharing demand~, predicting social events~ and missing label classification on videos~.", "cites": [7239, 25], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions how non-relational data can be transformed into a dynamic graph and cites two papers as examples, but it does not synthesize their approaches or connect them to a broader theme. It lacks critical evaluation or comparison of the methods and offers only a surface-level description of the idea with minimal abstraction."}}
{"id": "d818651f-3683-4cad-9a3c-7a60cb94c5f4", "title": "Problem Formulation and Output for Dynamic Graph Embedding", "level": "subsection", "subsections": ["49101fcb-04b3-46cf-9018-c3362eaab443", "d77bce95-cde0-465f-93c7-310f9099a3d3"], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Problem Formulation and Output for Dynamic Graph Embedding"]], "content": "\\label{subsection:dynamic_graph_embedding}\nIt is important to mathematically formulate dynamic graph embedding to understand its outputs. Given a dynamic graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathcal{T})$, where $\\mathcal{V} = \\{V(t)\\}_{t \\in \\mathcal{T}}$ and $\\mathcal{E} = \\{E(t)\\}_{t \\in \\mathcal{T}}$, and an embedding dimension $d$, the problem of embedding dynamic graph is regarded as learning how to map $\\mathcal{G}$ into a $d$-dimensional vector space over time, in which both topological information and temporal dependencies of the network are captured, either by learning representations able to reconstruct the dynamic graph $\\mathcal{G}$, to predict the behavior of the network at timestamps outside the lifespan $\\mathcal{T}$, or to directly handle a task-driven application such as node classification. When the graph topology evolves, two possible interpretations are possible for the evolution of embeddings: (i)~the vector representations move along the embedding space, making it possible to trace the trajectory of each node; or (ii)~the embedding space itself evolves in time, thus being possible to learn mappings between embedding spaces in consecutive timestamps~.\nThe time-domain of vector representations and the network does not need to be identical, i.e., $\\mathbb{T} \\ne \\mathcal{T}$. For instance, a dynamic graph may have daily information about interactions between users in a social network, but the network analytics and inference are more interested in capturing weekly or even monthly features. Hence, even though the network life span is given by daily timestamps, vector representations are extracted for a coarser temporal granularity.\nTherefore, to define different dynamic graph embedding outputs, it is important to separate between (i) \\textbf{topological embedding}, which is similar to the definitions for static graph embedding~ and concerns node embedding, edge embedding, substructure embedding, and graph snapshot embedding, all of them over time, and (ii) \\textbf{temporal embedding}, regarding the relation between network temporal granularity given by $\\mathcal{T}$ and embedding temporal granularity given by $\\mathbb{T}$. The complete classification we propose is shown in Figure~\\ref{fig:output} and is further discussed in the following topics.\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.45\\linewidth]{figs/FigDynGraphEmbOutput.pdf}\n    \\caption{Dynamic graph embedding output taxonomy proposed in this survey.}\n    \\label{fig:output}\n\\end{figure}", "cites": [215], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear and structured formulation of dynamic graph embedding, introducing a novel taxonomy for outputs and distinguishing between topological and temporal embedding. While it synthesizes ideas reasonably well and builds on the cited survey, it lacks in-depth critical evaluation of specific approaches. The abstraction level is strong, as it generalizes concepts to propose a framework for understanding the temporal and topological dimensions of embedding outputs."}}
{"id": "d77bce95-cde0-465f-93c7-310f9099a3d3", "title": "Topological Embedding", "level": "subsubsection", "subsections": [], "parent_id": "d818651f-3683-4cad-9a3c-7a60cb94c5f4", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Problem Formulation and Output for Dynamic Graph Embedding"], ["subsubsection", "Topological Embedding"]], "content": "\\label{subsubsec:topological}\nEmbedding topological properties of a dynamic graph is similar to Fig.~\\ref{fig:staticgraphembedding}, and the main difference is the coupling between the temporal embedding discussed above and each graph structure, as presented in the right branch of Fig.~\\ref{fig:output}. WLoG, we are considering $\\mathcal{V} = V \\times \\mathcal{T}$ and $\\mathcal{E} = E \\times \\mathcal{T}$ to simplify notations.\n\\begin{definition}\nA \\textbf{dynamic node embedding} is a mapping $\\nu_{n}: V \\times \\mathcal{T} \\to \\mathbb{R}^{d} \\times \\mathbb{T}$. Therefore, each node at time $t \\in \\mathbb{T}$ is represented as a vector in a low dimensional space.\n\\end{definition}\nNode embedding over time is useful for several applications such as time-dependent node classification, network clustering evolution, and link prediction. It also allows one to track node trajectories in the embedding space and extract information about node behavior and roles in the network.~.\n\\begin{definition}\nA \\textbf{dynamic edge embedding} is a mapping $\\nu_{e}: E \\times \\mathcal{T} \\to \\mathbb{R}^{d} \\times \\mathbb{T}$. Therefore, each edge at time $t \\in \\mathbb{T}$ is represented as a vector in a low dimensional space.\n\\end{definition}\nIn addition to edge embedding, a few methods map both edges and nodes, in particular for embedding dynamic knowledge graphs or user-item interaction graphs~.\n\\begin{definition}\nA \\textbf{dynamic substructure embedding} is a mapping $\\nu_{h}: S(\\mathcal{G}) \\times \\mathcal{T} \\to \\mathbb{R}^{d} \\times \\mathbb{T}$, where $S(\\mathcal{G})$ is a set of induced subgraphs in $G$ at each time $t \\in \\mathcal{T}$. Therefore, each substructure at time $t \\in \\mathcal{T}$ is represented as a vector in a low dimensional space.\n\\end{definition}\nSeveral dynamic graph embedding methods generalize traditional node or link prediction tasks to consider joint prediction over larger $k$-node induced subgraphs~ and graphlets~.\n\\begin{definition}\nA \\textbf{snapshot embedding} is a mapping $\\nu_{\\mathcal{G}}: \\mathcal{G} \\times \\mathcal{T} \\to \\mathbb{R}^{d} \\times \\mathbb{T}$, where $\\mathcal{G} = \\{G_{t}\\}_{t \\in \\mathcal{T}}$. Hence, each graph snapshot at time $t \\in \\mathcal{T}$ is represented as a vector in low dimensional space.\n\\end{definition}\nSnapshot embeddings are useful for tracking network behavior over time, when the topological structure of the network is related to some emerging property or global interpretation of node interactions~.", "cites": [7237, 7240], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of different types of dynamic graph embeddings (node, edge, substructure, and snapshot) with mathematical definitions, but it lacks synthesis across the cited papers. It mentions applications and some methods but does not connect these ideas to the cited works in a meaningful way. There is no critical analysis or broader abstraction beyond the definitions and use cases."}}
{"id": "cf7e4388-7913-4587-beef-fb1c0236e00d", "title": "Feature Evolution", "level": "subsubsection", "subsections": [], "parent_id": "e0d8f8a9-c6dd-4b47-b621-411172538fbb", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Dynamic Behaviors"], ["subsubsection", "Feature Evolution"]], "content": "In many problems involving heterogeneous graphs, it is assumed nodes have fixed labels. However, it is also possible to observe a \\textbf{label evolution}: In a citation network, an author may have as its main research area a different topic compared to previous years. Such changes are linked in some way to the topological evolution of the network. In node classification tasks, each node in a graph has a class label, hence it is possible to predict the class label for the nodes in a graph $G(t_{k})$ using previous graphs $G(t_{0}),...,G(t_{k-1})$~. \nEven more, in several real-world networks, nodes and edges may have rich attributes (i.e. additional information) that are changing over time in addition to the network structure and the topology may influence attribute modification. Therefore, embedding methods may also capture \\textbf{information evolution}~. Furthermore, edge weights may also change over time in weighted networks, and their changes may be handled by embedding methods~.", "cites": [7235, 7240], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of feature evolution in dynamic graphs and briefly mentions how node labels and attributes can change over time. It cites two papers but does not synthesize their contributions or connect them meaningfully to the discussion. There is minimal critical analysis or abstraction beyond the surface-level description of the problem."}}
{"id": "40f251f6-6eb6-4b96-9985-ae4ba684809f", "title": "Stability and Temporal Smoothness", "level": "subsection", "subsections": [], "parent_id": "056e1eb8-8ad4-4f8e-9c39-595e8e3a2949", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Fundamentals Behind the Embedding of Dynamic Graphs"], ["subsection", "Stability and Temporal Smoothness"]], "content": "\\label{subsection:stability}\nA successful dynamic graph embedding algorithm should create stable embeddings over time. In other words, an embedding should be able to learn similar representations at consecutive timestamps if the underlying graph changes only slightly. More specifically, given the dynamic graph $\\mathcal{G}$, if the graph snapshot $G(t_{k+1})$ is similar to $G(t_{k})$ (for instance, adjacency matrices $A(t_{k+1})$ and $A(t_{k})$), the embedding matrix $Z(t_{k+1})$ is expected to be similar to $Z(t_{k})$. Goyal et al.~ propose the stability constant $K_{A}(\\nu)$ as a metric to evaluate the stability of a dynamic graph embedding function $\\nu$ in terms of the adjacency matrix $A$ over time. More specifically, the authors consider the stability of any embedding at a given timestamp as the ratio of the Frobenius norm of the difference between embedding matrices and the Frobenius norm of the difference between adjacency matrices, at consecutive timestamps. Then, the stability constant is the maximum difference between stabilities calculated along the entire life span $\\mathcal{T}$ of the network, and the authors claim that a dynamic embedding $\\nu$ is stable as long as the stability constant is small. It is possible to further extend these proposals to include any similarity matrix $S$, and to take the limit of two consecutive timestamps $t_{k+1} - t_{k} = \\Delta t \\to 0$ in order to analyze continuity aspects of embeddings (see future directions in Sec.~\\ref{sec:conc}).\nAlthough the above discussion is related to the global behavior of embedding, most of the proposed embedding methods seek local stability, i.e., for each node in the network. If the local topological structure around a node $v$ has undergone few changes, it is expected that the representations $z_{v}(t_{k+1})$ and $z_{v}(t_{k})$ are similar, assuming a \\textbf{temporal smoothness} in the embedding space~.", "cites": [7241, 7232, 7235], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of stability from multiple cited papers into a general discussion on temporal smoothness, connecting the idea of embedding consistency over time. It introduces the stability constant and discusses its implications, showing some abstraction by linking the concept to broader properties like continuity. However, the critical analysis is limitedâ€”there is no detailed evaluation of the proposed stability measures or their limitations in different contexts."}}
{"id": "08d8737f-a239-4840-abf3-27d1eafb336f", "title": "Techniques for the Embedding of Dynamic Graphs", "level": "section", "subsections": ["33570251-4240-40a6-95aa-81e640282eef", "af0d1cef-d47c-4800-a9e3-56dfd218a903", "885545d5-2a95-4d96-9338-c320e0def984", "f9af3d04-7d83-49d7-9e03-93e265d28723", "ac0ee22a-3061-4076-8385-8077f204a126", "74768985-5fe0-494f-bdbc-162bfc50f104", "7f915d82-56ae-414b-8a7a-d5c03c9b0602", "90f3cc53-7774-4488-a99b-609561275f7c"], "parent_id": "cc73abb4-e745-4f3e-9227-7cfc6839570d", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"]], "content": "\\label{sec:tech}\nIn this section, we propose a taxonomy of dynamic graph embedding techniques, summarized in Figure~\\ref{fig:techniques}. This taxonomy has been inspired by previous static graph embedding classifications~. Classifications for dynamic graphs can be seen as extensions of static graph methods: (i)~\\textbf{matrix factorization approaches}; (ii)~\\textbf{deep learning approaches}; (iii)~\\textbf{random walk-based methods} (which may be understood as node sequence sampling methods); (iv)~\\textbf{optimization based on edge reconstruction}, which also leverages \\textbf{temporal smoothness}, and (v) \\textbf{graph kernel methods}. Here we include \\textbf{tensor factorization approaches}, fusing them with matrix factorization approaches and defining general \\textbf{factorization based approaches}, and introduce two novel paradigms: (i)~\\textbf{temporal point process based} methods, which handles similarity matrix changes as stochastic processes; and (ii)~\\textbf{agnostic models}, which learn embeddings over time independent of the approach used for each graph snapshot in dynamic networks.\nEarlier approaches to map dynamic networks into vector spaces proposed learning independent vector representations of each snapshot employing static graph embedding methods. However, since representation learning assumes that the probability mass of the data concentrates in manifolds that have much smaller dimensionality than the original space where the data lives, the evolution of the network may cause two relevant impacts on the embedding space: (i)~the embedding vectors move on the manifold; and (ii)~the manifold itself evolves in time~. Therefore, integrating spatial topology of network and the temporal network evolution into feature vectors encompassing these temporal correlations enhances the performance of prediction, classification and many other temporal network analysis problems~.\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{figs/FigTechniques.pdf}\n    \\caption{Proposed taxonomy for dynamic graph embedding techniques, organized by algorithmic approaches.}\n    \\label{fig:techniques}\n\\end{figure}\nNext, we present each classification proposed by our taxonomy, discussing its insights, how it leverages temporal dependence in addition to topological structure, and describing several methods following each approach.Concrete examples of applications using the methods covered here are presented only in the following section.", "cites": [219, 215], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts from the cited static graph embedding surveys and extends them to dynamic graphs by proposing a novel taxonomy. It provides an analytical perspective by explaining how temporal evolution affects embedding manifolds and why integrating temporal dependencies is important. However, it lacks deeper critical evaluation of the methods and does not extensively compare the strengths and weaknesses of the cited approaches."}}
{"id": "91c4e471-e9a5-4eea-a07c-4c7b7f2edd50", "title": "Matrix factorization approaches", "level": "subsubsection", "subsections": [], "parent_id": "33570251-4240-40a6-95aa-81e640282eef", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Factorization-based methods"], ["subsubsection", "Matrix factorization approaches"]], "content": "\\label{subsub:matrixfactorization}\nMatrix factorization approaches express the evolutionary structure of networks in the form of matrices, therefore leveraging the time-dependent structural correlation among pairs of nodes. Dynamic graph embedding based on matrix factorization may be classified, as in static graphs, according to which type of loss function the approach minimizes. While most approaches factorize the similarity matrix and define an inner-product function between node embeddings to approximate the proximity measure~, graph Laplacian Eigenmaps can be used to reconstruct time-dependent adjacency matrix from an eigendecomposition of Laplacian matrix~. The novelty in the embedding of dynamic networks consists in the way of propagating the factorization over time, maintaining the stability of the representations while inserting temporal dependence into matrix decomposition. It is possible to distinguish three paradigms based on the modeling for the connections between different timestamps: (i)~adding temporal smoothing directly to the loss function, thus ensuring the stability of embeddings over time and focusing on node trajectories; (ii)~updating the similarity matrix by using matrix perturbation theory, assuming that temporal evolution changes network topology slightly; and (iii)~defining a temporal matrix factorization, decomposing the similarity matrix into a constant term and a time-dependent term. Figure~\\ref{fig:matrixfactorization} shows these two classifications.\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.45\\linewidth]{figs/FigMatrixFactorization.pdf}\n    \\caption{Classification of matrix factorization techniques for dynamic graph embedding, taking into account the type of cost function (similar to Cai et al.~) and how temporal dependence is leveraged into matrix decomposition.}\n    \\label{fig:matrixfactorization}\n\\end{figure}\n\\begin{compactitem}\n    \\item \\textbf{Jointly optimizing loss function and temporal smoothing:} The methods based on the above insight perform the matrix factorization of each snapshot, in addition to bound the representations through a loss function term that is minimized when, for each node, the embeddings at two consecutive timestamps are similar. In other words, these approaches assume temporal smoothness in the embedding space, and the optimal embedding should jointly reconstruct the similarity matrix over time while being continuous. Given the loss function $\\mathcal{L}_{0}(t)$ minimized by a matrix factorization approach on static graphs at each timestamp $t \\in \\mathcal{T}$, the jointly similarity reconstruction and temporal smoothing loss function for a dynamic graph can be defined as:\n    \\begin{equation}\n        \\mathcal{L} = \\sum_{t \\in \\mathcal{T}} \\mathcal{L}_{0}(t) + \\tau \\sum_{t \\in \\mathcal{T}}\\sum_{t' \\in \\mathcal{T} | (t' - t) \\le \\Delta t} \\mathcal{L}_{\\Delta t} (t', t),\n    \\end{equation}\n    where $\\Delta t$ is the time interval between consecutive timestamps in $\\mathcal{T}$, $\\mathcal{L}_{\\Delta t}(t', t)$ is a loss function for each pair of consecutive timestamps ($t$ and $t'$) implying temporal smoothness. $\\tau > 0$ regulates the temporal smoothness term contribution. Some approaches following this paradigm include Ferreira et al.~ and Zhu et al.~.\n    One can implement two generalizations of the loss function: (i) the contribution of each timestamp may not be homogeneous, since the future in general depends more directly on the recent past than on the distant past, weighing the loss function $\\mathcal{L}_{0}$ with a function $f(t)$ that is close to $1$ when $t \\approx \\tau$ (where $\\tau$ is the last timestamp, i.e. $t_{N_{S}-1}$ for snapshot model), and close to $0$ if $t \\ll \\tau$~(as an example, $f(t) = e^{\\tau - t}$); and (ii) the dataset may contain non-homogeneous time intervals, i.e., a function $g(\\Delta t)$ may be defined in order to leverage different values of $\\Delta t$, and the smoothness of embeddings at timestamps $t$ and $t + \\Delta t$ may be less important when $\\Delta t$ is large.\n    \\item \\textbf{Incremental updates on embeddings:} In these methods, the initial graph snapshot $G_{0}$ gives the initial similarity matrix $S(0)$ using a standard matrix decomposition. Afterwards, the embeddings for subsequent timestamp are updated by assuming that $||S(\\Delta t) - S(0)|| < \\epsilon$ for some small $\\epsilon$, i.e. the similarity matrix $S(\\Delta t)$ is a perturbation of the initial matrix $S(0)$. Therefore, it is possible to update a low dimensional representation of the nodes using first-order matrix perturbation theory in symmetric matrices iteratively~. Li et al.~ apply matrix perturbation theory for Laplacian Eigenmaps, whereas Zhang et al.~ propose TIMERS to update SVD decompositions incrementally. \n    It is noteworthy that these approaches accumulate errors due to the perturbative approach made, and it may not be very effective if the network evolves intensely over time. Nevertheless, since for many real networks the temporal evolution is quite sparse (i.e. the number of added or removed edges is much lower than the total number of edges), these methods present promising results in dynamic link prediction, node clustering and node classification. Moreover, these strategies may reduce error accumulation over time, setting a restart time, i.e. a time interval at which the algorithm recalculates the factorization instead of the incremental update~.\n    \\item \\textbf{Temporal Matrix Factorization}: In these methods, the time-dependent similarity matrix $S_{t}$ is decomposed by a temporal rank-$k$ matrix factorization model as follows~:\n    \\begin{equation}\n    S(t) = h(U \\times V(t)^{T}),\n    \\end{equation}\n    where both $U$ and $V(t)$ are $|V| \\times k$ matrices, $U$ is a constant matrix, $V(t)$ is a time-dependent matrix and $h(\\cdot)$ is an element-wise function. For undirected networks, $S(t)$ is symmetric and it is possible to (i)~average $U V(t)^{T}$ and its transpose as the prediction of $S(t)$; or (ii)~factorize $S(t)$ as the product of a time-dependent matrix $V(t)$ and its transpose~. Therefore, this method learns two types of embedding: (i)~a constant term embedding, given by the rows of $U$ and represents persistent properties between pairs of nodes; and (ii)~a time-varying embedding, given by the rows of $V(t)$ and represents changes in topology over time. A non-linearity may be inserted by the function $h$, e.g. a logistic function to interpret the reconstruction $U \\times V(t)^{T}$ as a probability measure for the similarity.\n    The main challenge to handle this approach is to describe the time-dependent matrices $V(t)$ for each timestamp. The dynamic behavioral mixed-membership model~(DBMM) proposed by Rossi~et~al.~ was the first  to employ this factorization, and proposed: (i)~a transition matrix $T$ in order to bound $V(t+\\Delta t) \\approx V(t) T$, (ii)~a stacked transition model, which bounds training examples from $l$ previous timestamps, and (iii)~a summary transition model, defining $V(t)$ at a specific timestamp as a linear combination of time-dependent matrices at previous timestamps. Yu et al.~, who developed the formal description of the temporal matrix factorization approach described above, represented $V(t)$ as a polynomial function over time of order $p$, i.e. $V(t) = \\sum_{i=0}^{p} W^{(i)} t^{i}$, where $\\{W^{(i)}\\}_{i=0}^{p}$ are $|V| \\times k$ matrices which need to be learnt from the model along with $U$. The LIST model~ leverages a temporal matrix factorization to learn the feature vector of each node by simultaneously optimizing the temporal smoothness constraint and network propagation constraint, ensuring that two vertices that are connected are likely to share similar features.\n\\end{compactitem}\nTable~\\ref{tab:matrixfactorization} compares the approaches based on matrix factorization discussed in this section. Some combinations of the matrix factorization approaches were not yet explored, such as graph Laplacian eigenmaps including a temporal smoothing term and a Laplacian temporal matrix factorization.\n\\begin{table}[H]\n\\caption{Matrix factorization based Dynamic Graph Embedding.}\n\\scriptsize\n\\vspace{-3mm}\n\\label{tab:matrixfactorization}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{2}{|c|}{} & \\multicolumn{7}{|c|}{\\textbf{Matrix Factorization based Dynamic Graph Embedding}} \\\\ \\cline{3-9}\n \\multicolumn{2}{|c|}{}& BCGD~ &  & DANE~ & TIMERS~ & DBMM~ & TMF~ & LIST~ \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{2}{*}{Cost Function Type}} & Laplacian Eigenmaps &  &  & \\checkmark &  &  &  & \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Similarity Matrix Factorization & \\checkmark & \\checkmark &  & \\checkmark & \\checkmark & \\checkmark & \\checkmark \\\\ \\hline\n\\multirow{3}{*}{Temporal Dependence} & Temporal Smoothing & \\checkmark & \\checkmark &  &  &  & & \\\\ \\cline{2-9} \n & Incremental Updates &  &  & \\checkmark & \\checkmark &  & & \\\\ \\cline{2-9} \n & Temporal Matrix Factorization &  &  &  &  & \\checkmark & \\checkmark & \\checkmark \\\\ \\hline\n\\end{tabular}\n\\end{table}", "cites": [7231, 215], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key matrix factorization approaches from multiple sources, connecting their mathematical formulations and assumptions to build a coherent framework. It provides critical insights into the limitations of perturbation-based methods, such as error accumulation, and highlights the conditions under which these approaches are effective. The abstraction level is strong, as the section identifies broader paradigms (temporal smoothing, incremental updates, and temporal matrix factorization) and generalizes how temporal dependencies are modeled in dynamic graph embedding."}}
{"id": "af0d1cef-d47c-4800-a9e3-56dfd218a903", "title": "Approaches based on Deep Learning", "level": "subsection", "subsections": ["a80c04af-ca0c-4460-a8a4-546147bb1ad4", "d9e5aa24-f5a4-4921-8ddd-c5d341fb9683"], "parent_id": "08d8737f-a239-4840-abf3-27d1eafb336f", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Approaches based on Deep Learning"]], "content": "\\label{subsec:deeplearning}\nDeep Learning has shown a remarkable performance in a wide variety of research fields, including computer vision and language modeling, and it also benefits several applications related to representation learning and other tasks over graphs. Many successful static graph embedding methods have been proposed in the last few years, from Graph Neural Networks~ and Convolutional Neural Networks on Graphs~, to autoencoders, such as Structural Deep Network Embedding (SDNE)~.\nDifferent architectures based on neural networks are used both to extract the topological properties of a network and to capture temporal dependencies therein. Some preliminary works developed a fully-connected neural networks, either using them as an autoencoder or as a part of the decoding process~. Architectures based on recurrent neural networks~(RNNs), including long short-term memory units~(LSTMs)~ and gated recurrent units~(GRUs)~, leverage a sequence of graphs, or their representations, in order to learn or enhance embeddings taking into consideration temporal correlation, and store information over time to handle more complex correlations beyond consecutive timestamps~. Attention mechanisms~ are used to further improve understanding of the most relevant time points for each representation~. Convolutional neural networks~(CNNs)~ for graphs have been widely adopted in order to handle topological properties, including Graph Convolutional Networks~(GCNs)~. Iterative propagation procedures have been also employed to learn the graph topology, as in Graph Neural Networks~(GNNs)~, GraphSAGE~, and Gated Graph Neural Networks~(GGNNs), the former also being able to learn the reachability across the nodes in a graph using GRUs~. These approaches have been also explored by several dynamic graph embedding methods~. Even further, instead of using RNNs to explore the characteristics of the network over time, many succeeding models also employ convolutional networks~(for instance, 1D CNNs) to leverage temporal dependencies~.\nNeural networks may also learn an approximation of the network distribution by using generative models, such as the variational autoencoders (VAEs)~ and generative adversarial networks (GANs)~. These approaches learn low-dimensional latent representations of the training data that store information about the type of output the model needs to generate, using a generative network to capture the data distribution and a recognition network~(or a discriminator network) to estimate the probability that a sample came from the data distribution. Variational graph autoencoder~(VGAE)~ and Graph-GAN~ employ these approaches to static graphs. These generative models are used in dynamic graphs to learn these data distributions over time~.\nIn order to define a taxonomy that categorizes every method with minimal overlap between different categories, we classify the approaches according to \nthe general architecture of the neural network. We have identified two main general architectures: (i)~encoder-decoder perspective, which holds the majority of works concerning learning representations and decoding them for an application~(i.e. network reconstruction, link prediction, or node/graph classification); and (ii)~encoder, sampling, and decoder perspective, which contains the generative models and handles representations as probability distributions~(see Figure~\\ref{fig:deeplearning} for the complete proposed taxonomy for deep learning based approaches). In the following, we further detail each of these methods and also present the most important existing works for each of them.\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.45\\linewidth]{figs/FigDeepLearning.pdf}\n    \\caption{Classification of deep learning techniques for dynamic graph embedding, dividing into encoder-decoder perspective and generative models.}\n    \\label{fig:deeplearning}\n\\end{figure}", "cites": [1000, 28, 243, 168, 7244, 229, 7232, 5680, 7243, 211, 7242, 7235, 242, 7212, 7241], "cite_extract_rate": 0.4838709677419355, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a range of deep learning-based dynamic graph embedding approaches by organizing them into a structured taxonomy centered on neural network architectures. It abstracts common themes such as temporal modeling, topological learning, and generative approaches. While it provides a clear analytical framework, it could offer deeper critical evaluation of the methods' limitations or comparative performance across different settings."}}
{"id": "a80c04af-ca0c-4460-a8a4-546147bb1ad4", "title": "Encoder-Decoder Architecture", "level": "subsubsection", "subsections": [], "parent_id": "af0d1cef-d47c-4800-a9e3-56dfd218a903", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Approaches based on Deep Learning"], ["subsubsection", "Encoder-Decoder Architecture"]], "content": "The encoder-decoder architecture consists of an encoder, which maps the data into a low-dimensional representation, and a decoder, that aims to either (i)~reconstruct the original data; or (ii)~solve an application-driven problem, such as a binary or multi-class classification problem. In the first case, the network is called an autoencoder, since the encoding seeks to be as lossless as possible. In the second case, the encoding is lossy and the original data cannot be fully recovered. \nConcerning dynamic graphs, an encoder receives graph snapshots as input and the decoder may exhibit three distinct outputs: (i)~traditional autoencoders, whose representations reconstruct each graph snapshot, hence following the lossless encoding paradigm; (ii)~dynamic autoencoders, whose representations do not reconstruct each graph snapshot whereas instead they reconstruct a snapshot in a future timestamp, therefore predicting the network structure; and (iii)~discriminator networks, whose embeddings do not reconstruct the network topology at all, but are intended to learn node labels, node clusters or a global network property, and the loss functions are usually application-driven.\n\\begin{compactitem}\n    \\item \\textbf{Traditional Autoencoders}: These architectures are applied for each graph snapshot, similar to the SDNE in static graphs~. DynGEM~ builds a fully-connected autoencoder for each graph snapshot, using a transfer learning paradigm to share parameters between two consecutive autoencoders and a strategy that allows the autoencoder network to widen its layers and inserts new layers in order to handle a growing number of nodes in the graph. LDANE~ follows a similar strategy, and handles node attributes by adding a margin-based ranking loss term in the loss function that ensures the embeddings of two similar nodes are closer than the embedding of two non-similar nodes.\n    Models employing recurrent neural networks and their hidden states to encode dynamic graph structure have also been proposed. For instance, Taheri et al.~ developed DyGGNN, which leverages Gated Graph Neural Networks~(GGNNs) to capture graph topology and couples it with an LSTM encoder to handle graph dynamics, and with an LSTM decoder to reconstruct the structure of the dynamic graph at each timestamp. Other approaches include DySAT~ and DGNN~. \n    Table~\\ref{tab:traditionalautoencoders} summarizes the deep learning approach employed by each of these methods.\n    \\begin{table*}[h]\n    \\caption{Traditional Autoencoders for Dynamic Graph Embedding.}\n    \\scriptsize\n    \\vspace{-3mm}\n    \\label{tab:traditionalautoencoders}\n    \\centering\n    \\tabcolsep=0.05cm\n    \\renewcommand{\\arraystretch}{1.1}\n    \\begin{tabular} {  |c|c| } \\hline\n    \\textbf{Algorithm}& \\textbf{Deep Learning Model} \\\\ \\hline\n    DynGEM~ & Fully-Connected Autoencoder (based on SDNE~) \\\\ \\hline\n    LDANE~ & Similar to DynGEM, also handles node attributes (margin-based ranking loss term)  \\\\ \\hline\n    DyGGNN~ & Gated Graph Neural Networks and LSTMs \\\\ \\hline\n    DySat~ & Structural and Temporal Self-Attention \\\\ \\hline\n    DGNN~ & Attentive LSTMs \\\\ \\hline\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\end{table*}\n    \\item \\textbf{Dynamic Autoencoders}: The input of these methods based on dynamic autoencoders is the historical records of the network. The output is the reconstructed graph at a future time. Bonner et al.~ regarded this approach as the temporal graph offset reconstruction problem, i.e. creating temporal graph embeddings that recreate a future timestamp of the graph.\n    Several embedding methods developed a recurrent architecture to capture the dynamics of the network in order to predict its future state, such as Goyal et al.~, which propose three different strategies for taking a set of graph snapshots, from autoencoders (dyngraph2vecAE) to LSTM networks (dyngraph2vecRNN), and the combination of both of them (dyngraph2vecAERNN). Chen et al.~ propose an encoder-LSTM-decoder (E-LSTM-D), which resembles dyngraph2vecAERNN, but uses rectified linear units~(ReLUs) as the activation function for each encoder/decoder layer, and adds a regularization term to prevent overfitting. AdaNN~ employs a triple attention module, leveraging topology, node attributes and temporal attention to further feed them into two connected GRUs and concatenating them into a joint state vector. TRRN~ adopts memories to enhance temporal capacity, applying multi-head self-attention and learning contextualized representations feeding different factors (including node features and topological features) and updated memories into LSTMs.\n    Graph convolution combined with recurrent units have been exploited for building dynamic autoencoders. GC-LSTM~ uses convolutions to extract topological features while coupling with an LSTM in order to learn temporal features of the dynamic network. EvolveGCN~ proposes two versions that follow a similar approach: (i)~EvolveGCN-H, where the GCN parameters are hidden states of GRUs that take node embeddings as input; and (ii)~EvolveGCN-O, where the GCN parameters are input/output of an LSTM unit. T-GCN~ uses GCNs to learn topological structures, then passing these features to GRUs in order to extract temporal dependencies. \n    Table~\\ref{tab:dynamicautoencoders} summarizes the deep learning approach employed by each of the methods based on dynamic autoencoders.\n    \\begin{table*}[h]\n    \\scriptsize\n    \\caption{Dynamic Autoencoders for Dynamic Graph Embedding.}\n    \\vspace{-3mm}\n    \\label{tab:dynamicautoencoders}\n    \\centering\n    \\tabcolsep=0.05cm\n    \\renewcommand{\\arraystretch}{1.1}\n    \\begin{tabular} {  |c|c| } \\hline\n    \\textbf{Algorithm}& \\textbf{Deep Learning Model} \\\\ \\hline\n    TO-GAE~ & GCNs over time \\\\ \\hline\n    dyngraph2vecAE~ & Fully-Connected (FC) Autoencoders \\\\ \\hline\n    dyngraph2vecRNN~ & Sparsely Connected LSTMs \\\\ \\hline\n    dyngraph2vecAERNN~ & FC Encoder, LSTMs and FC Decoder \\\\ \\hline\n    E-LSTM-D~ & FC Encoder, LSTMs and FC Decoder \\\\ \\hline\n    AdaNN~ & Spatial, Attribute-Topology and Temporal Attention, and GRUs \\\\ \\hline\n    TRRN~ & FC Encoder, Transformer-Style Self-Attention and LSTMs \\\\ \\hline\n    GC-LSTM~ & Graph Convolutions and LSTMs \\\\ \\hline\n    EvolveGCN-H~ & GCNs and GRUs \\\\ \\hline\n    EvolveGCN-O~ & GCNs and LSTMs \\\\ \\hline\n    T-GCN~ & GCNs and GRUs \\\\ \\hline\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\end{table*}\n    \\item \\textbf{Discriminator Networks:} This approach considers that a neural network must learn application-driven representations, such as properly classifying nodes or graphs/subgraphs over time~, extracting network properties~, or predicting a specific global feature~. Several approaches combine GCNs and recurrent networks, such as DynGraph2Seq~,     TSGNet~ and NAAM~. Topological features may also be extracted by other techniques as an alternative to GCNs, as Xu et al. showed by implementing STAR~. Other approaches use convolutions for both spatial and temporal feature extraction, including Spatio-Temporal Graph Convolutional Network~(STGCN)~.\n    These discriminator networks are commonly employed to embed graphs constructed from non-relational data. For instance, DynamicGCN~ extracts and learns graph representations from historical event documents, encoding the input data into a sequence of graphs with node embeddings, and developing a graph convolutional network model to predict the occurrence of certain type of events. TD-Graph LSTM~, in the other hand, is applied to action-driven video object detection, passing each frame through a spatial convolutional network in order to detect similar regions in consecutive frames, and construct a temporal graph structure by connecting semantically similar regions. LSTM units take the spatial visual features as the input states, incorporating temporal motion patterns for participating objects in the action while minimizing an action-driven object categorization loss. Li et al.~ propose a spatial-temporal graph embedding model called STG2Vec, which includes a temporal attention and incorporates multi-source information to fed a collaborative temporal modeling based on LSTMs.\n    Table~\\ref{tab:discriminatornetworks} lists the approaches described above and summarizes the deep learning approach employed by each of them, along with the specific task each method attempts to solve.\n    \\begin{table*}[h]\n    \\scriptsize\n    \\caption{Discriminator Networks for Dynamic Graph Embedding.}\n    \\vspace{-3mm}\n    \\label{tab:discriminatornetworks}\n    \\centering\n    \\tabcolsep=0.05cm\n    \\renewcommand{\\arraystretch}{1.1}\n    \\begin{tabular} {  |c|c|c| } \\hline\n    \\textbf{Algorithm}& \\textbf{Deep Learning Model} & \\textbf{Task} \\\\ \\hline\n    DynGraph2Seq~ & GCNs, LSTMs and Hierarchical Attention & Sequence of Target Health Stages \\\\ \\hline\n    TSGNet~ & GCNs and LSTMs & Node Classification \\\\ \\hline\n    NAAM~ & GCNs, LSTMs (or BiLSTMs) and Temporal Attention & Forecasting User Interactions \\\\ \\hline\n    STAR~ & Spatio-Temporal Attention and GRUs & Node Classification \\\\ \\hline\n    STGCN~ & GCNs and Gated Convolutional Neural Networks & Traffic Forecasting \\\\ \\hline\n    DynamicGCN~ & GCNs with Updates from Previous Timestamps & Event Prediction \\\\ \\hline\n    TD-Graph LSTM~ & CNNs and LSTMs & Missing Label Classification \\\\ \\hline\n    STG2Vec~ & Temporal Attention and LSTMs & Bike-sharing Demand \\\\ \\hline\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\end{table*}\n\\end{compactitem}", "cites": [25, 28, 7244, 7232, 7242, 7239, 7241, 7238], "cite_extract_rate": 0.34782608695652173, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple works on encoder-decoder-based dynamic graph embedding, connecting them through a structured taxonomy of autoencoders and discriminator networks. While it provides a clear analytical framework by grouping methods according to their objectives and mechanisms, the critical analysis is limited to surface-level distinctions (e.g., lossless vs. lossy encoding). It identifies patterns such as the use of attention and recurrent components but stops short of deep evaluation or identifying broader principles in the field."}}
{"id": "d9e5aa24-f5a4-4921-8ddd-c5d341fb9683", "title": "Generative Models", "level": "subsubsection", "subsections": [], "parent_id": "af0d1cef-d47c-4800-a9e3-56dfd218a903", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Approaches based on Deep Learning"], ["subsubsection", "Generative Models"]], "content": "Generative algorithms attempt to predict features given a certain label, i.e. to learn the data distribution patterns, in contrast to discriminative models, which attempt to learn representations and classify each input data. Therefore, these approaches have the power to synthesize data in addition to compress and to learn embeddings. Regarding generative neural networks for dynamic graph embedding, two groups of approaches arise: (i)~methods based on variational autoencoders~(VAEs), which encodes an input as a distribution over the latent space; and (ii)~methods based on generative adversarial networks~(GANs), which train both a generator network to synthesize graphs and a discriminator network to distinguish between true graphs and generated ones.\n\\begin{compactitem}\n    \\item \\textbf{Based on Variational Autoencoders:} The encoder for a variational autoencoder takes a data point and produces a distribution, usually parameterized as a multivariate Gaussian. In this case, the encoder predicts the mean and standard deviation of the Gaussian distribution, and the lower-dimensional embedding is sampled from this distribution. The decoder is a variational approximation, which takes an embedding and produces an output~. \n    Every variational autoencoder for dynamic graphs is inspired by VGAE~ to address the encoding of each snapshot, differing on how to handle the graph evolution. These methods include (i) Dyn-VGAE~, which addresses a temporal smoothness loss term, (ii) TO-GVAE~, which applies VGAE over time to reconstruct subsequent snapshots, and (iii) VGRNN~, which adopts VGAE whose prior distribution parameters are based on the hidden states in previous timestamps. \n    There are several proposals to enhance the encoder model. For instance, Bonner et al.~ propose a Temporal Neighbourhood Aggregation (TNA) block to comprise a GCN with a GRU in the encoder, controlling the combination of topological and temporal learning via a final linear layer. Zhao et al.~ have developed a framework called BurstGraph, which splits the adjacency matrix of a graph into a standard adjacency matrix and a burst adjacency matrix to pick up unexpected behavior within a time duration. A variational autoencoder is employed using GraphSAGE~ as the encoder, and two decoders: a standard decoder, which learns representations $Z^{v}$, and a bursty decoder, which learns sparse embeddings~$Z^b$.\n    \\item \\textbf{Based on Generative Adversarial Networks:} \n    Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other by designing a game-theoretical minimax game to combine generative and discriminative models. This approach has been applied for graph representation learning by a framework called GraphGAN~. While a generator network attemps to approximate the true graph connectivity distribution, the discriminator network aims to discriminate the connectivity of each node pair. Therefore, the generator network  tries to deceive the discriminator network, whereas the discriminator network improves itself to distinguish better and better between true edges and generated edges.\n    Inspired by these recent approach for graph representation learning, some methods arise for dynamic graph embedding. DynGraphGAN~ designs the discriminator network with the following components: (i)~a GCN to encode neighborhood features of nodes; and (ii)~CNNs to learn temporal graph evolution along the time dimension. The generator network implements a sigmoid function of the inner product of two node's embeddings at a timestamp~$t$ to estimate the probability distribution of an edge connecting these nodes at time~$t$.  GCN-GAN~ displays an architecture whose generative network consists of a GCN layer, an LSTM layer and a fully-connected output layer, and the discriminator network is a fully-connected feedforward neural network. \n    Each of the GAN-based methods previously mentioned are trained with a single graph. Therefore, the trained model is capable of generating artificial snapshots following a similar structure and dynamic of the original graph used during training. Also, note that the resulting model is limited to creating snapshots with the same number of nodes as the graph used in training, given that number of parameters of the model is proportional to the number of nodes in the TVG. This dependency is observed, for example, for the last layer of the generative model of the GCN-GAN model, which builds an adjacency matrix with dimensions $N\\times N$ from an embedding vector (outputted by the LSTM layer).\n\\end{compactitem}\nTable~\\ref{tab:generative} summarizes the generative model and architectures employed by  the methods discussed in this section.\n\\begin{table*}[h]\n\\caption{Generative Models for Dynamic Graph Embedding.}\n\\scriptsize\n\\vspace{-3mm}\n\\label{tab:generative}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c|c|c| } \\hline\n\\textbf{Algorithm}& \\textbf{VAE}& \\textbf{GAN} & \\textbf{Deep Learning Models} \\\\ \\hline\nTO-GVAE~ & \\checkmark & & GCNs  \\\\ \\hline\nDyn-VGAE~ & \\checkmark & & Original VAE with Temporal Smoothness \\\\ \\hline\nTNA~ & \\checkmark & &  GCNs and GRUs   \\\\ \\hline\n & \\checkmark & & GraphSAGE  and RNNs  \\\\ \\hline\nVGRNN~ & \\checkmark & & GCNs and LSTMs  \\\\ \\hline\nDynGraphGAN~ & & \\checkmark & GCNs and CNNs  \\\\ \\hline\nGCN-GAN~ & & \\checkmark & GCNs and LSTMs \\\\\\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}", "cites": [7235, 242, 229, 1000, 5680, 7243], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple generative approaches for dynamic graph embedding, integrating insights on VAE and GAN methods, and highlighting their structural and temporal components. It provides a basic level of critical evaluation by pointing out limitations, such as the dependency on the number of nodes in training data. The abstraction level is moderate, with some pattern recognition (e.g., recurrent structures in VAEs) but not a full meta-level theoretical framework."}}
{"id": "885545d5-2a95-4d96-9338-c320e0def984", "title": "Random Walk Approaches", "level": "subsection", "subsections": ["4fb2bc6b-6bc6-4163-9d5a-4589c2ef8fa7", "3469d2dc-6aad-4f80-8eeb-aa5fbf8345dc", "e6b75282-559b-4fe0-96b0-65b9226397f1", "06429ecb-de73-46d9-ae4f-c1f218bd6915"], "parent_id": "08d8737f-a239-4840-abf3-27d1eafb336f", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Random Walk Approaches"]], "content": "Another class of methods for graph embedding functions relies on random walks. Multiple random walks of fixed length $L$ are considered sentences, generating a context for each node and trying to extract higher-order dependencies without adjacency matrices. \nThe node sequence matrix is therefore generated and factorized, usually, by applying a neural network architecture, the most popular being the Skip-Gram~, to produce low dimensional vector representations for each node while maintaining their proximity in the new embedded space.\nRandom walks applied to dynamic graphs must generate time-dependent contexts $C(t)$ in addition to sequences that capture topological dependencies. Then, the methods based on random walks are separated according to how they include the temporal aspect into the calculation: (i)~random walk on snapshots, where a time-dependent node sequence matrix is generated by applying random walks starting on each node at each snapshot, and further optimizing a joint problem that takes into account the temporal dependency; (ii)~evolving random walks, where the node sequences are generated for the initial time (first snapshot), then the method incrementally updates node representation by updating random walks starting on nodes affected by topological evolution; and (iii)~temporal random walks, which define time-dependent context matrices by allowing random walks across consecutive timestamps and considering time ordering restriction. Also, other node sequence sampling methods besides random walks may be applied to generate contexts, including neighborhood aggregation. In the following, we further detail each of these methods.", "cites": [1684, 7165], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces random walk approaches for dynamic graph embedding and categorizes them into three types, but it does so primarily through description rather than synthesis or critique. It integrates the basic idea of Skip-Gram from the cited papers but does not connect them to broader themes or trends in the field. There is minimal critical evaluation or identification of overarching principles."}}
{"id": "4fb2bc6b-6bc6-4163-9d5a-4589c2ef8fa7", "title": "Random Walks on Snapshots", "level": "subsubsection", "subsections": [], "parent_id": "885545d5-2a95-4d96-9338-c320e0def984", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Random Walk Approaches"], ["subsubsection", "Random Walks on Snapshots"]], "content": "\\label{subsub:rws}\nThis approach performs random walks on each snapshot of a dynamic graph, obtaining vector representations by optimizing a joint problem taking into account temporal dependencies. It is important to note that methods following this approach generate contexts whose temporal connection between two matrices at consecutive time points is not modeled by random walks. Instead, the temporal dependency is defined later in the generation of the embeddings, taking into account the temporal smoothness. For instance, embeddings may be learned for each graph snapshot independently using methods including node2vec and DeepWalk, and afterward, the representations may be combined using operations, from simple vector concatenation to dynamic embeddings and orthogonal transformations to align embedding vectors at consecutive timestamps.\nDe Winter et al.~ and Dyn2Vec~ apply vector concatenation to node representations over time. The former applies node2vec for each snapshot, whereas the latter employs a DeepWalk variant, whose probability of choosing a certain edge depends on the normalized edge weight. Chen et al.~ initialize node embeddings by using a Gaussian prior with a diagonal covariance, and learn representations over time using dynamic Bernoulli embeddings, considering the rows of the node sequence matrix as the context for each node. tNodeEmbed~ preserves static network neighborhoods of nodes in a $d$-dimensional feature space by using Orthogonal Procrustes, and optimizes a LSTM for specific tasks~(i.e., link prediction and multi-label node classification). DynSEM~ train node embeddings for each timestamp using node2vec, align node embeddings into a common space using Orthogonal Procrustes, and optimize a joint loss function taking into account temporal smoothness.\nTable~\\ref{tab:randomwalk} lists the methods described above and points out both static embedding method applied for each snapshot and how they handle temporal dependencies.\n\\begin{table*}[h]\n\\scriptsize\n\\caption{Random Walks on Snapshots.}\n\\vspace{-3mm}\n\\label{tab:randomwalk}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c|c| } \\hline\n\\textbf{Algorithm}& \\textbf{Static Embedding Method}& \\textbf{Temporal Dependence Handling} \\\\ \\hline\n(Static)   & node2vec & Vector Concatenation \\\\ \\hline\nDyn2Vec  & DeepWalk variant & Vector Concatenation \\\\ \\hline\n & Gaussian Initialization & Dynamic Bernoulli Embeddings  \\\\ \\hline\ntNodeEmbed  & node2vec & Orthogonal Procrustes and LSTM \\\\ \\hline\nDynSEM  & node2vec & Orthogonal Procrustes and Temporal Smoothing Loss Function \\\\ \\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}", "cites": [478, 7234], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methods that use random walks on graph snapshots, listing approaches and how they handle temporal dependencies. It integrates some ideas from the cited papers, such as vector concatenation and alignment techniques, but lacks critical evaluation or deeper synthesis into a novel framework. The abstraction level is limited to rephrasing existing methods without identifying broader patterns or principles."}}
{"id": "3469d2dc-6aad-4f80-8eeb-aa5fbf8345dc", "title": "Evolving Random Walks", "level": "subsubsection", "subsections": [], "parent_id": "885545d5-2a95-4d96-9338-c320e0def984", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Random Walk Approaches"], ["subsubsection", "Evolving Random Walks"]], "content": "\\label{subsub:erw}\nGenerating random walks for every timestamp is an expensive time-consuming process. Several approaches first generate embeddings for the initial timestamp by using a static random walk approach, then incrementally update node representations taking into account that, in general, only a few nodes are influenced by topological evolution. Dynnode2vec~ follows this approach by sampling node sequences for only evolving nodes instead of generating random walks for all nodes in a given timestamp and afterward feeding these sequences as an input to a dynamic Skip-Gram model~, which is initialized at the first snapshot and used for weight initialization of the Skip-Gram of subsequent timestamps.\nOther approaches include EvoNRL~, which initially employs node2vec, stores the random walks in memory, and updates the set of random walks when a single new edge arrives in the network. EvoNRL uses a similar dynamic Skip-Gram model previously mentioned~. Sajjad~et~al.~ follow the same Skip-Gram implementation over time and proposes random walk update algorithms aiming to be statistically indistinguishable from a set of random walks generated from scratch on the new graph. NetWalk~ also proposes a network embedding algorithm inspired by the Skip-Gram architecture (which the authors call Clique Embedding). It uses a deep autoencoder neural network to learn vector representations through a stream of random walks while minimizes the pairwise distance among all nodes in each walk. Evolving random walks are used in order to mitigate the computational cost of performing a full random walk in every snapshot. The former is not expected to be as precise as the latter, as shown in some approaches~. But the small loss in accuracy is compensated by the huge computational efficiency shown by these methods.\nTable~\\ref{tab:evolvingwalk} lists the methods based on evolving random walks and points out both static embedding methods applied for each snapshot and how they update random walks and vector representations.\n\\begin{table*}[h]\n\\caption{Methods based on Evolving Random Walks.}\n\\scriptsize\n\\vspace{-3mm}\n\\label{tab:evolvingwalk}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c|c| } \\hline\n\\textbf{Algorithm}& \\textbf{Static Embedding Method} & \\textbf{Update Method} \\\\ \\hline\ndynnode2vec~ & node2vec & Dynamic Skip-Gram Model \\\\ \\hline\nEvoNRL~ & node2vec & Skip-Gram Model over Time \\\\ \\hline\n & DeepWalk with Unbiased Random Walk Updates & Skip-Gram Model over Time \\\\ \\hline\nNetWalk~ & Clique Embedding (AutoEncoder) & Vertex Reservoir and Walk Updating \\\\ \\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}", "cites": [7245, 479, 7246], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of evolving random walk approaches for dynamic graph embedding, listing the methods and their update strategies. It integrates some cited papers by highlighting common themes such as the use of Skip-Gram models and computational efficiency, but lacks deeper comparative analysis or critical evaluation of their strengths and weaknesses. The abstraction level is limited to identifying that evolving random walks reduce computational cost at the expense of some accuracy."}}
{"id": "e6b75282-559b-4fe0-96b0-65b9226397f1", "title": "Temporal Random Walk Methods", "level": "subsubsection", "subsections": [], "parent_id": "885545d5-2a95-4d96-9338-c320e0def984", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Random Walk Approaches"], ["subsubsection", "Temporal Random Walk Methods"]], "content": "The methods described in Sections~\\ref{subsub:rws} and \\ref{subsub:erw} consider that random walks and their updates are made to each snapshot separately. However, one way to include the time dependency directly in a sequence of nodes generated by random walks is to build a method to create a corpus of walks over time, respecting the temporal flux. In the literature, these walks are regarded as temporal walks~.\nIt is possible to generalize the Skip-Gram architecture to handle continuous-time dynamic networks, as described by Nguyen et al.~. In particular, the authors propose a general framework called CTDNE for learning time-preserving embeddings and propose several methods to select the subsequent nodes from a starting node, thus performing a temporal random walk: (i)~an unbiased temporal neighbor selection; (ii)~a biased selection, which may be based on temporal exponentially-weighted decay~(i.e., older timestamps have an exponentially lower contribution to the selection) or on temporal linearly-weighted decay. \nSeveral approaches follow CTDNE's paradigm, including Wu et al.~, which developed T-EDGE to encompass weighted networks, and De Winter et al.~, proposing a continuous-time version of node2vec. STWalk2 proposed by Pandhre et al.~, on the other hand, generates a spatial walk for each snapshot and a temporal walk, employing a Skip-Gram network to combine the two learned embeddings to get node representations. LSTM-node2vec~ trains an LSTM autoencoder with node sequences generated by temporal random walks, and afterward initialize node2vec with the input layer weights of the trained LSTM encoder for each snapshot at time~$t$.\nDiffusion prediction problems are related to temporal random walks, but the exact timestamp of diffusion is not necessarily known. instead only the temporal ordering is defined~(i.e., typically one does not know exactly when some information have passed from a node to another, but the source and the target of the diffusion process is known). Models following this objective include (i) DeepCas~, which uses GRUs and attention mechanisms to predict the future size of the cascade, (ii) DAN~, which outputs the probability distribution of the next infected node leveraging feed-forward neural networks and attention mechanism, and (iii) Topo-LSTM~, employing LSTMs to handle temporal dependence over diffusion. Moreover, Yang et al.~ implement GRUs, GCNs and GraphSAGE to predict next affected nodes, and a reinforcement learning framework to predict cascade size.\nTemporal Random Walks represents a more natural way to deal with dynamic continuous graphs~, since it does not require any time discretization of the graph into snapshots. It is also the ideal approach for diffusion problems, as we have shown.\nTable~\\ref{tab:temporalrandomwalk} lists the temporal random walk methods and points out both static embedding method applied for each snapshot and how they update random walks and vector representations.\n\\begin{table*}[t]\n\\scriptsize\n\\caption{Temporal Random Walk Methods.}\n\\vspace{-3mm}\n\\label{tab:temporalrandomwalk}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c|c| } \\hline\n\\textbf{Algorithm}& \\textbf Neural Network Model & Comments \\\\ \\hline\nCTDNE~ & Skip-Gram model & Defined temporal random walk embedding methods \\\\ \\hline\nT-EDGE~ & Skip-Gram model & Encompasses weighted dynamic networks \\\\ \\hline\n (Dynamic) & node2vec & Continuous version of the first random walks on snapshots \\\\ \\hline\nSTWalk2~ & Skip-Gram model & Separates temporal random walks and spatial random Walks \\\\ \\hline\nLSTM-node2vec~ & node2vec and LSTMs & Handles both temporal sequences and static sequences \\\\ \\hline\nDeepCas~ & DeepWalk, GRUs and Attention Mechanism & Diffusion cascades \\\\ \\hline\nDAN~ & Feed-forward neural network and attention mechanism & Diffusion cascades\\\\ \\hline\n & GCNs and GraphSAGE & Diffusion cascades \\\\ \\hline\nTopo-LSTM~ & LSTMs & Diffusion cascades \\\\ \\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}", "cites": [7247, 7248], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of temporal random walk methods, citing several relevant papers and briefly explaining their approaches. It attempts basic synthesis by grouping these methods under the CTDNE paradigm and discussing their shared goal of handling continuous-time dynamics. However, it lacks critical evaluation of the strengths or weaknesses of each method and offers limited abstraction beyond the specific techniques described."}}
{"id": "06429ecb-de73-46d9-ae4f-c1f218bd6915", "title": "Other Node Sequence Sampling Methods", "level": "subsubsection", "subsections": [], "parent_id": "885545d5-2a95-4d96-9338-c320e0def984", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Random Walk Approaches"], ["subsubsection", "Other Node Sequence Sampling Methods"]], "content": "Some techniques exhibit steps or insights given by two different random walk based approaches. For instance, several models create a graph containing the nodes at a given time $t$ and their neighbors at the same timestamp $t$ and the previous timestamps in a defined time window, and employ random walk procedures leveraging temporal ordering~. In particular, DHNE~ gives exponential-decaying weights for edges connecting nodes and past neighbors. These approaches share elements from random walks over snapshots and temporal random walks. Furthermore, StreamWalk algorithm introduced by Beres et al.~ compresses both evolving random walks and temporal random walks by updating the weight for walks to handle more recent edges.\nAlthough most node sequence sampling methods are based on random walks, some techniques rely on other ways to aggregate the neighborhood. Liu et al.~ develop a spatial-temporal neural attention mechanism to estimate the co-occurrence matrix and guide the embedding algorithm to focus on the context information with higher importance. Dynamic Knowledge Graph Embedding~(DKGE)~, on the other hand, applies an attentive GCN~(AGCN) to learn contextual subgraph embeddings over knowledge graphs, integrating them with knowledge embedding of entities and relations to build the joint representation of each object in the graph. The temporal evolution is leveraged by an online learning strategy that learns knowledge embeddings and contextual element embeddings of emerging entities and relations, as well as knowledge embeddings of existing entities and relations with changed contexts~(i.e. whose induced subgraphs are changed). Torricelli et al.~ introduce weg2vec, which takes a dynamic network and project it into a weighted link stream (the authors called weighted event graph), sampling neighborhoods for events (i.e. the edges of the original dynamic graph) from the link stream (i.e. to create a graph that connects edges concerning involved nodes, co-occurrence, and event time difference), and inputting sequences of connected events to a Skip-Gram model.\nTable~\\ref{tab:othernodesequencesampling} lists the methods described in this section and shows comments about their peculiarities, i.e., how they leverage temporal dependence or how they choose node context to apply the Skip-Gram model.\n\\begin{table*}[t]\n\\caption{Other Node Sequence Sampling Methods.}\n\\scriptsize\n\\vspace{-3mm}\n\\label{tab:othernodesequencesampling}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c| } \\hline\n\\textbf{Algorithm}& Comments \\\\ \\hline\nDHNE~ & Historical-current graphs \\\\ \\hline\nSTWalk1~ & Similar to historical-current graphs \\\\ \\hline\nDyAne~ & Supra-adjacency representation \\\\ \\hline\nStreamWalk~ & Temporal random walks updated for affected nodes \\\\ \\hline\nDKGE~ & Attentive GCNs over subgraphs, and online learning \\\\ \\hline\n & Co-ocurrence matrix using spatial-temporal neural attention model\\\\ \\hline\nweg2vec~ & Weighted event graphs \\\\ \\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}", "cites": [7249], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of various node sequence sampling methods, connecting some works by mentioning shared elements like temporal random walks and Skip-Gram usage. However, it lacks deeper integration or a novel framework. Critical analysis is minimal, as the section mainly describes the methods without evaluating their strengths or limitations. Abstraction is limited to a small extent, with some mention of temporal dependencies, but no overarching principles or meta-level insights are presented."}}
{"id": "f9af3d04-7d83-49d7-9e03-93e265d28723", "title": "Edge Reconstruction based Optimization with Temporal Smoothing", "level": "subsection", "subsections": [], "parent_id": "08d8737f-a239-4840-abf3-27d1eafb336f", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Edge Reconstruction based Optimization with Temporal Smoothing"]], "content": "\\label{subsec:edgereconstruction}\nFollowing the taxonomic approach proposed by Cai et al.~ for static graphs, some methods for dynamic graphs have been identified whose approach is similar to techniques that directly optimize an objective function based on edge reconstruction. In addition to either maximizing edge reconstruction probability or minimizing edge reconstruction loss, these approaches also preserve temporal smoothness. It is noteworthy that these methods may be understood as reconstructing temporal edges between a node $v$ at a given time $t_{i}$ and the same node at the subsequent timestamp, therefore justifying this category even more adequately for embedding methods.\nDynamicTriad~ is a representative method of this category, aiming to preserve both structural information and evolution patterns of a network by modeling how a closed triad (i.e. three vertices connected) develops from an open triad (i.e. three vertices where two of them are not connected). The authors define the probability that an open triad $(v,u,w)$ (where $v$ and $u$ are not connected) evolves into a closed triad, and the probability of the edge $(v,u)$ will not be created, joining these probabilities into a distance-based loss function. Moreover, the model supposes that highly connected nodes should be embedded closely in the low-dimensional vector space and imposes this condition by a margin-based rank loss function, and finally considers temporal smoothness at consecutive time stamps.\nOther approaches, solely based on a distance loss function include DNE~, and Liu et al.~. Time-Aware KB Embedding~ learns node embeddings by modeling relationships as translation operators in the low-dimensional vector space~ and optimizes a joint margin-based ranking loss function concerning both temporal order score function (the temporal encoding) and translation embeddings (the topological encoding). Table~\\ref{tab:edgereconstruction} lists the methods described above, and the loss function each technique aims to minimize, pointing how they handle temporal dependence.\n\\begin{table*}[t]\n\\scriptsize\n\\caption{Edge Reconstruction based Optimization.}\n\\vspace{-3mm}\n\\label{tab:edgereconstruction}\n\\centering\n\\tabcolsep=0.05cm\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular} {  |c|c|c| } \\hline\n\\textbf{Algorithm}& \\textbf{Temporal Dependence Handling} & \\textbf{Loss Function} \\\\ \\hline\nDNE~ & Delta of Theoretical Optimal Solution~ and Temporal Smoothness & Based on LINE~\\\\ \\hline\nDynamicTriad  & Temporal Smoothness & Triadic Closure and Social Homophily \\\\ \\hline\n & Temporal Smoothness & Based on Laplacian Eigenmaps  \\\\ \\hline\nTime-Aware KB Embedding~ & Temporal Order Score Function & Joint Margin-Based Rank \\\\ \\hline\n\\end{tabular}\n\\vspace{-1mm}\n\\end{table*}\nNote that the temporal smoothing given by a distance-based loss is similar to both matrix factorization problems and skip-gram based models. Indeed, there is a general view that demonstrates the relationship between network embedding approaches, matrix factorization, and Skip-Gram models~. Even further, Liu et al.~ provide a fundamental connection from an optimization perspective, which is the fundamental idea of edge reconstruction based methods. In this survey, these approaches are separated in the taxonomy to follow more strictly algorithmic properties rather than theoretical aspects of loss functions.", "cites": [215, 282], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several dynamic graph embedding methods by highlighting their shared focus on edge reconstruction and temporal smoothing, integrating concepts from different papers into a coherent category. It abstracts by connecting these approaches to broader concepts such as matrix factorization and skip-gram models. However, the critical analysis is limited, with no explicit discussion of weaknesses or trade-offs among the cited methods."}}
{"id": "ac0ee22a-3061-4076-8385-8077f204a126", "title": "Methods based on Graph Kernel", "level": "subsection", "subsections": [], "parent_id": "08d8737f-a239-4840-abf3-27d1eafb336f", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Methods based on Graph Kernel"]], "content": "\\label{subsec:graphkernels}\nAs presented by Cai et al.~ for static graphs, a few methods handle elementary substructures that are decomposed from a whole graph structure. They incorporate topological attributes built in the network processing step, including graphlet transitions count~, graphlet frequencies over time~ and adjacency matrix summation~, to learn representations capable of reconstructing such elaborate attributes using a shallow approach of an autoencoder. Hence, since these substructures are used as a topological building block of a static network, dynamic graph embedding takes into account the transitions between different elementary structures. In addition, BÃ©res et al.~ developed an online second-order similarity (SecondOrder) that learns neighborhood similarity by Min-Hash fingerprinting, modifying the embedding vector whenever a neighbor of $v_{i}$ gets more similar to $v_{j}$ after adding the edge $(v_{i},v_{j})$ into the network, which may be regarded as a graph kernel based approach.", "cites": [215, 7250], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methods based on graph kernels for dynamic graph embedding, mentioning key concepts from Cai et al. and BÃ©res et al. However, it lacks synthesis by not clearly connecting or contrasting the two works, and it does not offer critical evaluation or identify broader patterns or principles. The narrative remains at a surface level without deep analytical insight."}}
{"id": "74768985-5fe0-494f-bdbc-162bfc50f104", "title": "Methods based on Temporal Point Process", "level": "subsection", "subsections": [], "parent_id": "08d8737f-a239-4840-abf3-27d1eafb336f", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Techniques for the Embedding of Dynamic Graphs"], ["subsection", "Methods based on Temporal Point Process"]], "content": "Several dynamic graph embedding techniques, consider that interactions between nodes are stochastic processes whose probabilities depend on the topological structure of the network, on node features, and the network history. For these methods, it is assumed that an event influences a given node and, consequently, it can interact with other nodes in the network, if they are susceptible to the influence of the current node. Therefore, there is a probability that the event will propagate based on the mathematical definitions presented in Section~\\ref{subsec:temporalpointprocess}, such as the conditional intensity function.\nMain methods in this category include DyRep~, handling a continuous-time deep model of a temporal point process using a conditional intensity function modeling the occurrence of an event $p$ with time scale $k$ between nodes $v_{i}$ and $v_{j}$, and DeepCoEvolve~, modeling the user-item interaction as a multidimensional temporal point process. Other approaches include KnowEvolve~, modeling a fact in a knowledge graph as a temporal point process, M$^{2}$DNE~, capturing edge evolution by a temporal point process with an attentive mechanism, in addition to a general dynamics equation concerning the linking rate, and HTNE~ with an attention mechanism for neighborhood formation sequence of a node as a counting process. Furthermore, Knyazev et al.~ extend DyRep~, replacing the original encoder with a procedure that, given an event between nodes $v_{i}$ and $v_{j}$: (i)~calculates representations of all nodes at time $t_{k-1}$; (ii)~returns an edge embedding for all pair of nodes; (iii)~updates the embedding of node $v_{j}$ based on all edges connected to it; and (iv)~updates the edge embedding between nodes $v_{i}$ and $v_{j}$.\nMHDNE~ models the edge formation process as two temporal sequences with historical edge information and network evolution information therein, respectively. In particular, the network evolution is based on open triangles and triadic closure problem~, and the intensity function for a new edge creation at time $t$ is given by a Hawkes process, leveraging a term dependent on node embeddings, a time decay function on an exponential form, and the distance between node's neighborhood. Wu et al.~ propose a Graph Biased Temporal Point Process (GBTPP), which aims to compute the probability of an event propagating to nodes $v_{j} \\in \\mathcal{N}(v_{i})$ in the future timestamp $t_{k+1}$ given event propagation history and node $v_{i}$, which is influenced by the event at time $t_{k}$.", "cites": [7251, 7236, 7238], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several methods using temporal point processes by connecting them through the concept of event propagation and the use of conditional intensity functions. It provides a coherent narrative by categorizing these methods under a shared framework. However, while it highlights methodological components, it lacks deeper critical evaluation of their limitations or trade-offs. The abstraction is moderate, as it identifies general patterns like the use of attention mechanisms and Hawkes processes across the cited works."}}
{"id": "8fcfb4be-e561-450b-819f-c8982e39871e", "title": "Node-Related Applications", "level": "subsection", "subsections": [], "parent_id": "a4e1d817-74e3-47ef-8bd4-d1ba30553664", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Dynamic Graph Embedding Applications"], ["subsection", "Node-Related Applications"]], "content": "Node embeddings are used for various purposes on network analysis, with applications already known in static graphs, but handled over time, including \\textbf{node classification}, \\textbf{node clustering}, and \\textbf{recommendation systems}, up to novel applications, specific within the dynamic scenario, such as the \\textbf{node feature prediction} and \\textbf{trajectory tracking}.\n\\begin{compactitem}\n    \\item \\textbf{Node Classification:} The node classification problem focuses on the assignment of a class label to each node in a graph based on the rules learned from the labelled nodes. In a dynamic network, it is possible to (i)~classify nodes whose labels are unknown in a given timestamp~$t$, considering behavior and labels of the other nodes in the network; or (ii)~to predict the classification of a node in the future, given that node labels can vary over time~. Approaches concerning node classification apply a classifier on labeled node embeddings for training, including: (i)~linear layer~; (ii)~SVM~; (iii)~logistic regression~; (iv)~softmax~; and (v)~random forests and gradient boosting techniques~.\n    \\item \\textbf{Node Clustering:} While the classification task is supervised, clustering similar nodes is an unsupervised task, aiming to group similar vertices when information about labels is unavailable. An important challenge in this task is to ensure that the embeddings of similar nodes are close to each other in the vector space while being able to capture possible node transitions between different clusters over time, as in DynGem~ and dyngraph2vec variations~. Clusters in the embedding space can also represent different behavior patterns of nodes over time, which is defined by Rossi et al.~ as an analysis of the role evolution of each node in the network.\n    \\item \\textbf{Recommender Systems:} A dynamic network consisting of users, items, and timestamped interactions between users and items may be explored by embedding methods to recommend items to users according to their interests over time. As discussed by Kazemi et al.~a, recommendations may suggest items not exactly similar to user's interest in order to attract the user to a novel interest, and recommendations may arouse a future desire for items of certain type even if the user does not display any immediate interest.\n   \\item \\textbf{Node Attribute Prediction:} \nIt is important to predict time-varying attributes of network nodes. Formally, given a time-varying graph $\\mathcal{G} = \\{G(t_{0}),...,G(t_{N_{S}-1})\\}$ with additional node attributes $X(t) \\in \\mathbb{R}^{f}$ (where $f$ is the number of these attributes) as the training data, this task aims to estimate the real-valued variable $X(t_{N_{S}}) \\in \\mathbb{R}^{f}$ at time $t_{N_{S}}$ for each node in the graph~. This problem is also known as relational time series regression, and node embeddings over time can serve as input to time series prediction models, such as ARIMA or recurrent neural networks, to enhance prediction by leveraging topological information along with attribute evolution.\n    \\item \\textbf{Trajectory Tracking:} From the representations obtained by embedding over time, it is possible to make an analysis of the trajectories of each entity. Ferreira et al.~ capture ideological changes of two diverse party systems~(Brazil and the United States), as expressed by membersâ€™ voting behavior, by mapping the network into a temporal latent ideological space. Therefore, the authors have tracked individual members overtime in the low-dimensional vector space, analyzing how vector representations of individual members change and then measure ideological shifts over time. Although this task shares similarities with the node clustering problem, the focus of trajectory tracking is not on the groups themselves, but on the transitions between them.\n\\end{compactitem}", "cites": [7247, 7231, 478, 7245, 7232, 7250, 7251, 7239, 7235, 7249, 7238], "cite_extract_rate": 0.36666666666666664, "origin_cites_number": 30, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited works to present a coherent overview of node-related applications in dynamic graph embedding, such as classification, clustering, and trajectory tracking. It abstracts the common themes and tasks across different methods and integrates them into a structured discussion. However, while it mentions some techniques and their applications, it lacks deeper critical evaluation or comparative analysis of their strengths and weaknesses."}}
{"id": "d03536ec-25bc-42ef-ab4e-63b7f9e2fa71", "title": "Edge-Related Applications", "level": "subsection", "subsections": [], "parent_id": "a4e1d817-74e3-47ef-8bd4-d1ba30553664", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Dynamic Graph Embedding Applications"], ["subsection", "Edge-Related Applications"]], "content": "\\label{subsec:edgerelated}\nEdge-related tasks comprise the most commonly explored problem by the techniques presented in this survey: \\textbf{dynamic link prediction}. However, a novel application, when compared with static methods, appears in the literature: \\textbf{event time prediction}, whose focus is on the detection of the time instant when a new edge should appear. Finally, the \\textbf{edge classification} may also be treated on heterogeneous dynamic graphs where edges have labels.\n\\begin{compactitem}\n    \\item \\textbf{Dynamic Link Prediction:} Link prediction in dynamic networks is more complex than in static networks, since it comprises two different tasks: (i)~\\textbf{temporal link prediction}, (the prediction of new edges)~, where, given a sequence of snapshots of an evolving network $\\mathcal{G} = \\{G(t_{0}), ..., G(t_{N_{S} -1})\\}$, aims to predict the links in $G(t_{N_{S}})$ (where $N_{S}$ is the total number of snapshots), i.e. to construct a function $f(v,u)$ that predicts whether an edge $(v,u)$ exists between nodes $v$ and $u$ at time $t_{N_{S}}$~; and (ii)~\\textbf{link completion} (prediction of previously observed edges)~, which consists of finding the missing links along the evolving network. Most approaches consider link prediction as a classification task, where labels are (i)~existence; or (ii)~non-existence of an edge. Junuthula et al.~ provides a deeper discussion regarding evaluation of link prediction on dynamic networks, and Yu et al generalizes temporal link prediction to include: (i)~prediction of link weights, considering henceforth the aforementioned definition as a particular case when the link weight is restricted to 0 or 1, and (ii)~prediction at timestamp $N(S)+\\alpha$, where $\\alpha \\ge 0$, and the classical definition above regards the special case $\\alpha = 0$.\n    \\item \\textbf{Event Time Prediction:} In dynamic networks it is valid to question at which timestamp a given interaction can occur, configuring the event time prediction task. Methods based on temporal point process have a mathematical formulation to predict the next time point $t_{k}$ for an event given a pair of nodes $v$ and $u$ and network history (i.e. a list of interactions over time), including DyRep~ and DeepCoevolve~. \n    \\item \\textbf{Edge Classification:} When the edges of a network can belong to certain classes, the problem of classifying the edges shares similarities with the node classification task. For instance, interactions between nodes may be associated with a trustworthy rating (i.e. users who trust others, or not), or sentiment analysis (i.e. a post on a social network). Therefore, predicting the label of an edge $(v_{i}, v_{j})$ over time may be done concatenating node embeddings or operating over them to obtain edge embeddings, and applying a classifier afterward~. \n\\end{compactitem}", "cites": [7236, 7238], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of edge-related applications in dynamic graph embedding, including dynamic link prediction, event time prediction, and edge classification. While it introduces the concept of event time prediction and references specific papers, the integration of ideas is minimal and largely descriptive. There is little critical evaluation or abstraction beyond the specific tasks mentioned."}}
{"id": "76d792b0-e435-4094-a0c5-cc7251be03de", "title": "Node- and Edge-Related Tasks", "level": "subsection", "subsections": [], "parent_id": "a4e1d817-74e3-47ef-8bd4-d1ba30553664", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Dynamic Graph Embedding Applications"], ["subsection", "Node- and Edge-Related Tasks"]], "content": "\\label{subsec:nodeedgerelated}\nSome tasks can be applied to both nodes and edges in a graph, depending on how the problem is formulated. In this context, two tasks have been identified so far: (i)~\\textbf{anomaly detection}, which can be related to a node with anomalous behavior or to an unwanted or unexpected edge; and (ii)~\\textbf{diffusion prediction}, which can either identify which nodes will be affected by a diffusion process or detect edges most likely to diffuse information.\n\\begin{compactitem}\n    \\item \\textbf{Anomaly Detection:} Anomaly detection is an important application for detecting malicious activity in networks. These anomalies can be detected by unexpected changes in the vector representation of nodes and edges, suggesting that some non-expected activity is arising at some timestamp. Goyal et al.~ propose the definition of $\\delta_{t} = \\norm{Z(t_{k+1}) - Z(t_{k})}_{F}$ as the change in embedding between time $t_{k}$ and $t_{k+1}$, and a threshold to consider the node behavior as anomalous when its embeddings change above this value. This threshold value can be calibrated diferently to each specific problem, and can be tuned to identify certain types of anomalies based on node behavior changes, where each type of behavior is encoded differently. A similar approach is presented by Rossi et al.~, which defines several groups of behaviors for the nodes of the network based on node embeddings (i.e. similar to node clustering), hence the authors claim to detect anomalous behaviors from abrupt node transitions between different clusters.  Khoshraftar et al.~ formulate the anomaly detection as a classification task, where edges may belong to anomalous or normal class labels, therefore using node embeddings to compute edge representations and then using their method (LSTM-Node2vec) to classify the edges. \n    \\item \\textbf{Diffusion Prediction:} Diffusion problems solved by embeddings methods may be categorized as (i)~a sequence prediction problem (i.e. a microscopic diffusion problem), willing to predict the future affected node given the previously affected ones~; or (ii)~a regression problem, which predicts the future numerical properties of the network (e.g. the total number of infected nodes in a macroscopic diffusion problem)~.  \n\\end{compactitem}", "cites": [7247, 7252, 7248, 7232], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of node- and edge-related tasks in dynamic graph embedding, particularly focusing on anomaly detection and diffusion prediction. It integrates a few cited works but does so at a surface level without drawing broader connections or offering a novel synthesis. There is minimal critical evaluation or comparison of the approaches, and the abstraction remains limited to task-level generalizations without deeper conceptual insights."}}
{"id": "1e3df9b7-c962-4bd8-aa55-3ec57e396a28", "title": "Graph-Related Applications", "level": "subsection", "subsections": [], "parent_id": "a4e1d817-74e3-47ef-8bd4-d1ba30553664", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Dynamic Graph Embedding Applications"], ["subsection", "Graph-Related Applications"]], "content": "Problems related to the whole graph usually analyze the network globally, therefore dealing with tasks that are not centered on vertices or edges. The main examples verified in the literature are: \\textbf{graph classification}, \\textbf{network visualization}, and \\textbf{graph reconstruction}.\n\\begin{compactitem}\n\\item \\textbf{Graph Classification:} Classifying the whole graph over time into one class from a set of predefined categories $\\mathcal{L}^{G}$ is a relevant problem when the topological structure and possible attributes of each node in a network configure a global classifiable behavior~. By obtaining a whole graph embedding over time (either by aggregating node embeddings, or using graph kernels, as in Section~\\ref{subsec:graphkernels}), it is possible to use the same classifiers presented in the node classification task to either perform classifications at known timestamps or to predict the classification of the graph in the future.\n\\item \\textbf{Network Visualization:} It is also possible to visualize dynamic graphs in 2D or 3D space by applying dimensionality reduction techniques that preserve the embedding structure, such as t-SNE~, to node embeddings. Goyal et al.~ state that, to avoid visualization instability (i.e. embedding instability over time), t-SNE needs to be initialized with an identical random state for all timestamps. \n\\item \\textbf{Graph Reconstruction:} The learned vector representations may reconstruct the dynamic graph through operations in the vector space that decode similarity information between pairs of nodes, such as a dot product or pairwise distance to estimate the adjacency matrix or the weight matrix. Goyal et al.~ propose a methodology to rank pair of nodes according to their corresponding reconstructed similarity, then define the reconstruction precision as the ratio of real links in the top $k$ pair of nodes. \n\\end{compactitem}", "cites": [7232], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic overview of three graph-related applications of dynamic graph embeddingâ€”classification, visualization, and reconstructionâ€”but does so primarily through description rather than synthesis or critique. While it references Goyal et al. and mentions DynGEM, the connections between the cited papers and the broader context of the field are minimal. There is some abstraction in grouping these tasks under a common theme, but the analysis remains shallow and lacks a comparative or evaluative perspective."}}
{"id": "d42d226c-7dac-4062-bcd6-56b6381ae455", "title": "Conclusion and Future Directions", "level": "section", "subsections": [], "parent_id": "cc73abb4-e745-4f3e-9227-7cfc6839570d", "prefix_titles": [["title", "A Survey on Embedding Dynamic Graphs"], ["section", "Conclusion and Future Directions"]], "content": "\\label{sec:conc}\nIn this survey, we conducted a comprehensive review of the literature in embedding methods for dynamic graphs. We defined the problem of embeddings for dynamic graphs inspired by previous surveys on static graph embeddings, whereas introducing some important concepts to consider scenarios for dynamic graphs. We proposed a taxonomy to classify the problem settings for dynamic graph embedding problems, broadening the design presented in the static scenario by introducing fundamental time aspects in embeddings, such as the different dynamic graph models that can be embedded, in addition to embedding outputs that aggregate temporal information or track representation trajectories. We also proposed a taxonomy for the different embedding paradigms for dynamic graphs, classifying them according to the methodology they use, topological-temporal properties that preserve, and assumptions made for the method to be valid. After that, we summarized the applications that the embedding of dynamic graphs enables.\n\\begin{compactitem}\n    \\item \\textbf{Development and Expansion of Libraries and Frameworks for Dynamic Graph Embedding:} With the increasing number of dynamic graph embedding techniques, it is interesting to invest in developing and expanding a framework capable of unifying the different algorithms, applications, and standard benchmark datasets. Goyal et al.~ propose DynamicGEM, an open-source Python library consisting of state-of-the-art algorithms for dynamic graph embeddings, focusing on node embeddings. The library contains the evaluation framework for graph reconstruction, static and temporal link prediction, node classification, and temporal visualization, with various metrics implemented to evaluate the state-of-the-art methods, and examples of evolving networks from various domains. Since DynamicGEM provides a template to add new algorithms, it would be interesting to invest in further developing the package so as to incrementally insert new techniques. In addition, expanding the framework to include embeddings of other types (such as edges, hybrids, and whole graphs), as well as methods for continuous-time dynamic graphs. Building a reference package of embedding methods for dynamic graphs~(be it DynamicGEM or other) would benefit the community interested in this topic. \n    \\item \\textbf{Mathematical Analysis and Different Stability Metrics:} To define useful and accurate stability metrics is a future research field in the area, with a more theoretical focus. Goyal et al.~ suggest a metric considering the adjacency matrix and snapshot models. Nevertheless, it is necessary to explore these metrics in more detail, by testing them on real-world networks and checking the behavior of embeddings for different methods and problems. In addition, a more sophisticated mathematical analysis may promote a research field direction to improve understanding of the relationship between representations and the evolution of dynamic graphs. \n    \\item \\textbf{Temporal Multiscale Evolution Embedding:} In real networks, the phenomena of temporal evolution may be associated with different scales (e.g. daily, weekly, monthly, and yearly phenomena). It would be interesting to investigate embedding techniques capable of efficiently capturing these peculiarities in a methodology that deals with temporal multiscale evolution. Trivedi et al.~ make an important step forward in this direction by considering two different timescales: for dynamics on the network and dynamics of the network.\n    \\item \\textbf{Dynamic Hypergraph Embedding:} The existing models for dynamic graphs only consider edges connecting two nodes in the graph, therefore not being able to expand to hypergraphs, where sets of nodes form connections without necessarily having binary ordered relations. Although there are a few works in hypergraph embedding  and even fewer that are extended to dynamic hypergraphs , a promising area of research is to develop new methods for dynamic hypergraphs, as well as extending some of the existing dynamic graph embedding methods, allowing them to handle hyperedges.\n    \\item \\textbf{Capturing Latencies and Spatial-Temporal Edge Patterns:} Although the methods described in the survey capture dynamic behaviors such as topological evolution, feature evolution and processes on the network, more sophisticated temporal models take into account that nodes and edges are not created or removed instantaneously in the network~. Thus, latency is an important feature of dynamic networks, as it carries information about the affinity between nodes or a node within the network, and no method described above has a methodology for dealing with such factors. In addition, spatio-temporal edges make embedding of dynamic graphs more complex than extracting information from snapshots, or from timestamped edges, as temporal correlations are more complex between non-consecutive timestamps~.\n    \\item \\textbf{Generalization of Graph Embedding to Higher-Order Dimensional Networks:} With the diversity of models for dynamic graphs, it is noted that there is no consensus to define a more general model and, consequently, an embedding that can be generalized to as many dynamic graphs as possible. Keeping this in mind, there are graph generalization models, including MAGs~ and Stream Graphs , and a future direction of research may be the application of embedding methods in such models. Even more, it is interesting to suggest extending embedding for higher-order graphs, allowing not only the capture of temporal and topological properties but also of multilayer structures at a different time and connectivity scales.\n    \\item \\textbf{Generation of Property-Preserving Network Evolution in Embedding Space:} Several complex network properties, such as pathways, degree distribution, and scale invariance, may change over time, and finding out which patterns of temporal evolution conserve or change these properties is challenging. Cheng et al.~ propose a structure-preserving model reduction procedure  developed for linear network systems, whereas Rossi et al.~ propose a matrix factorization to discover roles of certain vertices in a network and study possible changes in these roles over time. One possible direction for future research is to use representations in low-dimensional spaces to study and generate evolutions in a network that are capable of preserving certain properties of interest. One possible way to pursue this idea is to explore generative models, such as variational autoencoders and GANs, and use learned distributions of input data to generate new networks that are similar to training, and thereby capture patterns associated with their characteristics.\n\\end{compactitem}\n\\section*{Acknowledgment}\nThis work has been partially supported by CAPES, CNPq, FAPEMIG, and FAPERJ. Moreover, this paper is dedicated to the memory of our dear co-worker Artur Ziviani, who passed away while this paper was being peer-reviewed. Artur was a brilliant researcher and dedicated advisor.\n\\bibliographystyle{unsrt}  \n\\bibliography{reference}\n\\end{document}\n\\endinput", "cites": [7253, 477, 7232], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple ideas from the cited works and integrates them into a coherent set of future research directions. It critically identifies limitations in current methods (e.g., lack of handling latencies, hypergraphs, and temporal multiscale phenomena), and offers thoughtful abstractions by proposing generalization strategies and broader modeling approaches. The insights go beyond description to highlight gaps and suggest novel research paths."}}
