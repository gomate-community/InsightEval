{"id": "1f54d4f8-ff8d-4fb2-9b18-c68ee9e185fc", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "d8acb238-d81f-45f2-82ee-ec0eb9a6e814", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\nThe prevalent applications\nof machine learning\narouse natural concerns about trustworthiness. \nSafety-critical applications such as \nself-driving systems  \nand medical treatments , \nincrease the importance of behaviour relating to correctness, robustness, privacy, efficiency and fairness.\nSoftware testing refers to any activity that aims to detect the differences between existing and required behaviour~.\nWith the recent rapid rise in interest and activity,\ntesting has been demonstrated to be an effective way to expose problems and potentially facilitate to improve the trustworthiness of machine learning systems.\nFor example, DeepXplore~, a differential white-box testing technique for deep learning, revealed thousands of incorrect corner\ncase behaviours in autonomous driving learning systems;\nThemis~, a fairness testing technique for detecting causal discrimination, \ndetected significant ML model discrimination towards gender, marital status, or race for as many as 77.2\\% of the individuals in datasets to which it was applied.\nIn fact, some aspects of the testing problem for machine learning systems are shared with well-known solutions already widely studied in the software engineering literature.\nNevertheless, the statistical nature of machine learning systems and their ability to make autonomous decisions raise additional, and challenging, research questions for software testing . \nMachine learning testing poses challenges that arise from the fundamentally different nature and construction of machine learning systems, compared to traditional (relatively more deterministic and less statistically-orientated) software systems.\nFor instance, a machine learning system inherently follows a data-driven programming paradigm, where the decision logic is obtained via a training procedure from training data under the machine learning algorithm's architecture~. \nThe model's behaviour may evolve over time, in response to the frequent provision of new data~. \nWhile this is also true of traditional software systems, the core underlying behaviour of a traditional system does not typically change in response to new data, in the way that a machine learning system can.\nTesting machine learning also suffers from a particularly pernicious instance of the \\emph{Oracle Problem} .\nMachine learning systems are difficult to test because they are designed to provide an answer to a question for which no previous answer exists~.\nAs Davis and Weyuker said~, for these kinds of systems `There would be no need to write such programs, if the correct answer were known'. \nMuch of the literature on testing machine learning systems seeks to find techniques that can tackle the Oracle problem, often drawing on traditional software testing approaches.\nThe behaviours of interest for machine learning systems are also typified by emergent properties, the effects of which can only be fully understood by considering the machine learning  system as a whole.\nThis makes testing harder, because it is less obvious how to break the system into smaller components that can be tested, as units, in isolation.\nFrom a testing point of view, this emergent behaviour has a tendency to migrate testing challenges from the unit level to the integration and system level.\nFor example, low accuracy/precision of a machine learning model is typically a composite effect, arising from a combination of the behaviours of different components such as the training data, the learning program, and even the learning framework/library~. \nErrors may propagate to become amplified or suppressed, inhibiting the tester's ability to decide where the fault lies.\nThese challenges also apply in more traditional software systems, where, for example, previous work has considered failed error propagation \nand the subtleties introduced by\nfault masking . \nHowever, these problems are far-reaching in machine learning systems, since they arise out of the nature of the machine learning approach and fundamentally affect all behaviours, rather than arising as a side effect of traditional data and control flow .\nFor these reasons, machine learning systems are thus sometimes regarded as \\emph{`non-testable'}  software.\nRising to these challenges, the literature has seen considerable progress and a notable upturn in interest and activity:\nFigure~\\ref{fig:numberofpublications} shows the cumulative number of publications on the topic of testing machine learning systems between 2007 and June 2019 \\jie{(we introduce how we collected these papers in Section~\\ref{sec:papercollection})}. \nFrom this figure, we can see that 85\\% of papers have appeared since 2016, testifying to the emergence of new software testing domain of interest: machine learning testing. \n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[scale=0.48]{figures/totalnumberpubs_overyear}\n\t\\caption{Machine Learning Testing Publications (accumulative) during 2007-2019}\n\t\\label{fig:numberofpublications}\n\\end{figure}\nIn this paper, we use the term `\\textbf{Machine Learning Testing}' (\\textbf{ML testing}) to refer to\nany activity aimed at detecting differences between existing and required behaviours of machine learning systems.\n\\mlt{} is different from testing approaches that use machine learning or those that are guided by machine learning, which should be referred to as `machine learning-based testing'.\nThis nomenclature accords with previous usages in the software engineering literature. \nFor example, the literature uses the terms `state-based testing'  \nand `search-based testing'  \nto refer to testing techniques that make use of concepts of state and search space, \nwhereas we use the terms `GUI testing' \nand `unit testing'  \nto refer to test techniques that tackle challenges of testing GUIs (Graphical User Interfaces) and code units. \nThis paper seeks to provide a comprehensive survey of \\mlt{}.\nWe draw together the aspects of previous work that specifically concern software testing, while simultaneously covering all types of approaches to machine learning that have hitherto been tackled using testing.\nThe literature is organised according to four different aspects: the testing properties (such as correctness, robustness, and fairness), machine learning components (such as the data, learning program, and framework), testing workflow (e.g., test generation, test execution, and test evaluation), and application scenarios (e.g., autonomous driving and machine translation).\nSome papers address multiple aspects.\nFor such papers, we mention them in all the aspects correlated (in different sections).\nThis ensures that each aspect is complete. \nAdditionally, we summarise research distribution (e.g., among testing different machine learning categories), trends, and datasets.\nWe also identify open problems and challenges for the emerging research community working at the intersection between techniques for software testing and problems in machine learning testing. \nTo ensure that our survey is self-contained, we aimed to include sufficient material to fully orientate software engineering researchers who are interested in testing and curious about testing techniques for machine learning applications.\nWe also seek to provide machine learning researchers with a complete survey of software testing solutions for improving the trustworthiness of machine learning systems.\nThere has been previous work that discussed or surveyed aspects of the literature related to \\mlt{}.\nHains et al.~, Ma et al.~, and Huang et al.~\nsurveyed secure deep learning, \\jie{in which the focus was deep learning security with testing being one of the assurance techniques}.\nMasuda et al.~ outlined their collected papers on software quality for machine learning applications in a short paper. \nIshikawa~ discussed the foundational concepts that might be used in any and all \\mlt{} approaches.\nBraiek and Khomh~ discussed defect detection in machine learning data and/or models in their review of 39 papers.\nAs far as we know, no previous work has provided a comprehensive survey particularly focused on machine learning testing.\nIn summary, the paper makes the following contributions:\n1) \\textbf{Definition}. The paper defines Machine Learning Testing (\\mlt{}), \noverviewing the concepts, testing workflow, testing properties, and testing components related to machine learning testing. \n2) \\textbf{Survey}. The paper provides a comprehensive survey of \\papernum machine learning testing papers, across various publishing areas such as software engineering, artificial intelligence, systems and networking, and data mining.\n3) \\textbf{Analyses}. The paper analyses and reports data on the research distribution, datasets, and trends that characterise the machine learning testing literature. We observed a pronounced imbalance in the distribution of research efforts: among the \\papernum papers we collected,\naround 120 of them tackle supervised learning testing, three of them tackle unsupervised learning testing, and only one paper tests reinforcement learning.\nAdditionally, \nmost of them (93) centre on correctness and robustness, \nbut only a few papers test interpretability, privacy, or efficiency.\n4) \\textbf{Horizons}. The paper identifies challenges, open problems, and promising research directions for \\mlt{}, with the aim of facilitating and stimulating further research.\nFigure~\\ref{fig:sectiontreegraph} depicts the paper structure. \nMore details of the review schema can be found in Section~\\ref{sec:schema}.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[scale=0.55]{figures/treegraph.pdf}\n\t\\caption{Tree structure of the contents in this paper}\n\t\\label{fig:sectiontreegraph}\n\\end{figure}", "cites": [6005, 6006, 885, 6007, 6004, 6003], "cite_extract_rate": 0.24, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction section effectively synthesizes multiple cited works to establish the context and challenges of ML testing, particularly highlighting the Oracle problem and emergent behaviors. It provides critical analysis by distinguishing ML testing from ML-based testing and noting the limitations and challenges unique to ML systems. The section abstracts these ideas to define broader principles and research directions, offering a conceptual framework for understanding the domain."}}
{"id": "58a9b56e-4c50-434a-b721-33c30dca7815", "title": "Preliminaries of Machine Learning", "level": "section", "subsections": [], "parent_id": "d8acb238-d81f-45f2-82ee-ec0eb9a6e814", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Preliminaries of Machine Learning"]], "content": "\\label{sec:preliminaries}\nThis section reviews the fundamental terminology in machine learning so as to make the survey self-contained. \nMachine Learning (ML) is a type of artificial intelligence technique that makes decisions or predictions from data~. \nA machine learning system is typically composed from following elements or terms.\n\\noindent \\textbf{Dataset}: A set of instances for building or evaluating a machine learning model. \nAt the top level, the data could be categorised as:\n\\begin{itemize}\n\\item\\textbf{Training data}: the data used to `teach' (train) the algorithm to perform its task.\n\\item \\textbf{Validation data}: the data used to tune the hyper-parameters of a learning algorithm.\n\\item \\textbf{Test data}: the data used to validate machine learning model behaviour. \n\\end{itemize}\n\\noindent \\textbf{Learning program}: the code written by developers to build and validate the machine learning system.\n\\noindent \\textbf{Framework}: the library, or platform being used when building a machine learning model, such as \\emph{Pytorch}~, \\emph{TensorFlow}~, \\emph{Scikit-learn}~, \\emph{Keras}~, and \\emph{Caffe}~.\nIn the remainder of this section, we give definitions for other ML terminology used throughout the paper.\n\\noindent \\textbf{Instance}: a piece of data recording the information about an object. \n\\noindent \\textbf{Feature}: a measurable property or characteristic of a phenomenon being observed to describe the instances.\n\\noindent \\textbf{Label}: value or category assigned to each data instance. \n\\noindent \\textbf{Test error}: the difference ratio between the real conditions and the predicted conditions.\n\\noindent \\textbf{Generalisation error}: the expected difference ratio between the real conditions and the predicted conditions of any valid data.\n\\noindent \\textbf{Model}: the learned machine learning artefact that encodes decision or prediction logic which is trained from the training data, the learning program, and frameworks. \nThere are different types of machine learning.\nFrom the perspective of training data characteristics, machine learning includes:\n\\noindent \\textbf{Supervised learning}: a type of machine learning that learns from training data with labels as learning targets. \nIt is the most widely used type of machine learning~.\n\\noindent \\textbf{Unsupervised learning}: a learning methodology that learns from training data without labels and relies on understanding the data itself.\n\\noindent \\textbf{Reinforcement learning}: a type of machine learning where the data are in the form of sequences of\nactions, observations, and rewards, and the learner learns how to take actions to interact in a specific environment so as to maximise the specified rewards. \nLet $\\mathcal{X}=(x_1,...,x_m)$ be the set of unlabelled training data.\nLet $\\mathcal{Y}=(c(x_1),...,c(x_m))$ be the set of labels corresponding to each piece of training data $x_i$.\nLet concept $C: \\mathcal{X}\\rightarrow \\mathcal{Y}$ be the mapping from $\\mathcal{X}$ to $\\mathcal{Y}$ (the real pattern).\nThe task of supervised learning is to learn a mapping pattern, i.e., a model, $h$ based on $\\mathcal{X}$ and $\\mathcal{Y}$ so that the learned model $h$ is similar to its true concept $C$ with a very small generalisation error. \nThe task of unsupervised learning is to learn patterns or clusters from the data without knowing the existence of labels $\\mathcal{Y}$.\n\\jienew{Reinforcement learning guides and plans with the learner actively interacting with the environment to achieve a certain goal.\nIt is usually modelled as a Markov decision process.\nLet $S$ be a set of states, $A$ be a series of actions.\nLet $s$ and $s'$ be two states. \nLet $r_a(s,s')$ be the reward after transition from $s$ to $s'$ with action $a \\in A$.\nReinforcement learning is to learn how to take actions in each step to maximise the target awards.  \n}\nMachine learning can be applied to the following typical tasks~\\footnote{\\jienew{These tasks are defined based on the nature of the problems solved instead of specific application scenarios such as language modelling.}}:\n1) \\textbf{Classification}: to assign a category to each data instance; E.g., image classification, handwriting recognition.\n2) \\textbf{Regression}: to predict a value for each data instance; \nE.g., temperature/age/income prediction.\n3) \\textbf{Clustering}: to partition instances into homogeneous regions;\nE.g., pattern recognition, market/image segmentation.\n4) \\textbf{Dimension reduction}: to reduce the training complexity;\nE.g., dataset representation, data pre-processing.\n5) \\textbf{Control}: to control actions to maximise rewards;\nE.g., game playing.\nFigure~\\ref{fig:mltypeandtask} shows the relationship between different categories of machine learning and the five machine learning tasks.\nAmong the five tasks, classification and regression belong to supervised learning;\nClustering and dimension reduction belong to unsupervised learning.\nReinforcement learning is widely adopted to control actions, such as to control AI-game players to maximise the rewards for a game agent.\n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[scale=0.63]{figures/ML_type_task.pdf}\n\t\\caption{Machine learning categories and tasks}\n\t\\label{fig:mltypeandtask}\n\\end{figure}\nIn addition, machine learning can be classified into \\textbf{classic machine learning} and \\textbf{deep learning}.\nAlgorithms like Decision Tree~, SVM~, linear regression~, and Naive Bayes~ all belong to classic machine learning.\nDeep learning~ applies Deep Neural Networks (DNNs) that uses multiple layers of nonlinear processing units for feature extraction and transformation. \nTypical deep learning algorithms often follow some widely used neural network structures like Convolutional Neural Networks (CNNs)~\nand Recurrent Neural Networks (RNNs)~.\nThe scope of this paper involves both classic machine learning and deep learning.", "cites": [8688, 1159, 166, 2671], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear and factual overview of machine learning concepts and tasks but does not engage in meaningful synthesis of the cited papers. It mentions the papers primarily to support definitions (e.g., CNNs, RNNs, Caffe) without connecting their contributions or contrasting their approaches. There is minimal critical analysis or abstraction to broader principles, making the section primarily descriptive in nature."}}
{"id": "2adc1ae6-e37b-4a78-a376-6c489532b41e", "title": "Role of Testing in ML Development", "level": "subsubsection", "subsections": [], "parent_id": "34fe8310-4367-4844-ab58-340f55e6537c", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Workflow"], ["subsubsection", "Role of Testing in ML Development"]], "content": "Figure~\\ref{fig:mltestroles} shows the life cycle of deploying a machine learning system with ML testing activities involved.\nAt the very beginning, a prototype model is generated based on historical data;\nbefore deploying the model online, one needs to conduct offline testing, such as cross-validation, to make sure that the model meets the required conditions.\nAfter deployment, the model makes predictions, yielding new data that can be analysed via online testing to evaluate how the model interacts with user behaviours.\nThere are several reasons that make online testing essential.\nFirst, offline testing usually relies on test data, while test data usually fails to fully represent future data~;\nSecond, offline testing is not able to test some circumstances that may be problematic in real applied scenarios, such as data loss and call delays.\nIn addition, offline testing has no access to some business metrics such as open rate, reading time, and click-through rate.\nIn the following, we present an \\mlt{} workflow adapted from classic software testing workflows.\nFigure~\\ref{fig:testworkflow} shows the workflow, including both offline testing and online testing.", "cites": [6008], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the role of testing in ML development and mentions a few limitations of offline testing, but it does not deeply synthesize or integrate the cited paper. The single citation is not effectively connected to the broader discussion, and there is little critical analysis or identification of overarching principles or trends."}}
{"id": "7e2eab0c-b4a5-4351-ac3c-bf87ce66fd41", "title": "Online Testing", "level": "subsubsection", "subsections": [], "parent_id": "34fe8310-4367-4844-ab58-340f55e6537c", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Workflow"], ["subsubsection", "Online Testing"]], "content": "Offline testing tests the model with historical data without in the real application environment.\nIt also lacks the data collection process of user behaviours.\nOnline testing complements the shortage of offline testing, and aims to detect bugs after the model is deployed online.\nThe workflow of online testing is shown by the bottom of Figure~\\ref{fig:testworkflow}.\n\\jienew{There are different methods of conducting online testing for different purposes. For example, runtime monitoring keeps checking whether the running ML systems meet the requirements or violate some desired runtime properties. \nAnother commonly used scenario is to monitor user responses, based on which to find out whether the new model is superior to the old model under certain application contexts.\nA/B testing is one typical type of such online testing~.\nIt splits customers to compare two versions of the system (e.g., web pages).\nWhen performing A/B testing on ML systems, the sampled users will be split into two groups using the new and old ML models separately. \n}\nMAB (Multi-Armed Bandit) is another online testing approach~. \nIt first conducts A/B testing for a short time and finds out the best model, then put more resources on the chosen model.", "cites": [6009], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of online testing methods in ML systems, including A/B testing and MAB. It integrates minimal information from the cited paper and lacks deeper synthesis or comparison of approaches. There is no critical evaluation or abstraction of broader principles, and the narrative remains largely factual and surface-level."}}
{"id": "a0f5118e-557e-49ef-914a-5a33a0d2af51", "title": "ML Testing Components", "level": "subsection", "subsections": [], "parent_id": "008f5609-e19c-4a43-8776-0b259dddd696", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Components"]], "content": "\\label{sec:mltestingcomponents}\nTo build a machine learning model, an ML software developer usually needs to collect data, label the data, design learning program architecture, and implement the proposed architecture based on specific frameworks. \nThe procedure of machine learning model development requires interaction with several components such as data, learning program, and learning framework, while each component may contain bugs.\nFigure~\\ref{fig:newmlcomponent} shows the basic procedure of building an ML model and the major components involved in the process.\nData are collected and pre-processed for use;\nthe learning program is the code for running to train the model;\nthe framework (e.g., Weka, scikit-learn, and TensorFlow) offers algorithms and other libraries for developers to choose from, when writing the learning program.\nThus, when conducting \\mlt{}, developers may need to try to find bugs in every component including the data, the learning program, and the framework.\nIn particular, error propagation is a more serious problem in ML development because the components are more closely bonded with each other than traditional software~, which indicates the importance of testing each of the ML components.\nWe introduce the bug detection in each ML component below:\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.48]{figures/ml-three-component.pdf}\n\t\\caption{Components (shown by the grey boxes) involved in ML model building.}\n\t\\label{fig:newmlcomponent}\n\\end{figure}\n\\noindent \\textbf{\\textbf{Bug Detection in Data}}.\nThe behaviours of a machine learning system largely depends on data~.\nBugs in data affect the quality of the generated model, and can be amplified to yield more serious problems over a period a time~.\nBug detection in data checks problems such as whether the data is sufficient for training or test a model (also called completeness of the data~), whether the data is representative of future data, \nwhether the data contains a lot of noise such as biased labels,\nwhether there is skew between training data and test data~, \nand whether there is data poisoning~ or adversary information that may affect the model's performance.\n\\noindent \\textbf{\\textbf{Bug Detection in Frameworks}}.  \nMachine Learning requires a lot of computations.\nAs shown by Figure~\\ref{fig:newmlcomponent}, ML frameworks offer algorithms to help write the learning program, and platforms to help train the machine learning model, making it easier for developers to build solutions for designing, training and validating algorithms and models for complex problems.\nThey play a more important role in ML development than in traditional software development.\nML Framework testing thus checks whether the frameworks of machine learning have bugs that may lead to problems in the final system~.\n\\noindent \\textbf{\\textbf{Bug Detection in Learning Program}}. \nA learning program can be classified into two parts: the algorithm designed by the developer or chosen from the framework, and the actual code that developers write to implement, deploy, or configure the algorithm.\nA bug in the learning program may arise either because the algorithm is designed, chosen, or configured improperly, or because the developers make typos or errors when implementing the designed algorithm.", "cites": [6010], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general overview of ML testing components (data, learning program, and framework) and lists common types of bugs associated with each. However, it does not effectively synthesize information from the cited papers, as it only mentions one paper in passing without integrating its specific contributions or insights. There is minimal critical analysis or abstraction, with the discussion remaining mostly descriptive and lacking a deeper evaluation or meta-level understanding of the field."}}
{"id": "62443559-5e83-4943-ab40-58a4808ed30c", "title": "Model Relevance", "level": "subsubsection", "subsections": [], "parent_id": "e0bd2235-a0fb-453b-99b3-ece324d0df54", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Properties"], ["subsubsection", "Model Relevance"]], "content": "A machine learning model comes from the combination of a machine learning algorithm and the training data.\nIt is important to ensure that\nthe adopted machine learning algorithm be not over-complex than just needed~.\nOtherwise, the model may fail to have good performance on future data, or have very large uncertainty.\n\\jienew{\nThe algorithm capacity\nrepresents the number of functions that a machine learning model can select (based on the training data at hand) as a possible solution.\nIt is usually approximated by VC-dimension~ or Rademacher Complexity~ for classification tasks.\nVC-dimension is the cardinality of the largest set of points that the algorithm can shatter.\nRademacher Complexity is the cardinality of the largest set of training data with fixed features that the algorithm shatters.\n}\n\\jienew{We define the relevance between a machine learning algorithm capacity and the data distribution as the problem of \\emph{model relevance}.}\n\\begin{definition}[Model Relevance]\nLet $\\mathcal{D}$ be the training data distribution.\nLet $R(\\mathcal{D}, \\mathcal{A})$ be the simplest required capacity of any machine learning algorithm $\\mathcal{A}$ for $\\mathcal{D}$.\n$R'(\\mathcal{D},\\mathcal{A'})$ is the capacity of the machine learning algorithm $\\mathcal{A'}$ under test.\nModel relevance is the difference between \n$R(\\mathcal{D}, \\mathcal{A})$ and $R'(\\mathcal{D}, \\mathcal{A'})$.\n\\begin{equation}\nf = |(R(\\mathcal{D}, \\mathcal{A})-R'(\\mathcal{D}, \\mathcal{A'})|\n\\end{equation}\t\n\\end{definition}\n\\jie{Model relevance aims to measure how well a machine learning algorithm fits the data. A low model relevance is usually caused by overfitting, where the model is too complex for the data, which thereby fails to generalise to future data or to predict observations robustly.}\nOf course, $R'(\\mathcal{D}, \\mathcal{A})$, the minimum complexity that is `just sufficient' is hard to determine, and is typically approximate~.\nWe discuss more strategies that could help alleviate the problem of overfitting in Section~\\ref{sec:overfitting}.", "cites": [6008], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section introduces the concept of model relevance by connecting it to algorithm capacity and overfitting, and it cites one relevant paper to support the discussion. While it integrates the idea of overfitting with testing principles, it lacks synthesis from multiple sources and deeper comparative or critical analysis. However, it does abstract the problem into a formal definition and highlights a broader issue in ML testing."}}
{"id": "2d99b00a-8833-4779-89b3-ac353b074fff", "title": "Robustness", "level": "subsubsection", "subsections": [], "parent_id": "e0bd2235-a0fb-453b-99b3-ece324d0df54", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Properties"], ["subsubsection", "Robustness"]], "content": "Robustness is defined by the IEEE standard glossary of software engineering terminology~ as:\n`The degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions'.\nAdopting a similar spirit to this definition, we define the robustness of ML as follows:\n\\begin{definition}[Robustness]\nLet $S$ be a machine learning system. \nLet $E(S)$ be the correctness of $S$. \nLet $\\delta(S)$ be the machine learning system with perturbations on any machine learning components such as the data, the learning program, or the framework.\nThe robustness of a machine learning system is a measurement of the difference between $E(S)$ and $E(\\delta(S))$:\n\\begin{equation}\nr = E(S)-E(\\delta(S))\n\\end{equation}\t\n\\end{definition}\nRobustness thus measures the resilience of an ML system's correctness in the presence of perturbations.\nA popular sub-category of robustness is called \\emph{adversarial robustness}. \nFor adversarial robustness, the perturbations are designed to be hard to detect.\nFollowing the work of Katz et al.~, we classify adversarial robustness into local adversarial robustness and global adversarial robustness.\nLocal adversarial robustness is defined as follows.\n\\begin{definition}[Local Adversarial Robustness]\nLet $x$ a test input for an ML model $h$. \nLet $x'$ be another test input generated via conducting adversarial perturbation on $x$.\nModel $h$ is $\\delta$-local robust at input $x$ if for any $x'$. \n\\begin{equation}\n \\forall x': ||x-x'||_p \\leq \\delta \\rightarrow h(x)=h(x') \n\\end{equation}\n\\end{definition}\n$||\\cdot||_p$ represents $p$-norm for distance measurement. The commonly used $p$ cases in machine learning testing are 0, 2, and $\\infty$. For example, when $p=2$, i.e.\\, $||x-x'||_2$ represents the Euclidean distance of $x$ and $x'$.\nIn the case of $p=0$, it calculates the element-wise difference between $x$ and $x'$.\nWhen $p=\\infty$, it measures the the largest element-wise distance among all elements of $x$ and $x'$.\nLocal adversarial robustness concerns the robustness at one specific test input, while global adversarial robustness measures robustness against all inputs. \nWe define global adversarial robustness as follows.\n\\begin{definition}[Global Adversarial Robustness]\nLet $x$ a test input for an ML model $h$. \nLet $x'$ be another test input generated via conducting adversarial perturbation on $x$.\nModel $h$ is $\\epsilon$-global robust if for any $x$ and $x'$.\n\\begin{equation}\n\\forall x, x':  ||x-x'||_p \\leq \\delta \\rightarrow h(x)-h(x') \\leq \\epsilon\n\\end{equation}\n\\end{definition}", "cites": [6011], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical treatment of robustness in ML testing by introducing formal definitions and distinguishing between local and global robustness, influenced by cited work such as Katz et al. It synthesizes the concept across multiple components but focuses more on foundational ideas rather than comparing or critiquing different approaches in depth. Some abstraction is achieved through general definitions, though broader trends or limitations of the approaches are not deeply explored."}}
{"id": "0c4299b5-f134-40e2-8028-3981cb3f43bb", "title": "Efficiency", "level": "subsubsection", "subsections": [], "parent_id": "e0bd2235-a0fb-453b-99b3-ece324d0df54", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Properties"], ["subsubsection", "Efficiency"]], "content": "The efficiency of a machine learning system refers to its construction or prediction speed. An efficiency problem happens when the system executes slowly or even infinitely during the construction or the prediction phase.\nWith the exponential growth of data and complexity of systems, efficiency is an important feature to consider for model selection and framework selection, sometimes even more important than accuracy~.\nFor example, to deploy a large model to a mobile device, optimisation, compression, and device-oriented customisation may be performed to make it feasible for the mobile device execution in a reasonable time, but accuracy may sacrifice to achieve this.", "cites": [8088], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section on Efficiency primarily describes the concept and its relevance, but it fails to effectively synthesize or integrate the cited paper. There is no discussion of how the paper contributes to the understanding of efficiency in ML testing. The section lacks critical evaluation and abstraction, offering only a superficial explanation without broader insights or comparative analysis."}}
{"id": "35103353-9b9e-4d82-98c0-3a1fe03eb881", "title": "Interpretability", "level": "subsubsection", "subsections": [], "parent_id": "e0bd2235-a0fb-453b-99b3-ece324d0df54", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Machine Learning Testing"], ["subsection", "ML Testing Properties"], ["subsubsection", "Interpretability"]], "content": "Machine learning models are often applied to assist/make decisions in medical treatment, income prediction, or personal credit assessment.\nIt may be important for humans to understand the `logic' behind the final decisions, so that they can build trust over the decisions made by ML~.\nThe motives and definitions of interpretability are diverse and still somewhat discordant~.\nNevertheless, unlike fairness, a mathematical definition of ML interpretability remains elusive~.\nReferring to the work of Biran and Cotton~ as well as the work of Miller~, \\jienew{we describe the interpretability of ML as the degree to which an observer can understand the cause of a decision made by an ML system.}\nInterpretability contains two aspects: transparency (how the model works) and post hoc explanations (other information that could be derived from the model)~.\nInterpretability is also regarded as a request by regulations like GDPR~, where the user has the legal `right to explanation' to ask for an explanation of an algorithmic decision that was made about them.\nA thorough introduction of ML interpretability can be referred to in the book of Christoph~.", "cites": [8500, 1798, 7621, 6976, 6012], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on interpretability, connecting ideas such as the lack of consensus on definitions, the role of transparency and post hoc explanations, and legal implications from GDPR. It provides a critical note by highlighting the elusiveness of a mathematical definition and the underspecified nature of interpretation. While it identifies some broader themes, such as the importance of interpretability for trust and regulation, it does not fully abstract or propose a novel framework for understanding the concept."}}
{"id": "d71b45f7-54e2-4ccd-bd92-0159c4c6dd31", "title": "Survey Scope", "level": "subsection", "subsections": [], "parent_id": "a5671273-3b09-4bb3-8465-635b2b5aa165", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Paper Collection and Review Schema"], ["subsection", "Survey Scope"]], "content": "\\label{sec:surveyscope}\nAn ML system may include both hardware and software. \nThe scope of our paper is software testing (as defined in the introduction) applied to machine learning.\nWe apply the following inclusion criteria when collecting papers.\nIf a paper satisfies any one or more of the following criteria, we will include it.\nWhen speaking of related `aspects of ML testing', we refer to the ML properties, ML components, and ML testing procedure introduced in Section~~\\ref{sec:preliminaries}.\n\\noindent 1) The paper introduces/discusses the general idea of ML testing or one of the related aspects of ML testing.\n\\noindent 2) The paper proposes an approach, study, or tool/framework that targets testing one of the ML properties or components.\n\\noindent 3) The paper presents a dataset or benchmark especially designed for the purpose of ML testing.\n\\noindent 4) The paper introduces a set of measurement criteria that could be adopted to test one of the ML properties.\nSome papers concern traditional validation of ML model performance such as the introduction of precision, recall, and cross-validation.\nWe do not include these papers because they have had a long research history and have been thoroughly and maturely studied.\nNevertheless, for completeness, we include the knowledge when introducing the background to set the context.\nWe do not include the papers that adopt machine learning techniques for the purpose of traditional software testing and also those target ML problems, which do not use testing techniques as a solution. \n\\jienew{\nSome recent papers also target the formal guarantee on the desired properties of a machine learning system, i.e., to formally verify the correctness of the machine learning systems as well as other properties.\nTesting and verification of machine learning, as in traditional testing, have their own advantages and disadvantages. For example, verification usually requires a white-box scenario,\nbut suffers from poor scalability, while testing may scale, but lacks completeness.\nThe size of the space of potential behaviours may render current approaches to verification infeasible in general~, but specific safety critical properties will clearly benefit from focused research activity on scalable verification, as well as testing.\nIn this survey, we focus on the machine learning testing.\nMore details for the literature review of the verification of machine learning systems can be found in the recent work of Xiang et al.~.\n}", "cites": [8970, 6013], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the scope of ML testing by integrating inclusion criteria and connecting them to broader concepts like ML properties and components. It also critically distinguishes between ML testing and traditional model validation, as well as ML verification, offering a balanced perspective on their respective strengths and limitations. The abstraction is strong, as it frames the discussion in terms of general principles (e.g., white-box vs. black-box, scalability vs. completeness) that apply beyond the cited works."}}
{"id": "36e0c35d-35d2-4978-8517-e685ccbc3a40", "title": "Domain-specific Test Input Synthesis", "level": "subsubsection", "subsections": [], "parent_id": "296787e0-cbc4-441f-8f84-1f5b6c995a69", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Input Generation"], ["subsubsection", "Domain-specific Test Input Synthesis"]], "content": "Test inputs of \\mlt{} can be classified into two categories: adversarial inputs and natural inputs.\nAdversarial inputs are perturbed based on the original inputs. \nThey may not belong to normal data distribution (i.e., maybe rarely exist in practice), but could expose robustness or security flaws. \nNatural inputs, instead, are those inputs that belong to the data distribution of a practical application scenario.\nHere we introduce the related work that aims to generate natural inputs via domain-specific test input synthesis.\nDeepXplore~ proposed a white-box differential testing technique to generate test inputs for a deep learning system.\nInspired by test coverage in traditional software testing, the authors proposed neuron coverage to drive test generation (we discuss different coverage criteria for \\mlt{} in Section~\\ref{sec:modelevaluationmetrics}).\nThe test inputs are expected to have high neuron coverage.\nAdditionally, the inputs need to expose differences among different DNN models, as well as be like real-world data as much as possible.\nThe joint optimisation algorithm iteratively uses a gradient search to find a modified input that satisfies all of these goals.\nThe evaluation of DeepXplore indicates that it covers 34.4\\% and 33.2\\% more neurons than the same number of randomly picked inputs and adversarial inputs.\nTo create useful and effective data for autonomous driving systems, \\emph{DeepTest}~ performed greedy search with nine different realistic image transformations: changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect.\nThere are three types of image transformation styles provided in OpenCV\\footnote{\\url{https://github.com/itseez/opencv}(2015)}: linear, affine, and convolutional.\nThe evaluation of DeepTest uses the Udacity self-driving car challenge dataset~.\nIt detected more than 1,000 erroneous behaviours on CNNs and RNNs with low false positive rates\\footnote{The examples of detected erroneous behaviours are available at \\url{https://deeplearningtest.github.io/deepTest/}.}.\nGenerative adversarial networks (GANs)~ are algorithms to generate models that approximate the manifolds and distribution on a given set of data. GAN has been successfully applied to advanced image transformation~(e.g., style transformation, scene transformation) that look at least superficially authentic to human observers.\nZhang et al.~ applied GAN to deliver driving scene-based test generation with various weather conditions.\nThey sampled images from Udacity Challenge dataset~ and YouTube videos (snowy or rainy scenes), and fed them into the UNIT framework\\footnote{A recent\nDNN-based method to perform image-to-image transformation~} for training. \nThe trained model takes the whole Udacity images as the seed inputs and yields transformed images as generated tests.\nZhou et al.~ proposed \\emph{DeepBillboard} to generate real-world adversarial billboards that can trigger potential steering errors of autonomous driving systems.\nTo test audio-based deep learning systems, Du et al.~ designed a set of transformations tailored to audio inputs considering background noise and volume variation. They first abstracted and extracted a probabilistic transition model from an RNN. Based on this, stateful testing criteria are defined and used to guide test generation for stateful machine learning system.\nTo test the image classification platform when classifying biological cell images, Ding et al.~ built a testing framework for biological cell classifiers.\nThe framework iteratively generates new images and uses metamorphic relations for testing.\nFor example, they generate new images by increasing the number/shape of artificial mitochondrion into the biological cell images, which can arouse easy-to-identify changes in the classification results.\nRabin et al.~ discussed the possibilities of testing code2vec (a code embedding approach~) with semantic-preserving program transformations serving as test inputs.\nTo test machine translation systems, Sun et al.~ automatically generate test inputs via mutating the words in translation inputs.\nIn order to generate translation pairs that ought to yield consistent translations, their approach conducts word replacement based on word embedding similarities.\nManual inspection indicates that the test generation has a high precision (99\\%) on generating input pairs with consistent translations.", "cites": [6017, 6016, 3500, 1208, 6007, 6015, 6018, 6014], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple works by organizing them under a common theme of domain-specific test input generation and connecting their goals (e.g., neuron coverage, real-world relevance). It provides critical insights by evaluating the effectiveness of methods like DeepXplore and DeepTest through reported metrics. However, while it identifies patterns across application domains, it does not fully abstract to overarching principles or theories about ML testing workflows."}}
{"id": "a921e924-23bd-4f53-af03-65976f6f6b5a", "title": "Fuzz and Search-based Test Input Generation", "level": "subsubsection", "subsections": [], "parent_id": "296787e0-cbc4-441f-8f84-1f5b6c995a69", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Input Generation"], ["subsubsection", "Fuzz and Search-based Test Input Generation"]], "content": "Fuzz testing is a traditional automatic testing technique that generates random data as program inputs to detect crashes, memory leaks, failed (built-in) assertions, etc, with many sucessfully application to system security and vulnerability detection~.\nAs another widely used test generation technique, search-based test generation often uses metaheuristic search techniques to guide the fuzz process for more efficient and effective test generation~.\nThese two techniques have also been proved to be effective in exploring the input space of \\mlt{}:\nOdena et al.~ presented \\emph{TensorFuzz}.\n\\emph{TensorFuzz} used a simple nearest neighbour hill climbing approach to explore achievable coverage over valid input space for Tensorflow graphs, and to discover numerical errors, disagreements between neural networks and their quantized versions, and surfacing undesirable behaviour in RNNs.\nDLFuzz, proposed by Guo et al.~, is another fuzz test generation tool based on the implementation of DeepXplore with nueron coverage as guidance.\nDLFuzz aims to generate adversarial examples.\nThe generation process thus does not require similar functional deep learning systems for\ncross-referencing check like DeepXplore and TensorFuzz.\nRather, it needs only minimum changes over the original inputs to find those new inputs that improve neural coverage but have different predictive results from the original inputs.\nThe preliminary evaluation on MNIST and ImageNet shows that compared with DeepXplore, DLFuzz is able to generate 135\\% to 584.62\\% more inputs with 20.11\\% less time consumption.\nXie et al.~ presented a metamorphic transformation based coverage guided fuzzing technique, DeepHunter, which leverages both neuron coverage and coverage criteria  presented by DeepGauge~.\nDeepHunter uses a more fine-grained metamorphic mutation strategy to generate tests, which demonstrates the advantage in reducing the false positive rate. It also demonstrates its advantage in achieving high coverage and bug detection capability. \nWicker et al.~ proposed feature-guided test generation.\nThey adopted Scale-Invariant Feature Transform (SIFT) to identify features that represent an image with a Gaussian mixture model, then transformed the problem of finding adversarial examples into a two-player turn-based stochastic game.\nThey used Monte Carlo Tree Search to identify those elements of an image most vulnerable as the means of generating adversarial examples.\nThe experiments show that their black-box approach is competitive with some state-of-the-art white-box methods.\nInstead of targeting supervised learning, Uesato et al.~ proposed to evaluate reinforcement learning with adversarial example generation.\nThe detection of catastrophic failures is expensive because failures are rare. \nTo alleviate the consequent cost of finding such failures, the authors proposed to use a failure probability predictor to estimate the probability that the agent fails, which was demonstrated to be both effective and efficient.\nThere are also fuzzers for specific application scenarios other than image classifications.\nZhou et al.~ \ncombined fuzzing and metamorphic testing to test the LiDAR obstacle-perception module of real-life self-driving cars, and reported previously unknown software faults.\nJha et al.~ investigated how to generate the most effective test cases (the faults that are most likely to lead to violations of safety conditions) via modelling the fault injection as a Bayesian network. \nThe evaluation, based on two production-grade AV systems from NVIDIA and Baidu, revealed many situations where faults lead to safety violations. \nUdeshi and Chattopadhyay~ generate inputs for text classification tasks and produce a fuzzing approach that considers the grammar under test as well as the distance between inputs.\nNie et al.~ and Wang et al.~ mutated the sentences in NLI (Natural Language Inference) tasks to generate test inputs for robustness testing.\nChan et al.~ generated adversarial examples for DNC to expose its robustness problems.\nUdeshi et al.~ focused much on individual fairness and generated test inputs that highlight the discriminatory nature of the model under test.\nWe give details about these domain-specific fuzz testing techniques in Section~\\ref{sec:applicationsenario}.\nTuncali et al.~ proposed a framework for testing autonomous driving systems.\nIn their work they compared three test generation strategies: \nrandom fuzz test generation,\ncovering array~ $+$ fuzz test generation,\nand covering array $+$ search-based test generation (using Simulated Annealing algorithm~).\nThe results indicated that the test generation strategy with search-based technique involved has the best performance in detecting glancing behaviours.", "cites": [8090, 6019, 6020, 6023, 6024, 8089, 6022, 6025, 6021], "cite_extract_rate": 0.45, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple fuzz and search-based test generation techniques effectively, connecting them under the broader theme of exploring input space in ML systems. It includes some critical evaluation, such as noting that DLFuzz does not require cross-referencing systems and that certain techniques reduce false positives. However, it mainly presents findings and results rather than offering deep, nuanced critiques or proposing a novel overarching framework."}}
{"id": "46f322b4-15b3-4f37-8783-c760b1e75115", "title": "Symbolic Execution Based Test Input Generation", "level": "subsubsection", "subsections": [], "parent_id": "296787e0-cbc4-441f-8f84-1f5b6c995a69", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Input Generation"], ["subsubsection", "Symbolic Execution Based Test Input Generation"]], "content": "Symbolic execution is a program analysis technique to test\nwhether certain properties can be violated by the software under test~. \nDynamic Symbolic Execution (DSE, also called concolic testing) is a technique used to automatically generate test inputs that achieve high code coverage. \nDSE executes the program under test with random test inputs and performs symbolic execution in parallel to collect symbolic constraints obtained from predicates in branch statements along the execution traces. \nThe conjunction of all symbolic constraints along a path is called a path condition. \nWhen generating tests, DSE randomly chooses one test input from the input domain,\nthen uses constraint solving to reach a target branch condition in the path~.\nDSE has been found to be accurate and effective, and has been the primary technique used by some vulnerability discovery tools~.\nIn ML testing, the model's performance is decided, not only by the code, but also by the data, and thus symbolic execution has two application scenarios: either on the data or on the code.\nSymbolic analysis was applied to generate more effective tests to expose bugs by Ramanathan and Pullum~.\nThey proposed a combination of symbolic and statistical approaches to efficiently find test cases.\nThe idea is to distance-theoretically abstract the data using symbols to help search for those test inputs where minor changes in the input will cause the algorithm to fail.\nThe evaluation of the implementation of a $k$-means algorithm indicates that the approach is able to detect subtle errors such as bit-flips. \nThe examination of false positives may also be a future research interest.\nWhen applying symbolic execution on the machine learning code, there are many challenges.\nGopinath~ listed three such challenges for neural networks in their paper, which work for other ML modes as well:\n(1) the networks have no explicit branching;\n(2) the networks may be highly non-linear, with no well-developed solvers for constraints; and \n(3) there are scalability issues because the structures of the ML models are usually very complex and are beyond the capabilities of current symbolic reasoning tools.\nConsidering these challenges, Gopinath~ introduced DeepCheck.\nIt transforms a Deep Neural Network (DNN) into a program to enable symbolic execution to find pixel attacks that have the same activation pattern as the original image.\nIn particular, the activation functions in DNN follow an IF-Else branch structure, which can be viewed as a path through the translated program.\nDeepCheck is able to create 1-pixel and 2-pixel attacks by identifying most of the pixels or pixel-pairs that the neural network fails to classify the corresponding modified images.\nSimilarly, Agarwal et al.~ apply LIME~, a local explanation tool that approximates a model with linear models, decision trees, or falling rule lists, to help get the path used in symbolic execution.\nTheir evaluation based on 8 open source fairness benchmarks shows that the algorithm generates 3.72 times more successful test cases than the random test generation approach THEMIS~.\nSun et al.~ presented DeepConcolic, \na dynamic symbolic execution testing method for DNNs.\nConcrete execution is used to direct the symbolic analysis to particular MC/DC criteria' condition, through concretely evaluating given properties of the ML models. \nDeepConcolic explicitly takes coverage requirements as input.\nThe authors report that it yields over 10\\% higher neuron coverage than DeepXplore for the evaluated models.", "cites": [6027, 6026, 8971, 7507, 6006], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key ideas from multiple papers, connecting symbolic execution techniques with their adaptation to ML testing, particularly for neural networks. It provides a critical view by highlighting challenges such as non-linear structures and scalability, and discusses the limitations of current approaches. While it abstracts some principles (e.g., using symbolic execution on data or code), the analysis remains grounded in specific tools and methods without reaching a meta-level generalization."}}
{"id": "cad0a412-7e2d-476e-a016-41c3462d7e54", "title": "Metamorphic Relations as Test Oracles", "level": "subsubsection", "subsections": [], "parent_id": "8071c099-7c4d-4166-8ef9-25c61a5f9aa4", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Oracle"], ["subsubsection", "Metamorphic Relations as Test Oracles"]], "content": "\\label{sec:metamorphicrelations}\nMetamorphic relations was proposed by Chen et al.~ to ameliorate the test oracle problem in traditional software testing.\nA metamorphic relation refers to the relationship between the software input change and output change during multiple program executions.\nFor example, to test the implementation of the function $sin(x)$, one may check how the function output changes when the input is changed from $x$ to $\\pi -x$. \nIf $sin(x)$ differs from $sin(\\pi - x)$, this observation signals an error without needing to examine the specific values computed by the implementation.\n$sin(x) = sin(\\pi - x)$ is thus a metamorphic relation that plays the role of test oracle (also named `pseudo oracle') to help bug detection.\nIn \\mlt{}, metamorphic relations are widely studied to tackle the oracle problem.\nMany metamorphic relations are based on transformations of training or test data that are expected to yield unchanged or certain expected changes in the predictive output.\nThere are different granularities of data transformations when studying the corresponding metamorphic relations.\nSome transformations conduct coarse-grained changes such as enlarging the dataset or changing the data order, without changing each single data instance. \nWe call these transformations `Coarse-grained data transformations'.\nSome transformations conduct data transformations via smaller changes on each data instance, such as mutating the attributes, labels, or pixels of images, and are referred to as `fine-grained' data transformations in this paper.\nThe related works of each type of transformations are introduced below.\n\\noindent \\textbf{Coarse-grained Data Transformation}.\nAs early as in 2008, Murphy et al.~ discuss the properties of machine learning algorithms that may be adopted as metamorphic relations.\nSix transformations of input data are introduced: additive, multiplicative, permutative, invertive,\ninclusive, and exclusive.\nThe changes include adding a constant to numerical values; \nmultiplying numerical values by a constant; \npermuting the order of the input data; \nreversing the order of the input data; \nremoving a part of the input data; \nadding additional data.\nTheir analysis is on MartiRank, SVM-Light, and PAYL.\nAlthough unevaluated in the initial 2008 paper, this work provided a foundation for determining the relationships and transformations that can be used for conducting metamorphic testing for machine learning.\nDing et al.~ proposed 11 metamorphic relations to test deep learning systems.\nAt the dataset level, the metamorphic relations were also based on training data or test data transformations that were not supposed to affect classification accuracy, such as adding 10\\% training images into each category of the training data set or removing one category of data from the dataset.\nThe evaluation is based on a classification of biological cell images.\nMurphy et al.~ presented function-level metamorphic relations. The evaluation on 9 machine learning applications indicated that functional-level properties were 170\\% more effective than application-level properties.\n\\noindent \\textbf{Fine-grained Data Transformation}.\nIn 2009, Xie et al.~ proposed to use metamorphic relations that were specific to a certain model to test the implementations of supervised classifiers.\nThe paper presents five types of metamorphic relations that enable the prediction of expected changes to the output (such as changes in classes, labels, attributes) based on particular changes to the input.\nManual analysis of the implementation of KNN and Naive Bayes from Weka~ indicates that not all metamorphic relations are necessary.\nThe differences in the metamorphic relations between SVM and neural networks are also discussed in ~.\nDwarakanath et al.~ applied metamorphic relations to image classifications with SVM and deep learning systems.\nThe changes on the data include changing the feature or instance orders, linear scaling of the test features, normalisation or scaling up the test data, or changing the convolution operation order of the data. \nThe proposed MRs are able to find 71\\% of the injected bugs.\nSharma and Wehrheim~ considered fine-grained data transformations such as changing feature names, renaming feature values to test fairness. They studied 14 classifiers, none of them were found to be sensitive to feature name shuffling.\nZhang et al.~ proposed Perturbed Model Validation (PMV) which combines metamorphic relation and data mutation to detect overfitting.\nPMV mutates the training data via injecting noise in the training data to create perturbed training datasets, then checks the training accuracy decrease rate when the noise degree increases.\nThe faster the training accuracy decreases, the less the machine learning model overfits. \nAl-Azani and Hassine~ studied the metamorphic relations of Naive Bayes, $k$-Nearest Neighbour, as well as their ensemble classifier.\nIt turns out that the metamorphic relations necessary for Naive Bayes and k-Nearest Neighbour may be not necessary for their ensemble classifier.\nTian et al.~ and Zhang et al.~ stated that the autonomous vehicle steering angle should not change significantly or stay the same for the transformed images under different weather conditions. \nRamanagopal et al.~ used the classification consistency of similar images to serve as test oracles for testing self-driving cars. \nThe evaluation indicates a precision of 0.94 when detecting errors in unlabelled data. \nAdditionally, Xie et al.~ proposed METTLE, a metamorphic testing approach for unsupervised learning validation.\nMETTLE has six types of different-grained metamorphic relations that are specially designed for unsupervised learners.\nThese metamorphic relations manipulate instance order, distinctness, density, attributes, or inject outliers of the data. The evaluation was based on synthetic data generated by Scikit-learn, showing that METTLE is practical and effective in validating unsupervised learners.\nNakajima et al.~ discussed the possibilities of using different-grained metamorphic relations to find problems in SVM and neural networks, such as to manipulate instance order or attribute order and to reverse labels and change attribute values, or to manipulate the pixels in images.\n\\noindent \\textbf{Metamorphic Relations between Different Datasets}.\nThe consistency relations between/among different datasets can also be regarded as metamorphic relations that could be applied to detect data bugs. \nKim et al.~ and Breck et al.~ studied the metamorphic relations between training data and new data. If the training data and new data have different distributions, the training data may not be adequate.\nBreck et al.~ also studied the metamorphic relations among different datasets that are close in time: these datasets are expected to share some characteristics because it is uncommon to have frequent drastic changes to the data-generation code.\n\\noindent \\textbf{Frameworks to Apply Metamorphic Relations}.\nMurphy et al.~ implemented a framework called Amsterdam to automate the process of using metamorphic relations to detect ML bugs. \nThe framework reduces false positives via setting thresholds when doing result comparison.\nThey also developed Corduroy~, which extended Java Modelling Language to let developers specify metamorphic properties and generate test cases for ML testing.\nWe introduce more related work on domain-specific metamorphic relations of testing autonomous driving, Differentiable Neural Computer (DNC)~, machine translation systems~, biological cell classification ~, and audio-based deep learning systems~ in Section~\\ref{sec:applicationsenario}.", "cites": [6018, 6019, 8972, 6029, 3500, 6015, 6028], "cite_extract_rate": 0.2916666666666667, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured synthesis of metamorphic relations in ML testing, categorizing them by transformation granularity and application. It connects multiple works to highlight trends and differences in how these relations are applied across ML models and systems. While there is some critical evaluation (e.g., noting that not all MRs are necessary for certain models), the analysis remains largely descriptive in comparing methods rather than deeply critiquing their limitations or implications."}}
{"id": "1cfbed13-3d8c-4c04-8bfb-250878e9f6d7", "title": "Cross-Referencing as Test Oracles", "level": "subsubsection", "subsections": [], "parent_id": "8071c099-7c4d-4166-8ef9-25c61a5f9aa4", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Oracle"], ["subsubsection", "Cross-Referencing as Test Oracles"]], "content": "\\label{sec:oracle_differential}\nCross-Referencing is another type of test oracle for ML testing, including differential Testing and N-version Programming.\nDifferential testing is a traditional software testing technique that detects bugs by observing whether similar applications yield different outputs regarding identical inputs~.\nIt is a testing oracle for detecting compiler bugs~.\nAccording to the study of Nejadgholi and Yang~, 5\\% to 27\\% test oracles for deep learning libraries use differential testing.\nDifferential testing is closely related with N-version programming~: N-version programming aims to generate multiple functionally-equivalent programs based on one specification, so that the combination of different versions are more fault-tolerant and robust.\nDavis and Weyuker~ discussed the possibilities of differential testing for `non-testable' programs. \nThe idea is that if multiple implementations of an algorithm yield different outputs on one identical input, then at least one of the implementation contains a defect.\nAlebiosu et al.~ evaluated this idea on machine learning, and successfully found 16 faults from 7 Naive Bayes implementations and 13 faults from 19 $k$-nearest neighbour implementation.\nPham et al.~ also adopted cross referencing to test ML implementations, but focused on the implementation of deep learning libraries.\nThey proposed CRADLE, the first approach that focuses on finding and localising bugs in deep learning software libraries. The evaluation was conducted on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. It turned out that CRADLE detects 104 unique inconsistencies and 12 bugs.\nDeepXplore~ and DLFuzz~ used differential testing as test oracles to find effective test inputs. Those test inputs causing different behaviours among different algorithms or models were preferred during test generation. \nMost differential testing relies on multiple implementations or versions, while \nQin et al.~ used the behaviours of `mirror' programs, generated from the training data as pseudo oracles.\nA mirror program is a program generated based on training data, so that the behaviours of the program represent the training data.\nIf the mirror program has similar behaviours on test data, it is an indication that the behaviour extracted from the training data suit test data as well.\nSun et al.~ applied cross reference in repairing machine translation systems.\nTheir approach, TransRepair, compares the outputs (i.e., translations) of different mutated inputs, and picks the output that shares the most similarity with others as a superior translation candidate.", "cites": [6007, 6024, 6015], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes well by connecting cross-referencing techniques like differential testing and N-version programming with ML testing, and by showing how these are applied across different contexts. It critically references the application and outcomes of these techniques, such as the success of CRADLE in finding bugs. The section abstracts some of the broader principles of using multiple implementations or mirror programs for testing, though it primarily focuses on concrete examples rather than developing a high-level theoretical framework."}}
{"id": "014a6b7e-0959-4772-913e-f9b6b618e905", "title": "Measurement Metrics for Designing Test Oracles", "level": "subsubsection", "subsections": [], "parent_id": "8071c099-7c4d-4166-8ef9-25c61a5f9aa4", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Oracle"], ["subsubsection", "Measurement Metrics for Designing Test Oracles"]], "content": "\\label{sec:modelevaluationmetrics}\nSome work has presented definitions or statistical measurements of non-functional features of ML systems including robustness~, fairness~, and interpretability~.\nThese measurements are not direct oracles for testing, but are essential for testers to understand and evaluate the property under test, and to provide some actual statistics that can be compared with the expected ones.\nFor example, the definitions of different types of fairness ~ (more details are in Section~\\ref{sec:fairnessdefinitions}) define the conditions an ML system has to satisfy without which the system is not fair. \nThese definitions can be adopted directly to detect fairness violations. \nExcept for these popular test oracles in \\mlt{}, \nthere are also some domain-specific rules that could be applied to design test oracles.\nWe discussed several domain-specific rules that could be adopted as oracles to detect data bugs in Section~\\ref{sec:bugdetectionintrainingdata}.\nKang et al.~ discussed two types of model assertions under the task of car detection in videos:\nflickering assertion to detect the flickering in car bounding box, and multi-box assertion to detect nested-car bounding. \nFor example, if a car bounding box contains other boxes, the multi-box assertion fails.\nThey also proposed some automatic fix rules to set a new predictive result when a test assertion fails.\nThere has also been a discussion about the possibility of evaluating ML learning curve in lifelong machine learning as the oracle~.\nAn ML system can pass the test oracle if it can grow and increase its knowledge level over\ntime.", "cites": [6032, 8500, 3899, 8703, 6031, 6030], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section lists various measurement metrics used in designing test oracles for ML systems, citing papers on fairness, interpretability, and robustness. While it makes some basic connections between these metrics and their use in ML testing, it primarily describes them individually without in-depth comparison or critique. There is minimal abstraction beyond specific examples, limiting the broader insights into overarching principles or trends."}}
{"id": "d7f738e2-172f-4f12-9766-4aa0d6b80da3", "title": "Test Coverage", "level": "subsubsection", "subsections": [], "parent_id": "03a34345-79fd-4f04-bc4a-46227e60d2cc", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Adequacy"], ["subsubsection", "Test Coverage"]], "content": "In traditional software testing, code coverage measures the degree to which the source code of a program is executed by a test suite~. \nThe higher coverage a test suite achieves, it is more probable that the hidden bugs could be uncovered. In other words, covering the code fragment is a necessary condition to detect the defects hidden in the code. It is often desirable to create test suites to achieve higher coverage.\nUnlike traditional software, code coverage is seldom a demanding criterion for ML testing, since the decision logic of an ML model is not written manually but rather it is learned from training data.\nFor example, in the study of Pei et al.~, 100\\,\\% traditional code coverage is easy to achieve with a single randomly chosen test input.\nInstead, researchers propose various types of coverage for ML models beyond code coverage.\n\\noindent \\textbf{Neuron coverage}.\nPei et al.~ proposed the first coverage criterion, neuron coverage, particularly designed for deep learning testing.\nNeuron coverage is calculated as the ratio of the number of unique neurons activated by all test inputs and the total number of neurons in a DNN. In particular, a neuron is activated if its output value is larger than a user-specified threshold. \nMa et al.~ extended the concept of neuron coverage. They first profile a DNN based on the training data, so that they obtain the activation behaviour of each neuron against the training data. Based on this, they propose more fine-grained criteria, $k$-multisection neuron coverage, neuron boundary coverage, and strong neuron activation coverage, to represent the major functional behaviour and corner behaviour of a DNN.\n\\noindent \\textbf{MC/DC coverage variants}.\nSun et al.~ proposed four test coverage criteria that are tailored to the distinct features of DNN inspired by the MC/DC coverage criteria~.\nMC/DC observes the change of a Boolean variable, while their proposed criteria observe a sign, value, or distance change of a neuron, in order to capture the causal changes in the test inputs.\nThe approach assumes the DNN to be a fully-connected network, and does not consider the context of a neuron in its own layer as well as different neuron combinations within the same layer~.\n\\noindent\\textbf{Layer-level coverage}.\nMa et al.~ also presented layer-level coverage criteria, which considers the top hyperactive neurons and their combinations (or the sequences) to characterise the behaviours of a DNN.\nThe coverage is evaluated to have better performance together with neuron coverage based on dataset MNIST and ImageNet.\nIn their following-up work~, they further proposed combinatorial testing coverage, which checks the combinatorial activation status of the neurons in each layer via checking the fraction of neurons activation interaction in a layer.\nSekhon and Fleming~ defined a coverage criteria that looks for 1) all pairs of neurons in the same layer having all possible value combinations, and 2) all pairs of neurons in consecutive layers having all possible value combinations. \n\\noindent \\textbf{State-level coverage}.\nWhile aftermentioned criteria, to some extent, capture the behaviours of feed-forward neural networks, they do not explicitly characterise stateful machine learning system like recurrent neural network (RNN).\nThe RNN-based ML approach has achieved notable success in applications that handle sequential inputs, e.g., speech audio, natural language, cyber physical control signals.\nIn order to analyse such stateful ML systems, Du et al.~ proposed the first set of testing criteria specialised for RNN-based stateful deep learning systems.\nThey first abstracted a stateful deep learning system as a probabilistic transition system. \nBased on the modelling, they proposed criteria based on the state and traces of the transition system, to capture the dynamic state transition behaviours.\n\\noindent\\textbf{Limitations of Coverage Criteria}.\nAlthough there are different types of coverage criteria, most of them focus on DNNs.\nSekhon and Fleming~ examined the existing testing methods for DNNs and discussed the limitations of these criteria.\nMost proposed coverage criteria are based on the structure of a DNN. Li et al.~ pointed out the limitations of structural coverage criteria for deep networks caused by the fundamental differences\nbetween neural networks and human-written programs.\nTheir initial experiments with natural inputs found no strong correlation between the number of misclassified\ninputs in a test set and its structural coverage.\nDue to the black-box nature of a machine learning system, it is not clear how such criteria directly relate to the system's decision logic.", "cites": [6034, 6033, 8090, 6007, 6018, 8091], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple coverage criteria for ML testing, connecting neuron coverage, MC/DC variants, layer-level, and state-level coverage across cited works. It critically discusses limitations of structural coverage criteria for DNNs and notes their lack of strong correlation with system correctness. While it identifies some broader patterns (e.g., the shift from code coverage in traditional testing to neuron-based coverage in ML), it could further abstract by proposing a unifying framework for ML test coverage."}}
{"id": "a889f84a-92a8-48b6-9e3e-3f2a3d637fa0", "title": "Mutation Testing", "level": "subsubsection", "subsections": [], "parent_id": "03a34345-79fd-4f04-bc4a-46227e60d2cc", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Adequacy"], ["subsubsection", "Mutation Testing"]], "content": "In traditional software testing, mutation testing evaluates the fault-revealing ability of a test suite via injecting faults~. \nThe ratio of detected faults against all injected faults is called the \\emph{Mutation Score}.\nIn \\mlt{}, the behaviour of an ML system depends on, not only the learning code, but also data and model structure.\nMa et al.~ proposed DeepMutation, which mutates DNNs at the source level or model level, to make minor perturbation on the decision boundary of a DNN.\nBased on this, a mutation score is defined as the ratio of test instances of which results are changed against the total number of instances. \nShen et al.~ proposed five mutation operators for DNNs and evaluated properties of mutation on the MINST dataset.\nThey pointed out that domain-specific mutation operators are needed to enhance mutation analysis.\nCompared to structural coverage criteria, mutation testing based criteria are more directly relevant to the decision boundary of a DNN. For example, an input data that is near the decision boundary of a DNN, could more easily detect the inconsistency between a DNN and its mutants.", "cites": [8092], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of mutation testing in traditional software engineering and connects it to ML testing by citing Ma et al. and Shen et al., providing a coherent explanation of how it applies to DNNs. It offers some abstraction by highlighting the relevance of mutation testing to decision boundaries. However, it lacks deeper critical analysis of the cited approaches, such as limitations or trade-offs, and does not fully compare or contrast different methods or research directions."}}
{"id": "2e66719f-364d-4981-9fb9-b9ac881169c0", "title": "Surprise Adequacy", "level": "subsubsection", "subsections": [], "parent_id": "03a34345-79fd-4f04-bc4a-46227e60d2cc", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Adequacy"], ["subsubsection", "Surprise Adequacy"]], "content": "Kim et al.~ introduced \n\\textbf{surprise adequacy} to measure the coverage of discretised input surprise range for deep learning systems.\nThey argued that \\jie{test diversity is more meaningful when being measured with respect to the training data.}\nA `good' test input should be `sufficiently but not overly surprising' comparing with the training data.\nTwo measurements of surprise were introduced: one is based on Keneral Density Estimation~(KDE) to approximate the likelihood of the system having seen a similar input during\ntraining, \nthe other is based on the distance between vectors representing\nthe neuron activation traces of the given input and the training data (e.g., Euclidean distance).\n\\jie{These criteria can be adopted to detect adversarial examples. Further investigation is required to determine whether such criteria enable the behaviour boundaries of ML models to be approximated in terms of surprise. It will also be interesting for future work to study the relationship between adversarial examples, natural error samples, and surprise-based criteria.}", "cites": [6028], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the core idea from Kim et al. regarding surprise adequacy and its two measures (KDE and neuron activation distance). It also offers some critical observations by noting the need for further investigation into the relationship between surprise-based criteria and adversarial examples. The discussion generalizes to suggest broader research directions, indicating an analytical rather than purely descriptive approach."}}
{"id": "99cffe8e-de76-477b-9188-33133a5afac5", "title": "Test Prioritisation and Reduction", "level": "subsection", "subsections": [], "parent_id": "ceeeb49e-d57d-42cc-87d5-2633d953501b", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Test Prioritisation and Reduction"]], "content": "\\label{sec:testprioritisation}\nTest input generation in ML has a very large input space to cover.\nOn the other hand, we need to label every test instance so as to judge predictive accuracy. \nThese two aspects lead to high test generation costs.\nByun et al.~ used DNN metrics like cross entropy, surprisal, and Bayesian uncertainty to prioritise test inputs.\nThey experimentally showed that these are good indicators of inputs that expose unacceptable behaviours, which are also useful for retraining.\nGenerating test inputs is also computationally expensive.\nZhang et al.~ proposed to reduce costs by identifying those test instances that denote the more effective adversarial examples.\nThe approach is a test prioritisation technique that ranks the test instances based on their sensitivity to noise, because the instances that are more sensitive to noise is more likely to yield adversarial examples.\nLi et. al~ focused on test data reduction in operational DNN testing.\nThey proposed a sampling technique guided by the neurons in the last hidden layer of a DNN, using a cross-entropy minimisation based distribution approximation technique.\nThe evaluation was conducted on pre-trained models with three image datasets: MNIST, Udacity challenge, and ImageNet. \nThe results show that, compared to random sampling, their approach samples only half the test inputs, yet it achieves a similar level of precision.\nMa et al.~ proposed a set of test selection metrics based on the notion of model confidence.\nTest inputs that are more uncertain to the models are preferred, because they are more informative and should be used to improve the model if being included during retraining.\nThe evaluation shows that their test selection approach has 80\\% more gain than random selection.", "cites": [6035, 6036, 8973], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes three distinct approaches to test prioritisation and reduction, linking them through the common goal of reducing test generation costs while maintaining effectiveness. It includes some critical evaluation by highlighting the performance gains of the proposed methods over random sampling. However, it lacks deeper comparative analysis or discussion of limitations, and while it identifies a trend towards using model uncertainty and sensitivity metrics, it does not abstract these ideas into a broader framework or principle."}}
{"id": "2c332c26-695c-4344-8364-172ee7d75463", "title": "Debug and Repair", "level": "subsection", "subsections": [], "parent_id": "ceeeb49e-d57d-42cc-87d5-2633d953501b", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "Debug and Repair"]], "content": "\\label{sec:debug}\n\\noindent \\textbf{Data Resampling}.\nThe generated test inputs introduced in Section~\\ref{sec:input} only expose ML bugs, but are also studied as a part of the training data and can improve the model's correctness through retraining.\nFor example, DeepXplore achieves up to 3\\% improvement in classification accuracy by retraining a deep learning model on generated inputs.\nDeepTest~ improves the model's accuracy by 46\\%.\nMa et al.~ identified the neurons responsible for the misclassification and call them `faulty neurons'.\nThey resampled training data that influence such faulty neurons to help improve model performance.\n\\noindent\\textbf{Debugging Framework Development}.\n\\jie{\nDutta et al.~ proposed Storm, a program transformation framework to generate smaller programs that can support debugging for machine learning testing. \nTo fix a bug, developers usually need to shrink the program under test to write better bug reports and to facilitate debugging and regression testing. Storm applies program analysis and probabilistic reasoning to simplify probabilistic programs, which helps to pinpoint the issues more easily.\n}\nCai et al.~ presented \\emph{tfdbg},\na debugger for ML models built on TensorFlow, containing three key components:\n1) the Analyzer, which makes the structure and intermediate state of the runtime graph visible;\n2) the NodeStepper, which enables clients to pause, inspect, or modify at a given node of the graph;\n3) the RunStepper, which enables clients to take higher level actions between iterations of model training.\nVartak et al.~ proposed the MISTIQUE system to capture, store, and query model intermediates to help the debug.\nKrishnan and Wu~ presented PALM, a tool that explains a complex model with a two-part surrogate model: a meta-model that partitions the training data and a set of sub-models that approximate the patterns within each partition.\nPALM helps developers find out the training data that impacts prediction the most, and thereby target the subset of training data that account for the incorrect predictions to assist debugging.\n\\noindent \\textbf{Fix Understanding}.\nFixing bugs in many machine learning systems is difficult because bugs can occur at multiple points in different components.\nNushi et al.~ proposed a human-in-the-loop approach that simulates potential fixes in different components through human computation tasks: humans were asked to simulate improved component states.\nThe improvements of the system are recorded and compared, to provide guidance to designers about how they can best improve the system. \n\\jienew{\n\\noindent \\textbf{Program Repair}.\nAlbarghouthi et al.~ proposed a distribution-guided inductive synthesis approach to repair decision-making programs such as machine learning programs. \nThe purpose is to construct a new program with correct predictive output, but with similar semantics with the original program.\nTheir approach uses sampled instances and the predicted outputs to drive program synthesis in which the program is encoded based on SMT. \n}", "cites": [3500, 6037], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various debugging and repair tools for ML systems, with limited synthesis across the cited works. It mentions different techniques and tools but does not compare or contrast them in depth, nor does it critically evaluate their limitations or effectiveness. Some abstraction is attempted (e.g., `faulty neurons`), but the section remains largely focused on summarizing individual methods without highlighting broader trends or principles."}}
{"id": "285b516a-be31-4d6a-85d1-4dd46996d081", "title": "General Testing Framework and Tools", "level": "subsection", "subsections": [], "parent_id": "ceeeb49e-d57d-42cc-87d5-2633d953501b", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Workflow"], ["subsection", "General Testing Framework and Tools"]], "content": "\\label{sec:generalframework}\nThere has also been work focusing on providing a testing tool or framework that helps developers to implement testing activities in a testing workflow.\nThere is a test framework to generate and validate test inputs for security testing~.\nDreossi et al.~ presented a CNN testing framework that consists of three main modules: an image generator, a collection of sampling methods, and a suite of visualisation tools. \nTramer et al.~ proposed a comprehensive testing tool to help developers to test and debug fairness bugs with an easily interpretable bug report.\nNishi et al.~ proposed a testing\nframework including different evaluation aspects such as allowability, achievability, robustness, avoidability and improvability.\nThey also discussed different levels of ML testing, such as system, software, component, and data testing.\nThomas et al. ~ recently proposed a framework for designing machine learning algorithms, which simplifies the regulation of undesired behaviours. \nThe framework is demonstrated to be suitable for regulating regression, classification, and reinforcement algorithms.\nIt allows one to learn from (potentially biased) data\nwhile guaranteeing that, with high\nprobability, the model will exhibit no bias when applied to unseen data.  \nThe definition of bias is user-specified, allowing for a large family of definitions.  \nFor a learning algorithm and a training data, the framework will either returns a model with this guarantee, or a warning that it fails to find such a model with the required\nguarantee.", "cites": [6038, 6039], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various testing frameworks and tools, citing specific papers but without deeply connecting or synthesizing their contributions. It mentions the components and purposes of each framework but lacks critical evaluation or comparison of their strengths and weaknesses. There is minimal abstraction or generalization beyond the individual tools, limiting the insight quality."}}
{"id": "f7ecc544-7176-46d8-b793-11a615ebeb55", "title": "Correctness", "level": "subsection", "subsections": [], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Correctness"]], "content": "\\label{sec:effectiveness} \nCorrectness concerns the fundamental function accuracy of an ML system.\nClassic machine learning validation is the most well-established and widely-used technology for correctness testing.\nTypical machine learning validation approaches are cross-validation and bootstrap. The principle is to isolate test data via data sampling to check whether the trained model fits new cases.\nThere are several approaches to perform cross-validation.\nIn hold out cross-validation~, the data are split into two parts: one part becomes the training data and the other part becomes test data\\footnote{Sometimes a validation set is also needed to help train the model, in which circumstances the validation set will be isolated from the training set.}.\nIn $k$-fold cross-validation, the data are split into $k$ equal-sized subsets: one subset used as the test data and the remaining $k-1$ as the training data. \nThe process is then repeated $k$ times, with each of the subsets serving as the test data.\nIn leave-one-out cross-validation, $k$-fold cross-validation is applied, where $k$ is the total number of data instances.\nIn Bootstrapping, the data are sampled with replacement~, and thus the test data may contain repeated instances.\nThere are several widely-adopted correctness measurements such as accuracy, precision, recall, and Area Under Curve (AUC).\nThere has been work~ analysing the disadvantages of each measurement criterion. \nFor example, accuracy does not distinguish between the types of errors it makes (False Positive versus False Negatives). \nPrecision and Recall may be misled when data is unbalanced. \nAn implication of this work is that we should carefully choose performance metrics.\nChen et al. studied the variability of both training data and test data when assessing the correctness of an ML classifier~.\nThey derived analytical expressions for the variance of the estimated performance and provided an open-source software implemented with an efficient computation algorithm.\nThey also studied the performance of different statistical methods when comparing AUC, and found that the $F$-test has the best performance~. \nTo better capture correctness problems, Qin et al.~ proposed to generate a mirror program from the training data, and then use the behaviours this mirror program to serve as a correctness oracle.\nThe mirror program is expected to have similar behaviours as the test data.\nThere has been a study of the prevalence of correctness problems among all the reported ML bugs: \nZhang et al.~ studied 175 Tensorflow bug reports from StackOverflow QA (Question and Answer) pages and from Github projects.\nAmong the 175 bugs, 40 of them concern poor correctness.\nAdditionally, there have been many works on \ndetecting data bug that may lead to low correctness~ (see more in Section~\\ref{sec:datatesting}),\ntest input or test oracle design~,\nand test tool design~ (see more in Section~\\ref{sec:testingworkflow}).", "cites": [6037, 6040], "cite_extract_rate": 0.1, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of correctness testing in ML, describing validation methods and citing specific works. It makes minimal connections between the cited papers and does not integrate them into a broader framework. While it touches on limitations of performance metrics and mentions specific approaches by Chen et al. and Qin et al., the critique is shallow and lacks in-depth evaluation or comparison."}}
{"id": "ae502e52-b694-4bcd-9ffa-9c1c1a66b86a", "title": "Model Relevance", "level": "subsection", "subsections": [], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Model Relevance"]], "content": "\\label{sec:overfitting} \n\\jienew{Model relevance evaluation detects mismatches between model and data. A poor model relevance is usually associated with overfitting or underfitting.}\nWhen a model is too complex for the data, even the noise of training data is fitted by the model~. \nOverfitting can easily happen, especially when the training data is insufficient, ~.\nCross-validation is traditionally considered to be a useful way to detect overfitting.\nHowever, it is not always clear how much overfitting is acceptable and cross-validation may be unlikely to detect overfitting if the test data is unrepresentative of potential unseen data.\nZhang et al.~ introduced Perturbed Model Validation (PMV) to help model selection. \nPMV injects noise to the training data, re-trains the model against the perturbed data, then uses the training accuracy decrease rate to detect overfitting/underfitting.\nThe intuition is that an overfitted learner tends to fit noise in the training sample, while an underfitted learner will have low training accuracy regardless the presence of injected noise.\nThus, both overfitting and underfitting tend to be less insensitive to noise and exhibit a small accuracy decrease rate against noise degree on perturbed data.\nPMV was evaluated on four real-world datasets (breast cancer, adult, connect-4, and MNIST) and nine synthetic datasets in the classification setting. \nThe results reveal that PMV has better performance and provides a more recognisable signal for detecting both overfitting/underfitting compared to 10-fold cross-validation.\nAn ML system usually gathers new data after deployment, which will be added into the training data to improve correctness. \nThe test data, however, cannot be guaranteed to represent the future data.\nWerpachowski et al.~ presents an overfitting detection approach via generating adversarial examples from test data.\nIf the reweighted error estimate on adversarial examples is sufficiently different from that of the original test set, overfitting is detected.\nThey evaluated their approach on ImageNet and CIFAR-10.\nGossmann et al. ~ studied the threat of test data reuse practice in the medical domain with extensive simulation\nstudies, and found that the repeated reuse of the same test data will inadvertently result in overfitting under all considered simulation settings.\nKirk~ mentioned that we could use training time as a complexity proxy for an ML model;\nit is better to choose the algorithm with equal correctness but relatively small training time.\nMa et al.~ tried to relieve the overfitting problem via re-sampling the training data.\nTheir approach was found to improve test accuracy from 75\\% to 93\\% on average, based on an evaluation using three image classification datasets.", "cites": [6008], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss overfitting/underfitting as aspects of model relevance, creating a coherent narrative around detection methods and their limitations. It includes critical evaluation of traditional approaches like cross-validation and introduces comparative strengths of newer methods like PMV and adversarial example-based detection. While it identifies broader themes (e.g., the insensitivity of overfitted/underfitted models to noise), it stops short of offering a meta-level framework or novel conceptual model."}}
{"id": "fdff1cc6-101b-4a5e-a606-39266cb8ac71", "title": "Robustness Measurement Criteria", "level": "subsubsection", "subsections": [], "parent_id": "5b8c22fd-274d-4ad8-b6eb-d18dc0f79b5d", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Robustness and Security"], ["subsubsection", "Robustness Measurement Criteria"]], "content": "Unlike correctness or overfitting, robustness is a non-functional characteristic of a machine learning system.\nA natural way to measure robustness is to check the correctness of the system with the existence of noise~; a robust system should maintain performance in the presence of noise.\nMoosavi-Dezfooli et al.~ proposed DeepFool that computes perturbations (added noise) that `fool' deep networks so as to quantify their robustness.\nBastani et al.~ presented three metrics to measure robustness: \n1) pointwise robustness, indicating the minimum input change a classifier fails to be robust;\n2) adversarial frequency, indicating how often changing an input changes a classifier's results;\n3) adversarial severity, indicating the distance between an input and its nearest adversarial example.\nCarlini and Wagner~ created a set of attacks that can be used\nto construct an upper bound on the robustness of a neural network.\nTjeng et al.~ proposed to use the distance between a test input and its closest adversarial example to measure robustness.\nRuan et al.~ provided global robustness lower and upper bounds based on the test data to quantify the robustness. \nGopinath~ et al. proposed DeepSafe, a data-driven approach for assessing DNN robustness: inputs that are clustered into the same group should share the same label.\nMore recently, Mangal et al.~ proposed the definition of probabilistic robustness. \nTheir work used abstract interpretation to approximate the behaviour of a neural network and to compute an over-approximation of the input regions where the network may exhibit non-robust behaviour.\nBanerjee et al.~ explored the use of Bayesian Deep Learning to model the propagation of errors inside deep neural networks to mathematically model the sensitivity of neural networks to hardware errors, without performing extensive fault injection experiments.", "cites": [3870, 906, 890, 6031], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of robustness measurement approaches from several papers, but it lacks deeper synthesis or comparison of these methods. While it introduces multiple criteria (e.g., pointwise robustness, adversarial frequency), it does not analyze how they relate, contrast, or build on each other. There is minimal abstraction or critical evaluation of the limitations or strengths of the cited works."}}
{"id": "839f0eaa-06b1-4b54-8e4a-b1f09c68d72b", "title": "Perturbation Targeting Test Data", "level": "subsubsection", "subsections": [], "parent_id": "5b8c22fd-274d-4ad8-b6eb-d18dc0f79b5d", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Robustness and Security"], ["subsubsection", "Perturbation Targeting Test Data"]], "content": "The existence of adversarial examples allows attacks that may lead to serious consequences in safety-critical applications such as self-driving cars.\nThere is a whole separate literature on adversarial example generation that deserves a survey of its own, and so this paper does not attempt to fully cover it. Rather, we focus on those promising aspects that could be fruitful areas for future research at the intersection of traditional software testing and machine learning.\nCarlini and Wagner~\ndeveloped adversarial example generation approaches using distance metrics to quantify similarity.\nThe approach succeeded in generating adversarial examples for all images on the recently proposed defensively distilled networks~. \nAdversarial input generation has been widely adopted to test the robustness of autonomous driving systems~.\nThere has also been research on generating adversarial inputs for NLI models~(Section~\\ref{sec:NLI}), malware detection~, and Differentiable Neural Computer (DNC)~.\nPapernot et al.~ designed a library to standardise the implementation of adversarial example construction.\nThey pointed out that standardising adversarial example generation is important because `benchmarks constructed without a\nstandardised implementation of adversarial example construction are not\ncomparable to each other': it is not easy to tell whether a good result is caused by a high level of robustness or by the differences in the adversarial example construction procedure.\nOther techniques to generate test data that check the neural network robustness include symbolic execution~, fuzz testing~, combinatorial Testing~,\nand abstract interpretation~. \nIn Section~\\ref{sec:input}, we cover these test generation techniques in more detail.", "cites": [6027, 8971, 6034, 6024, 6041, 3500, 6025, 6023, 8090, 6007, 890, 6019], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various adversarial testing and input mutation techniques for evaluating ML robustness, citing multiple papers. It briefly connects some methods (e.g., symbolic execution, fuzz testing) as part of a broader set of techniques for robustness testing but does so in a largely listing manner without a deeper synthesis. There is minimal critical evaluation of the papers, and while it touches on standardization and challenges in testing, the abstraction remains limited to identifying a few general themes."}}
{"id": "ea59b2f9-ab32-43f3-a3d2-022c4b2b9f28", "title": "Efficiency", "level": "subsection", "subsections": [], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Efficiency"]], "content": "\\label{sec:efficiency} \nThe empirical study of Zhang et al.~ on Tensorflow bug-related artefacts (from StackOverflow QA page and Github) found that nine out of 175 ML bugs (5.1\\%) belong to efficiency problems.\nThe reasons may either be that efficiency problems rarely occur or that these issues are difficult to detect.\nKirk~ pointed out that it is possible to use the efficiency of different machine learning algorithms when training the model to compare their complexity.\nSpieker and Gotlieb~ studied three training data reduction approaches, the goal of which was to find a smaller subset of the original training data set with similar characteristics during model training, so that model building speed could be improved for faster machine learning testing.", "cites": [6042], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions a few papers but fails to synthesize their ideas into a cohesive narrative. It provides factual descriptions without critical evaluation or comparison, and does not abstract to broader patterns or principles in ML testing efficiency. As a result, the insight remains minimal and descriptive."}}
{"id": "91c84637-6462-4dcf-ae0b-3b4491589336", "title": "Fairness", "level": "subsection", "subsections": ["c0fc1ff4-0a12-472c-8fcd-2e7a1c1f5d00", "0a6021ba-e9a2-4fbd-bf4b-35aaa158ff14"], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Fairness"]], "content": "\\label{sec:fairness}\nFairness is a relatively recently-emerging non-functional characteristic. \nAccording to the work of Barocas and Selbst~, there are the following five major causes of unfairness.\n\\noindent1) \\emph{Skewed sample}: once some initial bias happens, such bias may compound over time.\n\\noindent2) \\emph{Tainted examples}: the data labels are biased because of biased labelling activities of humans.\n\\noindent3) \\emph{Limited features}: features may be less informative or reliably collected, misleading the model in building the connection between the features and the labels.\n\\noindent4) \\emph{Sample size disparity}: if the data from the minority group and the majority group are highly imbalanced, ML model may the minority group less well.\n\\noindent5) \\emph{Proxies}: some features are proxies of sensitive attributes (e.g., neighbourhood in which a person resides), and may cause bias to the ML model even if sensitive attributes are excluded.\nResearch on fairness focuses on measuring, discovering, understanding, and coping with the observed differences regarding different groups or individuals in performance.\nSuch differences are associated with fairness bugs, which can offend and even harm users, and cause programmers and businesses embarrassment, mistrust, loss of revenue, and even legal violations~.", "cites": [6039], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the five causes of unfairness in machine learning based on Barocas and Selbst, and briefly mentions the focus of fairness research. However, it does not effectively synthesize information from multiple sources or connect different ideas. There is minimal critical analysis or identification of broader patterns or principles in the field."}}
{"id": "c0fc1ff4-0a12-472c-8fcd-2e7a1c1f5d00", "title": "Fairness Definitions and Measurement Metrics", "level": "subsubsection", "subsections": [], "parent_id": "91c84637-6462-4dcf-ae0b-3b4491589336", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Fairness"], ["subsubsection", "Fairness Definitions and Measurement Metrics"]], "content": "\\label{sec:fairnessdefinitions}\nThere are several definitions of fairness proposed in the literature, yet no firm consensus~.\nNevertheless, these definitions can be used as oracles to detect fairness violations in \\mlt{}.\nTo help illustrate the formalisation of ML fairness, we use \n$X$ to denote a set of individuals,\n$Y$ to denote the true label set when making decisions regarding each individual in $X$.\nLet $h$ be the trained machine learning predictive model.\nLet $A$ be the set of sensitive attributes, and $Z$ be the remaining attributes.\n\\noindent \\textbf{1) Fairness Through Unawareness}.\nFairness Through Unawareness (FTU) means that an algorithm is fair so long as the protected\nattributes are not explicitly used in the decision-making process~.\nIt is a relatively low-cost way to define and ensure fairness. \nNevertheless, sometimes the non-sensitive attributes in $X$ may contain\ninformation correlated to sensitive attributes that may thereby lead to discrimination~. \nExcluding sensitive attributes may also impact model accuracy and yield less effective predictive results~.\n\\noindent \\textbf{2) Group Fairness}.\nA model under test has group fairness if groups selected based on \nsensitive attributes have an equal probability of decision outcomes.\nThere are several types of group fairness.\n\\textbf{\\emph{Demographic Parity}} is a popular group fairness measurement~. \nIt is also named \\emph{Statistical Parity} or \\emph{Independence Parity}.\nIt requires that a decision should be independent of the protected attributes. \nLet $G_1$ and $G_2$ be the two groups belonging to $X$ divided by a sensitive attribute $a\\in A$.\nA model $h$ under test satisfies demographic parity if $P\\{h(x_i)=1|x_i\\in G_1\\} = P\\{h(x_j)=1|x_j \\in G_2\\}$.\n\\textbf{\\emph{Equalised Odds}} is another group fairness approach proposed by Hardt et al.~.\nA model under test $h$ satisfies \\emph{Equalised Odds} if $h$ is independent of the protected attributes when a target label $Y$ is fixed as $y_i$:\n$P\\{h(x_i)=1|x_i\\in G_1,Y=y_i\\} = P\\{h(x_j)=1|x_j \\in G_2,Y=y_i\\}$.\nWhen the target label is set to be positive, Equalised Odds becomes \\textbf{\\emph{Equal Opportunity}}~.\nIt requires that the true positive rate should be the same for all the groups.\nA model $h$ satisfies \\emph{Equal Opportunity} if $h$ is independent of the protected attributes when a target class $Y$ is fixed as being positive:\n$P\\{h(x_i)=1|x_i\\in G_1,Y=1\\} = P\\{h(x_j)=1|x_j \\in G_2,Y=1\\}$.\n\\noindent \\textbf{3) Counter-factual Fairness}.\nKusner et al.~ introduced \\textbf{\\emph{Counter-factual Fairness}}.\nA model satisfies Counter-factual Fairness if its output remains the same when the protected\nattribute is flipped to a counter-factual value, and other variables are modified as determined by the assumed causal model.\n\\jie{Let $a$ be a protected attribute, $a'$ be the counterfactual attribute of $a$, $x_i'$ be the new input with $a$ changed into $a'$.\nModel $h$ is counter-factually fair if, for any input $x_i$ and protected attribute $a$:\n}\n$P\\{h(x_i)_{a}=y_i|a\\in A,x_i \\in X\\} = P\\{h(x_i')_{a'}=y_i|a \\in A,x_i \\in X\\}$.\nThis measurement of fairness additionally provides a mechanism to interpret the causes of bias, \\jie{because the variables other than the protected attributes are controlled, and thus the differences in $h(x_i)$ and $h(x_i')$ must be caused by variations in $A$.}\n\\noindent \\textbf{4) Individual Fairness}.\nDwork et al.~ proposed a use task-specific similarity metric to describe the pairs of individuals that should be regarded as similar. \nAccording to Dwork et al., a model $h$ with individual fairness should give similar predictive results among similar individuals: $P\\{h(x_i)|x_i \\in X\\} = P\\{h(x_j)=y_i|x_j \\in X\\}$ iff $d(x_i,x_j)<\\epsilon$, where $d$ is a distance metric for individuals that measures their similarity, and $\\epsilon$ is tolerance to such differences.\n\\noindent \\textbf{Analysis and Comparison of Fairness Metrics}.\nAlthough there are many existing definitions of fairness, each has its advantages and disadvantages.\nWhich fairness is the most suitable remains controversial.\nThere is thus some work surveying and analysing the existing fairness metrics, or investigate and compare their performance based on experimental results, as introduced below.\nGajane and Pechenizkiy~ surveyed how fairness is defined and formalised in the literature.\nCorbett-Davies and Goel~ studied three types of fairness definitions: anti-classification, classification parity, and calibration.\nThey pointed out the deep statistical limitations of each type with examples.\nVerma and Rubin~ explained and illustrated the existing most prominent fairness definitions based on a common, unifying dataset.\nSaxena et al.~ investigated people's perceptions of three of the fairness definitions. \nAbout 200 recruited participants from Amazon's Mechanical Turk were asked to choose their preference over three allocation rules on two individuals having each applied for a loan. \nThe results demonstrate a clear preference for the way of allocating resources in proportion to the applicants' loan repayment rates.\n\\noindent \\textbf{\\jienew{Support for Fairness Improvement.}}\n\\jie{\nMetevier et al.~ proposed RobinHood, an algorithm that supports multiple user-defined fairness definitions under the scenario of offline contextual bandits\\footnote{\\jienew{A contextual bandit is a type of algorithm that learns to take actions based on rewards such as user click rate~.}}.\nRobinHood makes use of concentration inequalities~ to calculate high-probability bounds and to search for solutions that satisfy the fairness requirements. It gives user warnings when the requirements are violated. \nThe approach is evaluated under three application scenarios: a tutoring system, a loan approval setting, and the criminal recidivism, all of which demonstrate the superiority of RobinHood over other algorithms.\n}\n\\jie{Albarghouthi and Vinitsky~ proposed the concept of `fairness-aware programming', in which fairness is a first-class concern. \nTo help developers define their own fairness specifications, they developed a specification language.\nLike assertions in traditional testing, the fairness specifications are developed into the run-time monitoring code to enable multiple executions to catch violations.\nA prototype was implemented in Python.\n}\n\\jienew{\nAgarwal et al.~ proposed to reduce fairness classification into a problem of cost-sensitive classification (where the costs of different types of errors are differentiated).\nThe application scenario is binary classification, with the underlying classification method being treated as a black box.\nThe reductions optimise the trade-off between accuracy and fairness constraints. \n}\n\\jienew{Albarghouthi et al.~ proposed an approach to repair decision-making programs using distribution-guided inductive synthesis.\n}", "cites": [7759, 3899, 8703, 6044, 8691, 6043, 3905], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple fairness definitions and measurement approaches from the cited papers into a structured overview, effectively connecting different perspectives (e.g., group vs. individual fairness). It includes critical analysis by highlighting the limitations and controversies of each fairness metric. The section also abstracts from individual papers to identify broader categories and principles of fairness, such as the trade-offs between fairness and model accuracy, and the interpretability of bias sources."}}
{"id": "0a6021ba-e9a2-4fbd-bf4b-35aaa158ff14", "title": "Test Generation Techniques for Fairness Testing", "level": "subsubsection", "subsections": [], "parent_id": "91c84637-6462-4dcf-ae0b-3b4491589336", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Fairness"], ["subsubsection", "Test Generation Techniques for Fairness Testing"]], "content": "Galhotra et al.~ proposed Themis which considers group fairness using causal analysis~.\nIt defines fairness scores as measurement criteria for fairness and uses random test generation techniques to evaluate the degree of discrimination (based on fairness scores).\nThemis was also reported to be more efficient on systems that exhibit more discrimination.\nThemis generates tests randomly for group fairness, while Udeshi et al.~ proposed Aequitas, focusing on test generation to uncover discriminatory inputs and those inputs essential to understand individual fairness.\nThe generation approach first randomly samples the input space to discover the presence of discriminatory inputs, then searches the neighbourhood of these inputs to find more inputs.\nAs well as detecting fairness bugs, \\emph{Aeqitas} also retrains the machine-learning models and reduce discrimination in the decisions made by these models.\nAgarwal et al.~ used symbolic execution together with local explainability to generate test inputs.\nThe key idea is to use the local explanation, specifically Local Interpretable\nModel-agnostic Explanations\\footnote{Local Interpretable\nModel-agnostic Explanations produces decision trees corresponding to an input that could provide paths in symbolic execution~} to identify whether factors that drive decisions include protected attributes.\nThe evaluation indicates that the approach generates 3.72 times more successful test cases than THEMIS across 12 benchmarks.\nTramer et al.~ were the first to proposed the concept of `fairness bugs'.\nThey consider a statistically significant association between a protected attribute and an algorithmic output to be a fairness bug, specially named `Unwarranted Associations' in their paper.\nThey proposed the first comprehensive testing tool, aiming to help developers test and debug fairness bugs with an `easily interpretable' bug report.\nThe tool is available for various application areas including image classification, income prediction, and health care prediction.\nSharma and Wehrheim~ sought to identify causes of unfairness  via checking whether the algorithm under test is sensitive to training data changes.\nThey mutated the training data in various ways to generate new datasets, such as changing the order of rows, columns, and shuffling feature names and values.\n12 out of 14 classifiers were found to be sensitive to these changes.", "cites": [6039, 6026, 7507, 6045, 6006, 6021], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several fairness testing approaches effectively, highlighting how each contributes to uncovering different types of discrimination (group vs. individual) and the methods they employ (random sampling, causal analysis, symbolic execution). It includes some critical evaluation, such as noting the efficiency of Themis on discriminatory systems and the relative success of Agarwal et al.'s approach. While it identifies general trends in test generation strategies, the abstraction remains at a moderate level, focusing more on methodological distinctions than on overarching theoretical principles."}}
{"id": "eadef988-b4f8-4f9d-9918-8968d6a2b3d1", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Interpretability"]], "content": "\\label{sec:inter}\n\\noindent\\textbf{Manual Assessment of Interpretability}.\nThe existing work on empirically assessing the interpretability property usually includes humans in the loop. \nThat is, manual assessment is currently the primary approach to evaluate interpretability.\nDoshi-Velez and Kim~ gave a taxonomy of evaluation (testing) approaches for interpretability: application-grounded, human-grounded, and functionally-grounded.\nApplication-grounded evaluation involves human experimentation with a real application scenario.\nHuman-grounded evaluation uses results from human evaluation on simplified tasks.\nFunctionally-grounded evaluation requires no human experiments but uses a quantitative metric as a proxy for explanation quality, for example, a proxy for the explanation of a decision tree model may be the depth of the tree. \nFriedler et al.~ introduced two types of interpretability:\nglobal interpretability means understanding the entirety of a trained model;\nlocal interpretability means understanding the results of a trained model on a specific input and the corresponding output.\nThey asked 1000 users to produce the expected\noutput changes of a model given an input change, and then recorded accuracy and completion time over\nvaried models.\nDecision trees and logistic\nregression models were found to be more locally interpretable than neural networks.\n\\noindent\\textbf{Automatic Assessment of Interpretability}.\nCheng et al.~ presented a metric to understand the behaviours of an ML model.\nThe metric measures whether the learner has learned the object in object identification scenario via occluding the surroundings of the objects.\nChristoph~ proposed to measure interpretability based on the category of ML algorithms.\nHe claimed that `the easiest way to achieve interpretability is to use only a subset of algorithms that create interpretable models'. \nHe identified several models with good interpretability, including\nlinear regression, logistic regression and decision tree models.\nZhou et al.~ defined the concepts of Metamorphic Relation Patterns (MRPs) and Metamorphic Relation Input Patterns (MRIPs) that can be adopted to help end users understand how an ML system works.\nThey conducted case studies of various systems, including large commercial websites, Google Maps navigation, Google Maps location-based search, image analysis for face recognition (including Facebook, MATLAB, and OpenCV), and the Google video analysis service Cloud Video Intelligence. \n\\noindent\\textbf{Evaluation of Interpretability Improvement Methods}.\nMachine learning classifiers are widely used in many medical applications, yet the clinical meaning of the predictive outcome is often unclear.\nChen et al.~ investigated several interpretability-improving methods which transform classifier scores to a probability of disease scale. \nThey showed that classifier scores on arbitrary scales can be calibrated to the probability scale without affecting their discrimination performance.", "cites": [8500, 6010, 8093], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to create a structured overview of interpretability testing, distinguishing between manual and automatic assessment approaches. It offers some critical evaluation, such as noting the limitations of functionally-grounded evaluation by relying solely on proxies. The section also identifies broader patterns, like the relative interpretability of different model types, but stops short of forming a novel conceptual framework."}}
{"id": "c5104242-8273-48ff-83de-f40d3dcea444", "title": "Privacy", "level": "subsection", "subsections": [], "parent_id": "28829c2c-e616-463a-9bfb-a2290eb03f88", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Properties to be Tested"], ["subsection", "Privacy"]], "content": "\\label{sec:privacy}\n\\jienew{ \nDing et al.~ treat programs as grey boxes, and detect differential privacy violations via statistical tests. \nFor the detected violations, they generate counter examples to illustrate these violations as well as to help developers understand and fix bugs.\nBichsel et al. ~ proposed to estimate the $\\epsilon$ parameter in differential privacy, aiming to find a triple $(x,x',\\Phi)$ that witnesses the largest possible privacy violation, where $x$ and $x'$ are two test inputs and $\\Phi$ is a possible set of outputs.}", "cites": [6046], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of two papers on privacy testing in ML but lacks synthesis of their ideas or a broader narrative. It does not critically evaluate the methods, compare them, or discuss their limitations. There is minimal abstraction or generalization beyond the specific systems described."}}
{"id": "b24ab7fe-0aa9-4e47-9ec3-babc97874f09", "title": "Bug Detection in Training Data", "level": "subsubsection", "subsections": [], "parent_id": "4d6122da-e041-48d5-abec-79bf93ea6077", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Data"], ["subsubsection", "Bug Detection in Training Data"]], "content": "\\label{sec:bugdetectionintrainingdata}\n\\noindent \\textbf{Rule-based Data Bug Detection}. Hynes et al.~ proposed \\emph{data linter}-- an ML tool, inspired by code linters, to automatically inspect ML datasets.\nThey considered three types of data problems:\n1) miscoded data, such as mistyping a number or date as a string;\n2) outliers and scaling, such as uncommon list length;\n3) packaging errors, such as duplicate values, empty examples, and other data organisation issues.\nCheng et al.~ presented a series of metrics to evaluate whether the training data have covered all important scenarios. \n\\noindent \\textbf{Performance-based Data Bug Detection}.\nTo solve the problems in training data, Ma et al.~ proposed MODE.\nMODE identifies the `faulty neurons' in neural networks that are responsible for the classification errors, and tests the training data via data resampling to analyse whether the faulty neurons are influenced.\nMODE allows to improve test effectiveness from 75\\,\\% to 93\\,\\% on average, based on evaluation using the MNIST, Fashion MNIST, and CIFAR-10 datasets.", "cites": [6010], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual overview of two approaches for bug detection in training data but lacks deeper synthesis of ideas or broader conceptual connections. It does not critically evaluate the methods, compare their strengths and weaknesses, or highlight limitations. The content remains descriptive and does not generalize to higher-level patterns or principles."}}
{"id": "14418dd7-98ab-4646-9688-21730499e972", "title": "Bug Detection in Test Data", "level": "subsubsection", "subsections": [], "parent_id": "4d6122da-e041-48d5-abec-79bf93ea6077", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Data"], ["subsubsection", "Bug Detection in Test Data"]], "content": "Metzen et al.~ proposed to augment\nDNNs with a small sub-network, specially designed to distinguish genuine data from data containing adversarial perturbations.\nWang et al.~ used DNN model mutation to expose adversarial examples motivated by their observation that adversarial samples are more sensitive to perturbations~.\nThe evaluation was based on the MNIST and CIFAR10 datasets.\nThe approach detects 96.4\\,\\%/90.6\\,\\% adversarial samples with 74.1/86.1 mutations for MNIST/CIFAR10.\nAdversarial examples in test data raise security risks. Detecting adversarial examples is thereby similar to bug detection.\nCarlini and Wagner~ surveyed ten proposals that are designed for detecting adversarial examples and compared their efficacy.\nThey found that detection approaches rely on loss functions and can thus be bypassed when constructing new loss functions. \nThey concluded that adversarial examples are significantly harder to detect than previously appreciated.\nThe insufficiency of the test data may not able to detect overfitting issues, and\ncould also be regarded as a type of data bugs.\nThe approaches for detecting test data insufficiency were discussed in Section~\\ref{sec:evaluation}, such as coverage~ and mutation score~.", "cites": [888, 6047, 3500, 8092, 6048, 8090, 6007, 927], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section shows moderate synthesis by connecting bug detection in test data with adversarial examples and overfitting, drawing on several related papers. It provides critical analysis, particularly in highlighting the limitations of adversarial example detection and the difficulty of identifying data insufficiency. The abstraction is somewhat limited, as it primarily focuses on specific methods and findings rather than articulating overarching principles or theoretical frameworks."}}
{"id": "285bbd24-6b44-4d55-a1ac-58635b801b66", "title": "Skew Detection in Training and Test Data", "level": "subsubsection", "subsections": [], "parent_id": "4d6122da-e041-48d5-abec-79bf93ea6077", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Data"], ["subsubsection", "Skew Detection in Training and Test Data"]], "content": "The training instances and the instances that the model predicts should exhibit consistent features and distributions.\nKim et al.~ proposed two measurements to evaluate the skew between training and test data:\none is based on Kernel Density Estimation~(KDE) to approximate the likelihood of the system having seen a similar input during\ntraining, \nthe other is based on the distance between vectors representing\nthe neuron activation traces of the given input and the training\ndata (e.g., Euclidean distance).\nBreck~ investigated the skew in training data and serving data (the data that the ML model predicts after deployment).\nTo detect the skew in features, they do key-join feature comparison.\nTo quantify the skew in distribution,\nthey argued that general approaches such as KL divergence or cosine similarity might not be sufficiently intuitive for produce teams. \nInstead, they proposed to use the largest change in probability as a value in the two distributions as a measurement of their distance.", "cites": [6028], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of skew detection techniques from two cited works but lacks deeper synthesis or comparison between them. It does not evaluate the strengths and weaknesses of the methods, nor does it abstract them into broader principles or trends. The content remains descriptive without offering a critical or analytical perspective."}}
{"id": "2fedadd7-0bf7-402a-8cbf-daa6595f4b80", "title": "Frameworks in Detecting Data Bugs", "level": "subsubsection", "subsections": [], "parent_id": "4d6122da-e041-48d5-abec-79bf93ea6077", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Data"], ["subsubsection", "Frameworks in Detecting Data Bugs"]], "content": "Breck et al.~ proposed a data validation system for detecting data bugs. \nThe system applies constraints (e.g., type, domain, valency) to find bugs in single-batch (within the training data or new data),\nand quantifies the distance between training data and new data.\nTheir system is deployed as an integral part of TFX (an end-to-end machine learning platform at Google).\nThe deployment in production provides evidence that the system helps early detection and debugging of data bugs.\nThey also summarised the type of data bugs, in which new feature column, unexpected string values, and missing feature columns are the three most common.\nKrishnan et al.~ proposed a model training framework, ActiveClean, that allows for iterative data cleaning while preserving provable convergence properties.\nActiveClean suggests a sample of data to clean based on the data's value to the model and the likelihood that it is `dirty'. \nThe analyst can then apply value transformations and filtering operations to the sample to `clean' the identified dirty data.\nIn 2017, Krishnan et al.~ presented a system named BoostClean to detect domain value violations (i.e., when an attribute value is outside of an allowed domain) in training data.\nThe tool utilises the available cleaning resources such as Isolation Forest~ to improve a model's performance.\nAfter resolving the problems detected, the tool is able to improve prediction accuracy by up to 9\\% in comparison to the best non-ensemble alternative.\nActiveClean and BoostClean may involve a human in the loop of testing process.\nSchelter et al.~ focus on the automatic `unit' testing of large-scale datasets.\nTheir system provides a declarative API that combines common as well as user-defined quality constraints for data testing.\nKrishnan and Wu~ also targeted automatic data cleaning and proposed AlphaClean.\nThey used a greedy tree search algorithm to automatically tune the parameters for data cleaning pipelines.\nWith AlphaClean, the user could focus on defining cleaning requirements and let the system find the best configuration under the defined requirement.\nThe evaluation was conducted on three datasets, demonstrating that AlphaClean finds solutions of up to 9X better than state-of-the-art parameter tuning methods.\nTraining data testing is also regarded as a part of a whole machine learning workflow in the work of Baylor et al.~.\nThey developed a machine learning platform that enables data testing, based on a property description schema that captures properties such as the features present in the data and the expected type or valency of each feature.\nThere are also data cleaning technologies such as statistical or learning approaches from the domain of traditional database and data warehousing.\nThese approaches are not specially designed or evaluated for ML, but they can be re-purposed for \\mlt{}~.", "cites": [6040, 6049], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several works on data bug detection and cleaning, connecting them under the broader theme of ML testing components. It highlights common approaches (e.g., constraints, active cleaning, declarative APIs) and identifies recurring patterns such as the use of human-in-the-loop and domain-specific tools. While it offers some abstraction by categorizing these methods, it lacks deeper critical analysis of their limitations or systematic comparison of their effectiveness."}}
{"id": "7e80fb33-a0d6-4241-bea9-941d2160e065", "title": "Bug Detection in Learning Program", "level": "subsection", "subsections": [], "parent_id": "92a8da34-103a-478f-be7a-0d73eb37d03b", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Learning Program"]], "content": "\\label{sec:trainingprogramtesting}\nBug detection in the learning program checks whether the algorithm is correctly implemented and configured, e.g., the model architecture is designed well, and whether there exist coding errors.\n\\noindent \\textbf{Unit Tests for ML Learning Program}.\nMcClure~ introduced ML unit testing with TensorFlow built-in testing functions to help ensure that `code will function as expected' to help build developers' confidence.\nSchaul et al.~ developed a collection of unit tests specially designed for stochastic optimisation. \nThe tests are small-scale, isolated with well-understood difficulty. \nThey could be adopted in the beginning learning stage to test the learning algorithms to detect bugs as early as possible.\n\\noindent \\textbf{Algorithm Configuration Examination}.\nSun et al.~ and Guo et al.~ identified operating systems, language, and hardware Compatibility issues.\nSun et al.~ studied 329 real bugs from three machine learning frameworks: \nScikit-learn, Paddle, and Caffe. \nOver 22\\% bugs are found to be compatibility problems due to incompatible operating systems, language versions, or conflicts with hardware.\nGuo et al.~ investigated deep learning frameworks such as TensorFlow, Theano, and Torch.\nThey compared the learning accuracy, model size, robustness with different models classifying dataset MNIST and CIFAR-10.\nThe study of Zhang et al.~ indicates that the most common learning program bug is due to the change of TensorFlow API when the implementation has not been updated accordingly.\nAdditionally, 23.9\\% (38 in 159) of the bugs from ML projects in their study built based on TensorFlow arise from problems in the learning program.\nKarpov et al.~ also highlighted testing algorithm parameters in all neural network testing problems. \nThe parameters include the number of neurons and their types based on the neuron layer types, the ways the neurons interact with each other, the synapse weights, and the activation functions.\nHowever, the work currently remains unevaluated.\n\\noindent \\textbf{Algorithm Selection Examination}.\nDevelopers usually have more than one learning algorithm to choose from.\nFu and Menzies~ compared deep learning and classic learning on the task of linking Stack Overflow questions, and found that classic learning algorithms (such as refined SVM) could achieve similar (and sometimes better) results at a lower cost. \nSimilarly, the work of Liu et al.~ found that the $k$-Nearest Neighbours (KNN) algorithm achieves similar results to deep learning for the task of commit message generation.\n\\noindent \\textbf{Mutant Simulations of Learning Program Faults}.\nMurphy et al.~ used mutants to simulate programming code errors to investigate whether the proposed metamorphic relations are effective at detecting errors.\nThey introduced three types of mutation operators: switching comparison operators, mathematical operators, and off-by-one errors for loop variables.\nDolby et al.~ extended WALA to support static analysis of the behaviour of tensors in Tensorflow learning programs written in Python.\nThey defined and tracked tensor types for machine learning, and changed WALA to produce a dataflow graph to abstract possible program behavours.", "cites": [6052, 6051, 6050], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several papers to discuss bug detection in ML learning programs, connecting ideas on unit testing, compatibility issues, and algorithm parameter testing. It provides some critical points, such as noting that Karpov et al.'s work remains unevaluated, but lacks deeper evaluation or comparison of the methodologies. The abstraction level is moderate, as it identifies recurring themes like API changes and compatibility bugs without proposing a unified framework or high-level principle."}}
{"id": "3ad9d2f6-8492-4c7a-bc7e-c588fe110c1a", "title": "Study of Framework Bugs", "level": "subsubsection", "subsections": [], "parent_id": "a9535316-bc6e-48af-bb33-b8c2e9e035a5", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "ML Testing Components"], ["subsection", "Bug Detection in Framework"], ["subsubsection", "Study of Framework Bugs"]], "content": "\\label{sec:studyframeworkbugs}\nXiao et al.~ focused on the security vulnerabilities of popular deep learning frameworks including Caffe, TensorFlow, and Torch.\nThey examined the code of popular deep learning frameworks.\nThe dependency of these frameworks was found to be very complex.\nMultiple vulnerabilities were identified in their implementations. \nThe most common vulnerabilities are bugs that cause programs to crash, non-terminate, or exhaust memory.\nGuo et al.~ tested deep learning frameworks, including TensorFlow, Theano, and Torch, by comparing their runtime behaviour, training accuracy, and robustness, under identical algorithm design and configuration. \nThe results indicate that runtime training behaviours are different for each framework, while the prediction accuracies remain similar.\nLow Efficiency is a problem for ML frameworks, which may directly lead to inefficiency of the models built on them. \nSun et al.~ found that approximately 10\\% of reported framework bugs concern low efficiency.\nThese bugs are usually reported by users.\nCompared with other types of bugs, they may take longer for developers to resolve.", "cites": [6053, 6051], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of three papers related to framework bugs in ML, mentioning findings such as security vulnerabilities, runtime differences, and efficiency issues. However, it lacks deeper synthesis across the cited works and does not offer a critical evaluation or broader abstraction of the trends or implications in ML framework testing. The narrative is primarily factual and does not connect the studies into a more insightful or comprehensive understanding."}}
{"id": "b70d1943-138a-46e9-bc8f-2dc7e6f4d70c", "title": "Autonomous Driving", "level": "subsection", "subsections": [], "parent_id": "9e6122c5-5885-4682-ad91-5e005fa6406a", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Application Scenarios"], ["subsection", "Autonomous Driving"]], "content": "\\label{sec:autodriving}\nTesting autonomous vehicles has a comparatively long history. For example, in 2004, Wegener and B\\\"uhler compared different fitness functions when evaluating the tests of autonomous car parking systems~.\nTesting autonomous vehicles also has many research opportunities and open questions, as pointed out and discussed by Woehrle et al.~.\n\\jienew{\nMore recently, search-based test generation for AV testing has been successfully applied.\nAbdessalem et al.~ focused on improving the efficiency and accuracy of search-based testing of advanced driver assistance systems (ADAS) in AVs.\nTheir algorithms use classification models to improve the efficiency of the search-based test generation for critical scenarios. Search algorithms are further used to refine classification models to improve their accuracy.\nAbdessalem et al.~ also proposed FITEST, a multi-objective search algorithm that searches feature interactions which violate system requirements or lead to failures.\n}\nMost of the current autonomous vehicle systems that have been put into the market are semi-autonomous vehicles, which require a human driver to serve as a fall-back~, as was the case with the work of Wegener and B\\\"uhler~.\nAn issue that causes the human driver to take control of the vehicle is called a \\emph{disengagement}.\nBanerjee et al.~ investigated the causes and impacts of 5,328 disengagements from the data of 12 AV manufacturers for 144 vehicles that drove a cumulative 1,116,605 autonomous miles, 42 (0.8\\%) of which led to accidents.\nThey classified the causes of disengagements into 10 types.\n64\\% of the disengagements were found to be caused by the bugs in the machine learning system, among which the behaviours of image classification (e.g., improper detection of traffic lights, lane markings, holes, and bumps) were the dominant causes accounting for 44\\% of all reported disengagements.\nThe remaining 20\\% were due to the bugs in the control and decision framework such as improper motion planning.\nPei et al.~ used gradient-based differential testing to generate test inputs to detect potential DNN bugs and leveraged neuron coverage as a guideline.\nTian et al.~ proposed to use a set of image transformation to generate tests, which simulate the potential noise that could be present in images obtained from a real-world camera.\nZhang et al.~ proposed DeepRoad, a GAN-based approach to generate test images for real-world driving scenes. Their approach is able to support two weather conditions (i.e., snowy and rainy).\nThe images were generated with the pictures from YouTube videos.\nZhou et al.~ proposed DeepBillboard, which generates real-world adversarial billboards that can trigger potential steering errors of autonomous driving systems. \nIt demonstrates the possibility of generating continuous and realistic physical-world tests for practical autonomous-driving systems.\nWicker et al.~ used feature-guided Monte Carlo Tree Search to identify elements of an image that are most vulnerable to a self-driving system; adversarial examples.\nJha et al.~ accelerated the process of finding `safety-critical' issues via analytically modelling the injection of faults into an AV system as a Bayesian network.\nThe approach trains the network to identify safety critical faults automatically. \nThe evaluation was based on two production-grade AV systems from NVIDIA and Baidu, indicating that the approach can find many situations where faults lead to safety violations. \nUesato et al.~ aimed to find catastrophic failures in safety-critical agents like autonomous driving in reinforcement learning. \nThey demonstrated the limitations of traditional random testing, then proposed a predictive adversarial example generation approach to predict failures and estimate reliable risks.\nThe evaluation on TORCS simulator indicates that the proposed approach is both effective and efficient with fewer Monte Carlo runs.\nTo test whether an algorithm can lead to a problematic model, Dreossi et al.~ proposed to generate training data as well as test data.\nFocusing on Convolutional Neural Networks (CNN), they build a tool to generate natural images and visualise the gathered information to detect blind spots or corner cases under the autonomous driving scenario. \nAlthough there is currently no evaluation, the tool has been made available\\footnote{\\url{https://github.com/shromonag/FalsifyNN}}.\nTuncali et al.~ presented a framework that supports both system-level testing and the testing of those properties of an ML component.\nThe framework also supports fuzz test input generation and search-based testing using approaches such as Simulated Annealing and Cross-Entropy optimisation.\nWhile many other studies investigated DNN model testing for research purposes,\nZhou et al.~ \ncombined fuzzing and metamorphic testing to test \nLiDAR, which is an obstacle-perception module of real-life self-driving cars, and detected real-life fatal bugs.\nJha et al. presented AVFI~ and Kayotee~, which are fault injection-based tools to systematically inject faults into autonomous driving systems to assess their safety and reliability.\n\\jienew{\nO'Kelly et al.~ proposed a `risk-based framework' for AV testing to predict the probability of an accident in a base distribution of traffic behaviour (derived from the public traffic data collected by the US Department of Transportation).\nThey argued that formally verifying correctness of an AV system is infeasible due to the challenge of formally defining ``correctness'' as well as the white-box requirement. \nTraditional testing AVs in a real environment requires prohibitive amounts of time.\nTo tackle these problems, they view AV testing as  rare-event simulation problem, then evaluate the accident probability to accelerate AV testing.\n}", "cites": [6014, 8970, 3500, 6007, 6020, 6025, 6038], "cite_extract_rate": 0.3684210526315789, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes a range of testing approaches for autonomous driving, from search-based test generation to adversarial testing and fault injection, and connects them to broader ML testing themes like robustness and safety. It includes some critical perspectives (e.g., limitations of random testing, infeasibility of formal verification), though these are not deeply elaborated. The section identifies patterns such as the use of DNN testing techniques for real-world AV safety, but stops short of articulating a meta-level framework or overarching principle."}}
{"id": "ea1c81c8-5fa7-4c32-8105-22265d8dcc06", "title": "Machine Translation", "level": "subsection", "subsections": [], "parent_id": "9e6122c5-5885-4682-ad91-5e005fa6406a", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Application Scenarios"], ["subsection", "Machine Translation"]], "content": "\\label{sec:machinetranslation}\nMachine translation automatically translates text or speech from one language to another.\nThe BLEU (BiLingual Evaluation Understudy) score~ is a widely-adopted measurement criterion to evaluate machine translation quality.\nIt assesses the correspondence between a machine's output and that of a human.\nZhou et al.~ used self-defined metamorphic relations in their tool `MT4MT' to test the translation consistency of machine translation systems.\nThe idea is that some changes to the input should not affect the overall structure of the translated output.\nTheir evaluation showed that Google Translate outperformed Microsoft Translator for long sentences whereas the latter outperformed the former for short and simple sentences. \nThey hence suggested that the quality assessment of machine translations should consider multiple dimensions and multiple types of inputs.\nSun et al.~ combine mutation testing and metamorphic testing to test and repair the consistency of machine translation systems.\nTheir approach, TransRepair, enables automatic test input generation, automatic test oracle generation, as well as automatic translation repair. \nThey first applied mutation on sentence inputs to find translation inconsistency bugs, then used translations of the mutated sentences to optimise the translation results in a black-box or grey-box manner. \nEvaluation demonstrates that TransRepair fixes 28\\% and 19\\% bugs on average for Google Translate and Transformer.\nCompared with existing model retraining approaches, TransRepair has the following advantages: \n1) more effective than data augmentation; 2) source code in dependant (black box); \n3) computationally cheap (avoids space and time expense of data collection and model retraining); \n4) flexible (enables repair without touching other well-formed translations). \nThe work of Zheng et al.~ proposed two algorithms for\ndetecting two specific types of machine translation violations:\n(1) under-translation, where some words/phrases\nfrom the original text are missing in the translation, and (2)\nover-translation, where some words/phrases from the original\ntext are unnecessarily translated multiple times.\nThe algorithms are based on a statistical analysis of both the original texts and the translations, to check whether there are violations of one-to-one mappings in words/phrases.", "cites": [6054, 6015], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section synthesizes the contributions of Zhou et al. and Sun et al., connecting their use of metamorphic and mutation testing to the broader context of ML testing for consistency. It critically evaluates the effectiveness and advantages of TransRepair over traditional retraining methods. While it identifies some patterns in testing strategies, the abstraction remains grounded in specific algorithms and does not fully generalize to broader ML testing principles."}}
{"id": "ccd97035-6c5c-4fc4-9409-ae566209ce9d", "title": "Natural Language Inference", "level": "subsection", "subsections": [], "parent_id": "9e6122c5-5885-4682-ad91-5e005fa6406a", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Application Scenarios"], ["subsection", "Natural Language Inference"]], "content": "\\label{sec:NLI}\nA Nature Language Inference (NLI) task judges the inference relationship of a pair of natural language sentences.\nFor example, the sentence `A person is in the room' could be inferred from the sentence `A girl is in the room'.\nSeveral works have tested the robustness of NLI models.\nNie et al.~ generated sentence mutants (called `rule-based adversaries' in the paper) to test whether the existing NLI models have semantic understanding.\nSeven state-of-the-art NLI models (with diverse architectures) were all unable to recognise simple semantic differences when the word-level information remains unchanged.\nSimilarly, Wang et al.~ mutated the inference target pair by simply swapping them.\nThe heuristic is that \na good NLI model should report comparable accuracy between the original and swapped test set for contradictory pairs and for neutral pairs,\nbut lower accuracy in swapped test set for entailment pairs (the hypothesis may or may not be true given a premise).", "cites": [6023], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two studies by connecting their findings on testing NLI models through mutation techniques, showing some integration of ideas. It also provides a basic level of critical analysis by explaining the implications of the observed model behaviors. However, it lacks deeper evaluation of the methods or broader theoretical abstraction, focusing mainly on the specific experiments and results described in the cited works."}}
{"id": "efd4ab8e-0b07-44d8-bc28-57594bfd59e6", "title": "Timeline", "level": "subsection", "subsections": [], "parent_id": "9627d962-475a-4b32-b22c-50df9ccf8fff", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Analysis of Literature Review"], ["subsection", "Timeline"]], "content": "\\label{sec:timeline}\nFigure~\\ref{fig:timeline} shows several key contributions in the development of \\mlt{}.\nAs early as in 2007, Murphy et al.~ mentioned the idea of testing machine learning applications.\nThey classified machine learning applications as `non-testable' programs considering the difficulty of getting test oracles. \nThey primarily consider the detection of implementation bugs, described as ``to ensure that an application using the algorithm correctly implements the specification and fulfils the users' expectations''.\nAfterwards, Murphy et al.~ discussed the properties of machine learning algorithms that may be adopted as metamorphic relations to detect implementation bugs.\nIn 2009, Xie et al.~ also applied metamorphic testing on supervised learning applications.\nFairness testing was proposed in 2012 by Dwork et al.~;\nthe problem of interpretability was proposed in 2016 by Burrell~.\nIn 2017, Pei et al.~ published the first white-box testing paper on deep learning systems.\nTheir work pioneered to propose coverage criteria for DNN.\nEnlightened by this paper, a number of machine learning testing techniques have emerged, such as DeepTest~, DeepGauge~, DeepConcolic~, and DeepRoad~.\nA number of software testing techniques has been applied to \\mlt{},\nsuch as different testing coverage criteria~, mutation testing~, combinatorial testing~, metamorphic testing~, and fuzz testing~.\n\\begin{figure*}[h!]\n  \\centering\n\\includegraphics[scale=0.85]{figures/timeline}\n\\caption{Timeline of \\mlt{} research}\n  \\label{fig:timeline}\n\\end{figure*}", "cites": [8971, 8703, 8089, 3500, 8092, 6019, 6007, 8090, 8091], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a chronological overview of key papers in ML testing but mainly describes each contribution without deeper synthesis or critical evaluation. It lists events and papers in a timeline fashion, with only minimal integration of ideas (e.g., noting that Pei et al.'s work 'pioneered' coverage criteria for DNNs). There is no substantial abstraction or identification of broader patterns or trends in the field."}}
{"id": "8031faad-973f-4a10-bdda-3aa0588fbc1f", "title": "Research Distribution among Supervised/Unsupervised/Reinforcement Learning Testing", "level": "subsubsection", "subsections": [], "parent_id": "354f0384-4a6a-463e-9cb3-eb13167d272a", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Analysis of Literature Review"], ["subsection", "Research Distribution among Machine Learning Categories"], ["subsubsection", "Research Distribution among Supervised/Unsupervised/Reinforcement Learning Testing"]], "content": "We further classified the papers based on the three machine learning categories: \n1) supervised learning testing, 2) unsupervised learning testing, and 3) reinforcement learning testing.\nOne striking finding is that almost all the work we identified in this survey focused on testing supervised machine learning. \nAmong the \\papernum related papers, there are currently only three papers testing unsupervised machine learning: \nMurphy et al.~ introduced metamorphic relations that work for both supervised and unsupervised learning algorithms.\nRamanathan and Pullum~ proposed  a combination of symbolic and statistical approaches to test the $k$-means clustering algorithm.\nXie et al.~ designed metamorphic relations for unsupervised learning.\nWe were able to find out only one paper that focused on reinforcement learning testing: \nUesato et al.~ proposed a predictive adversarial example generation approach to predict failures and estimate reliable risks in reinforcement learning.\nBecause of this notable imbalance, we sought to understand whether there was also an imbalance of research popularity in the machine learning areas.\nTo approximate the research popularity of each category, we searched terms `supervised learning', `unsupervised learning', and `reinforcement learning' in Google Scholar and Google.\nTable~\\ref{tab:researchhit} shows the results of search hits.\nThe last column shows the number/ratio of papers that touch each machine learning category in ML testing.\nFor example, 119 out of \\papernum papers were observed for supervised learning testing purpose.\nThe table suggests that testing popularity of different categories is not related to their overall research popularity. \nIn particular, reinforcement learning has higher search hits than supervised learning, but we did not observe any related work that conducts direct reinforcement learning testing.\nThere may be several reasons for this observation.\nFirst, supervised learning is a widely-known learning scenario\nassociated with classification, regression, and ranking problems~. \nIt is natural that researchers would emphasise the testing of widely-applied, known and familiar techniques at the beginning.\nSecond, supervised learning usually has labels in the dataset. \nIt is thereby easier to judge and analyse test effectiveness.\n\\begin{table}[t]\\small\t\n\t\\center\n\t\\caption{\\label{tab:researchhit}Search hits and testing distribution of supervised/unsupervised/reinforcement Learning }\n\t\\begin{tabular}{lrrr}\n\t\t\\toprule\n\t\tCategory&Scholar hits& Google hits& Testing hits\\\\ \\hline\n\t\tSupervised&1,610k&73,500k&119/\\papernum \\\\\n\t\tUnsupervised&619k&17,700k&3/\\papernum \\\\ \n\t\tReinforcement&2,560k&74,800k&1/\\papernum \\\\\n\t\t\\bottomrule\t\t\n\t\\end{tabular}\n\\end{table}\nNevertheless, many opportunities clearly remain for research in the widely-studied areas of unsupervised learning and reinforcement learning (we discuss more in Section~\\ref{sec:researchdirection}).", "cites": [6020], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information by categorizing ML testing papers and connecting them to broader ML research areas, but it remains largely descriptive of the cited works. It provides some critical analysis by highlighting the imbalance in testing research and offering tentative explanations for it. The abstraction is moderate, as it identifies a general trend and contrasts it with the overall popularity of the respective ML categories."}}
{"id": "87e05641-cecb-454a-b387-a1c7857a21c8", "title": "Datasets Used in ML Testing", "level": "subsection", "subsections": [], "parent_id": "9627d962-475a-4b32-b22c-50df9ccf8fff", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Analysis of Literature Review"], ["subsection", "Datasets Used in ML Testing"]], "content": "\\label{sec:dataset}\n\\begin{table*}[h!]\\small\n\t\\center\n\t\\caption{\\label{tab:dataset1}Datasets (1/4): Image Classification}\n\t\\begin{tabular}{p{3.9cm}| p{7.5cm}| p{2cm}| p{2.8cm}}\n\t\t\\toprule\n\t\t\\textbf{Dataset}&\\textbf{Description}&\\textbf{Size}&\\textbf{Usage}\\\\ \\midrule \n\t\tMNIST~&Images of handwritten digits&60,000+10,000&correctness, overfitting, robustness \\\\ \\midrule\n\t\tFashion MNIST~&MNIST-like dataset of fashion images&70,000&correctness, overfitting\\\\ \\midrule\n\t\tCIFAR-10~&General images with 10 classes&50,000+10,000&correctness, overfitting, robustness\\\\ \\midrule\n\t\tImageNet~&Visual recognition challenge dataset &14,197,122&correctness, robustness\\\\ \\midrule\n\t\tIRIS flower~&The Iris flowers&150&overfitting\\\\ \\midrule\n\t\tSVHN~&House numbers&73,257+26,032&correctness,robustness\\\\ \\midrule\n\t\tFruits 360~&Dataset with 65,429 images of 95 fruits&65,429&correctness,robustness\\\\ \\midrule\n\t\tHandwritten Letters~&Colour images of Russian letters&1,650&correctness,robustness\\\\ \\midrule\n\t\tBalance Scale~&Psychological experimental results&625&overfitting\\\\ \\midrule\n\t\tDSRC~&Wireless communications between vehicles and road side units&10,000&overfitting, robustness\\\\ \\midrule\n\t\tUdacity challenge~&Udacity Self-Driving Car Challenge images &101,396+5,614&robustness\\\\ \\midrule\n\t\tNexar traffic light challenge~&Dashboard camera& 18,659+500,000&robustness\\\\ \\midrule\n\t\tMSCOCO~&Object recognition \t&160,000&correctness\\\\ \\midrule\n\t\tAutopilot-TensorFlow~& Recorded to test the NVIDIA Dave model&45,568&robustness \\\\ \\midrule\n\t\tKITTI~&Six different scenes\ncaptured by a VW Passat station wagon equipped with four\nvideo cameras\t&14,999 &robustness \\\\  \n\t\t\\bottomrule\t\t\n\t\\end{tabular}\n\\end{table*}\nTables~\\ref{tab:dataset1} to ~\\ref{tab:dataset5} \nshow details concerning widely-adopted datasets used in \\mlt{} research.\nIn each table, the first column shows the name and link of each dataset.\nThe next three columns give a brief description, the size (the ``+'' connects training data and test data if applicable), the testing problem(s), the usage application scenario of each dataset\\footnote{These tables do not list datasets adopted in data cleaning evaluation, because such studies usually involve hundreds of data sets~}.\nTable~\\ref{tab:dataset1} shows the datasets used for image classification tasks.\nDatasets can be large (e.g., more than 1.4 million images in ImageNet).\nThe last six rows show the datasets collected for autonomous driving system testing.\nMost image datasets are adopted to test correctness, overfitting, and robustness of ML systems.\nTable~\\ref{tab:dataset3} shows datasets related to natural language processing. \nThe contents are usually text, sentences, or text files, applied to scenarios like robustness and correctness.\n\\begin{table*}[h!]\\small\n\t\\center\n\t\\caption{\\label{tab:dataset3}Datasets (2/4): Natural Language Processing}\n\t\\begin{tabular}{p{3.9cm}|  p{7.5cm}| p{2cm}| p{2.9cm}}\n\t\t\\toprule\n\t\t\\textbf{Dataset}&\\textbf{Description}&\\textbf{Size}&\\textbf{Usage}\\\\ \\midrule \n\t\t\tbAbI~&questions and answers for NLP&1000+1000&robustness\\\\ \\midrule\n\t\t\tTiny Shakespeare~&Samples from actual Shakespeare&100,000 character&correctness\\\\  \\midrule\n\t\tStack Exchange Data Dump~&Stack Overflow questions and answers&365 files&correctness\\\\ \\midrule\n\t\tSNLI~&Stanford Natural Language Inference Corpus&570,000&robustness\\\\ \\midrule\n\t\tMultiNLI~& Crowd-sourced collection of sentence pairs annotated with textual entailment information&433,000&robustness\\\\  \\midrule\n\t\tDMV failure reports~&AV failure reports from 12 manufacturers in California\\footnote{The 12 AV manufacturers are: Bosch, Delphi Automotive, Google, Nissan, Mercedes- Benz, Tesla Motors, BMW, GM, Ford, Honda, Uber, and Volkswagen.} &keep  updating&correctness \\\\\n\t\t\\bottomrule\t\t\n\t\\end{tabular}\n\\end{table*}\nThe datasets used to make decisions are introduced in Table~\\ref{tab:dataset3}.\nThey are usually records with personal information, and thus are widely adopted to test the fairness of the ML models.\nWe also calculate how many datasets an \\mlt{} paper usually uses in its evaluation (for those papers with an evaluation).\nFigure~\\ref{fig:datasetnumber} shows the results.\nSurprisingly, most papers use only one or two datasets in their evaluation;\nOne reason might be training and testing machine learning models have high costs.\nThere is one paper with as many as 600 datasets, but that paper used these datasets to evaluate data cleaning techniques, which has relatively low cost~.\n\\begin{figure}[h!]\n  \\centering\n\\includegraphics[scale=0.46]{figures/dataset_number.pdf}\n\\caption{Number of papers with different amounts of datasets in experiments}\n  \\label{fig:datasetnumber}\n\\end{figure}\nWe also discuss research directions of building dataset and benchmarks  for ML testing in Section~\\ref{sec:researchdirection}.\n\\begin{table*}[h!]\\small\n\t\\center\n\t\\caption{\\label{tab:dataset4}Datasets (3/4): Records for Decision Making}\n\t\\begin{tabular}{p{3.9cm}|  p{7.5cm}| p{2cm}| p{2.9cm}}\n\t\t\\toprule\n\t\t\\textbf{Dataset}&\\textbf{Description}&\\textbf{Size}&\\textbf{Usage}\\\\ \\midrule \n\t\tGerman Credit~&Descriptions of customers with good and bad credit risks&1,000&fairness\\\\ \\midrule\n\t\tAdult ~&Census income& 48,842&fairness\\\\ \\midrule\n\t\tBank Marketing~&Bank client subscription term deposit data& 45,211&fairness\\\\ \\midrule\n\t\tUS Executions~&Records of every execution performed in the United States&1,437&fairness\\\\ \\midrule\n\t\tFraud Detection~&European Credit cards transactions&284,807 &fairness\\\\ \\midrule\n\t\tBerkeley Admissions Data~&Graduate school applications to the six largest departments at University of California, Berkeley in 1973&4,526&fairness\\\\ \\midrule\n\t\tBroward County COMPAS~&Score to determine whether to release a defendant & 18,610&fairness\\\\ \\midrule\n\t\tMovieLens Datasets~&People's preferences for movies& 100k-20m&fairness\\\\ \\midrule\n\t\tZestimate~&data about homes and Zillow's in-house price and predictions&2,990,000&correctness\\\\  \\midrule\n\t\tFICO scores~&United States credit worthiness&301,536 &fairness\\\\  \\midrule\n\t\tLaw school success~&Information concerning law students from 163 law schools in the United States& 21,790&fairness\\\\\n\t\t\\bottomrule\t\t\n\t\\end{tabular}\n\\end{table*}\n\\begin{table*}[h!]\\small\n\t\\center\n\t\\caption{\\label{tab:dataset5}Datasets (4/4): Others}\n\t\\begin{tabular}{p{3.9cm}|  p{7.5cm}| p{2cm}| p{2.9cm}}\n\t\t\\toprule\n\t\t\\textbf{Dataset}&\\textbf{Description}&\\textbf{Size}&\\textbf{Usage}\\\\ \\midrule \n\t\tVirusTotal~&Malicious PDF files&5,000&robustness \\\\ \\midrule\n\t\tContagio~&Clean and malicious files&28,760&robustness  \\\\  \\midrule\n\t\tDrebin~&Applications from different malware families&123,453&robustness\\\\ \\midrule\n\t\tChess~ &Chess game data: King+Rook versus King+Pawn on a7&3,196&correctness\\\\ \\midrule\n\t\tWaveform~&CART book's generated waveform data&5,000&correctness\\\\\n\t\t\\bottomrule\t\t\n\t\\end{tabular}\n\\end{table*}", "cites": [3197, 2911, 1174, 3870, 6055, 652, 6040], "cite_extract_rate": 0.1891891891891892, "origin_cites_number": 37, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the datasets used in ML testing across different domains, listing their properties and usage in a tabular format. While it mentions some trends (e.g., most papers use only one or two datasets), the connections between the cited works are minimal and the analysis remains superficial. There is little generalization or critical evaluation of the underlying issues in dataset usage."}}
{"id": "e0f14c49-54e6-4457-87d6-3180e8980e75", "title": "Challenges in ML Testing", "level": "subsection", "subsections": [], "parent_id": "309b9b2b-98c6-417d-bcfc-cbeed30897dc", "prefix_titles": [["title", "Machine Learning Testing: \\\\Survey, Landscapes and Horizons"], ["section", "Challenges and Opportunities"], ["subsection", "Challenges in ML Testing"]], "content": "\\label{sec:challenges}\nAs this survey reveals, ML testing has experienced rapid recent growth.\nNevertheless, ML testing remains at an early stage in its development, with many challenges and open questions lying ahead.\n\\noindent\\textbf{Challenges in Test Input Generation}.\nAlthough a range of test input generation techniques have been proposed (see more in Section~\\ref{sec:input}), test input generation remains  challenging because of the large behaviour space of ML models. \nSearch-based Software Test generation (SBST)~ uses a meta-heuristic optimising search technique, such as a Genetic Algorithm, to automatically generate test inputs.\nIt is a test generation technique that has been widely used in research (and deployment~) for traditional software testing paradigms.\nAs well as generating test inputs for testing functional properties like program correctness, SBST has also been used to explore tensions in algorithmic fairness in requirement analysis.~.\nSBST has been successfully applied in testing autonomous driving systems~.\nThere exist many research opportunities in applying SBST on generating test inputs for testing other ML systems, since there is a clear apparent fit between SBST and ML; SBST adaptively searches for test inputs in large input spaces.\nExisting test input generation techniques focus on generating adversarial inputs to test the robustness of an ML system.\nHowever, adversarial examples are often criticised because they do not represent real input data.\nThus, an interesting research direction is to how to generate natural test inputs and how to automatically measure the naturalness of the generated inputs. \nThere has been work that tries to generate test inputs to be as natural as possible under the scenario of autonomous driving, such as DeepTest~, DeepHunter~ and DeepRoad~, yet the generated images could still suffer from unnaturalness: sometimes even humans may not recognise the images generated by these tools.\nIt is both interesting and challenging to explore whether such kinds of test data that are meaningless to humans should be adopted/valid in \\mlt{}.\n\\noindent\\textbf{Challenges on Test Assessment Criteria}.\nThere has been a lot of work exploring how to assess the quality or adequacy of test data (see more in Section~\\ref{sec:evaluation}).\nHowever, there is still a lack of systematic evaluation about how different assessment metrics correlate, or how these assessment metrics correlate with tests' fault-revealing ability, a topic that has been widely studied in traditional software testing~.\n\\jienew{The relation between test assessment criteria and test sufficiency remains unclear.\nIn addition, assessment criteria may provide a way of interpreting and understanding behaviours of ML models, which might be an interesting direction for further exploration.}\n\\noindent\\textbf{Challenges Relating to The Oracle Problem}.\nThe oracle problem remains a challenge in ML testing. \nMetamorphic relations are effective pseudo oracles but, in most cases, they need to be defined by human ingenuity . \nA remaining challenge is thus to automatically identify and construct reliable test oracles for ML testing.\nMurphy et al.~ discussed how flaky tests are likely to arise in metamorphic testing whenever floating point calculations are involved.\nFlaky test detection is a challenging problem in traditional software testing~.\nIt is perhaps more challenging in ML testing because of the oracle problem.\nEven without flaky tests, pseudo oracles may be inaccurate, leading to many false positives.\nThere is, therefore, a need to explore how to yield more accurate test oracles and how to reduce the false positives among the reported issues.\nWe could even use ML algorithm $b$ to learn to detect false-positive oracles when testing ML algorithm $a$.\n\\noindent\\textbf{Challenges in Testing Cost Reduction}.\nIn traditional software testing, the cost problem remains a big problem, yielding many cost reduction techniques such as test selection, test prioritisation, and predicting test execution results. \nIn ML testing, the cost problem could be more serious, especially when testing the ML component, because ML component testing usually requires model retraining or repeating of the prediction process. It may also require data generation to explore the enormous mode behaviour space.\nA possible research direction for cost reduction is to represent an ML model as an intermediate state to make it easier for testing.\nWe could also apply traditional cost reduction techniques such as test prioritisation or minimisation to reduce the size of test cases without affecting the test correctness.\nMore ML solutions are deployed to diverse devices and platforms~(e.g., mobile device, IoT edge device). Due to the resource limitation of a target device, how to effectively test ML model on diverse devices as well as the deployment process would be also a challenge.", "cites": [3500, 8090], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and integrates information from multiple sources, particularly highlighting the role of SBST and works like DeepTest and DeepGauge in shaping current testing practices. It also provides a critical analysis by identifying limitations such as the unnaturalness of adversarial inputs and the oracle problem. The section abstracts these challenges into broader issues in ML testing, such as the need for natural test inputs and accurate oracles, offering meta-level insights into the field's future directions."}}
