{"id": "3fe27568-e32e-45b1-9483-5471c981f453", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "11d83682-7593-45c6-99d3-daa7a02d3b91", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Introduction"]], "content": "Machine learning algorithms have penetrated every aspect of our lives. \nAlgorithms make movie recommendations, suggest products to buy, and who to date. They are increasingly used in high-stakes scenarios such as loans  and hiring decisions .\nThere are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored~, and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions ``unfair''~.\nIn the context of decision-making, fairness is the \\textit{absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics}. Thus, an unfair algorithm is one \nwhose decisions are skewed \ntoward a particular group of people. A canonical example comes \nfrom a tool \nused by courts in the United States to make pretrial detention and release decisions. The software, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), measures the \nrisk of a person to recommit another crime. \nJudges use COMPAS to decide whether to release an offender, or to keep him or her in prison. \nAn investigation into the software found \na bias against \nAfrican-Americans:\\footnote{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}\nCOMPAS is more likely to \nhave higher false positive rates for African-American offenders than Caucasian offenders \nin falsely predicting them to be at a higher risk of recommitting a crime or recidivism.\nSimilar findings have been made in other areas, such as an AI system that judges beauty pageant winners but was biased against darker-skinned contestants,\\footnote{https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people} or facial recognition software in digital cameras that overpredicts Asians as blinking.\\footnote{http://content.time.com/time/business/article/0,8599,1954643,00.html} These biased predictions stem from the hidden or neglected biases in data or algorithms.\nIn this survey we identify two potential sources of unfairness in machine learning outcomes---those that arise from biases in the data and those that arise from the algorithms. We review research investigating how biases in data skew what is learned by machine learning algorithms, and \nnuances in the way the algorithms themselves work to prevent them from making fair decisions---even when the data is unbiased. Furthermore, we observe that biased algorithmic outcomes might impact user experience, thus generating a feedback loop between data, algorithms and users that can perpetuate and even amplify existing sources of bias. \nWe begin the review with several highly visible real-world cases of where unfair machine learning algorithms have led to suboptimal and discriminatory outcomes in Section~\\ref{sec:examples}. In Section~\\ref{sec:loop}, we describe the different types and sources of biases that occur within the data-algorithms-users loop mentioned above. Next, in Section~\\ref{sec:alg-fairness}, we present the different ways that the concept of fairness has been operationalized and studied in the literature. We discuss the ways in which these two concepts are coupled. Last, we will focus on different families of machine learning approaches, how fairness manifests differently in each one, and the current state-of-the-art for tackling them in Section~\\ref{sec:methods}, followed by potential areas of future work in each of the domains in Section~\\ref{sec:future}.", "cites": [3890], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the topic of bias and fairness in machine learning with a few real-world examples and outlines the structure of the survey. It mentions one cited paper to discuss candidate screening and fairness, but integration of ideas is minimal and not connected to the broader themes in the section. There is limited critical analysis, as the paper is not evaluated or compared with other approaches. The section offers some abstraction by identifying a data-algorithms-users feedback loop, but the overall insight remains at a descriptive-analytical level without deep synthesis or meta-level generalizations."}}
{"id": "a40d4e0e-669a-4d7c-a837-1a1f2f5b3614", "title": "Systems that Demonstrate Discrimination", "level": "subsection", "subsections": [], "parent_id": "773207d9-2d70-4cc6-a5c5-44a367db114c", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Real-World Examples of Algorithmic Unfairness"], ["subsection", "Systems that Demonstrate Discrimination"]], "content": "COMPAS is an exemplar of a discriminatory system. In addition to this, discriminatory behavior was also evident in an algorithm that would deliver advertisements promoting jobs in Science, Technology, Engineering, and Math (STEM) fields . This advertisement was designed to deliver advertisements in a gender-neutral way. However, less women compared to men saw the advertisement due to gender-imbalance which would result in younger women being considered as a valuable subgroup and more expensive to show advertisements to. This optimization algorithm would deliver ads in a discriminatory way although its original and pure intention was to be gender-neutral. Bias in facial recognition systems  and recommender systems  have also been largely studied and evaluated and in many cases shown to be discriminative towards certain populations and subgroups. In order to be able to address the bias issue in these applications, it is important for us to know where these biases are coming from and what we can do to prevent them. \nWe have enumerated the bias in COMPAS, which is a widely used commercial risk assessment software. In addition to its bias, it also contains performance issues when compared to humans. When compared to non-expert human judgment in a study, it was discovered to be not any better than a normal human . It is also interesting to note that although COMPAS uses 137 features, only 7 of those were presented to the people in the study.  further argues that COMPAS is not any better than a simple logistic regression model when making decisions. We should think responsibly, and recognize that the application of these tools, and their subsequent decisions affect peoples' lives; therefore, considering fairness constraints is a crucial task while designing and engineering these types of sensitive tools. In another similar study, while investigating sources of group unfairness (unfairness across different groups is defined later), the authors in  compared SAVRY, a tool used in risk assessment frameworks that includes human intervention in its process, with automatic machine learning methods in order to see which one is more accurate and more fair. Conducting these types of studies should be done more frequently, but prior to releasing the tools in order to avoid doing harm.", "cites": [3891], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of several systems that demonstrate discrimination, including COMPAS, STEM job advertisement algorithms, facial recognition, and recommender systems. While it mentions some findings from studies, it does not deeply synthesize or connect these ideas across sources. The critical analysis is limited, and the abstraction to broader patterns or principles is minimal, focusing largely on individual examples."}}
{"id": "fa071114-208a-4f50-a618-d3a786adeb52", "title": "Assessment Tools", "level": "subsection", "subsections": [], "parent_id": "773207d9-2d70-4cc6-a5c5-44a367db114c", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Real-World Examples of Algorithmic Unfairness"], ["subsection", "Assessment Tools"]], "content": "An interesting direction that researchers have taken is introducing tools that can assess the amount of fairness in a tool or system. For example, Aequitas~ is a toolkit that lets users to test models with regards to several bias and fairness metrics for different population subgroups. Aequitas produces reports from the obtained data that helps data scientists, machine learning researchers, and policymakers  to make conscious decisions and avoid harm and damage toward certain populations. AI Fairness 360 (AIF360) is another toolkit developed by IBM in order to help moving fairness research algorithms into an industrial setting and to create a benchmark for fairness algorithms to get evaluated and an environment for fairness researchers to share their ideas  . These types of toolkits can be helpful for learners, researchers, and people working in the industry to move towards developing fair machine learning application away from discriminatory behavior.", "cites": [3892], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces two fairness assessment toolkits (Aequitas and AI Fairness 360) but does so in a largely descriptive manner without integrating or contrasting them. It lacks critical evaluation of their strengths, weaknesses, or effectiveness, and does not abstract broader principles or trends in fairness assessment tools from the cited work."}}
{"id": "3696cf56-dad7-4a61-bb83-8b4a2e09f2b1", "title": "Data to Algorithm", "level": "subsubsection", "subsections": [], "parent_id": "8bcfb982-01c4-4d6d-85a4-68ce6643f0f8", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Types of Bias"], ["subsubsection", "Data to Algorithm"]], "content": "In this section we talk about biases in data, which, when used by ML training algorithms, might result in biased algorithmic outcomes.\n\\begin{enumerate}\n    \\item \\textbf{Measurement Bias.} \\textit{Measurement, or reporting, bias arises from how we choose, utilize, and measure particular features} . An example of  this type of bias was observed in the recidivism risk prediction tool COMPAS, where \n    prior arrests and friend/family arrests were used as proxy variables to measure level of ``riskiness'' or ``crime''----which on its own can be viewed as mismeasured proxies. This is partly due to the fact that minority communities are controlled and policed more frequently, so they have higher arrest rates. However, one should not conclude that because people coming from minority groups have higher arrest rates therefore they are more dangerous as there is a difference in how these groups are assessed and controlled .\n    \\item \\textbf{Omitted Variable Bias.} \\textit{Omitted variable bias\\textsuperscript{\\ref{fl}} occurs when one or more important variables are left out of the model} . An example for this case would be when someone designs a model to predict, with relatively high accuracy, the annual percentage \n    rate at which customers will stop subscribing to a service, but soon observes that the majority of users are canceling their subscription without receiving any warning from the designed model. Now imagine that the reason for canceling the subscriptions is appearance of a new strong competitor in the market which offers the same solution, but for half the price. The appearance of the competitor was something that the model was not ready for; therefore, it is considered to be an omitted variable. \n    \\item \\textbf{Representation Bias.} \\textit{Representation bias arises from how we sample from a population during data collection process}  . Non-representative samples lack the diversity of the population, with missing subgroups and other anomalies. Lack of geographical diversity in datasets like ImageNet (as shown in Figures \\ref{imagenet1} and \\ref{imagenet2}) \n    results in demonstrable bias towards Western cultures.\n     \\item \\textbf{Aggregation Bias.} \\textit{Aggregation bias (or ecological fallacy) arises when false conclusions are drawn \n     about individuals from observing the entire population.} \n     An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders. Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore,  \n     a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population~.\n     This is true even when they are represented equally in the training data. Any general assumptions about  \n     subgroups within the population can result in aggregation bias.\n    \\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.48\\columnwidth]{example_equal_size}\n    \\includegraphics[width=0.48\\columnwidth]{example_unbalanced}\n    \\caption{Illustration of biases in data.\n    The red line shows the regression (MLR) for the entire population, while dashed green lines are regressions for each subgroup, and the solid green line is the unbiased regression. (a) When all subgroups are of equal size, then MLR shows a positive relationship between the outcome and the independent variable. (b) Regression shows almost no relationship  in less balanced data. The relationships between  variables within each subgroup, however, remain the same. (Credit: Nazanin Alipourfard) \\label{fig:explain}}\n    \\end{figure}\n\\begin{figure}[h]\n\\includegraphics[width=0.75\\textwidth, trim=2cm 5cm 0cm 6cm,clip=true]{imagenet2.pdf}\n\\caption{Fraction of each country, represented by their two-letter ISO codes, in Open Images and ImageNet image datasets. In both datasets, US and Great Britain represent the top locations, from  \\textsuperscript{\\textcopyright} Shreya Shankar.}\n\\label{imagenet1}\n\\end{figure}\n\\begin{figure}[h]\n\\includegraphics[width=0.75\\textwidth,trim=0cm 2cm 0cm 3.3cm,clip=true]{imagenet.pdf}\n\\caption{Geographic distribution of countries in the Open Images data set. In their sample, almost one third of the data was US-based, and 60\\% of the data was from the six most represented countries across North America and Europe, from  \\textsuperscript{\\textcopyright} Shreya Shankar.}\n\\label{imagenet2}\n\\end{figure}\n \\begin{enumerate}\n    \\item \\textbf{Simpson's Paradox.} Simpson's paradox is a type of aggregation bias that arises in the analysis of heterogeneous data~. \n    The paradox arises when an association observed in aggregated data disappears or reverses when the same data is disaggregated into its underlying subgroups (Fig.~\\ref{fig:explain}(a)). One of the better-known examples of the type of paradox arose during the gender bias lawsuit in university admissions against UC Berkeley~.\n    After analyzing graduate school admissions data, it seemed like there was bias toward women, a smaller fraction of whom were being admitted to graduate programs compared to their male counterparts. However, when admissions data was separated and analyzed over the departments, women applicants had equality and in some cases even a small advantage over men. The paradox happened as women tended to apply to departments with lower admission rates for both genders. Simpson's paradox has been observed in a variety of domains, including biology~, psychology~,  astronomy~,  and computational social science~. \n    \\item{\\textbf{Modifiable Areal Unit Problem}} is a statistical bias in geospatial analysis, which arises when modeling data at different levels of spatial aggregation~. This bias results in different trends learned when data is aggregated at different spatial scales.\n\\end{enumerate}\n   \\item \\textbf{Sampling Bias.} \\textit{Sampling bias is similar to representation bias, and it arises due to {non-random} sampling of subgroups.} As a consequence of sampling bias, the trends estimated for one population may not generalize to data collected from a new population. For the intuition, consider the example in {Figure~\\ref{fig:explain}}. The left plot represents data collected during a study from three subgroups, which were uniformly sampled (Fig.~\\ref{fig:explain}(a)). Suppose the next time the  study was conducted, one of the subgroups was sampled more frequently than the rest (Fig.~\\ref{fig:explain}(b)). The positive trend found by the regression model in the first study almost completely disappears (solid red line in plot on the right), although the subgroup trends (dashed green lines) are unaffected.\n    \\item \\textbf{Longitudinal Data Fallacy.}\n    Researchers analyzing temporal data must use \\emph{longitudinal analysis} to track cohorts over time to learn their behavior. Instead, temporal data is often modeled using cross-sectional analysis, which combines diverse cohorts at a single time point. The heterogeneous cohorts can bias cross-sectional analysis, leading to different conclusions than longitudinal analysis. \n    As an example, analysis of bulk Reddit data~ revealed that comment length decreased over time on average. However, bulk data represented a cross-sectional snapshot of the population, which in reality contained different cohorts who joined Reddit in different years. When data was disaggregated by cohorts, the comment length within each cohort was found to increase over time.\n    \\item \\textbf{Linking Bias.}  \\textit{Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users} . In  authors show how social networks can be biased toward low-degree nodes when only considering the links in the network and not considering the content and behavior of users in the network.  also shows that user interactions are significantly different from social link patterns that are based on features, such as method of interaction or time. The differences and biases in the networks can be a result of many factors, such as network sampling, as shown in , which can change the network measures and cause different types of problems.\n    \\end{enumerate}", "cites": [3894, 8699, 7758, 3893, 6984], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates several types of bias, connecting them to specific examples and citing relevant papers to support its claims. It demonstrates moderate synthesis by relating concepts like Simpson's paradox to broader issues in data analysis and ML fairness. Critical analysis is present but limited to pointing out issues (e.g., how models can be unprepared for new variables or how aggregation can distort conclusions), without deeper evaluation of methodological trade-offs. The section abstracts some patterns, such as the impact of data heterogeneity and sampling methods, but remains grounded in specific examples rather than presenting a meta-level framework."}}
{"id": "4145147d-ef17-4382-9a43-81bec8e4de7d", "title": "Algorithm to User", "level": "subsubsection", "subsections": [], "parent_id": "8bcfb982-01c4-4d6d-85a4-68ce6643f0f8", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Types of Bias"], ["subsubsection", "Algorithm to User"]], "content": "Algorithms modulate user behavior. Any biases in algorithms  might introduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic outcomes and affect user behavior as a consequence.\n\\begin{enumerate}\n \\item \\textbf{Algorithmic Bias.} \\textit{Algorithmic bias is when the bias is not present in the input data and is added purely by the algorithm}  . The algorithmic design choices, such as use of certain optimization functions, regularizations, choices in applying regression models on the data as a whole or considering subgroups, and the general use of statistically biased estimators in algorithms , can all contribute to biased algorithmic decisions that can bias the outcome of the algorithms.\n\\item \\textbf{User Interaction Bias.} \\textit{User Interaction bias is \n    a type of bias that can not only be observant on the Web but also get triggered from two sources---the user interface and through the user itself by imposing his/her self-selected biased behavior and interaction }. This type of bias can be influenced by other types and subtypes, such as presentation and ranking biases.\n    \\begin{enumerate}\n    \\item  \\textbf{Presentation Bias.} \\textit{Presentation bias is a result of how information is presented .} For example, on the Web \n    users can only click on content that they see, so the seen content gets clicks, while everything else gets no click. And it could be the case that the user does not see all the information on the Web . \n  \\item \\textbf{Ranking Bias.} \\textit{The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others}. This bias affects search engines~ and crowdsourcing applications~.\n \\end{enumerate}\n  \\item \\textbf{Popularity Bias}. \\textit{Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation---for example, by fake reviews or social bots}  . As an instance, this type of bias can be seen in search engines  or recommendation systems where popular objects would be presented more to the public. But this presentation may not be a result of good quality; instead, it may be due to other biased factors.\n    \\item \\textbf{Emergent Bias.} \\textit{Emergent bias occurs as a result of use and interaction with real users. This bias arises as a result of change in population, cultural values, or societal knowledge usually some time after the completion of design} . This type of bias is more likely to be observed in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of prospective users by design . This type of bias can itself be divided into more subtypes, as discussed in detail in .\n    \\item \\textbf{Evaluation Bias.} \\textit{Evaluation bias happens during model evaluation} . This includes the use of inappropriate and disproportionate benchmarks for evaluation of applications such as Adience and IJB-A benchmarks. These benchmarks are used in the  evaluation of facial recognition systems that were biased toward skin color and gender , and can serve as examples for this type of bias .\n\\end{enumerate}", "cites": [3895], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of different types of algorithm-to-user bias, such as algorithmic bias, user interaction bias, popularity bias, emergent bias, and evaluation bias. It integrates the cited work (e.g., on popularity bias) into a broader discussion of how algorithmic outputs influence user behavior. However, the critical analysis is limited, with few evaluations of the cited papers or deeper reflection on methodological strengths/weaknesses, and the abstraction is moderate as it identifies categories of bias but does not present a novel or meta-level conceptual framework."}}
{"id": "b8ade98c-508e-408d-98f6-8850f5561a48", "title": "User to Data", "level": "subsubsection", "subsections": [], "parent_id": "8bcfb982-01c4-4d6d-85a4-68ce6643f0f8", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Types of Bias"], ["subsubsection", "User to Data"]], "content": "Many data sources used for training ML models are user-generated. Any inherent biases in users might be reflected in the data they generate. Furthermore, when user behavior is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in the data generation process. Here we list several important types of such biases.\n\\begin{enumerate}\n     \\item  \\textbf{Historical Bias.}\\textit{ Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection } . An example of  this type of bias can be found in a 2018 image search result where searching for women CEOs ultimately resulted in fewer female CEO images due to the fact that only 5\\% of Fortune 500 CEOs were woman---which would cause the search results to be biased towards male CEOs . These search results were of course reflecting the reality, but whether or not the search algorithms should reflect this reality is an issue worth considering.\n    \\item \\textbf{Population Bias.} \\textit{Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population \n    of the platform from the original target population}  .\n    Population bias creates non-representative data. An example of this type of bias can arise  from different user demographics on different social platforms, such as women being more likely to use Pinterest, Facebook, Instagram, while men being more active in online forums like Reddit or Twitter. More such examples and statistics related to  social media use among young adults according to gender, race, ethnicity, and parental educational background can be found in .\n    \\item \\textbf{Self-Selection Bias.} \\textit{Self-selection bias\\footnote{\\label{fl}https://data36.com/statistical-bias-types-explained/} is a subtype of the  selection or sampling bias in which subjects of the research select themselves.} An example of this type of bias can be observed in an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic supporters are more likely to complete the poll. \n    \\item \\textbf{Social Bias.} \\textit{Social bias happens when others' actions affect our judgment.}  . An example of this type of bias can be a case where we want to rate or review an item with a low score, but when influenced by other high ratings, we change our scoring thinking that perhaps we are being too harsh .\n    \\item \\textbf{Behavioral Bias.} \\textit{Behavioral bias arises from different user behavior across platforms, contexts, or different datasets} . An example of this type of bias can be observed in , where authors show how differences in emoji representations among platforms can result in different reactions and behavior from people and sometimes even leading to communication errors.\n    \\item \\textbf{Temporal Bias.} \\textit{Temporal bias arises from differences in populations and behaviors over time} . An example can be observed in Twitter where  people talking about a particular topic start using a hashtag at some point to capture attention, then continue the discussion about the event without using the hashtag .\n    \\item \\textbf{Content Production Bias.} \\textit{Content Production bias arises from structural, lexical, semantic, and syntactic differences in the contents generated by users} . An example of this type of bias can be seen in  where the differences in use of language across different gender and age groups is discussed. The differences in use of language can also be seen across and within countries and populations.\n     \\end{enumerate}\nExisting work tries to categorize these bias definitions into groups, such as definitions falling solely under data or user interaction. However, due to the existence of the feedback loop phenomenon , \nthese definitions are intertwined, and we need a categorization which closely models this situation. This feedback loop is not only existent between the data and the algorithm, but also between the algorithms and user interaction . Inspired by these papers, we modeled categorization of bias definitions, as shown in Figure \\ref{fig:cycle}, and grouped these definitions on the arrows of the loop where we thought they were most effective. We emphasize the fact again that these definitions are intertwined, and one should consider how they affect each other in this cycle, and address them accordingly.", "cites": [3896, 3897], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes ideas from multiple sources to present a coherent categorization of user-to-data biases, particularly leveraging the concept of feedback loops. While it offers some abstraction by modeling a bias cycle and emphasizing interdependencies, the critical analysis is limited to pointing out the limitations of existing categorizations without deeper evaluation of the cited papers' strengths or weaknesses."}}
{"id": "dcbc2a62-b715-430b-ab11-8564b21ba7cd", "title": "Examples of Bias in Machine Learning Data", "level": "subsubsection", "subsections": [], "parent_id": "eeba8824-a85d-4e70-9634-424eff121afd", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Data Bias Examples"], ["subsubsection", "Examples of Bias in Machine Learning Data"]], "content": "In , the authors show that datasets like IJB-A and Adience are imbalanced and  contain mainly light-skinned subjects---79.6\\% in IJB-A and 86.2\\% in Adience. This can bias the analysis towards dark-skinned groups who are underrepresented in the data. In another instance, the way we use and analyze our data can create bias when we do not consider different subgroups in the data. In , the authors also show that considering only male-female groups is not enough, but there is also a need to use race to further subdivide the gender groups into light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. It's only in this case that we can clearly observe the bias towards dark-skinned females, as previously dark-skinned males would compromise for dark-skinned females and would hide the underlying bias towards this subgroup. Popular machine-learning datasets that serve as a base for most of the developed algorithms and tools can also be biased---which can be harmful to the downstream applications that are based on these datasets. For instance, ImageNet  and Open Images  are two widely used datasets in machine-learning. In , researchers showed that these datasets suffer from representation bias and advocate for the need to incorporate geographic diversity and inclusion while creating such datasets. In addition, authors in~ write about the existing representational biases in different knowledge bases that are widely used in Natural Language Processing (NLP) applications for different commonsense reasoning tasks.", "cites": [895], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of data bias in machine learning by discussing specific examples from different datasets and linking them to the broader issue of subgroup representation. It synthesizes ideas from the cited works to highlight how biases can be hidden or exacerbated when subgroups are not considered, but it does not offer a novel framework or deep critique of the papers. The abstraction is limited to identifying the need for subgroup analysis and geographic diversity, without reaching meta-level insights about the systemic nature of data bias."}}
{"id": "c4519ec0-517e-4923-bddd-da5b2d77f693", "title": "Explainable Discrimination", "level": "subsubsection", "subsections": [], "parent_id": "710588a1-1fac-410b-a0f6-79066f2a545b", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Discrimination"], ["subsubsection", "Explainable Discrimination"]], "content": "Differences in treatment and outcomes amongst different groups can be justified and explained via some attributes in some cases. In situations where these differences are justified and explained, it is not considered to be illegal discrimination and hence called explainable . For instance, authors in  state that in the UCI Adult dataset , a widely used dataset in the fairness domain, males on average have a higher annual income than females. However, this is because on average females work fewer hours than males per week. Work hours per week is an attribute that can be used to explain low income which needs to be considered. If we make decisions, without considering working hours, such that males and females end up averaging the same income, we will lead to reverse discrimination since we would cause male employees to get lower salary than females. Therefore, explainable discrimination is acceptable and legal as it can be explained through other attributes like working hours. In , authors present a methodology to quantify the explainable and illegal discrimination in data. They argue that methods that do not take the explainable part of the discrimination into account may result in non-desirable outcomes, so they introduce a \\textit{reverse} discrimination which is equally harmful and undesirable. \nThey explain how to quantify and measure discrimination in data or a classifier's decisions which directly considers illegal and explainable discrimination.", "cites": [8700], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of the concept of explainable discrimination, integrating the idea with examples from the UCI Adult dataset and referencing a methodology to quantify it. However, it only cites one paper in depth and does not explicitly synthesize multiple sources or engage in a deeper critical evaluation of the cited work. The abstraction is limited, as it focuses on a specific concept without clearly elevating it to broader principles or frameworks."}}
{"id": "715cbfa5-3902-4d23-b1d2-f5afdcb0de91", "title": "Unexplainable Discrimination", "level": "subsubsection", "subsections": [], "parent_id": "710588a1-1fac-410b-a0f6-79066f2a545b", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Bias in Data, Algorithms, and User Experiences"], ["subsection", "Discrimination"], ["subsubsection", "Unexplainable Discrimination"]], "content": "In contrast to explainable discrimination, there is unexplainable discrimination in which the discrimination toward a group is unjustified and therefore considered illegal. Authors in  also present local techniques for removing only the illegal or unexplainable discrimination, allowing only for explainable differences in decisions. These are preprocessing techniques that change the training data such that it contains no unexplainable discrimination. We expect classifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination. Unexplainable discrimination consists of  \\textit{direct} and \\textit{indirect}  discrimination.\n\\begin{enumerate}\n\\item{\\textbf{Direct Discrimination.}}\nDirect discrimination happens when protected attributes of individuals explicitly result in non-favorable outcomes toward them . Typically, there are some traits identified by law on which it is illegal to discriminate against, and it is usually these traits that are considered to be ``protected'' or ``sensitive'' attributes in computer science literature. A list of some of these protected attributes is provided in Table \\ref{electionexample} as specified in the Fair Housing and Equal Credit Opportunity Acts (FHA and ECOA) .\n\\item{\\textbf{Indirect Discrimination.}}\nIn indirect discrimination, individuals appear to be treated based on seemingly neutral and non-protected attributes; however, protected groups, or individuals still get to be treated unjustly as a result of implicit effects from their protected attributes (e.g., the residential zip code of a person can be used in decision making processes such as loan applications. However, this can still lead to racial discrimination, such as redlining, as despite the fact that zip code appears to be a non-sensitive attribute, it may correlate with race because of the population of residential areas.) .\n\\end{enumerate}", "cites": [3898, 8700, 8701], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition of unexplainable discrimination and differentiates it from explainable discrimination, citing relevant papers. However, it mainly describes concepts and examples without effectively synthesizing insights across the cited works or offering critical evaluation. There is minimal abstraction or generalization beyond the specific papers."}}
{"id": "243cac1a-256a-4808-9104-c1f13fedb69b", "title": "Definitions of Fairness ", "level": "subsection", "subsections": [], "parent_id": "4ce9b956-164a-4fdf-a103-b11a8fd6f2e6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Algorithmic Fairness"], ["subsection", "Definitions of Fairness "]], "content": "In , authors studied fairness definitions in political philosophy and tried to tie them to machine-learning. Authors in  studied the 50-year history of fairness definitions in the areas of education and machine-learning. In , authors listed and explained some of the definitions used for fairness in algorithmic classification problems. In , authors studied the general public's perception of some of these fairness definitions in computer science literature. Here we will reiterate and provide some of the most widely used definitions, along with their explanations inspired from .\\\\\n\\\\\n\\textbf{Definition 1.} \\textit{(Equalized Odds). The definition of equalized odds, provided by , states that ``A predictor \\^{Y} satisfies equalized odds with respect to protected attribute A and outcome Y, if \\^{Y} and A are independent conditional on Y. P(\\^{Y}=1|A=0,Y =y) = P(\\^{Y}=1|A=1,Y =y) , y$\\in$\\{0,1\\}''}. This means that the probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members . In other words, the equalized odds definition states that the protected and unprotected groups should have equal rates for true positives and false positives.\n\\\\\n\\\\\n\\textbf{Definition 2.} \\textit{(Equal Opportunity). ``A binary predictor \\^{Y} satisfies equal opportunity with respect to A and Y if P(\\^{Y}=1|A=0,Y=1) = P(\\^{Y}=1|A=1,Y=1)''}\n. This means that the probability of a person in a positive class being assigned to a positive outcome should be equal for both protected and unprotected (female and male) group members . In other words, the equal opportunity definition states that the protected and unprotected groups should have equal true positive rates.\n\\\\\n\\\\\n\\textbf{Definition 3.} \\textit{(Demographic Parity). Also known as statistical parity. ``A predictor \\^{Y} satisfies demographic parity if P(\\^{Y} |A = 0) = P(\\^{Y}|A = 1)''} . The likelihood of a positive outcome  should be the same regardless of whether the person is in the protected (e.g., female) group.\n\\\\\n\\\\\n\\textbf{Definition 4.} \\textit{(Fairness Through Awareness). ``An algorithm is fair if it gives similar predictions to similar individuals''} . In other words, any two individuals who are similar with respect to a similarity (inverse distance) metric defined for a particular task should receive a similar outcome.\\\\\n\\\\\n\\textbf{Definition 5.} \\textit{(Fairness Through Unawareness). ``An algorithm is fair as long as any protected attributes A are not explicitly used in the decision-making process''} .\\\\\n\\\\\n\\textbf{Definition 6.} \\textit{(Treatment Equality).  ``Treatment equality is achieved when the ratio of false negatives and false positives is the same for both protected group categories''} .  \\\\\n\\\\\n\\textbf{Definition 7.} \\textit{(Test Fairness). ``A score S = S(x) is test fair (well-calibrated) if it reflects the same likelihood of recidivism irrespective of the individual's group membership, R. That is, if for all values of s,\nP(Y =1|S=s,R=b)=P(Y =1|S=s,R=w)''} . In other words, the test fairness definition states that for any predicted probability score S, people in both protected and unprotected groups must have equal probability of correctly belonging to the positive class .\\\\\n\\\\\n\\textbf{Definition 8.} \\textit{(Counterfactual Fairness). ``Predictor \\^{Y} is counterfactually fair if under any context X =x and A=a,\nP($\\hat{Y}_{A\\xleftarrow{}a }$(U)=y|X =x,A=a)=P($\\hat{Y}_{A\\xleftarrow{}a'}$(U)=y|X =x,A=a), (for all y and for any value $a'$ attainable by A''} . The counterfactual fairness definition is based on the ``intuition that a decision is fair towards an individual if it is the same in both the actual world and a counterfactual world where the individual belonged to a different demographic group.''\n\\\\\n\\\\\n\\textbf{Definition 9.} \\textit{(Fairness in Relational Domains). ``A notion of fairness that is able to capture the relational structure in a domain---not only by taking attributes of individuals into consideration but by taking into account the social, organizational, and other connections between individuals''} .\\\\\n\\\\\n\\textbf{Definition 10.} \\textit{(Conditional Statistical Parity). For a set of legitimate factors L, predictor \\^{Y} satisfies conditional statistical parity if P(\\^{Y} |L=1,A = 0) = P(\\^{Y}|L=1,A = 1)} . Conditional statistical parity states that people in both protected and unprotected (female and male) groups should have equal probability of being assigned to a positive outcome given a set of legitimate factors L .\n\\\\\n\\\\\nFairness definitions fall under different types as follows:\n\\begin{enumerate}\n\\item{\\textbf{Individual Fairness.} Give similar predictions to similar individuals }.\n\\item{\\textbf{Group Fairness.} Treat different groups equally }.\n\\item{\\textbf{Subgroup Fairness}. Subgroup fairness intends to obtain the best properties of the group and individual notions of fairness. It is different than these notions but uses them in order to obtain better outcomes. It picks a group fairness constraint like equalizing false positive and asks whether this constraint holds over a large collection of subgroups }.\n\\end{enumerate}\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{ |p{5cm}||p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}\n \\hline\nName & Reference &Group & Subgroup &Individual\\\\\n \\hline\nDemographic parity&&\\checkmark && \\\\[0.5pt]\n \\hline\n Conditional statistical parity&&\\checkmark&&\\\\[0.5pt]\n \\hline\n Equalized odds &&\\checkmark&&\\\\[0.5pt]\n  \\hline\n  Equal opportunity&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n  Treatment equality&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n  Test fairness&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n  Subgroup fairness & && \\checkmark&\\\\[0.5pt]\n  \\hline\n  Fairness  through  unawareness&&&&\\checkmark\\\\[0.5pt]\n  \\hline\n  Fairness  through  awareness&&&&\\checkmark\\\\[0.5pt]\n   \\hline\n   Counterfactual fairness&&&&\\checkmark\\\\[0.5pt]\n   \\hline\n\\end{tabular}\n\\caption{Categorizing different fairness notions into group, subgroup, and individual types.}\n\\label{fairtypes}\n\\end{table}\nIt is important to note that according to , it is impossible to satisfy some of the fairness constraints at once except in highly constrained special cases. \nIn , the authors show the inherent incompatibility of two conditions: calibration and balancing the positive and negative classes. These cannot be satisfied simultaneously with each other unless under certain constraints; therefore, it is important to take the context and application in which fairness definitions need to be used into consideration and use them accordingly .\nAnother important aspect to consider is time and temporal analysis of the impacts that these definitions may have on individuals or groups. In  authors show that current fairness definitions are not always helpful and do not promote improvement for sensitive groups---and can actually be harmful when analyzed over time in some cases. They also show that measurement errors can also act in favor of these fairness definitions; therefore, they show how temporal modeling and measurement are important in evaluation of fairness criteria and introduce a new range of trade-offs and challenges toward this direction. It is also important to pay attention to the sources of bias and their types when trying to solve fairness-related questions.\\\\", "cites": [3906, 3902, 3903, 3905, 8704, 3901, 3899, 3907, 3904, 8703, 3900, 8702], "cite_extract_rate": 0.7058823529411765, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple fairness definitions from the cited papers and organizes them into a coherent framework with a clear taxonomy (individual, group, and subgroup fairness). It also abstracts these ideas to identify broader patterns and relationships between fairness notions. Critical analysis is present, but is somewhat limited to pointing out incompatibilities between definitions and their temporal limitations without deeper evaluation of specific works."}}
{"id": "c4976b58-d2d7-4703-a545-b120b4c383c4", "title": "Methods for Fair Machine Learning", "level": "section", "subsections": ["b2cc4f1f-7f19-48f9-916c-9b2a12874118", "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "d135ff68-4939-4964-ba92-cd7df4c8db16", "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "6f88affc-4ee1-4764-9a6b-e5a86fce1a2d"], "parent_id": "11d83682-7593-45c6-99d3-daa7a02d3b91", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"]], "content": "\\label{sec:methods}\nThere have been numerous attempts to address bias in artificial intelligence in order to achieve fairness; these stem from domains of AI. In this section we will enumerate different domains of AI, and the work that has been produced by each community to combat bias and unfairness in their methods. Table~\\ref{domain} provides an overview of the different areas that we focus upon in this survey. \nWhile this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories: \n\\begin{enumerate}\n    \\item \\textbf{Pre-processing.} Pre-processing techniques try to transform the data so that the underlying discrimination is removed . If the algorithm is allowed to modify the training data, then pre-processing can be used . \n    \\item \\textbf{In-processing.} In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process . If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model--- either by incorporating changes into the objective function or imposing a constraint .\n    \\item \\textbf{Post-processing.} Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model . If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase .\n\\end{enumerate}\nExamples of some existing work and their categorization into these types is shown in Table \\ref{prepost}.\nThese methods are not just limited to general machine learning techniques, but because  of \nAI's  popularity, they have expanded to different domains such as natural language processing and deep learning. From learning fair representations  to learning fair word embeddings , debiasing methods have been proposed in different AI applications and domains.\nMost of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. In addition, some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes  during the training phase but avoiding them during prediction time . A list of protected or sensitive attributes is provided in Table \\ref{electionexample}. They point out what attributes should not affect the outcome of the decision in housing loan or credit card decision-making  according to the law. Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision. Different bias-mitigating methods and techniques are discussed below for different domains---each targeting a different problem in different areas of machine learning in detail. This can expand the horizon of the reader on where and how bias can affect the system and try to help researchers  carefully look at various new problems concerning  potential places where discrimination and bias can affect the outcome of a system.", "cites": [3908, 3909, 3912, 3913, 3112, 8701, 3911, 3910], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic overview of fair machine learning methods (pre-processing, in-processing, and post-processing) and references several relevant papers, but the synthesis remains shallow, primarily listing examples rather than connecting them into a deeper narrative. There is minimal critical analysis of the cited works, and the abstraction is limited to reiterating the general goals of fairness in ML without offering a meta-level framework or identifying overarching principles."}}
{"id": "b2cc4f1f-7f19-48f9-916c-9b2a12874118", "title": "Unbiasing Data", "level": "subsection", "subsections": [], "parent_id": "c4976b58-d2d7-4703-a545-b120b4c383c4", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Unbiasing Data"]], "content": "Every dataset is the result of several design decisions made by the data curator. Those decisions have consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In order to mitigate the effects of bias in data, some general methods have been proposed that advocate  having good practices while using data, such as having datasheets that would act like a supporting document for the data reporting the dataset creation method, its characteristics, motivations, and its skews .  proposes a similar approach for the NLP applications. A similar suggestion has been proposed for models in . Authors in  also propose having labels, just like nutrition labels on food, in order to better categorize each data for each task. In addition to these general techniques, some work has targeted more specific types of biases. For example,  has proposed methods to test for cases of Simpson's paradox in the data, and  proposed methods to discover Simpson's paradoxes in data automatically. Causal models and graphs were also used in some work to detect direct discrimination in the data along with its prevention technique that modifies the data such that the predictions would be absent from direct discrimination .  also worked on  preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. Other pre-processing approaches, such as messaging , preferential sampling , disparate impact removal , also aim to remove biases from the data.\n\\begin{table}[h]\n\\centering\n{\\begin{tabular}{ |p{4cm}||p{7cm}|}\n \\hline\n  \\textbf{Area}&\\textbf{Reference(s)}\\\\\n \\hline\n Classification&              \\\\\n \\hline\n Regression& \\\\\n \\hline\n PCA&\\\\\n \\hline\n  Community detection&\\\\\n \\hline\n Clustering& \\\\\n \\hline\n  Graph embedding&\\\\\n \\hline\n Causal inference&          \\\\\n \\hline\n  Variational auto encoders&   \\\\\n  \\hline\n   Adversarial learning& \\\\\n  \\hline\n Word embedding&      \\\\\n \\hline\n Coreference resolution& \\\\\n \\hline\nLanguage model&\\\\\n \\hline\n  Sentence embedding&\\\\\n \\hline\n Machine translation&\\\\\n \\hline\n  Semantic role labeling&\\\\\n \\hline\n Named Entity Recognition& \\\\\n \\hline\n\\end{tabular}}\n\\caption{List of papers targeting and talking about bias and fairness in different areas.}\n\\label{domain}\n\\end{table}\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{ |p{6cm}||p{2cm}|p{2cm}|}\n \\hline\n Attribute& FHA & ECOA\\\\\n \\hline\nRace&\\checkmark&\\checkmark\\\\[0.5pt]\n \\hline\n Color&\\checkmark&\\checkmark\\\\[0.5pt]\n \\hline\n National origin&\\checkmark&\\checkmark\\\\[0.5pt]\n  \\hline\n  Religion&\\checkmark&\\checkmark\\\\[0.5pt]\n  \\hline\n   Sex&\\checkmark&\\checkmark\\\\[0.5pt]\n  \\hline\n   Familial status&\\checkmark&\\\\[0.5pt]\n  \\hline\n   Disability&\\checkmark&\\\\[0.5pt]\n  \\hline\n   Exercised rights under CCPA&&\\checkmark\\\\[0.5pt]\n  \\hline\n   Marital status&&\\checkmark\\\\[0.5pt]\n  \\hline\n   Recipient of public assistance&&\\checkmark\\\\[0.5pt]\n  \\hline\n  Age&&\\checkmark\\\\[0.5pt]\n  \\hline\n\\end{tabular}\n\\caption{A list of the protected attributes as specified in the Fair Housing and Equal Credit Opportunity Acts (FHA and ECOA), from .}\n\\label{electionexample}\n\\end{table}", "cites": [3918, 3894, 3898, 3916, 3923, 3914, 3912, 3921, 3935, 3915, 3924, 3931, 3926, 3938, 3910, 3913, 3112, 3934, 3922, 3936, 1441, 3928, 3933, 7759, 3899, 3930, 8701, 3911, 3917, 3920, 3925, 3908, 8700, 3919, 8706, 3110, 3927, 8705, 3932, 3929, 3937, 7760, 6985, 7761], "cite_extract_rate": 0.71875, "origin_cites_number": 64, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various methods for unbiasing data, including general practices and specific techniques. While it cites a range of papers and mentions different approaches, it lacks a cohesive synthesis of these ideas into a structured framework. There is minimal critical evaluation or abstraction beyond individual methods, and the narrative remains largely enumerative."}}
{"id": "f3f2696e-da02-4d46-b579-d4238f074625", "title": "Fair Classification", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Fair Classification"]], "content": "Since classification is a canonical task in machine learning and is widely used in different areas that can be in direct contact with humans, it is important that these types of methods be fair and be absent from biases that can harm some populations. Therefore, certain methods have been proposed  that satisfy certain definitions of fairness in classification. For instance, in  authors try to satisfy subgroup fairness in classification, equality of opportunity and equalized odds in , both disparate treatment and disparate impact in , and equalized odds in . Other methods try to not only satisfy some fairness constraints but to also be stable toward change in the test set . The authors in , propose a general framework for learning fair classifiers. This framework can be used for formulating fairness-aware classification with fairness guarantees. In another work , authors propose three different modifications to the existing Naive Bayes classifier for discrimination-free classification.  takes a new approach into fair classification by imposing fairness constraints into a Multitask learning (MTL) framework. In addition to imposing fairness during training, this approach can benefit the minority groups by focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy as a whole without attention to accuracy across different groups. In a similar work , authors propose a decoupled classification system where a separate classifier is learned for each group. They use transfer learning to reduce the issue of having less data for minority groups. In  authors propose to achieve fair classification by mitigating the dependence of the classification outcome on the sensitive attributes by utilizing the Wasserstein distance measure. In  authors propose the Preferential Sampling (PS) method to create a discrimination free train data set. They then learn a classifier on this discrimination free dataset to have a classifier with no discrimination. In~, authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for classification and that can provide interpretability.\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{ |p{3.6cm}||p{1.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}\n \\hline\n Algorithm&Reference&Pre-Processing&In-Processing& Post-Processing\\\\\n \\hline\n  Community detection&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n   Word embedding&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n   Optimized pre-processing&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n   Data pre-processing&&\\checkmark&&\\\\[0.5pt]\n  \\hline\n Classification&& &\\checkmark&\\\\[0.5pt]\n \\hline\n Regression&&&\\checkmark&\\\\[0.5pt]\n \\hline\n Classification&&&\\checkmark&\\\\[0.5pt]\n  \\hline\n  Classification&&&\\checkmark&\\\\[0.5pt]\n  \\hline\n  Adversarial learning&&&\\checkmark&\\\\[0.5pt]\n  \\hline\n  Classification&&&&\\checkmark\\\\[0.5pt]\n  \\hline\n  Word embedding&&&&\\checkmark\\\\[0.5pt]\n  \\hline\n  Classification&&&&\\checkmark\\\\[0.5pt]\n  \\hline\n   Classification&&&&\\checkmark\\\\[0.5pt]\n  \\hline\n\\end{tabular}\n\\caption{Algorithms categorized into their appropriate groups based on being pre-processing, in-processing, or post-processing.}\n\\label{prepost}\n\\end{table}", "cites": [3894, 3937, 3923, 3912, 3935, 3112, 3940, 3928, 7759, 3899, 3917, 3919, 3939, 3929, 7760], "cite_extract_rate": 0.64, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section primarily describes various fair classification methods from the cited papers without deeply connecting or synthesizing ideas across them. It lacks critical evaluation of the approaches and does not offer a broader conceptual framework or meta-level insights, focusing instead on listing techniques and their fairness properties."}}
{"id": "ed3d7f9a-9c0e-4566-9679-2996c8055c84", "title": "Fair Regression", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Fair Regression"]], "content": " proposes a fair regression method along with evaluating it with a measure introduced as the ``price of fairness'' (POF) to measure accuracy-fairness trade-offs. They introduce three fairness penalties as follows:\\\\\nIndividual Fairness: The definition for individual fairness as stated in~, ``for every cross pair $(x, y)\\in S_{1}$, $(x', y')\\in S_{2}$, a model $w$ is penalized for how differently it treats $x$ and $x'$ (weighted by a function of $|y - y'|$) where $S_{1}$ and $S_{2}$ are different groups from the sampled population.'' Formally, this is operationalized as\n\\[f_{1}(w,S)= \\frac{1}{n_{1}n_{2}} \\sum_{\\substack{(x_{i},y_{i})\\in S_{1}\\\\(x_{j},y_{j})\\in S_{2}}}d(y_{i},y_{j})(w.x_{i}-w.x_{j})^{2} \\]\nGroup Fairness: \"On average, the two groups' instances should have similar labels (weighted by the nearness of the labels of the instances)\" .\n\\[f_{2}(w,S)= \\Bigg(\\frac{1}{n_{1}n_{2}} \\sum_{\\substack{(x_{i},y_{i})\\in S_{1}\\\\(x_{j},y_{j})\\in S_{2}}}d(y_{i},y_{j})(w.x_{i}-w.x_{j})\\Bigg)^{2} \\]\nHybrid Fairness: \"Hybrid fairness requires both positive and both negatively labeled cross pairs to be treated similarly in an average over the two groups\" .\n\\[f_{3}(w,S)= \\Bigg ( \\sum_{\\substack{(x_{i},y_{i})\\in S_{1}\\\\(x_{j},y_{j})\\in S_{2}\\\\y_{i}=y_{j}=1}} \\frac{d(y_{i},y_{j})(w.x_{i}-w.x_{j})}{n_{1,1}n_{2,1}} \\Bigg)^{2} + \\Bigg ( \\sum_{\\substack{(x_{i},y_{i})\\in S_{1}\\\\(x_{j},y_{j})\\in S_{2}\\\\y_{i}=y_{j}=-1}} \\frac{d(y_{i},y_{j})(w.x_{i}-w.x_{j})}{n_{1,-1}n_{2,-1}} \\Bigg)^{2} \\]\nIn addition to the previous work,  considers the fair regression problem formulation with regards to two notions of fairness statistical (demographic) parity and bounded group loss.  uses decision trees to satisfy disparate impact and treatment in regression tasks in addition to classification.", "cites": [3939, 3930, 3912], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes three fairness penalties for fair regression from Paper 3 and briefly mentions related work from Papers 1 and 2. However, it primarily presents the methods and formulations without deeper synthesis, comparison, or critique. There is minimal abstraction or identification of broader patterns in the field."}}
{"id": "d75bb90a-926a-465c-a57f-14660004b6dc", "title": "Structured Prediction", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Structured Prediction"]], "content": "In , authors studied the semantic role-labeling models and a famous dataset, imSitu, and realized that only 33\\% of agent roles in cooking images are man, and the rest of 67\\% cooking images have woman as agents in the imSitu training set. They also noticed that in addition to the existing bias in the dataset, the model would amplify the bias such that after training a model\\footnote{Specifically, a Conditional Random Field (CRF)} on the dataset, bias is magnified for ``man'', filling only 16\\% of cooking images. Under these observations, the authors of the paper  show that structured prediction models have the risk of leveraging social bias. Therefore, they propose a calibration algorithm called RBA (reducing bias amplification); RBA is a technique for debiasing models by calibrating prediction in structured prediction. The idea behind RBA is to ensure that the model predictions follow the same distribution in the training data. They study two cases: multi-label object and visual semantic role labeling classification. They show how these methods amplify the existing bias in data.", "cites": [3936], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of a specific paper, highlighting both the dataset bias and the model's amplification of it. It synthesizes the key findings and method (RBA) from the cited work to present a coherent argument about bias in structured prediction. However, it lacks broader comparative context or deeper critique of the method's limitations and only marginally abstracts the issue to a general level, focusing mainly on the specific case study."}}
{"id": "0325d67a-b15b-45da-ad92-925d0fd5d9b3", "title": "Fair PCA", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Fair PCA"]], "content": "In  authors show that vanilla PCA can exaggerate the error in reconstruction in one group of people over a different group of equal size, so they propose a fair method to create representations with similar richness for different populations---not to make them indistinguishable, or to hide dependence on a sensitive or protected attribute. They show that vanilla PCA on the labeled faces in the wild (LFW) dataset~ has a lower reconstruction error rate for men than for women faces, even if the sampling is done with an equal weight for both genders. They intend to introduce a dimensionality reduction technique which maintains similar fidelity for different groups and populations in the dataset. Therefore, they introduce Fair PCA and define a fair dimensionality reduction algorithm. Their definition of Fair PCA (as an optimization function) is as follows, in which $A$ and $B$ denote two subgroups, $U_A$ and $U_B$ denote matrices whose rows correspond to rows of $U$ that contain members of subgroups $A$ and $B$ given $m$ data points in $R^n$:\\\\\n\\[ min_{U \\in R^{m \\times n}, rank(U) \\leq d} \\; max \\Bigg \\{ \\frac{1}{|A|} loss(A , U_{A}) ,  \\frac{1}{|B|} loss(B , U_{B})   \\Bigg \\} \\]\nAnd their proposed algorithm is a two-step process listed below:\n\\begin{enumerate}\n\\item{Relax the Fair PCA objective to a semidefinite program (SDP) and solve it.}\n\\item{Solve a linear program that would reduce the rank of the solution.}\n\\end{enumerate}", "cites": [8700], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the Fair PCA method introduced in the cited paper, including its motivation, objective function, and algorithmic steps. However, it lacks synthesis with other works, does not critically assess the methods limitations or implications, and offers minimal abstraction beyond the specific technique described."}}
{"id": "1488231d-641b-4417-a97d-d2b28f9bacd2", "title": "Community Detection/Graph Embedding/Clustering", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Community Detection/Graph Embedding/Clustering"]], "content": "Inequalities in online communities and social networks can also potentially be another place where bias and discrimination can affect the populations. For example, in online communities users with a fewer number of friends or followers face a disadvantage of being heard in online social media . In addition, existing methods, such as community detection methods, can amplify this bias by ignoring these low-connected users in the network or by wrongfully  assigning them to the irrelevant and small communities. In  authors show how this type of bias exists and is  perpetuated by the existing community detection methods. They propose a new attributed community detection method, called CLAN, to mitigate the harm toward disadvantaged groups in online social communities. CLAN is a two-step process that considers the network structure alongside node attributes to address exclusion bias, as indicated below:\n\\begin{enumerate}\n\\item{Detect communities using modularity values (Step 1-unsupervised using only network structure).}\n\\item{Train a classifier to classify users in the minor groups, putting them into one of the major groups using held-out node attributes (Step 2-supervised using other node attributes).}\n\\end{enumerate}\nFair methods in domains similar to community detection are also proposed, such as graph embedding  and clustering .", "cites": [1441, 3894, 3932, 3925], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by grouping the cited works under the umbrella of community detection, graph embedding, and clustering for fairness, but the integration remains shallow and lacks a deeper, unifying framework. There is minimal critical analysis of the approaches or their limitations, and the abstraction is limited to general observations without identifying broader principles or trends in fair ML methods."}}
{"id": "7bcd1575-144f-4d0e-9460-90e5e443aa39", "title": "Causal Approach to Fairness", "level": "subsubsection", "subsections": [], "parent_id": "e20fb19c-7e93-467e-a8cc-5fb0c9fcbfb6", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Machine Learning"], ["subsubsection", "Causal Approach to Fairness"]], "content": "Causal models can ascertain causal relationships between variables. Using causal graphs one can represent these causal relationships between variables (nodes of the graph) through the edges of the graph. These models can be used to remove unwanted causal dependence of outcomes on sensitive attributes such as gender or race in designing systems or policies . Many researchers have used causal models and graphs to solve  fairness-related concerns in machine learning. In , authors discuss in detail the subject of  causality and its importance while designing fair algorithms. There has been much research on discrimination discovery and removal that uses causal models and graphs in order to make decisions that are irrespective of sensitive attributes of groups or individuals. For instance, in  authors propose a causal-based framework that detects direct and indirect discrimination in the data along with their removal techniques.  is an extension to the previous work.  gives a nice overview of most of the previous work done in this area by the authors, along with discussing  system-, group-, and individual-level discrimination and solving each using their previous methods,  in addition to targeting direct and indirect discrimination. By expanding on the previous work and generalizing it, authors in  propose a similar pathway approach for fair inference using causal graphs; this would restrict certain problematic and discriminative pathways in the causal graph flexibly given any set of constraints. This holds when the path-specific effects can be identified from the observed distribution. In  authors introduce the path-specific counterfactual fairness definition which is an extension to counterfactual fairness definition  and propose a method to achieve it further extending the work in . In  authors extended a formalization of algorithmic fairness from their previous work to the setting of learning optimal policies that are subject to constraints based on definitions of fairness. They describe several strategies for learning optimal policies by modifying some of the existing strategies, such as Q-learning, value search, and G-estimation, based on some fairness considerations. In  authors only target discrimination discovery and no removal by finding instances similar to another instance and observing if a  change in the protected attribute will change the outcome of the decision. If so, they declare the existence of discrimination. In , authors define the following two notions of discrimination---unresolved discrimination and proxy discrimination---as follows: \\\\\n\\textbf{Unresolved Discrimination:} \"A variable V in a causal graph exhibits unresolved discrimination if there exists a directed path from A to V that is not blocked by a resolving variable, and V itself is non-resolving\" . \\\\\n\\textbf{Proxy Discrimination:} \"A variable V in a causal graph exhibits potential proxy discrimination, if there exists a directed path from A to V that is blocked by a proxy variable and V itself is not a proxy\" .\nThey proposed methods to prevent and avoid them. They also show that no observational criterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal reasoning framework needs to be incorporated. \\\\\nIn , Instead of using the usual risk difference $RD=p_{1}-p_{2}$, authors propose a causal risk difference $RD^{c}=p_{1}-p_{2}^{c}$ for causal discrimination discovery.\nThey define $p_{2}^{c}$ to be:\\\\\n\\[ p_{2}^{c} = \\frac{\\sum_{\\textbf{s} \\in S, dec(\\textbf{s})=\\ominus}w(\\textbf{s})}{\\sum_{\\textbf{s} \\in S}w(\\textbf{s})} \\]\n$RD^{c}$ not close to zero means that there is a bias in decision value due to group \nmembership (causal discrimination) or to covariates that have not been accounted \nfor in the analysis (omitted variable bias).\nThis $RD^{c}$ then becomes their causal discrimination measure for discrimination discovery.  is another work of this type that uses causal networks for discrimination discovery.", "cites": [3898, 3923, 3916, 3905, 8706, 8705, 3926], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple causal fairness works, linking concepts like path-specific effects, counterfactual fairness, and policy learning. It abstracts key notions such as unresolved and proxy discrimination, and presents a causal risk difference as a unifying measure. While there is some integration and high-level conceptual framing, critical evaluation of the methods and their limitations is more limited."}}
{"id": "1a51bd3a-ee8d-4a1b-943b-b6641dd5cd05", "title": "Variational Auto Encoders", "level": "subsubsection", "subsections": [], "parent_id": "d135ff68-4939-4964-ba92-cd7df4c8db16", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Representation Learning"], ["subsubsection", "Variational Auto Encoders"]], "content": "Learning fair representations and avoiding the unfair interference of sensitive attributes has been introduced in many different research papers. A well-known example is the Variational Fair Autoencoder introduced in . Here,they treat the sensitive variable as the nuisance variable, so that by removing the information about this variable they will get a fair representation. They use a maximum mean discrepancy regularizer to obtain invariance in the posterior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into the lower bound of their VAE architecture satisfies their proposed model for having the Variational Fair Autoencoder.\nSimilar work, but  not targeting fairness specifically, has been introduced in . In  authors also propose a debiased VAE architecture called DB-VAE which learns sensitive latent variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this DB-VAE using these latent variables to debias systems like facial detection systems. In  authors model their representation-learning task as an optimization objective that would minimize the loss of the mutual information between the encoding and the sensitive variable. The relaxed version of this assumption is shown in Equation \\ref{eq1}. They use this in order to learn fair representation and show that adversarial training is unnecessary and in some cases even counter-productive. In Equation \\ref{eq1}, c is the sensitive variable and z the encoding of x.\n\\begin{equation}\n   \\mathop{min}_{q} \\mathcal{L}(q,x)+\\lambda I(z,c)\n    \\label{eq1}\n\\end{equation}\nIn , authors introduce flexibly fair representation learning by disentanglement that disentangles information from multiple sensitive attributes. Their flexible and fair variational autoencoder is not only flexible with respect to downstream task labels but also flexible with respect to sensitive attributes. They address the demographic parity notion of fairness, which can target multiple sensitive attributes or any subset combination of them.", "cites": [3908, 3941, 3911, 3910], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of VAE-based fair representation learning by connecting the Variational Fair Autoencoder with related approaches like DB-VAE and flexibly fair VAE. It highlights shared objectives (e.g., removing sensitive information from latent representations) and introduces the optimization objective from one of the papers. However, it lacks deeper critical evaluation of the methods' strengths and weaknesses and does not fully abstract to higher-level principles of fair representation learning beyond the specific VAE techniques."}}
{"id": "89d06303-d503-484c-8790-9d581772abf4", "title": "Adversarial Learning", "level": "subsubsection", "subsections": [], "parent_id": "d135ff68-4939-4964-ba92-cd7df4c8db16", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair Representation Learning"], ["subsubsection", "Adversarial Learning"]], "content": "In  authors present a framework to mitigate bias in models learned from data with stereotypical associations. They propose a model in which they are trying to maximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary to predict the protected or sensitive variable (stereotyping variable z). The model consists of two parts---the predictor and the adversary---as shown in Figure \\ref{adversary1}. In their model, the predictor is trained to predict Y given X. With the help of a gradient-based approach like stochastic gradient descent, the model tries to learn the weights W by minimizing some loss function LP($\\hat{y}$, y). The output layer is passed to an adversary, which is another network. This network tries to predict Z. \nThe adversary may have different inputs depending on the fairness definition needing to be achieved. For instance, in order to satisfy \\textbf{Demographic Parity}, the adversary would try to predict the protected variable Z using only the predicted label $\\hat{Y}$ passed as an input to it, while preventing the adversary from learning this is the goal of the predictor. Similarly, to achieve \\textbf{Equality of Odds}, the adversary would get the true label Y in addition to the predicted label $\\hat{Y}$. To satisfy \\textbf{Equality of Opportunity} for a given class y, they would only select instances for the adversary where Y=y.  takes an interesting and different direction toward solving fairness issues using adversarial networks by introducing FairGAN which  generates synthetic data that is free from discrimination and is similar to the real data. They use their newly generated synthetic data from FairGAN, which is now debiased, instead of the real data for training and testing. They do not try to remove discrimination from the dataset, unlike many of the existing approaches, but instead generate new datasets similar to the real one which is debiased and preserves good data utility. The architecture of their FairGAN model is shown in Figure \\ref{adversary}. FairGAN consists of two components: a generator $G_{Dec}$ which generates the fake data conditioned on the protected attribute $P_{G}(x,y,s)=P_{G}(x,y|s)P_{G}(s)$ where $P_{G}(s)=P_{data}(s)$, and two discriminators $D_{1}$ and $D_{2}$. $D_{1}$ is trained to differentiate the real data denoted by $P_{data}(x,y,s)$ from the generated fake data denoted by $P_{G}(x,y,s)$. \n\\begin{figure}[H]\n\\includegraphics[width=.6\\textwidth,trim=10cm 0cm 5cm 1cm,clip=true]{adversary.pdf}\n\\caption{Structure of FairGAN as proposed in .}\n\\label{adversary}\n\\end{figure}\n\\begin{figure}[H]\n\\includegraphics[width=0.6\\textwidth,trim=0cm 12cm 0cm 12cm,clip=true]{adversary1.pdf}\n\\caption{The architecture of adversarial network proposed in  \\textsuperscript{\\textcopyright} Brian Hu Zhang.}\n\\label{adversary1}\n\\end{figure}\nIn addition to that, for achieving fairness constraints, such as statistical parity, $P_{G}(x,y|s=1)=P_{G}(x,y|s=0)$, the training of $D_{2}$ is such that it emphasizes differentiation of the two types of synthetic (generated by the model) samples $P_{G}(x,y|s=1)$ and $P_{G}(x,y|s=0)$ indicating if the synthetic samples are from the unprotected or protected groups. Here s denotes the protected or the sensitive variable, and we adapted the same notation as in .", "cites": [3933, 3919], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of adversarial learning methods for fairness, covering two papers that present different applications of adversarial networks. It explains the basic structure of the models and how fairness constraints are integrated, but lacks deeper synthesis of the underlying principles or comparative analysis between the approaches. There is minimal critical evaluation or abstraction to broader fairness concepts or frameworks."}}
{"id": "1fa2d297-36ec-4a86-acde-a856c21c507d", "title": "Word Embedding", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Word Embedding"]], "content": "In  authors noticed that while using state-of-the-art word embeddings in word analogy tests, ``man'' would be mapped to ``computer programmer'' and ``woman'' would be mapped to ``homemaker.'' This bias toward woman triggered the authors to propose a method to debias word embeddings by proposing a method that respects the embeddings for gender-specific words but debiases embeddings for gender-neutral words by following these steps: \\textit{(Notice that Step 2 has two different options. Depending on whether you target hard debiasing or soft debiasing, you would use either step 2a or 2b)}\n\\begin{enumerate}\n    \\item \\textbf{Identify gender subspace.} Identifying a direction of the embedding that captures the bias .\n    \\item \\textbf{Hard debiasing or soft debiasing:}\n    \\begin{enumerate}\n         \\item \\textbf{Hard debiasing (neutralize and equalize).} Neutralize puts away the gender subspace from gender-neutral words and makes sure that all the gender-neutral words are removed and zeroed out in the gender subspace .\n    Equalize makes gender-neutral words to be equidistant from the equality set of gendered words .\n    \\item \\textbf{Soft bias correction.} Tries to move as little as possible to retain its similarity to the original embedding as much as possible, while reducing the gender bias. This trade-off is controlled by a parameter . \n    \\end{enumerate}\n\\end{enumerate}\nFollowing on the footsteps of these authors, other future work  attempted to tackle this problem  by generating a gender-neutral version of (Glove called GN-Glove) that tries to retain gender information in some of the word embedding's learned dimensions, while ensuring that other dimensions are free from this gender effect. This approach primarily relies on Glove as its base model with gender as the protected attribute. However, a recent paper  argues against these debiasing techniques and states that many recent works on debiasing word embeddings have been superficial, that those techniques just hide the bias and don't actually remove it. A recent work  took a new direction and proposed a preprocessing method for the discovery of the problematic documents in the training corpus that have biases in them, and tried to debias the system by perturbing or removing these documents efficiently from the training corpus. In a very  recent work , authors target bias in ELMo's contextualized word vectors and attempt to analyze and mitigate the observed bias in the embeddings. They show that the corpus used for training of ELMo has a significant gender skew, with male entities being nearly three times more common than female entities. This automatically leads to gender bias in these pretrained contextualized embeddings. They propose the following two methods for mitigating the existing bias while using the pretrained embeddings in a downstream task, coreference resolution: \n(1) train-time data augmentation approach, and \n(2) test-time neutralization approach.", "cites": [3927, 3913, 3112, 3917], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes key debiasing approaches for word embeddings, linking methods like hard and soft debiasing to subsequent variations such as GN-Glove and ELMo-based techniques. It includes critical commentary by citing a paper that challenges the effectiveness of these methods, indicating an awareness of limitations. While it identifies a pattern of addressing gender bias, it stops short of offering a broader, more generalized framework or deeper abstraction."}}
{"id": "3d5f7546-001f-4675-83a5-334a56b66858", "title": "Coreference Resolution", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Coreference Resolution"]], "content": "The  paper shows that coreference systems have a gender bias. They introduce a benchmark, called WinoBias, focusing on gender bias in coreference resolution. In addition to that, they introduce a data-augmentation technique that removes bias in the existing state-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques. Their general approach is as follows: They first generate auxiliary datasets using a rule-based approach in which they replace all the male entities with female entities and the other way around. Then they train models with a combination of the original and the auxiliary datasets. They use the above solution in combination with word2vec debiasing techniques to generate word embeddings. They also point out sources of gender bias in coreference systems and propose solutions \nto them. They show that the first source of bias comes from the training data and propose a solution that generates an auxiliary data set by swapping male and female entities. Another case arises from the resource bias (word embeddings are bias), so the proposed solution is to replace Glove with a debiased embedding method. Last,  another source of bias can come from unbalanced gender lists, and balancing the counts in the lists is a solution they proposed. In another work , authors also show the existence of gender bias in three state-of-the-art coreference resolution systems by observing that for many occupations, these systems resolve pronouns in a biased fashion by preferring one gender over the other.", "cites": [3918, 3110], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key contributions of the cited papers, particularly connecting the bias sources and debiasing strategies in coreference resolution. It identifies patterns such as training data bias and resource bias, but lacks deeper critical analysis of the methods' effectiveness or broader implications. The abstraction is moderate, as it moves beyond individual papers to outline general sources of bias and approaches to address them."}}
{"id": "bd27d570-e2df-4ee8-8cd1-3441aa6394cc", "title": "Language Model", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Language Model"]], "content": "In  authors introduce a metric for measuring gender bias in a generated text from a language model based on recurrent neural networks that is trained on a text corpus along with measuring the bias in the training text itself. They use Equation \\ref{language_model_bias}, where $w$ is any word in the corpus, $f$  is a set of gendered words that belong to the female category, such as she, her, woman, etc., and $m$ to the male category, and measure the bias using the mean absolute and standard deviation of the proposed metric along with fitting a univariate linear regression model over it and then analyzing the effectiveness of each of those metrics while measuring the bias. \n\\begin{equation}\n    bias(w) = log (\\frac{P(w|f)}{P(w|m)})\n    \\label{language_model_bias}\n\\end{equation}\nIn their language model, they also introduce a regularization loss term that would minimize the projection of embeddings trained by the encoder onto the embedding of the gender subspace following the soft debiasing technique introduced in . Finally, they evaluate the effectiveness of their method on reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise on perplexity. They also point out the effectiveness of word-level bias metrics over the corpus-level metrics.", "cites": [3112, 3914], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the methodology of one specific paper (doc_id: 3914) and mentions the influence of another (doc_id: 3112), but it lacks deeper synthesis or comparison between the works. There is minimal critical evaluation of the approaches or their limitations, and the discussion remains focused on the specific details of the cited studies without generalizing to broader trends or principles in fair NLP."}}
{"id": "92c28f7a-822a-42cb-a6c5-c3ee17314fbb", "title": "Sentence Encoder", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Sentence Encoder"]], "content": "In  authors extend the research in detecting bias in word embedding techniques to that of sentence embedding. They try to generalize bias-measuring techniques, such as using the Word Embedding Association Test (WEAT ) in the context of sentence encoders by introducing their new sentence encoding bias-measuring techniques, the  Sentence Encoder Association Test (SEAT). They used state-of-the-art sentence encoding techniques, such as CBoW, GPT, ELMo, and BERT, and find that although there was  varying evidence of human-like bias in sentence encoders using SEAT, more recent methods like BERT are more immune to biases. That being said, they are not claiming that these models are bias-free, but state that more sophisticated bias discovery techniques may be used in these cases, thereby  encouraging more future work in this area.", "cites": [3110, 3938], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two papers to build a narrative on bias in sentence encoders, connecting the extension of WEAT to SEAT and showing how bias detection has evolved from word to sentence-level embeddings. It provides some critical analysis by noting that BERT is more immune to biases but not entirely bias-free, which encourages further research. However, the abstraction and synthesis are limited, as it does not fully generalize these findings into broader principles or frameworks."}}
{"id": "fd7d0053-4cc4-4281-8873-6d1faa820bc5", "title": "Machine Translation", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Machine Translation"]], "content": "In  authors  noticed that when translating the word \"friend\" in the following two sentences from English to Spanish, they achieved different results---although in both cases this word should be translated the same way.\\\\\n\"She works in a hospital, my friend is a nurse.\"\\\\\n\"She works in a hospital, my friend is a doctor.\"\\\\\nIn both of these sentences, \"friend\" should be translated to the female version of Spanish friend \"amiga,\" but the results were not reflecting this expectation. For the second sentence, friend was translated to \"amigo,\"---the male version of friend in Spanish. This is because doctor is more stereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it in its performance. To solve this, authors in  build an approach that leverages the fact that machine translation uses word embeddings. They use the existing debiasing methods in word embedding and apply them in the machine translation pipeline. This not only helped them to mitigate the existing bias in their system, but also boosted the performance of their system by one BLUE score. In  authors show that Google's translate system can suffer from gender bias by making sentences taken from the U.S. Bureau of Labor Statistics into a dozen languages that are gender neutral, including Yoruba, Hungarian, and Chinese, translating them into English, and showing that Google Translate shows favoritism toward males for stereotypical fields such as STEM jobs. In  authors annotated and analyzed the Europarl dataset , a large political, multilingual dataset used in machine translation, and discovered that with the exception of the youngest age group (20-30), which represents only a very small percentage of the total amount of sentences (0.71\\%), more male data is available in all age groups. They also looked at the entire dataset and showed that 67.39\\% of the sentences are produced by male speakers. Furthermore, to mitigate the gender-related issues and to improve morphological agreement in machine translation, they augmented every sentence with a tag on the English source side, identifying the gender of the speaker. This helped the system in most of the cases, but not always, so further work has been suggested for integrating speaker information in other ways.", "cites": [3943, 3921, 3942], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key findings from three different papers to illustrate gender bias in machine translation and attempts to connect the problem across models and datasets. It offers some critical perspective by noting the limitations of the proposed solutions (e.g., not always effective). However, the analysis remains at a moderate level and does not present a novel framework or deep abstraction of broader principles related to bias in NLP."}}
{"id": "480287cd-be97-4c66-a013-b3da567e6734", "title": "Named Entity Recognition", "level": "subsubsection", "subsections": [], "parent_id": "e1f0d57a-fd3f-44b7-bd5f-6d4762dd55e0", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Fair NLP"], ["subsubsection", "Named Entity Recognition"]], "content": "In , authors investigate a type of existing bias in various named entity recognition (NER) systems. In particular, they observed that in a context where an entity should be tagged as a person entity, such as \"John is a person\" or \"John is going to school\", more female names as opposed to male names are being tagged as non-person entities or not being tagged at all. To further formalize their observations, authors propose six different evaluation metrics that would measure amount of bias among different genders in NER systems. They curated templated sentences pertaining to human actions and applied these metrics on names from U.S census data incorporated into the templates. The six introduced measures each aim to demonstrate a certain type of bias and serve a specific purpose in showing various results as follows:\n\\begin{itemize}\n  \\item Error Type-1 Unweighted: Through this type of error, authors wanted to recognize the proportion of entities that are tagged as anything other than the person entity in each of the male vs female demographic groups. This could be the entity not being tagged or tagged as other entities, such as location.\n  \\[ \\frac{\\sum_{n \\in N_f}I(n_{type} \\neq PERSON)}{|N_f|}\\]\n  \\item Error Type-1 Weighted: This type of error is similar to its unweighted case except authors considered the frequency or popularity of names so that they could penalize if a more popular name is being tagged wrongfully.\n  \\[ \\frac{\\sum_{n \\in N_f}freq_f(n_{type} \\neq PERSON)}{\\sum_{n \\in N_f}freq_f(n)},\\]\nwhere $freq_f(\\cdot)$ indicates the frequency of a name for a particular year in the female census data. Likewise, $freq_m(\\cdot)$ indicates the frequency of a name for a particular year in the male census data.\n  \\item Error Type-2 Unweighted: This is a type of error in which the entity is tagged as other entities, such as location or city. Notice that this error does not count if the entity is not tagged.\n  \\[ \\frac{\\sum_{n \\in N_f}I(n_{type} \\notin \\{\\emptyset,PERSON\\})}{|N_f|},\\]\nwhere $\\emptyset$ indicates that the name is not tagged.\n  \\item Error Type-2 Weighted: This error is again similar to its unweighted case except the frequency is taken into consideration.\n  \\[ \\frac{\\sum_{n \\in N_f}freq_f(n_{type} \\notin \\{\\emptyset,PERSON\\})}{\\sum_{n \\in N_f}freq_f(n)}\\]\n  \\item Error Type-3 Unweighted: This is a type of error in which it reports if the entity is not tagged at all. Notice that even if the entity is tagged as a non-person entity this error type would not consider it.\n  \\[ \\frac{\\sum_{n \\in N_f}I(n_{type} = \\emptyset)}{|N_f|}\\]\n  \\item Error Type-3 Weighted: Again, this error is similar to its unweighted case with frequency taken into consideration.\n  \\[ \\frac{\\sum_{n \\in N_f}freq_f(n_{type} = \\emptyset)}{\\sum_{n \\in N_f}freq_f(n)}\\]\n\\end{itemize}\nAuthors also investigate the data that these NER systems are trained on and find that the data is also biased toward female gender by not including as versatile names as there should be to represent female names.", "cites": [3934], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of the bias issue in NER systems as discussed in the cited paper, and effectively explains the proposed evaluation metrics. However, it lacks critical analysis of the paper's methodology or limitations and does not compare or synthesize findings with other works on bias in NLP. The abstraction is limited to the specific case of gender bias in NER, without broader generalization."}}
{"id": "6f88affc-4ee1-4764-9a6b-e5a86fce1a2d", "title": "Comparison of Different Mitigation Algorithms", "level": "subsection", "subsections": [], "parent_id": "c4976b58-d2d7-4703-a545-b120b4c383c4", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Methods for Fair Machine Learning"], ["subsection", "Comparison of Different Mitigation Algorithms"]], "content": "The field of algorithmic fairness is a relatively new area of research and work still needs to be done for its improvement. With that being said, there are already papers that propose fair AI algorithms and bias mitigation techniques and compare different mitigation algorithms using different benchmark datasets in the fairness domain. For instance, authors in  propose a geometric solution to learn fair representations that removes correlation between protected and unprotected features. The proposed approach can control the trade-off between fairness and accuracy via an adjustable parameter. In this work, authors evaluate the performance of their approach on different benchmark datasets, such as COMPAS, Adult and German, and compare them against various different approaches for fair learning algorithms considering fairness and accuracy measures . In addition, IBM's AI Fairness 360 (AIF360) toolkit  has implemented many of the current fair learning algorithms and has demonstrated some of the results as demos which can be utilized by interested users to compare different methods with regards to different fairness measures.", "cites": [7759, 3941], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic comparison of mitigation algorithms by mentioning two papers and the AIF360 toolkit, but it lacks deeper synthesis of ideas across these works. It summarizes their methods and evaluation settings without critically analyzing their strengths, weaknesses, or trade-offs. There is no abstraction or generalization to broader principles or trends in fair machine learning."}}
{"id": "92291949-ae76-45f3-a36f-8985c3893828", "title": "Challenges", "level": "subsection", "subsections": [], "parent_id": "539e2025-f6c3-47b4-adeb-1aa1c312db9d", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Challenges and Opportunities for Fairness Research"], ["subsection", "Challenges"]], "content": "There are several remaining challenges to be addressed in the fairness literature. Among them are:\n\\begin{enumerate}\n\\item \\textbf{Synthesizing a definition of fairness.} Several definitions of what would constitute fairness from a machine learning perspective have been proposed in the literature. These definitions cover a wide range of use cases, and as a result are somewhat disparate in their view of fairness. Because of this, it is nearly impossible to understand how one fairness solution would fare under a different definition of fairness. Synthesizing these definitions into one remains an open research problem since it can make evaluation of these systems more unified and comparable. having a more unified fairness definition and framework can also help with the incompatibility issue of some current fairness definitions.\n\\item \\textbf{From Equality to Equity.} The definitions presented in the literature mostly focus on \\emph{equality}, ensuring that each individual or group is given the same amount of resources, attention or outcome. However, little attention has been paid to \\emph{equity}, which is the concept that each individual or group is given the resources they need to succeed~. Operationalizing this definition and studying how it augments or contradicts existing definitions of fairness remains an exciting future direction.\n\\item \\textbf{Searching for Unfairness.} Given a definition of fairness, it should be possible to identify instances of this unfairness in a particular dataset. Inroads toward this problem have been made in the areas of data bias by detecting instances of Simpson's Paradox in arbitrary datasets~; however, unfairness may require more consideration due to the variety of definitions and the nuances in detecting each one. \n\\end{enumerate}\n\\begin{figure}[h]\n\\includegraphics[width=\\textwidth,trim=0cm 0cm 0cm 0cm,clip=true]{new_heatmap.png}\n\\caption{Heatmap depicting distribution of previous work in fairness, grouped by domain and fairness definition.}\n\\label{heatmap}\n\\end{figure}", "cites": [7761, 3944], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a thoughtful analysis of challenges in fairness research, effectively synthesizing the cited papers by connecting the theme of Simpson's Paradox to the broader issue of detecting unfairness. It abstracts beyond specific papers to discuss conceptual challenges such as moving from equality to equity, suggesting a meta-level understanding. The critical analysis is present but could be deeper, particularly in evaluating the limitations of existing fairness definitions or methods."}}
{"id": "3dfcb96e-2c28-4414-9392-9efc71e93c4f", "title": "WinoBias", "level": "subsubsection", "subsections": [], "parent_id": "f668f915-0536-4f60-819c-548baf934ae5", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Appendix"], ["subsection", "Datasets for Fairness Research"], ["subsubsection", "WinoBias"]], "content": "The WinoBias dataset follows the winograd format and has 40 occupations in sentences that are referenced to human pronouns.\nThere are two types of challenge sentences in the dataset requiring linkage of gendered pronouns to either male or female stereotypical occupations. It was used in the coreference resolution study to certify if a system has gender bias or not---in this case, towards stereotypical occupations .", "cites": [3918], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the WinoBias dataset and its purpose in evaluating gender bias in coreference resolution. It integrates minimal information from the cited paper, primarily paraphrasing its use case. There is no critical analysis or abstraction to broader themes or principles in fairness research."}}
{"id": "6103003f-f238-4892-a6f8-7c7e72aa6493", "title": "Diversity in Faces Dataset", "level": "subsubsection", "subsections": [], "parent_id": "f668f915-0536-4f60-819c-548baf934ae5", "prefix_titles": [["title", "A Survey on Bias and Fairness in Machine Learning"], ["section", "Appendix"], ["subsection", "Datasets for Fairness Research"], ["subsubsection", "Diversity in Faces Dataset"]], "content": "The Diversity in Faces (DiF) is\nan image dataset collected for fairness research in face recognition. DiF is a large dataset containing one million annotations for face images. It is also a diverse dataset with diverse facial features, such as different \ncraniofacial distances, skin color, facial symmetry and contrast, age, pose, gender, resolution, along with diverse areas and ratios .\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{ |p{5.3cm}||p{1.3cm}|p{3.3cm}|p{3.3cm}|}\n \\hline\n Dataset Name& Reference & Size&Area\\\\\n \\hline\nUCI adult dataset& &48,842 income records&Social\\\\[0.5pt]\n \\hline\n German credit dataset&&1,000 credit records&Financial\\\\[0.5pt]\n \\hline\n Pilot parliaments benchmark dataset&&1,270 images &Facial images\\\\[0.5pt]\n  \\hline\n WinoBias&&3,160 sentences&Coreference resolution\\\\[0.5pt]\n  \\hline\n Communities and crime dataset&&1,994 crime records&Social\\\\[0.5pt]\n  \\hline\n COMPAS Dataset&& 18,610 crime records&Social\\\\[0.5pt]\n  \\hline\n  Recidivism in juvenile justice dataset&&4,753 crime records&Social\\\\[0.5pt]\n  \\hline\n  Diversity in faces dataset&&1 million images&Facial images\\\\[0.5pt]\n  \\hline\n\\end{tabular}\n\\caption{Most widely used datasets in the fairness domain with additional information about each of the datasets including their size and area of concentration.}\n\\label{dataset}\n\\end{table}\n\\bibliographystyle{ACM-Reference-Format.bst}\n\\bibliography{references}\n\\end{document}", "cites": [3918, 3945], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief factual description of the Diversity in Faces dataset but does not effectively integrate or synthesize information from the cited papers. There is no critical evaluation of the dataset or the related fairness challenges discussed in the cited works. The content remains at a surface level without abstracting broader patterns or principles in fairness research."}}
