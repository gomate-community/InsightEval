{"id": "9f59be43-5360-4147-bb35-346240b6dfde", "title": "Motivation \\& Background: Link to Learning", "level": "section", "subsections": ["6c7d9a6f-ee44-4bdc-9336-218eaa8c0fc6", "f5a73e1e-4bd5-4bf2-9c0a-519c9fe3d8ec"], "parent_id": "8f415ada-4643-4dae-9fb0-c6aa6d891278", "prefix_titles": [["title", "Abduction and Argumentation for Explainable Machine Learning: A Position Survey"], ["section", "Motivation \\& Background: Link to Learning"]], "content": "\\label{Section: Motivation and Background}\nReasoning and Learning have a synergistic inter-dependence: learning produces the knowledge that is assumed as given when reasoning; reasoning draws inferences that provide the inductive bias that is assumed as given when learning. How can we exploit this synergy, particularly with the reasoning processes of abduction and argumentation, so that we can enhance the learning process?\nThe acknowledgement of the existence of this inter-dependence becomes particularly important in the backdrop of a major recent development in Machine Learning and AI: the emergence of the need for explainability in Machine Learning (and Deep Learning, in particular), where decisions taken on the basis of learned models need to be transparent and comprehensible to human users , and where neural-symbolic integration can help develop such explainable systems, based both on lower-level sensory data and higher-level cognitive data. Although the detailed operation of a learned hypothesis may be unknown, it should be possible to explain its inferences at a level that is cognitively-compatible with the intended users or consumers of those inferences. Explanations should not require the users to have technological knowledge and should be offered at the high-level conceptual language of the application as used by the application experts and/or users. Abduction and argumentation, as mechanisms that generate explanations, can support directly the realization of this need for Explainable ML, achieving explainability by design of the way they reason over a learned theory.", "cites": [7303], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the interplay between reasoning and learning, particularly focusing on abduction and argumentation as enablers of explainability in ML. It synthesizes the concept of explainability from the cited survey paper and links it to the broader context of neural-symbolic integration. While it introduces a conceptual framework, it lacks deep comparative or critical analysis of the cited work and does not fully explore limitations or contrasting viewpoints."}}
{"id": "33b9680b-83bd-4a9a-b611-f71365ec34d1", "title": "Abduction in Neural-Based Learning", "level": "subsubsection", "subsections": [], "parent_id": "424603d7-fb15-43a5-a569-5c67641cbd5b", "prefix_titles": [["title", "Abduction and Argumentation for Explainable Machine Learning: A Position Survey"], ["section", "Structure of the (Machine) Learning Task"], ["subsection", "Abduction in Machine Learning"], ["subsubsection", "Abduction in Neural-Based Learning"]], "content": "In the context of building Explainable AI systems, Abduction can help generate cognitive explanations for Machine Learning predictions. Unlike explanations over logic-based or symbolic learning methods (using, e.g., decision trees), which can be directly obtained from the model , explanations over neural or sub-symbolic learning models (using, e.g., deep learning or Bayesian classifiers) are not naturally provided by the model, and need to be obtained by building a parallel symbolic model .\nIn an orthogonal direction, a symbolic module can be built on top of a neural module so that data are fed into the neural module, whose outputs are fed, in turn, into the symbolic module. The latter, then, computes the final outputs, or predictions, of the integrated system, and these predictions are expected to match the high-level labels of the data. In this setting, explanations of the symbolic module take on the role of providing cognitively understandable lower-level labels for the training of the neural module; see, e.g., .\nIt has been shown, in fact, that abduction supports a clean and compositional integration of the neural and symbolic modules, without imposing restrictions on their syntax and semantics . Unlike previous neural-symbolic integration approaches that generally assume that the symbolic module encodes a theory that is effectively differentiable --- and, thus, compatible with the typical neural learning process of backpropagation --- abduction accommodates any theory for the symbolic module, and utilizes the theory's abductive explanations to compute a loss function that is itself differentiable, even if the theory is not.\nIt is then easy to see how the particular framework can support an abduction-induction cycle applied on the symbolic module as follows: During the abduction part of the cycle, we assume the symbolic module is fixed, and we abduce labels that we use to train the neural module. During the induction part of the cycle, we assume the neural part is fixed, and we deduce through it data to train the symbolic module. Thus, the cycle iteratively improves how the neural module maps low-level data to high-level data, on which to train the symbolic module.", "cites": [7304], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited paper by framing abduction as a mechanism for neural-symbolic integration, connecting it to broader principles of explainability in machine learning. It offers a critical perspective by contrasting abduction-based approaches with traditional methods that require symbolic theories to be differentiable. The abstraction is strong, as it generalizes the role of abduction in creating a flexible and iterative framework for explanation and learning."}}
