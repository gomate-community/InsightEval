{"id": "cabc23b1-400a-4701-a498-c23e1b34971c", "title": "Motivation", "level": "subsection", "subsections": ["ee40e990-6526-45de-b456-9968a26b3bfa"], "parent_id": "b55a3b5a-525b-4e3f-8402-7d4b29224e9d", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Introduction: motivation for the survey and definitions"], ["subsection", "Motivation"]], "content": "Large Language Models (LLMs)~ have fueled dramatic progress in Natural Language Processing (NLP) and are already core in several products with millions of users, such as the coding assistant Copilot~, Google search engine\\footnote{See \\textit{e.g.} \\url{https://blog.google/products/search/search-language-understanding-bert/}} or more recently ChatGPT\\footnote{\\url{https://openai.com/blog/chatgpt/}}. Memorization~ combined with compositionality~ capabilities made LLMs able to execute various tasks such as language understanding or conditional and unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards higher-bandwidth human-computer interactions. \nHowever, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide non-factual but seemingly plausible predictions, often referred to as hallucinations~. This leads to many avoidable mistakes, for example in the context of arithmetics~ or within a reasoning chain~. Moreover, many LLMs groundbreaking capabilities seem to emerge with size, measured by the number of trainable parameters: for example,  demonstrate that LLMs become able to perform some BIG-bench tasks\\footnote{\\url{https://github.com/google/BIG-bench}} via few-shot prompting once a certain scale is attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest counterpart~, the size and need for data of LLMs can be impractical for training but also maintenance: continual learning for large models remains an open research question~. Other limitations of LLMs are discussed by~ in the context of \\textit{ChatGPT}, a chatbot built upon \\textit{GPT3}. \nWe argue these issues stem from a fundamental defect of LLMs: they are generally trained to perform statistical language modeling given (i) a single parametric model and (ii) a limited context, typically the $n$ previous or surrounding tokens. While $n$ has been growing in recent years thanks to software and hardware innovations, most models still use a relatively small context size compared to the potentially large context needed to always correctly perform language modeling. Hence, massive scale is required to store knowledge that is not present in the context but necessary to perform the task at hand. \nAs a consequence, a growing research trend emerged with the goal to solve these issues, slightly moving away from the pure statistical language modeling paradigm described above. \nFor example, a line of work circumvents the limited context size of LLMs by increasing its relevance: this is done by adding information extracted from relevant external documents. Through equipping LMs with a module that retrieves such documents from a database given a context, it is possible to match certain capabilities of some of the largest LMs while having less parameters~. Note that the resulting model is now non-parametric since it can query external data sources.\nMore generally, LMs can also improve their context via reasoning strategies~( \\textit{inter alia}) so that a more relevant context is produced in exchange for more computation before generating an answer. Another strategy is to allow LMs to leverage external tools~( \\textit{inter alia}) to augment the current context with important missing information that was not contained in the LM's weights. Although most of these works aim to alleviate the downfalls of LMs mentioned above separately, it is straightforward to think that more systematically augmenting LMs with both reasoning and tools may lead to significantly more powerful agents. We will refer to these models as \\textbf{Augmented Language Models (ALMs)}. As this trend is accelerating, keeping track and understanding the scope  of the numerous results becomes arduous. This calls for a taxonomy of ALMs works and definitions of technical terms that are used with sometimes different intents.", "cites": [2336, 679, 5556, 7957, 5558, 5561, 5557, 7, 1578, 5560, 424, 5559, 365, 8556, 7465, 1554, 2180, 7460], "cite_extract_rate": 0.9, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key insights from multiple papers to form a coherent narrative around the limitations of LLMs and the emerging trend of augmenting them with reasoning and tools. It critically engages with the trade-offs in model scale, parameter efficiency, and external knowledge integration. The section abstracts these ideas into a broader framework, defining ALMs and highlighting potential research directions."}}
{"id": "ee40e990-6526-45de-b456-9968a26b3bfa", "title": "Definitions.", "level": "paragraph", "subsections": ["886d8dd5-1d74-436e-8de8-f915c3750672", "54b6292f-5e3a-4799-9927-98e6f965e58a"], "parent_id": "cabc23b1-400a-4701-a498-c23e1b34971c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Introduction: motivation for the survey and definitions"], ["subsection", "Motivation"], ["paragraph", "Definitions."]], "content": "We now provide definitions for terms that will be used throughout the survey.\n\\begin{itemize}\n    \\item \\textbf{Reasoning.} In the context of ALMs, reasoning is decomposing a potentially complex task into simpler subtasks the LM can solve more easily by itself or using tools. There exist various ways to decompose into subtasks, such as recursion or iteration. In that sense, reasoning is akin to planning as defined for example in~. In this survey, reasoning will very often refer to the various strategies to improve reasoning skills in LMs, such as step-by-step reasoning using few-shot examples. It is not yet fully understood whether the LM is really reasoning, or simply producing a larger context that increases the likelihood of correctly predicting the missing tokens. We refer to~ for a discussion on this topic: although reasoning may currently be an abuse of language given the current state of the art, the term is already in use within the community. A more pragmatic definition of reasoning in the context in ALMs is giving more computation steps to the model before yielding the answer to a prompt.\n    \\item \\textbf{Tool.} For ALMs, a tool is an external module that is typically called using a rule or a special token and whose output is included in the ALM's context. The tool can gather external information, or have an effect on the virtual or physical world (generally perceived by the ALM). An example of a tool fetching external information is a document retriever, while a tool having an external effect is a robotic arm. A tool can be called at training or at inference time. More generally, learning to interact with a tool may consist in learning to call its API.\n    \\item \\textbf{Act.} For ALMs, calling a tool having an effect on the virtual or physical world and observing the result, typically by including it in the ALM's current context. \n    For example, some works from the survey discuss searching the web, or robotic arm manipulation via LMs. With a slight abuse of term, we will sometimes denote the call of a tool by an ALM as an action, even if it does not have an external effect.\n\\end{itemize}", "cites": [2183], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides analytical definitions of 'reasoning,' 'tool,' and 'act' in the context of ALMs, drawing from and engaging with the terminology used in the cited paper. It critically examines the concept of reasoning, questioning whether it reflects true reasoning or just better context prediction, and acknowledges potential abuses of terminology. While it does not deeply synthesize multiple sources or offer a novel framework, it generalizes ideas and identifies key characteristics of ALM augmentation strategies."}}
{"id": "65a82167-f9f6-499d-a245-1809ead9ad3c", "title": "Reasoning", "level": "section", "subsections": ["1b1f3f12-994b-492e-98bf-79b1122e83a3", "99f12445-2bb8-4f90-8420-04bf9be39dad", "6ce91381-830b-4590-b4cf-5ba3c4e4dfa1", "5cdeb1a2-ce9b-40c4-b12e-c5d765d55852"], "parent_id": "ed915511-87ce-430f-a250-1dd27eea9ea2", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"]], "content": "\\label{sec:reasoning}\nIn general, reasoning is the ability to make inferences using evidence and logic. Reasoning can be divided into multiple types of skills such as commonsense reasoning~, mathematical reasoning~, symbolic reasoning~, etc. Often, reasoning involves deductions from inference chains, called as multi-step reasoning. In the context of LMs, we will use the definition of reasoning provided in Section~\\ref{sec:intro}. Previous work has shown that LLMs can solve simple reasoning problems but fail at complex reasoning~: hence, this section focuses on various strategies to augment LM's reasoning skills. One of the challenges with complex reasoning problems for LMs is to correctly obtain the solution by composing the correct answers predicted by it to the sub-problems. \nFor example, a LM may correctly predict the dates of birth and death of a celebrity, but may not correctly predict the age.  call this discrepancy the compositionality gap for LMs.\nFor the rest of this section, we discuss the works related to three popular paradigms for eliciting reasoning in LMs. Note that~ propose a survey on reasoning in language models.  also propose a survey on reasoning albeit with a focus on prompting. Since our present work focuses on reasoning combined with tools, we refer the reader to~ for a more in-depth review of works on reasoning for LLMs.", "cites": [1578, 5559, 2183, 458, 7958, 5562], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of reasoning in LMs and introduces the concept of a compositionality gap, citing relevant papers. However, it lacks deep synthesis or integration of ideas across the cited works and merely lists or refers to them without combining insights into a novel narrative. There is minimal critical analysis or abstraction beyond the specific examples and papers mentioned."}}
{"id": "1b1f3f12-994b-492e-98bf-79b1122e83a3", "title": "Eliciting reasoning with prompting", "level": "subsection", "subsections": ["fe041d2e-c774-47cb-8e15-4025cf6b503f"], "parent_id": "65a82167-f9f6-499d-a245-1809ead9ad3c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Eliciting reasoning with prompting"]], "content": "In recent years, prompting LMs to solve various downstream tasks has become a dominant paradigm~. In prompting, examples from a downstream task are transformed such that they are formulated as a language modeling problem. Prompting typically takes one of the two forms: zero-shot, where the model is directly prompted with a test example's input; and few-shot, where few examples of a task are prepended along with a test example's input. This few-shot prompting is also known as in-context learning or few-shot learning. \nAs opposed to ``naive'' prompting that requires an input to be directly followed by the output/answer, elicitive prompts encourage LMs to solve tasks by following intermediate steps before predicting the output/answer.~ showed that elicitive prompting enables LMs to be better reasoners in a few-shot setting. Later,~ showed similar ability in a zero-shot setting. We discuss them in detail in the following paragraphs.", "cites": [1578, 679, 7094], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of eliciting reasoning through prompting, integrating concepts from multiple papers, particularly connecting chain-of-thought prompting (Paper 1) with broader prompting paradigms (Paper 2 and 3). It synthesizes the core idea of how prompting can enhance reasoning but lacks deeper critical evaluation of the limitations or comparative analysis of the approaches. The abstraction is moderate, as it identifies the general pattern of using prompting to elicit reasoning but does not rise to a meta-level framework."}}
{"id": "fe041d2e-c774-47cb-8e15-4025cf6b503f", "title": "Few-shot setting.", "level": "paragraph", "subsections": ["dcd1f7dc-ab1e-497f-9ea8-4c7b107ae02d"], "parent_id": "1b1f3f12-994b-492e-98bf-79b1122e83a3", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Eliciting reasoning with prompting"], ["paragraph", "Few-shot setting."]], "content": " introduced chain-of-thought (CoT), a few-shot prompting technique for LMs. The prompt consists of examples of a task, with inputs followed by intermediate reasoning steps leading to the final output, as depicted in Figure~\\ref{fig:fewshot_cot}. Table~\\ref{tab:reasoning_comparison} shows that CoT outperforms standard prompting methods.~ observe that the success of the few-shot strategy emerges with scale, while~ add that without fine-tuning, successful use of CoT generally requires 100B+ parameters LMs such as \\textit{LaMDA}~, \\textit{PaLM}~ or \\textit{GPT3}~, before proposing \\textit{UL2}, a 20B open source model that can perform CoT.\nUsing few-shot CoT prompting, \\textit{Minerva}~ achieves excellent performance on math benchmarks such as GSM8K~.  further improve CoT with \\textit{Self-consistency}: diverse reasoning paths are sampled from a given language model using CoT, and the most consistent answer is selected as the final answer.   introduce \\textit{Self-ask}, a prompt in the spirit of CoT. Instead of providing the model with a continuous chain of thought as in Figure~\\ref{fig:fewshot_cot}, \\textit{Self-ask} explicitly states the follow-up question before answering it and relies on a scaffold (e.g, \\textit{``Follow-up question:''} or \\textit{``So the final answer is:''}), so that the answers are more easily parseable. The authors demonstrate an improvement over CoT on their introduced datasets aiming at measuring the compositionality gap. They observe that this gap does not narrow when increasing the size of the model. Note that  focus on 2-hop questions, \\textit{i.e.}, questions for which the model only needs to compose two facts to obtain the answer. Interestingly, \\textit{Self-ask} can easily be augmented with a search engine (see Section~\\ref{sec:acting}).\n\\textit{ReAct}~ is another few-shot prompting approach eliciting reasoning that can query three tools throughout the reasoning steps: \\texttt{search} and \\texttt{lookup} in Wikipedia, and \\texttt{finish} to return the answer. \\textit{ReAct} will be discussed in more detail in the next sections. \n\\begin{figure}\n    \\centering\n    \\begin{tcolorbox}[colframe=RoyalBlue, colback=white]\n    \\textbf{Question:} Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each\n    can has 3 tennis balls. How many tennis balls does he have now? \\\\\n    \\textbf{Answer:} Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis\n    balls. 5 + 6 = 11. The answer is 11. \\\\\n    \\textbf{Question:} The cafeteria had 23 apples. If they used 20 to make lunch and bought\n    6 more, how many apples do they have? \\\\\n    \\textbf{Answer:}  \\\\\n    \\textbf{\\textcolor{RoyalBlue}{<LM>}}\n    \\end{tcolorbox}\n    \\caption{An example of few-shot Chain-of-Thought prompt. \\textbf{\\textcolor{RoyalBlue}{<LM>}} denotes call to the LM with the above prompt.}\n    \\label{fig:fewshot_cot}\n\\end{figure}", "cites": [1578, 679, 5559, 5563, 2191, 1554, 458, 2954, 1553, 364, 8556], "cite_extract_rate": 0.9166666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on few-shot reasoning prompting techniques, linking them through a coherent narrative on the emergence and evolution of these methods. It provides some critical evaluation, such as the observation that the compositionality gap does not narrow with model size, and highlights the parameter scale requirements for CoT effectiveness. However, while it identifies trends, it does not fully abstract to a higher-level framework or provide deep, nuanced critique of the underlying assumptions or limitations."}}
{"id": "dcd1f7dc-ab1e-497f-9ea8-4c7b107ae02d", "title": "Zero-shot setting.", "level": "paragraph", "subsections": [], "parent_id": "fe041d2e-c774-47cb-8e15-4025cf6b503f", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Eliciting reasoning with prompting"], ["paragraph", "Few-shot setting."], ["paragraph", "Zero-shot setting."]], "content": " extend the idea of eliciting reasoning in LMs to zero-shot prompting. Whereas few-shot provides examples of the task at hand, zero-shot conditions the LM on a single prompt that is not an example. Here,  simply append \\textit{Let's think step by step} to the input question before querying the model (see Figure~\\ref{fig:zeroshot_cot}), and demonstrate that zero-shot-CoT for large LMs does well on reasoning tasks such as GSM8K although not as much as few-shot-CoT.\n\\begin{figure}[ht]\n    \\centering\n    \\begin{tcolorbox}[colframe=RoyalBlue, colback=white]\n    \\textbf{Question:} The cafeteria had 23 apples. If they used 20 to make lunch and bought\n    6 more, how many apples do they have? \\\\\n    \\textbf{Answer:} Let's think step by step \\\\\n    \\textbf{\\textcolor{RoyalBlue}{<LM>}}\n    \\end{tcolorbox}\n    \\caption{An example of zero-shot Chain-of-Thought prompt. \\textbf{\\textcolor{RoyalBlue}{<LM>}} denotes call to the LM with the above prompt.}\n    \\label{fig:zeroshot_cot}\n\\end{figure}\n\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{l|c}\n        \\toprule\n        Model  &  Accuracy (\\%) \\\\\n        \\midrule\n        OpenAI (\\texttt{text-davinci-002})\\textsuperscript{[1]} &  15.6 \\\\\n        OpenAI (\\texttt{text-davinci-002}) + CoT\\textsuperscript{[1]} &  46.9 \\\\\n        OpenAI (\\texttt{text-davinci-002}) + CoT + Calculator\\textsuperscript{[1]} &  46.9 \\\\\n        OpenAI (\\texttt{code-davinci-002})\\textsuperscript{[1]} &  19.7 \\\\\n        OpenAI (\\texttt{code-davinci-002}) + CoT\\textsuperscript{[1]} &  63.1 \\\\\n        OpenAI (\\texttt{code-davinci-002}) + CoT + Calculator\\textsuperscript{[1]} &  65.4 \\\\\n        GPT-3 175B + FT + CoT + Calculator\\textsuperscript{[2]} &  34.0 \\\\\n        GPT-3 175B + FT + CoT + Calculator + Verifier\\textsuperscript{[2]} & 55.0 \\\\\n        PaLM 540B\\textsuperscript{[3]} & 17.0  \\\\\n        PaLM 540B+CoT\\textsuperscript{[3]} & 54.0 \\\\\n        PaLM 540B+CoT+Calculator\\textsuperscript{[3]} & 58.0 \\\\\n        PAL\\textsuperscript{[4]} & 72.0\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Evaluation of different reasoning methods on GSM8K, a popular reasoning benchmark. FT denotes fine-tuning and CoT denotes chain-of-thought. The reported accuracies are based on [1]:~; [2]:~; [3]:~; and [4]:~.}\n    \\label{tab:reasoning_comparison}\n\\end{table}", "cites": [1578, 7094, 1554, 458, 7957], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by comparing different reasoning methods under zero-shot settings and presenting their performance on GSM8K. However, it lacks deeper integration or a novel framework, and its critical analysis is limited to basic performance comparisons without addressing underlying strengths, weaknesses, or conceptual issues. The abstraction is minimal, as it focuses on specific results rather than deriving broader principles of reasoning in ALMs."}}
{"id": "99f12445-2bb8-4f90-8420-04bf9be39dad", "title": "Recursive prompting", "level": "subsection", "subsections": [], "parent_id": "65a82167-f9f6-499d-a245-1809ead9ad3c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Recursive prompting"]], "content": "Several works attempt to elicit intermediate reasoning steps by explicitly decomposing problems into sub-problems in order to solve the problem in a divide and conquer manner. This recursive approach can be especially useful for complex tasks, given that compositional generalization can be challenging for LMs . Methods that employ problem decomposition can either then solve the sub-problems independently, where these answers are aggregated to generate the final answer , or solve the sub-problems sequentially, where the solution to the next sub-problem depends on the answer to the previous ones . For instance, in the context of math problems, \\textit{Least-to-most} prompting~ allows a language model to solve harder problems than the demonstration examples by decomposing a complex problem into a list of sub-problems. It first employs few-shot prompting to decompose the complex problem into sub-problems, before sequentially solving the extracted sub-problems, using the solution to the previous sub-problems to answer the next one. \nWhile many earlier works include learning to decompose through distant supervision , like , many recent works employ in-context learning to do so . Among these, there are further differences. For instance,  is a follow-up work to~, but differs by using a series of prompts to perform recursive syntactic parses of the input rather than a linear decomposition, and also differs by choosing the exemplars automatically through various heuristics.  is concurrent work with ~ but differs by interweaving the question decomposition and answering stages, i.e., the next sub-question prediction has access to the previous questions and answers as opposed to generating all sub-questions independently of any previous answers. , on the other hand, decomposes using rule-based principles and slot-filling prompting to translate questions into a series of SQL operations.  also employs prompts to decompose into specific operations, but then allows each sub-problem to be solved using a library of specialized handlers, where each is devoted to a particular sub-task (e.g., retrieval).\n\\begin{figure}[ht]\n    \\centering\n    \\begin{tcolorbox}[colframe=RoyalBlue, colback=white]\n    \\textbf{Prompt 0} \\\\\n    \\\\\n    \\textbf{Question:} It takes Amy 4 minutes to climb to the top\n    of a slide. It takes her 1 minute to slide down.\n    The water slide closes in 15 minutes. How\n    many times can she slide before it closes? \\\\\n    \\textbf{\\textcolor{RoyalBlue}{<LM>}} \\\\\n    \\textbf{Answer:} To solve “\\colorbox{Peach}{How many times can she slide before it closes?}”, we need to first solve: “\\colorbox{YellowGreen}{How long does each trip take?}” \\\\\n    \\textbf{\\textcolor{RoyalBlue}{</LM>}} \\\\\n    \\\\\n    \\textbf{Prompt 1} \\\\ \n    \\\\\n    It takes Amy 4 minutes to climb to the top\n    of a slide. It takes her 1 minute to slide down.\n    The water slide closes in 15 minutes. \\\\\n    \\textbf{\\textcolor{YellowGreen}{Subquestion 1}}: \\colorbox{YellowGreen}{How long does each trip take?} \\\\\n    \\textbf{\\textcolor{RoyalBlue}{<LM>}} \\\\\n    \\textbf{\\textcolor{YellowGreen}{Answer 1}}: It takes Amy 4 minutes to\nclimb and 1 minute to slide\ndown. 4 + 1 = 5. So each trip\ntakes 5 minutes. \\\\\n\\textbf{\\textcolor{RoyalBlue}{</LM>}} \\\\\n\\\\\n    \\textbf{Prompt 2} \\\\\n    \\\\\n    It takes Amy 4 minutes to climb to the top of\na slide. It takes her 1 minute to slide down.\nThe slide closes in 15 minutes. \\\\\n\\textbf{\\textcolor{YellowGreen}{Subquestion 1}}: \\colorbox{YellowGreen}{How long does each trip take?} \\\\\n    \\textbf{\\textcolor{YellowGreen}{Answer 1}}: It takes Amy 4 minutes to\nclimb and 1 minute to slide\ndown. 4 + 1 = 5. So each trip\ntakes 5 minutes. \\\\\n \\textbf{\\textcolor{Peach}{Subquestion 2}}: \\colorbox{Peach}{How many times can she slide before it closes?} \\\\\n \\textbf{\\textcolor{RoyalBlue}{<LM>}} \\\\\n \\textbf{\\textcolor{Peach}{Answer 2}}: The water slide closes in\n15 minutes. Each trip takes 5\nminutes. So Amy can slide\n15 ÷ 5 = 3 times before it\ncloses. \\\\\n\\textbf{\\textcolor{RoyalBlue}{</LM>}} \\\\\n    \\end{tcolorbox}\n    \\caption{Recursive prompting example. \\textbf{\\textcolor{RoyalBlue}{<LM>}} denotes the start of the LM's output to the prompt, while \\textbf{\\textcolor{RoyalBlue}{</LM>}} denotes the end. The problem is first decomposed into subproblems in \\textbf{Prompt 0}. Then, \\textbf{\\textcolor{Peach}{Answer 2}} to  \\textbf{\\textcolor{Peach}{Subquestion 2}} and \\textbf{\\textcolor{YellowGreen}{Answer 1}} to \\textbf{\\textcolor{YellowGreen}{Subquestion 1}} are sequentially fed to \\textbf{Prompt 2} and \\textbf{Prompt 1}. The few-shot examples for each stage's prompt are omitted. Inspired from Figure 1 in~.\n    \\label{fig:my_label}} \n\\end{figure}", "cites": [424, 5567, 4316, 5565, 3384, 1144, 5566, 8929, 4286, 5564], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively integrates multiple cited papers to explain recursive prompting as a method for eliciting reasoning in LMs, particularly through problem decomposition. It draws connections between different approaches, such as least-to-most prompting, successive prompting, and unsupervised decomposition, showing how they vary in strategy and implementation. While it provides some comparative insights, such as differences in decomposition order and exemplar selection, it lacks deeper critical evaluation or meta-level abstraction to fully unify the field's insights."}}
{"id": "6ce91381-830b-4590-b4cf-5ba3c4e4dfa1", "title": "Explicitly teaching language models to reason", "level": "subsection", "subsections": [], "parent_id": "65a82167-f9f6-499d-a245-1809ead9ad3c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Explicitly teaching language models to reason"]], "content": "Despite their spectacular results, prompting approaches have some drawbacks in addition to requiring model scale. Namely, they require to discover prompts that elicit e.g. step-by-step reasoning, manually providing examples when it comes to few-shot for a new task. Moreover, prompting is computationally expensive in the case of long prompts, and it is harder to benefit from a relatively large number of examples due to limited context size of the model. \nRecent works suggest to circumvent these issues by training LMs to use, as humans, a working memory when more than one step are required to solve a task correctly.  introduce the notion of scratchpad, allowing a LM to better perform on multi-step computation tasks such as addition or code execution. More precisely, at training time, the LM sees input tasks such as addition along with associated intermediate steps: the ensemble is called a scratchpad. At test time, the model is required to predict the steps and the answer from the input task. Scratchpads differ from the above prompting strategies in that they are fine-tuned on example tasks with associated computation steps. Note however that~ also perform experiments in the few-shot regime.~ use a similar approach in the context of large LM pre-training: \\textit{Galactica} was trained on a corpus of scientific data including some documents where step-by-step reasoning is wrapped with a special token \\texttt{<work>} and \\texttt{</work>} to mimic an internal working memory. \nAt inference time, the model can be asked explicitly to activate this reasoning mode via the \\texttt{<work>} token.\n argue that one more problem arise when training on reasoning examples: many intermediate reasoning steps may be missing in the training data curated from the internet, as humans do not explicitly write all their reasoning steps.\nTo circumvent the issue of missing steps, the authors created datasets with detailed reasoning process.\nAn example of prompt seen during \\textit{Galactica}'s pre-training is presented in Figure~\\ref{fig:working_memory_example}. \nOther recent works improve the reasoning abilities of pre-trained LMs via fine-tuning.~ propose a bootstrap approach to generate reasoning steps (also called rationales) for a large set of unlabeled data and use that data to fine-tune the model.~ show that standard LM fine-tuning on reasoning tasks lead to better reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning, compared to pre-trained models. Further, several instruction fine-tuning approaches~ use chain-of-thought style prompts to achieve remarkable improvements on popular benchmarks such as BBH~ and MMLU~. Interestingly, all these works also show that small scale instruction-finetuned models can perform better than un-finetuned large scale models, especially in the tasks where instruction following is important. \n\\begin{figure}[ht]\n    \\centering\n    \\begin{tcolorbox}[colframe=RoyalBlue, colback=white]\n    \\textbf{Question:} A needle 35 mm long rests on a water surface at 20◦C. \n    What force over and above the needle’s weight is required to lift the needle from contact with the water surface? $\\sigma = 0.0728m$.\n    \\texttt{<work>}\n    \\begin{align*}\n    \\sigma & = 0.0728 N/m \\\\\n    \\sigma & = F/L \\\\\n    0.0728 & = F/(2 \\times 0.035) \\\\\n    F & = 0.0728(2 \\times 0.035)\n    \\end{align*} \n\\texttt{calculate.py\\\\\n```\\\\\nf = 0.0728*(2*0.035)\\\\\nwith open(\"output.txt\", \"w\") as file:\\\\\n    file.write(str(round(f, 5)))\\\\\n'''\\\\\n}\n\\\\\n«run: \\texttt{calculate.py}» \\\\\n\\\\\n«read: \\texttt{output.txt}» \\\\\n\\\\\n0.0051 \\\\\n\\\\\n\\texttt{</work>} \\\\\n\\\\\n\\textbf{Answer:} $F = 0.0051 N$\n    \\end{tcolorbox}\n    \\caption{Working memory example from~. This prompt and its output are seen during LM pre-training.}\n    \\label{fig:working_memory_example}\n\\end{figure}", "cites": [5560, 8930, 5569, 440, 7466, 7468, 364, 2215, 5568], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key approaches from multiple papers, connecting ideas like scratchpads and internal working memory to broader themes of explicit reasoning training. It also critically evaluates limitations (e.g., missing steps in training data) and highlights the potential of fine-tuning for improving reasoning. While it offers strong analytical insights, it could push further toward abstracting overarching principles or theoretical frameworks."}}
{"id": "5cdeb1a2-ce9b-40c4-b12e-c5d765d55852", "title": "Comparison and limitations of abstract reasoning", "level": "subsection", "subsections": [], "parent_id": "65a82167-f9f6-499d-a245-1809ead9ad3c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Reasoning"], ["subsection", "Comparison and limitations of abstract reasoning"]], "content": "Overall, reasoning can be seen as decomposing a problem into a sequence of sub-problems either iteratively or recursively.\\footnote{Here, reasoning is described as a sequential operation. However, other reasoning structures such as trees could be considered. For example,~ leverage trees to model the different strategies leading to a proof for a given theorem. A strategy is a set of intermediate results that must be either true or themselves proved, hence decomposed into another new subset of intermediate results.} Exploring as many reasoning paths as possible is hard and there is no guarantee that the intermediate steps are valid. A way to produce faithful reasoning traces is to generate pairs of questions and their corresponding answers for each reasoning step , but there is still no guarantee of the correctness of these intermediate steps. Overall, a reasoning LM seeks to improve its context by itself so that it has more chance to output the correct answer. To what extent LMs actually use the stated reasoning steps to support the final prediction remains poorly understood~.\nIn many cases, some reasoning steps may suffer from avoidable mistakes that compromise the correctness of the output. For example, mistakes on nontrivial mathematical operations in a reasoning step may lead to the wrong final output. The same goes with known facts such as the identity of a president at a given year. Some of the works studied above~ already leverage simple external tools such as a \\texttt{search engine} or a \\texttt{calculator} to validate intermediate steps. More generally, the next section of the survey focuses on the various tools that can be queried by LMs to increase the chance of outputting a correct answer.", "cites": [5559, 5571, 2954, 5570, 5568], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of abstract reasoning in ALMs, highlighting general challenges such as the difficulty of exploring multiple reasoning paths and the risk of errors in intermediate steps. It connects ideas from multiple papers, particularly linking the need for faithful reasoning and the role of tools in validation. While it identifies some limitations of current approaches, it lacks deeper critique or a fully novel synthesis of the field."}}
{"id": "51a580f9-615f-4370-bdc7-db68c16128aa", "title": "Iterative LM calling.", "level": "paragraph", "subsections": ["3e0d0f17-1e03-431c-a4e0-2669fdaf9cb7"], "parent_id": "2c37b943-ae87-40c6-9e30-6db4455a598c", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Calling another model"], ["paragraph", "Iterative LM calling."]], "content": "As an alternative to optimizing for a single, optimized prompt, an intuitive way to get better results from LMs consists of repeatedly calling the model to iteratively refine its output. \\textit{Re3}~ exploits this idea to automatically generate stories of over two thousand words. More precisely, \\textit{Re3} first generates a plan, setting, and characters by prompting \\textit{GPT3}~ with a premise. Then, \\textit{Re3} iteratively injects information from both the plan and current story state into a new \\textit{GPT3} prompt to generate new story passages. This work is improved upon in  with the use of a learned detailed outliner that iteratively expands the brief initial outline to any desired level of granularity. \nOther approaches that teach models to iteratively improve texts in an unsupervised fashion range from applications such as blank filling  to denoising a sequence of Gaussian vectors into word vectors ~.\n\\textit{PEER}~, for example, is a model initialized from \\emph{LM-Adapted T5}  and trained on Wikipedia edits, learning both how to carry out edits and how to plan for the next steps. Consequently, \\textit{PEER} is able to develop articles by repeatedly planning and editing as in Figure~\\ref{fig:peer}. The iterative approach has the additional benefit of allowing a complex task like story and article generation to be decomposed into smaller subtasks. Importantly and apart from \\textit{PEER}, the works mentioned above employ heuristics to call the LM. A future research direction may consist in allowing the LM to call itself repeatedly until the output satisfies a certain criterion. Rather than just calling a single model repeatedly,  propose an interactive interface for a pipeline allowing chaining of multiple LMs together, where the output of one step is passed as input to the next. Such contributions allow non-AI-experts to refine solutions to complex tasks that cannot be appropriately handled by a single LM.", "cites": [679, 7959, 5572, 9, 5573, 2180, 5574], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of iterative LM calling by connecting multiple cited works such as Re3, DOC, and PEER, and highlights how this approach enables decomposition of complex tasks. It offers some abstraction by identifying the common theme of task decomposition and iterative refinement. However, it lacks deeper critical analysis of limitations or trade-offs between the methods and could benefit from more explicit comparative insights or generalizations."}}
{"id": "3e0d0f17-1e03-431c-a4e0-2669fdaf9cb7", "title": "Leveraging other modalities.", "level": "paragraph", "subsections": [], "parent_id": "51a580f9-615f-4370-bdc7-db68c16128aa", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Calling another model"], ["paragraph", "Iterative LM calling."], ["paragraph", "Leveraging other modalities."]], "content": "Prompts under the form of text may not contain enough context to correctly perform a given task. For example, a question does not call for the same answer if it is asked with a serious or ironic tone. Including various modalities into the context would probably be useful for LMs such as chatbots. \nAs recently demonstrated by~ and~, LMs can also be used as a general-purpose interface with models pre-trained on different modalities. For example,~ take a number of pre-trained encoders that can process diverse modalities such as vision and language, and connect them to a LM that serves as a universal task layer. The interface and modular encoders are jointly pre-trained via a semi-causal language modeling objective. This approach combines the benefits of causal and non-causal language modeling, enabling both in-context learning and open-ended generation, as well as easy fine-tuning of the encoders. Similarly,~ introduce \\textit{Flamingo}, a family of Visual Language Models (VLMs) that can handle any interleaved sequences of visual and textual data. \\textit{Flamingo} models are trained on large-scale multimodal web corpora containing interleaved text and images, which enables them to display in-context few-shot learning capabilities of multimodal tasks. With only a handful of annotated examples, \\textit{Flamingo} can easily adapt to both generation tasks such as visual question-answering and captioning, as well as classification tasks such as multiple-choice visual question-answering.  introduce Socratic Models, a modular framework in which various models pre-trained on different modalities can be composed zero-shot. This allows models to exchange information with each other and acquire new multimodal capabilities without additional finetuning. Socratic Models enable new applications such as robot perception and planning, free-form question-answering about egocentric videos, or multimodal assistive dialogue by interfacing with external APIs and databases such as search engines. Interestingly, other modalities such as images can be incorporated to improve reasoning capabilities of moderate size LMs (1B)~.", "cites": [7565, 5576, 3408, 5575], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers effectively, connecting their ideas around using language models as general-purpose interfaces for multimodal tasks. It identifies a broader trend of modular and zero-shot integration of different modalities, which shows abstraction. While it includes some critical elements (e.g., noting the benefit of zero-shot composition in Socratic Models), deeper evaluation or limitations are not extensively discussed, limiting its critical score."}}
{"id": "081ac10f-66c6-4b8f-999e-ff41fa6b7cc4", "title": "Information retrieval", "level": "subsection", "subsections": ["e9d8432d-4193-47d6-abec-a0987cecf896", "57292b95-a1cc-488a-8a63-4f934dbcb8f7", "3d05b89c-317f-41e6-94e4-c3f07bb420ea"], "parent_id": "67c56c5f-2ce4-4d6b-80d1-215f4bdc9354", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Information retrieval"]], "content": "LMs can be augmented with memory units, for example via a neural cache of recent inputs~, to improve their reasoning abilities. Alternatively,\nknowledge in the form of natural language can be offloaded completely from the LM by retrieving from an external knowledge source. Memory augmentation strategies help the language model to avoid producing non-factual and out-of-date information as well as reducing the number of parameters required to achieve comparable performance to large LMs.", "cites": [461], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of how LMs can be augmented with memory units or external knowledge sources for information retrieval, but it does not synthesize multiple papers to form a broader narrative. It lacks critical evaluation of the cited work and offers minimal abstraction beyond the specific method described in the paper."}}
{"id": "fab674d1-5cf0-4b48-824c-ed50460da631", "title": "Conditioning LMs on retrieved documents.", "level": "paragraph", "subsections": [], "parent_id": "e9d8432d-4193-47d6-abec-a0987cecf896", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Information retrieval"], ["subsubsection", "Retrieval-augmented language models"], ["paragraph", "Conditioning LMs on retrieved documents."]], "content": "Various works augment LMs with a \\texttt{dense retriever} by appending the retrieved documents to the current context~. Even though the idea of retrieving documents to perform question answering is not new, retrieval-augmented LMs have recently demonstrated strong performance in other knowledge intensive tasks besides Q\\&A. These proposals close the performance gap compared to larger LMs that use significantly more parameters. \\textit{REALM}~ was the first method to jointly train end-to-end a retrieval system with an encoder LM. \\textit{RAG}~ jointly fine-tunes the retriever with a sequence-to-sequence model.  introduced a modification of the seq2seq architecture to efficiently process many retrieved documents.  focuses on an auto-regressive LM, called \\textit{RETRO}, and shows that combining a large-scale corpus with pre-trained frozen \\textit{BERT} embeddings for the retriever removes the need to further train the retriever while obtaining comparable performance to \\textit{GPT3} on different downstream tasks. The approach used in \\textit{RETRO} allows the integration of retrieval into existing pre-trained LMs. \\textit{Atlas}~ jointly trains a retriever with a sequence-to-sequence model to obtain a LM with strong few-shot learning capabilities in spite of being orders of magnitude smaller than many other large LMs. Table~\\ref{tab:retrieval_lm_comparison} compares the main characteristics of the models discussed, notably how the retrieval results are integrated into the LM's context. In all these cases, the query corresponds to the prompt.\n\\begin{table}[ht]\n\\small\n    \\centering\n    \\begin{tabular}{lcccc}\n    Model & \\# Retrieval tokens & Granularity & Retriever training & Retrieval integration \\\\\n    \\midrule\n    \\textit{REALM}~ & $O(10^9)$ & Prompt & End-to-End & Append to prompt \\\\\n    \\textit{RAG}~ & $O(10^9)$ & Prompt & Fine-tuning & Cross-attention \\\\\n    \\textit{RETRO}~ & $O(10^{12})$ & Chunk & Frozen & Chunked cross-attn. \\\\\n    \\textit{Atlas}~ & $O(10^9)$ & Prompt & Fine-tuning & Cross-attention \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison between database retrieval augmented languages models. Inspired by Table 3 from~.}\n    \\label{tab:retrieval_lm_comparison}\n\\end{table}", "cites": [365, 8418, 5579, 5557, 5577, 7225, 7960, 5578], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a comparative overview of retrieval-augmented LMs by summarizing their core ideas and differences through a table. It synthesizes information from multiple papers to highlight how retrieval is integrated into LMs, but the analysis remains relatively surface-level without deeper evaluation of trade-offs or limitations. The abstraction level is moderate, as it identifies some commonalities in retrieval integration methods but does not generalize to broader principles or frameworks."}}
{"id": "e3479de8-2972-4fcc-9488-ad58bdbb8352", "title": "Chain-of-thought prompting and retrievers.", "level": "paragraph", "subsections": [], "parent_id": "e9d8432d-4193-47d6-abec-a0987cecf896", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Information retrieval"], ["subsubsection", "Retrieval-augmented language models"], ["paragraph", "Chain-of-thought prompting and retrievers."]], "content": "Recent works ~ propose to combine a retriever with reasoning via chain-of-thoughts (CoT) prompting to augment a LM.  use the CoT prompt to generate reasoning paths consisting of an explanation and prediction pair. Then, knowledge is retrieved to support the explanations and the prediction that is mostly supported by the evidence is selected. This approach does not require any additional training or fine-tuning.  propose an information retrieval chain-of-thought approach (IRCoT) which consists of interleaving retrieval with CoT for multi-step QA. The idea is to use retrieval to guide the CoT reasoning steps and conversely, using CoT reasoning to guide the retrieval step.\nIn all these works, a retriever is systematically called for every query in order to get the corresponding documents to augment the LM. These approaches also assume that the intent is contained in the query. The query could be augmented with the user's intent by providing a natural language description of the search task (instruction) in order to disambiguate the intent, as proposed by . Also, the LM could query the retriever only occasionally---when a prompt suggests it to do so---which is discussed in the next subsection.", "cites": [5580, 386, 8931], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core ideas of the cited works, connecting CoT prompting and retrieval mechanisms to form a coherent narrative on how they augment LMs. It also identifies a limitation in the one-step retrieval approach and highlights the need for interleaving retrieval with reasoning. While it abstracts somewhat by generalizing the role of retrieval in reasoning, it does not offer a deep evaluative or meta-level analysis of the broader implications."}}
{"id": "57292b95-a1cc-488a-8a63-4f934dbcb8f7", "title": "Querying search engines", "level": "subsubsection", "subsections": [], "parent_id": "081ac10f-66c6-4b8f-999e-ff41fa6b7cc4", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Information retrieval"], ["subsubsection", "Querying search engines"]], "content": "A LM that only ingests a query can be seen as a passive agent. However, once it is given the ability to generate a query based on the prompt, the LM can enlarge its action space and become more active. \n\\textit{LaMDA} is one example of an agent-like LM designed for dialogue applications. The authors pre-train the model on dialog data as well as other public web documents. In addition to this, to ensure that the model is factually grounded as well as enhancing its conversational abilities, it is augmented with \\texttt{retrieval}, a \\texttt{calculator}, and a \\texttt{translator}~. Furthermore, to improve the model's safety, \\textit{LaMDA} is fine-tuned with annotated data. Another example is \\textit{BlenderBot}~, where the LM decides to generate a query based on a prompt. In this case, the prompt corresponds to the instruction of calling the search engine tool. \\textit{BlenderBot} is capable of open-domain conversation, it has been deployed on a public website to further improve the model via continual learning with humans in the loop. Similarly, \\textit{ReAct} uses few-shot prompting to teach a LM how to use different tools such as \\texttt{search} and \\texttt{lookup} in Wikipedia, and \\texttt{finish} to return the answer~. Similarly,  propose a model that learns to generate an internet search query based on the context, and then conditions on the search results to generate a response.\n \\textit{ReAct} interleaves reasoning and acting, allowing for greater synergy between the two and improved performance on both language and decision making tasks. \\textit{ReAct} performs well on a diverse set of language and decision making tasks such as question answering, fact verification, or web and home navigation. \nIn general, reasoning can improve decision making by making better inferences and predictions, while the ability to use external tools can improve reasoning by gathering additional information from knowledge bases or environments.", "cites": [2954, 1553, 3064], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches (LaMDA, BlenderBot, ReAct) into a coherent narrative about how LMs can become more active by generating and acting on search queries. It abstracts to a degree by highlighting the synergy between reasoning and acting, though the critical evaluation remains limited to general statements about benefits rather than in-depth assessment of limitations or trade-offs."}}
{"id": "3d05b89c-317f-41e6-94e4-c3f07bb420ea", "title": "Searching and navigating the web", "level": "subsubsection", "subsections": [], "parent_id": "081ac10f-66c6-4b8f-999e-ff41fa6b7cc4", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Information retrieval"], ["subsubsection", "Searching and navigating the web"]], "content": "\\label{subsec:search_navigate_web}\nIt is also possible to train agents that can navigate the open-ended internet in pursuit of specified goals such as searching information or buying items. For example, \\textit{WebGPT}~ is a LM-based agent which can interact with a custom text-based web-browsing environment in order to answer long-form questions. In contrast with other models that only learn how to query retrievers or search engines like \\textit{LaMDA}~ or \\textit{BlenderBot}~, \\textit{WebGPT} learns to interact with a web-browser, which allows it to further refine the initial query or perform additional actions based on its interactions with the tool. More specifically, \\textit{WebGPT} can \\texttt{search} the internet, \\texttt{navigate} webpages, \\texttt{follow} links, and \\texttt{cite} sources (see Table~\\ref{tab:actions_webgpt} for the full list of available actions). By accessing the internet, the agent is able to enhance its question-answering abilities, even surpassing those of humans as determined by human evaluators. The best model is obtained by fine-tuning \\textit{GPT3} on human demonstrations, and then performing rejection sampling against a reward model trained to predict human preferences. Similarly, WebShop~ is a simulated e-commerce website where an agent has to find, customize, and purchase a product according to a given instruction. To accomplish this, the agent must understand and reason about noisy text, follow complex instructions, reformulate queries, navigate different types of webpages, take actions to collect additional information when needed, and make strategic decisions to achieve its goals. Both the observations and the actions are expressed in natural language, making the environment well-suited for LM-based agents. The agent consists of a LM fine-tuned with behavior cloning of human demonstrations (\\textit{i.e.}, question-human demonstration pairs) and reinforcement learning using a hard-coded reward function that verifies whether the purchased item matches the given description. While there are other works on web navigation and computer-control, most of them assume the typical human interface, that takes as input images of a computer screen and output keyboard commands in order to solve digital tasks~. Since our survey focuses on LM-based agents, we will not discuss these works in detail.", "cites": [5583, 5582, 7961, 432, 5584, 5581, 3046, 1553, 3064], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how language models can be trained to search and navigate the web, with a focus on specific systems like WebGPT and WebShop. It integrates these works to a basic extent by highlighting the commonalities in their use of natural language interfaces and human demonstrations. However, it lacks deeper critical analysis or abstraction, as it primarily summarizes the methods without evaluating their strengths, limitations, or broader implications."}}
{"id": "a8121db0-5c65-440e-9ef9-c76082758b88", "title": "Computing via Symbolic Modules and Code Interpreters", "level": "subsection", "subsections": [], "parent_id": "67c56c5f-2ce4-4d6b-80d1-215f4bdc9354", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Computing via Symbolic Modules and Code Interpreters"]], "content": "Although recent LMs are able to correctly decompose many problems, they are still prone to errors when dealing with large numbers or performing complex arithmetics. For example, vanilla \\textit{GPT3} cannot perform out-of-distribution addition, \\textit{i.e.} addition on larger numbers than those seen during the training even when provided with examples with annotated steps~. In the context of reinforcement learning, the action space of a transformer agent is equipped with symbolic modules to perform \\textit{e.g.} arithmetic or navigation in~. \\textit{Mind's Eye}~ invokes a \\texttt{physics engine} to ground LMs physical reasoning. More precisely, a text-to-code LM is used to produce rendering code for the physics engine. The outcome of the simulation that is relevant to answer the question is then appended in natural language form to the LM prompt. As a result, \\textit{Mind's Eye} is able to outperform the largest LMs on some specific physical reasoning tasks while having two order of magnitude less parameters. \\textit{PAL}~ relies on CoT prompting of large LMs to decompose symbolic reasoning, mathematical reasoning, or algorithmic tasks into intermediate steps along with python code for each step (see Figure~\\ref{fig:fewshot_pal}). The python steps are then offloaded to a \\texttt{python interpreter} outputting the final result. They outperform CoT prompting on several benchmarks, especially on GSM-HARD, a version of GSM8K with larger numbers. See Table~\\ref{tab:reasoning_comparison} for a comparison between \\textit{PAL} and other models on GSM8K.\nSimilarly,  prompts \\textit{Codex}~ to generate executable code-based solutions to university-level problems, math word problems, or financial QA. \nIn the context of theorem proving,  uses large LMs to automatically formalize informal mathematical competition problem statements in Isabelle or HOL.  generate formal proof sketches, which are then fed to a prover. \n\\begin{figure}[ht]\n    \\centering\n    \\begin{tcolorbox}[colframe=RoyalBlue, colback=white]\n    \\textbf{Question:} Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? \\\\\n\\textbf{Answer:} \\colorbox{YellowGreen}{Roger started with 5 balls.}\\\\\n\\codeenv{tennis\\_balls = 5}\\\\\n\\colorbox{YellowGreen}{2 cans of 3 tennis balls each is} \\\\\n\\codeenv{bought\\_balls = 2 * 3} \\\\\n\\colorbox{YellowGreen}{tennis balls.} \\colorbox{YellowGreen}{The answer is} \\\\\n\\codeenv{answer = tennis\\_balls * bought\\_balls}\\\\\n\\textbf{Question:} The cafeteria had 23 apples. If they used 20 to make lunch and bought\n6 more, how many apples do they have? \\\\\n    \\textbf{Answer:}  \\\\\n    \\textbf{\\textcolor{RoyalBlue}{<LM>}}\n    \\end{tcolorbox}\n    \\caption{An example of few-shot PAL~ prompt. \\textbf{\\textcolor{RoyalBlue}{<LM>}} denotes call to the LM with the above prompt. The prompts are based on the chain-of-thoughts prompting shown on \n    Figure~\\ref{fig:fewshot_cot}, and the parts taken from it are \\colorbox{YellowGreen}{highlighted in green}. \n    In PAL, the prompts also contain \\codeenv{executable python code}, which performs operations and stores the results in the \\codeenv{answer} variable. When prompted with a new question, PAL generates a mix of executable code and explanation. The answer is obtained by executing the code and \\codeenv{print(answer)}.\n    \\label{fig:fewshot_pal}\n    }\n\\end{figure}", "cites": [5585, 5556, 7465, 7957, 5561, 3558, 5586], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by connecting them through the theme of using symbolic modules and code interpreters to enhance LMs' reasoning capabilities. It provides concrete examples and highlights specific techniques and performance differences, such as the efficiency of PAL compared to standard chain-of-thought prompting. However, while it identifies limitations (e.g., vanilla LMs failing with large numbers), it stops short of deeper critique or proposing a novel framework. It begins to abstract patterns, especially in the use of external modules for computation, but could offer more meta-level insights."}}
{"id": "529181c8-bcbe-4d20-a1c8-a3d300cdd8f3", "title": "Controlling Virtual Agents.", "level": "paragraph", "subsections": ["845ac684-d703-46b3-910d-6861dbd9431a"], "parent_id": "ce0ba1d8-25c4-48f3-9fa0-86260ace1ba8", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Acting on the virtual and physical world"], ["paragraph", "Controlling Virtual Agents."]], "content": "Recent works demonstrated the ability of LMs to control virtual agents in simulated 2D and 3D environments by outputting functions which can then be executed by computer in the corresponding environment, be it a simulation or the real-world. For example,~ fine-tune a pre-trained \\textit{GPT2}~ on sequential decision-making problems by representing the goals and observations as a sequence of embeddings and predicting the next action. This framework enables strong combinatorial generalization across different domains including a simulated household environment. This suggests that LMs can produce representations that are useful for modeling not only language but also sequential goals and plans, so that they can improve learning and generalization on tasks that go beyond language processing.\nSimilarly, ~ investigate whether it is possible to use the world knowledge captured by LMs to take specific actions in response to high-level tasks written in natural language such as ``make breakfast''. This work was the first to demonstrate that if the LM is large enough and correctly prompted, it can break down high-level tasks into a series of simple commands without additional training. However, the agent has access to a predetermined set of actions, so not all natural language commands can be executed in the environment. To address this issue, the authors propose to map the commands suggested by the LM into feasible actions for the agent using the cosine similarity function. The approach is evaluated in a virtual household environment and displays an improvement in the ability to execute tasks compared to using the plans generated by the LM without the additional mapping. While these works have demonstrated the usefulness of LMs for controlling virtual robots, the following paragraph cover works on physical robots.  combine a LM with a visual-language model (VLM) and a pre-trained language-conditioned policy for controlling a simulated robotic arm. The LM is used as a multi-step planner to break down a high-level task into subgoals, while the VLM is used to describe the objects in the scene. Both are passed to the policy which then executes actions according to the specified goal and observed state of the world.  use 7B and 70B \\textit{Chinchilla} as planners for an agent that acts and observes the result in a PycoLab environment. Additionally, a reporter module converts actions and observations from pixel to text space. Finally, the agent in~ uses a LM to generate action policies for text-based tasks. Interactively learning via online RL allows to ground the LM internal representations to the environment, thus partly departing from the knowledge \n about statistical surface structure of text that was acquired during pre-training.\n\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{ll}\n    Command & Effect\\\\\n    \\midrule\n    \\texttt{search <query>} & Send <query> to the Bing API and display a search results page\\\\\n    \\texttt{clicked on link <link ID>} & Follow the link with the given ID to a new page\\\\\n    \\texttt{find in page: <text>} &  Find the next occurrence of <text> and scroll to it\\\\\n    \\texttt{quote: <text>} &  If <text> is found in the current page, add it as a reference\\\\\n    \\texttt{scrolled down <1, 2, 3>} &  Scroll down a number of times\\\\\n    \\texttt{scrolled up <1, 2, 3>} &  Scroll up a number of times\\\\\n    \\texttt{Top} &  Scroll to the top of the page\\\\\n    \\texttt{back} &  Go to the previous page\\\\\n    \\texttt{end: answer} &  End browsing and move to answering phase\\\\\n    \\texttt{end: <nonsense, controversial>} &  End browsing and skip answering phase\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The actions \\textit{WebGPT} can perform, taken from ~.}\n    \\label{tab:actions_webgpt}\n\\end{table}", "cites": [5588, 432, 7962, 3034, 5587, 5575], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on controlling virtual agents, integrating different methods such as embedding-based decision-making, multi-step planning with VLMs, and online RL for grounding. It connects the common theme of LMs decomposing tasks and controlling agents in simulated environments. The critical analysis is moderate, pointing out limitations such as the need for predetermined action sets and the importance of grounding. The abstraction level is reasonable, as it generalizes the idea that LMs can model sequential goals and plans beyond language, but could offer deeper theoretical or conceptual generalization."}}
{"id": "845ac684-d703-46b3-910d-6861dbd9431a", "title": "Controlling Physical Robots.", "level": "paragraph", "subsections": [], "parent_id": "529181c8-bcbe-4d20-a1c8-a3d300cdd8f3", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Using Tools and Act"], ["subsection", "Acting on the virtual and physical world"], ["paragraph", "Controlling Virtual Agents."], ["paragraph", "Controlling Physical Robots."]], "content": " use a LM to write robot policy code given natural language commands by prompting the model with a few demonstrations. By combining classic logic structures and referencing external libraries, e.g., for arithmetic operations, LMs can create policies that exhibit spatial-geometric reasoning, generalize to new instructions, and provide precise values for ambiguous descriptions. The effectiveness of the approach is demonstrated on multiple real robot platforms. LMs encode common sense knowledge about the world which can be useful in getting robots to follow complex high-level instructions expressed in natural language. However, they lack contextual grounding which makes it difficult to use them for decision making in the real-world since they do not know what actions are feasible in a particular situation. To mitigate this problem, ~ propose to teach the robot a number of low-level skills (such as ``find a sponge'', ``pick up the apple'', ``go to the kitchen'') and learn to predict how feasible they are at any given state. Then, the LM can be used to split complex high-level instructions into simpler subgoals from the robot's repertoire. The LM can then select the most valuable yet feasible skills for the robot to perform. This way, the robot can use its physical abilities to carry out the LM's instructions, while the LM provides semantic knowledge about the task. The authors test their approach, called \\textit{SayCan}, on various real-world tasks and find that it can successfully complete long, abstract instructions in a variety of environments. To address the grounding problem,~ propose \\textit{NLMap-SayCan}, a framework to gather an integrate contextual information into LM planners. \\textit{NLMap} uses a Visual Language Model (VLM) to create an open-vocabulary queryable scene representation before generating a context-conditioned plan. \nAn alternative way of incorporating contextual information into the agent's decisions is to utilize linguistic feedback from the environment such as success detection, object recognition, scene description, or human interaction~. This results in improved performance on robotic control tasks such as table top rearrangement and mobile manipulation in a real kitchen. Finally, \\textit{RT-1}~ leverages large-scale, diverse, task-agnostic robotic datasets to learn a model that can follow over 700 natural language instructions, as well as generalize to new tasks, environments, and objects. \\textit{RT-1} makes use of \\textit{DIAL}~, an approach for automatically labeling robot demonstrations with linguistic labels via the vision-language alignment model \\textit{CLIP}~.", "cites": [7673, 2997, 7465, 7658, 5589, 3031], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers, connecting ideas around language model-based policy generation, contextual grounding, and task execution. It provides a coherent narrative about how LMs can be used to control robots by writing policy code and integrating with visual and environmental feedback. The section also identifies limitations, such as the lack of contextual grounding in LMs, and discusses solutions like SayCan and NLMap-SayCan, demonstrating both integration and critical evaluation."}}
{"id": "531968ce-08a1-42f7-a0c3-dc6a594fe617", "title": "Supervision", "level": "subsection", "subsections": ["303120e8-c267-4955-83aa-8211402bc0cc"], "parent_id": "24b48acb-9edc-419a-afa0-004d9652109b", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Supervision"]], "content": "\\label{sec:supervision}\nA straightforward way of teaching LMs both to reason and to act is by providing them with human-written demonstrations of the desired behaviours. Common ways of doing so are (i) via few-shot prompting as first suggested by , where the LM is provided a few examples as additional context during inference, but no parameter updates are performed, or (ii) via regular gradient-based learning. Typically, supervised learning is done \\emph{after} an initial pre-training with a language modeling objective ; an exception to this is recent work by , who propose to mix pre-training texts with human-annotated examples containing some form of explicit reasoning, marked with a special token. Some authors use supervised fine-tuning as an intermediate step, followed by reinforcement learning from human feedback ; see Section~\\ref{sec:reinforcement} for an in-depth discussion of such methods.", "cites": [5560, 679, 432, 7468, 364], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of how supervision is applied in augmented language models, referencing key papers on few-shot prompting, supervised fine-tuning, and human feedback. However, it lacks critical evaluation or synthesis of these ideas into a broader framework, merely listing approaches without analyzing their strengths, limitations, or relationships."}}
{"id": "303120e8-c267-4955-83aa-8211402bc0cc", "title": "Few-shot prompting.", "level": "paragraph", "subsections": ["71f912c7-69b6-4afb-8dc6-dbd759162600", "9d1d2f7b-8c4e-47ce-96b8-a99bd5b9a9e2", "d6412b33-7e1b-42a2-8631-9cace0e1f848"], "parent_id": "531968ce-08a1-42f7-a0c3-dc6a594fe617", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Supervision"], ["paragraph", "Few-shot prompting."]], "content": "Providing LMs with a few human-written \\emph{in-context} demonstrations of a desired behaviour is a common approach both for teaching them to reason  and for teaching them to use tools and act . This is mainly due to its ease of use: few-shot prompting only requires a handful of manually labeled examples and enables very fast experimentation as no model fine-tuning is required; moreover, it enables reusing the very same model for different reasoning tasks and tools, just by changing the provided prompt . On the other hand, the ability to perform reasoning with chain-of-thoughts from a few in-context examples only emerges as models reach a certain size , and performance depends heavily on the format in which examples are presented , the choice of few-shot examples, and the order in which they are presented . Another issue is that the amount of supervision that can be provided is limited by the number of examples that fit into the LM's context window; this is especially relevant if (i) a new behaviour is so difficult to learn that it requires more than a handful of examples, or (ii) we have a large space of possible actions that we want a model to learn. Beyond that, as no weight updates are performed, the LM's reasoning and acting abilities are tied entirely to the provided prompt; removing it also removes these abilities.", "cites": [1578, 2221, 679, 424, 5559, 7137, 5590, 7957, 2954, 7468, 8556], "cite_extract_rate": 0.9230769230769231, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to present a coherent narrative on few-shot prompting in ALMs, highlighting how it enables reasoning, tool use, and action without model retraining. It critically discusses limitations, such as model size dependency, context window constraints, and performance variability based on example format and order. It also abstracts to a broader understanding of the role of prompts in shaping ALM behavior, though it stops short of developing a comprehensive meta-framework."}}
{"id": "71f912c7-69b6-4afb-8dc6-dbd759162600", "title": "Fine-tuning.", "level": "paragraph", "subsections": [], "parent_id": "303120e8-c267-4955-83aa-8211402bc0cc", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Supervision"], ["paragraph", "Few-shot prompting."], ["paragraph", "Fine-tuning."]], "content": "As an alternative to few-shot prompting, the reasoning and acting abilities of a pre-trained LM can also be elicited by updating its parameters with standard supervised learning. This approach has been used both for teaching models to use tools, including search engines , web browsers , calculators and translation systems , and for improving reasoning abilities . For the latter, examples of reasoning are typically used in the larger context of \\emph{instruction tuning} , where, more generally, an LM's ability to follow instructions is improved based on human-labeled examples. Examples are typically collected from crowd workers. In some cases, they can instead be obtained automatically:  use execution traces as a form of supervision for reasoning, while  use heuristics to collect supervised data for teaching a language model to use a calculator.", "cites": [432, 8930, 2198, 3077, 1553, 7468, 364, 3064], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of how fine-tuning is used to elicit reasoning and tool-use capabilities in LMs, and mentions specific papers that contribute to this area. It makes minimal connections between the cited works, focusing mostly on listing examples rather than synthesizing a broader framework. There is little critical analysis or identification of limitations, and no clear abstraction of overarching principles or patterns."}}
{"id": "9d1d2f7b-8c4e-47ce-96b8-a99bd5b9a9e2", "title": "Prompt pre-training.", "level": "paragraph", "subsections": [], "parent_id": "303120e8-c267-4955-83aa-8211402bc0cc", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Supervision"], ["paragraph", "Few-shot prompting."], ["paragraph", "Prompt pre-training."]], "content": "A potential risk of finetuning \\emph{after} the pre-training phase is that the LM might deviate far from the original distribution and overfit the distribution of the examples provided during fine-tuning. To alleviate this issue,  propose to mix pre-training data with labeled demonstrations of reasoning, similar to how earlier work mixes pre-training data with examples from various downstream tasks ; however, the exact gains from this mixing, compared to having a separate fine-tuning stage, have not yet been empirically studied. With a similar goal in mind,  and  include examples from pre-training during the fine-tuning stage.", "cites": [5560, 364, 9, 2215], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of prompt pre-training, referencing a few relevant papers, but lacks meaningful synthesis or critical evaluation. It fails to connect the cited works into a coherent narrative and does not offer broader abstractions or insights into the underlying principles or trade-offs of the approach."}}
{"id": "d6412b33-7e1b-42a2-8631-9cace0e1f848", "title": "Bootstrapping.", "level": "paragraph", "subsections": [], "parent_id": "303120e8-c267-4955-83aa-8211402bc0cc", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Supervision"], ["paragraph", "Few-shot prompting."], ["paragraph", "Bootstrapping."]], "content": "As an alternative to standard fine-tuning, several authors propose to use \\emph{bootstrapping} techniques \\citep[e.g.][]{yarowsky-1995-unsupervised,brin1999extracting} to leverage some form of indirect supervision. This typically works by prompting a LM to reason or act in a few-shot setup followed by a final prediction; examples for which the actions or reasoning steps performed did \\emph{not} lead to a correct final prediction are then discarded. For example, STaR~ prompts a model to generate chain-of-thought reasoning sequences in a common sense question answering setup, but only keeps those chains that lead to the correct final answer for a given question. Finally, either the original LM or another (typically smaller) model is fine-tuned on all correct examples. As such, bootstrapping combines the data efficiency of few-shot prompting with some of the advantages of fine-tuning and can be successfully applied both to teach models to reason  and to use tools .", "cites": [7663], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of bootstrapping by connecting it to earlier work in few-shot prompting and indirect supervision, showing some integration of ideas. While it introduces the approach and gives an example (STaR), it does not deeply critique or compare the cited methods. The abstraction level is moderate, as it identifies the broader idea of combining data efficiency and fine-tuning benefits, but stops short of offering a meta-level framework."}}
{"id": "4ae30435-0227-4df3-8855-90a909076021", "title": "Reinforcement learning", "level": "subsection", "subsections": ["a4199090-dfb1-403b-8b3a-dc52ee056580"], "parent_id": "24b48acb-9edc-419a-afa0-004d9652109b", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Reinforcement learning"]], "content": "\\label{sec:reinforcement}\nSupervised learning from human-created prompts is effective to teach models to reason and act. However, such data is difficult and costly to obtain.\nHuman preference data~\\textemdash~such as rankings or likes/dislikes~\\textemdash~is much easier, faster, and cheaper to obtain than full demonstrations. \nFor instance, it might be easier for a human to evaluate the quality of a summary than write one from scratch. \nSuch data cannot be used in a supervised setting, but can provide rewards in the context of Reinforcement Learning (RL)~.\nRL has proven successful for learning complex behaviors through feedback-based interaction with an environment, and it has been us for applications such as playing  games~ or controlling robots~. When training a LM with RL, the LM can be considered an agent that learns a policy (i.e. a distribution over the model's vocabulary from which the next token is sampled) in order to optimize some reward function.\nMost of the existing work on RL and ALMs has focused on teaching LMs how to act rather than reason. The closest work on learning how to reason via RL is STaR~, a bootstrapping-based approach that is discussed in Section~\\ref{sec:supervision} \nRL is a natural framework for training LMs to act and use tools since many of these tools are non-differentiable (e.g. search engines, calculators or programming language interpreters). \nAdditionally, many tasks that benefit from interacting with tools resemble sequential decision making problems (e.g., navigating a web-browser to buy a specified product) and have a well-defined reward (e.g., $1$ if the model buys the correct product and $0$ otherwise).\nWhile there are early works focused on models that could interface with external tools, they employ ad-hoc tool-dependent architectures . \nWe do not cover them here since the main focus of our survey is instead on the acting and reasoning capabilities of standard general-purpose LM architectures trained with the language modeling objective.", "cites": [5593, 4208, 5595, 386, 5591, 5594, 5592], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent analytical overview of how reinforcement learning (RL) can be used to train augmented language models (ALMs) to act and use tools. It synthesizes multiple papers by connecting RL's suitability for non-differentiable tools and sequential decision-making tasks. It also critically notes limitations in prior work, such as ad-hoc architectures and a focus on acting rather than reasoning. The abstraction level is moderate, highlighting general patterns like the use of reward signals for complex behaviors, though it could offer a more unified framework for deeper insight."}}
{"id": "a4199090-dfb1-403b-8b3a-dc52ee056580", "title": "Hard-coded reward functions.", "level": "paragraph", "subsections": ["e0240c17-c60c-4bcd-b31d-85d52595b117"], "parent_id": "4ae30435-0227-4df3-8855-90a909076021", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Reinforcement learning"], ["paragraph", "Hard-coded reward functions."]], "content": "When teaching a LM how to use external tools, the standard practice is to update the weights of the model using a scalar reward generated by a hard-coded reward function. This task-dependent function is computed based on the tool output. The LM agent takes a textual input, which in RL terminology corresponds to the current state of the environment, and generates a sequence of tokens, or actions in RL terms. Optimization is done through policy gradient algorithms like REINFORCE~, PPO and similar variants~. \nInitial works on training LMs to use tools via RL mostly focused on searching and fetching additional factual information. Common tools for such information-seeking tasks are \\texttt{document retrievers}, \\texttt{question answering systems}, and \\texttt{search engines}. \nThe first two consist in retrieving document from a pre-defined set of text documents, or in retrieving an answer based on some input query. However, a search engine allows for more structured interactive search where, for instance, the model further refines the initial query or performs additional actions based on the initial output of the tool.\nFor example,~ perform conversational question-answering by teaching a LM via RL to rewrite queries in order to feed them to an off-the-shelf retriever. The reward function is a contrastive retrieval-accuracy metric based on the token overlap between following conversation rounds and\nretrieved passages. \nAnother example is the work from  : \\textit{RAINIER} is a LM able to generate contextually relevant questions that are optimized to query a frozen \\texttt{QA system}. After distilling knowledge from a larger \\textit{GPT3}~ model into a smaller \\textit{T5} model~, \\textit{RAINIER} is finetuned using PPO~ with feedback provided by the pre-trained question answering model from . Interestingly, this work is an example of a LM learning to use another frozen neural model as an external tool. \n use RL to teach a language model \nto navigate a \\texttt{virtual shop} and buy items constrained on attributes like color and price. Similar to \\textit{WebGPT}~, the model is given a goal in textual format and allowed to perform a limited set of actions. Prompted with a user-generated instruction, in a multi-task learning setup, the model needs to simultaneously understand the query and browse the web to search for the right product. The reward is a hard-coded text-matching function based on the similarity between the  model-purchased written description of the item and the given shopping instruction. Optimization is performed with the A3C algorithm~, a variant of the standard actor-critic method.  While the model still lags behind human experts, they found that fine-tuning with RL after training on human demonstrations improves performance. This provides additional evidence of the benefits of reward-based learning for endowing LMs with the ability to interact with external tools.\nWhile interacting with a \\texttt{search engine} or a \\texttt{document retriever} allows a model to augment its current context with additional input, it is often necessary to process structured information when interacting with tools like a \\texttt{knowledge base}.  train a LM to learn how to interface with a graph-based knowledge base by performing the text2graph and graph2text tasks.\nThe model, based on a \\textit{T5} architecture  and trained with the vanilla policy gradient algorithm REINFORCE~, can perform bidirectional generation of text and graphs and shows state-of-the-art performance on tasks related to knowledge base automated construction from text and vice versa.\nThe \\textit{T5}-based agent is trained to directly maximize graph2text metrics such as BLEU~, METEOR~, and chrF++~, or text2graph ones such as F1, Precision, and Recall.", "cites": [679, 38, 432, 9, 3046, 2257, 2219, 1390, 5596, 5597], "cite_extract_rate": 0.8, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to explain how hard-coded reward functions are used in RL to train language models to interact with tools, providing a coherent narrative on the methodological trends. It includes specific examples and techniques from cited works, but critical evaluation is limited to general observations (e.g., models lagging behind humans, benefits of reward-based learning) rather than in-depth critique. Some abstraction is present in highlighting common patterns in how reward functions enable tool interaction, but it remains grounded in specific case studies."}}
{"id": "e0240c17-c60c-4bcd-b31d-85d52595b117", "title": "Human feedback.", "level": "paragraph", "subsections": [], "parent_id": "a4199090-dfb1-403b-8b3a-dc52ee056580", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Reinforcement learning"], ["paragraph", "Hard-coded reward functions."], ["paragraph", "Human feedback."]], "content": "Evaluating the quality of machine-generated text is non-trivial because it can vary depending on the context, individual preferences, and user's intentions. For example, in some contexts, a user might require creative writing, while in others it may just require factual information. Model outputs should be judged accordingly and should be able to capture such intent differences. \nSeveral metrics based on heuristics like BLEU~ and ROUGE~ have been developed for comparing model outputs to reference texts.\nHowever, they fail to fully capture the quality of generations with respect to human intentions. Human feedback can be exploited to improve the quality of machine-generated text, for example for dialog agents~. In particular, Reinforcement Learning from Human Feedback (RLHF)~ aims to overcome these limitations by using human preferences as an evaluation metric and as an objective function to optimize the language model. Using RLHF allows LMs to be more closely aligned with complex human preferences and values which are difficult to capture by hard-coded reward functions.\nRLHF works by using a pre-trained LM to generate text, which is then evaluated by humans by, for example, ranking two model generations for the same prompt. This data is then collected to learn a reward model that predicts a scalar reward given any generated text. The reward captures human preferences when judging model output. Finally, the LM is optimized against such reward model using RL policy gradient algorithms like PPO~. \nRLHF can be applied directly on top of a general-purpose LM pre-trained via self-supervised learning. However, for more complex tasks, the model's generations may not be good enough. In such cases, RLHF is typically applied after an initial supervised fine-tuning phase using a small number of expert demonstrations for the corresponding downstream task~.\nA successful example of RLHF used to teach a LM to use an external tool stems from \\textit{WebGPT}~ (discussed in~\\ref{subsec:search_navigate_web}), a model capable of answering questions using a \\tl{search engine} and providing references to support such answers. The tool interface is a simplified text-based web-browser. The model architecture is based on \\textit{GPT3}~ and is trained to perform browsing actions expressed in natural language. The model is fine-tuned on question-human demonstration pairs, before further optimization via RLHF. On two QA datasets, \\textit{WebGPT}'s answers are preferred relative to human-generated ones and tend to be more factual than the original vanilla \\textit{GPT3} model. Similarly,  propose \\textit{GopherCite}, a \\textit{Gopher}-based LM model~ fine-tuned with RLHF that can cite supporting evidence when answering questions and abstain from answering when unsure. In contrast with \\textit{WebGPT}, \\textit{GopherCite} uses an information retrieval external module rather than a web-browser to find relevant information that improves its question answering capabilities. Besides learning to use external tools, RLHF has also proven useful for a wide range of language generation tasks, from summarization~ to training more helpful, harmless, and accurate assistants~. Since these works do not focus on training models to reason and act, they are out of the scope of this survey.", "cites": [679, 1354, 364, 432, 5598, 3568, 1339, 2498, 2219, 2500, 8472, 5599, 7461, 1352], "cite_extract_rate": 0.7, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to explain the role of human feedback in improving ALMs via RLHF, particularly in aligning models with complex human preferences and enabling factual, helpful responses. It critically points out limitations of heuristic-based metrics and highlights the advantages of human feedback. While it abstracts to some extent by emphasizing the broader purpose of RLHF in language modeling alignment, it does not fully develop a meta-level framework or compare approaches in depth."}}
{"id": "9bcdfb66-6695-4cbb-bc20-788d51428a57", "title": "Limitations and future directions", "level": "subsection", "subsections": [], "parent_id": "24b48acb-9edc-419a-afa0-004d9652109b", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Learning to reason, use tools, and act"], ["subsection", "Limitations and future directions"]], "content": "Despite recent algorithmic progress and performance improvements, current RL methods still suffer from instability issues which can make training difficult and slow~. While supervised learning has been an efficient and robust way to fine-tune language models on specific tasks~, this  assumes the existence of a large number of expert demonstrations, which can be difficult and costly to obtain. This is particularly true for tasks that require reasoning and acting where we do not have readily available data. A possible solution to the lack of quality data problem could come from bootstrapping methods and offline RL. They combine ``the best of both worlds\" by being more stable to train yet being able to improve via feedback and interaction, even without a large amount of examples for solving the task of interest. Recent works~ have shown that such approaches could reach performance that goes beyond that of the expert demonstrations or improve over initial model generations. For example,~ introduce a new offline RL algorithm called ILQL which learns from a static dataset of demonstrations and their associated rewards by estimating a value function and using it to optimize LM generations. ILQL combines online RL flexible optimization framework with the simplicity and ability to learn from existing datasets of supervised learning, resulting in good performance on dialogue tasks. As explained in Section~\\ref{sec:learning},~ employ a bootstrapping approach for teaching LMs to reason, which can be seen as an approximation to policy gradient algorithms. \nRecently,  proposed \\textit{Toolformer}, a model that teaches itself to use tools in a self-supervised way. This is achieved by first using the few-shot abilities of an existing LM to sample a large amount of potential tool uses. For instance, the model can call a calculator API to augment its context, e.g., ``\\emph{Out of 1400 participants, 400 (or [Calculator(400 / 1400)→ 0.29] 29\\% passed the test.}''\nThen, the model is fine-tuned on its own generations, filtering them based on whether they reduce perplexity for future tokens generations. This method enables using several tools (e.g., a \\texttt{calendar}, a \\texttt{calculator}, or an \\texttt{information retrieval system}). However, it was tested in a limited setup of using a single tool at once, since examples of tool use were independently sampled. We believe that studying how this approach could be extended to more complex multi-step tool uses is a promising research direction for a generalist LM-based agent.", "cites": [5585, 2499, 431], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from the cited papers, connecting supervised learning, reinforcement learning, and self-supervised tool use into a coherent narrative on ALM limitations and future directions. It also provides critical analysis by highlighting issues such as instability in RL and data scarcity, and proposes directions for improvement. Furthermore, it abstracts from specific methods (e.g., ILQL, Toolformer) to discuss broader possibilities for multi-step tool use and agent design."}}
{"id": "282aed9e-a17b-4ba7-a825-9f662e91cf36", "title": "Moving away from language modeling.", "level": "paragraph", "subsections": ["79b3f44a-960e-4661-a84d-875143e9d118"], "parent_id": "dee299b7-149a-48e7-9281-5f8620d57a25", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Discussion"], ["paragraph", "Moving away from language modeling."]], "content": "Is a model trained to do intermediate reasoning steps or having access to the internet still purely performing language modeling?\nIndeed, in NLP, language modeling~ is generally defined as the task of predicting missing tokens given a context\nand is relied heavily on for pre-training models. However, several techniques have been developed to later fine-tune models~ to perform various natural language tasks, which could be seen as moving away from traditional language modeling. In particular, the texts used to fine-tune LMs are not just found on the internet, but rather designed to explicitly inject some level of grounding. One of the argument advocated recently in~ is that ``\\textit{it might be much easier to learn from direct instructions like these than it is to learn from non-instruction data}''. This argument can be supported by the recent work of ~, showing both theoretically and in practice that even shallow looped transformers can follow instructions and be programmed as general purpose computers.\nIntuitively, a text is the result of complex intermediate thoughts that are hidden. Therefore, the superficial text used for supervision can be seen as representing only the logs of these thoughts, thus lacking of context. Conversely, with task-oriented supervised data, we can explicitly ground the answer with the intermediate steps. In this regard, the resulting model may not be considered as a language model. And yet, the task is still about predicting the next token given text only.\nThe argument is all the more true for ALMs since they can augment their context. In particular, tool-augmented LMs might actually lose the ability to assign a probability to the next token - which is at the core of language modeling: whereas a regular LM can easily compute $p(x_t \\mid x_1, \\ldots, x_{t-1})$, a tool-augmented LM has to consider all possible tool uses, e.g. $p(x_t \\mid x_1, \\ldots, x_{t-1}) = \\sum_{c} p(c) \\cdot p(x_t \\mid x_1, \\ldots, x_{t-1}, c)$ where $c$ is a tool, which might not be tractable.\nFor these reasons, we refer to Augmented Language Models (ALMs) \n in this survey, to distinguish from Language Modeling in the traditional sense.", "cites": [3568, 1587, 5600], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section thoughtfully synthesizes the cited papers to argue that ALMs are moving away from the traditional language modeling paradigm by incorporating reasoning and tool use. It critically engages with the notion of language modeling and highlights limitations, such as the intractability of probability computation with tool augmentation. The discussion abstracts the concept of language modeling to broader ideas about supervision, grounding, and the role of intermediate steps in learning."}}
{"id": "f2dcf046-0e3c-4685-a6bb-76965db2c936", "title": "Generalizing the non-parametric framework.", "level": "paragraph", "subsections": [], "parent_id": "79b3f44a-960e-4661-a84d-875143e9d118", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Discussion"], ["paragraph", "Moving away from language modeling."], ["paragraph", "A tradeoff between memorizing and querying tools."], ["paragraph", "Generalizing the non-parametric framework."]], "content": "A motivation behind information retrieval augmented LMs such as \\textit{RETRO}~ and \\textit{Atlas}~ is to develop a class of LM requiring less parameters through relying on an external non-parametric memory.\nThe motivation for using other kind of tools such as \\tl{code interpreter} or \\tl{calculator} has been slightly different so far: for instance,  use a calculator to improve accuracy on tasks requiring arithmetic. \nYet, the paradigm of tool-augmented LMs can be seen as a generalization of the non-parametric framework. Indeed, beyond information retrieval, LMs can delegate any kind of abilities such as calculus to the corresponding external tools. \nBy avoiding to store rarely accessed knowledge in their weights, tool-augmented LMs may have better scaling laws and thus yield smaller models retaining the capabilities of their largest counterpart. Combined with the possibility to access recent information from the external world thus avoiding frequent updates, non-parametric generalization holds great benefits for ALMs.", "cites": [365, 5557, 458], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from RETRO, Atlas, and a math reasoning study, connecting information retrieval with tool use to form a broader conceptual framework. It abstracts these examples into a general trend where LMs offload specialized tasks to external tools, suggesting broader implications for model efficiency and capability. However, the critical analysis is limited, as it primarily highlights benefits without deeply addressing limitations or contrasting different approaches."}}
{"id": "4e6ae953-3cec-49e3-a2ee-648276990320", "title": "Augmented Language Models benefits.", "level": "paragraph", "subsections": [], "parent_id": "79b3f44a-960e-4661-a84d-875143e9d118", "prefix_titles": [["title", "Augmented Language Models: a Survey"], ["section", "Discussion"], ["paragraph", "Moving away from language modeling."], ["paragraph", "A tradeoff between memorizing and querying tools."], ["paragraph", "Augmented Language Models benefits."]], "content": "Overall, ALMs offer many potential advantages over traditional LMs. \n\\begin{itemize}\n    \\item \\textit{Truthfulness}: As the current LM's training objective is arguably responsible for inciting the generation of seemingly plausible but not factual information, grounding the predictions through some tools should lead to more trustworthy models. However, although this conclusion is straightforward when equipping a LM with a calculator, there is surprisingly little evidence of it for information retrieval augmented LMs~. One of the reasons is the presence of a lot of non-truthful information in the web. Investigating this direction will be critical for making LM reliable. \n    \\item \\textit{Estimating and reducing uncertainty}: Extending the maximum-likelihood paradigm by letting the model reason and access additional information could help models to learn what they know and what they don’t. \n    Some papers suggest that LMs are already well calibrated~, i.e. there is a high correlation between the accuracy of their predictions and the corresponding likelihood. This uncertainty could be directly exploited by ALMs to know when to rely on their own weights, or when to query an external tool. \n    \\item \\textit{Interpretability}: Deep learning models are often considered to be black boxes, and their predictions are difficult to interpret. Providing intermediate reasoning steps and relying on tools should help to make ALMs more interpretable. In particular, we can expect that being able to cite the sources used to compose the answer to be critical. \n    However, some works  pointed out that chain-of-thoughts can lead to the correct predictions even though the intermediate reasoning doesn't make any sense, indicating clear challenges for researchers exploring this direction. \n    \\item \\textit{Enhanced capabilities}: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example, an ALM connected to a python interpreter can run code and experiments on a user's behalf, which a vanilla LM cannot do. In addition, a feedback loop can emerge between reasoning and acting, where each ability further improves the other~. Interacting with external tools, entities, and environments can improve reasoning since it allows the ALM to collect additional information and ground itself in the real-world. Similarly, reasoning can improve the ALM's decision making abilities such as when and how to use a certain tool.\n\\end{itemize}", "cites": [2954, 2391, 2376, 5563], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of the cited papers by linking the concepts of truthfulness, uncertainty estimation, interpretability, and enhanced capabilities to the broader theme of ALMs. It critically evaluates the effectiveness of these augmentations (e.g., noting lack of evidence for improved truthfulness in retrieval-based ALMs and issues with nonsensical reasoning steps). The section abstracts beyond individual papers to propose broader implications for model reliability and functionality, offering a principled analytical perspective."}}
