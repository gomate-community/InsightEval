{"id": "130eff31-5033-4384-9c99-729ff90fe873", "title": "Introduction", "level": "section", "subsections": ["67a93181-da5c-4a6d-95df-1dc33e363289", "feb06be2-5673-4204-8578-9022624f9d3c", "bb957f7c-33ba-4e63-98aa-f033211ce23b"], "parent_id": "5f69c7fa-bf2b-4523-bd34-303cffb73a99", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Introduction"]], "content": "Text generation, which is often formally referred as natural language generation (NLG), is one of the most important yet challenging tasks in natural language processing (NLP)~.\nNLG aims at producing understandable text in human language from linguistic or non-linguistic data in a variety of forms such as textual data, numerical data, image data, structured knowledge bases, and knowledge graphs.\nAmong these, text-to-text generation is one of the most important applications and thus often shortly referred as ``text generation''. Researchers have developed numerous technologies for this task in a wide range of applications~.\nText generation takes text (e.g., a sequence, keywords) as input, processes the input text into semantic representations, and generates desired output text.\nFor example, machine translation generates text in a different language based on the source text; summarization generates an abridged version of the source text to include salient information; question answering (QA) generates textual answers to given questions; dialogue system supports chatbots to communicate with humans with generated responses.\nWith the recent resurgence of deep learning technologies~, deep neural NLG models have achieved remarkable performance in enabling machines to understand and generate natural language. A basic definition of the text generation task is to generate an expected \\emph{output sequence} from a given \\emph{input sequence}, called sequence-to-sequence (Seq2Seq). \nThe Seq2Seq task and model were first introduced in 2014~. It maps an input text to an output text under encoder-decoder schemes. The encoder maps the input sequence to a fixed-sized vector, and the decoder maps the vector to the target sequence.\nSince then, developing NLG systems has rapidly become a hot topic. Various text generation models have been proposed under deep neural encoder-decoder architectures. Popular architectures include recurrent neural network (RNN) encoder-decoder~, convolutional neural network (CNN) encoder-decoder~, and Transformer encoder-decoder~.\nNevertheless, the input text alone contains limited knowledge to support neural generation models to produce the desired output.\nMeanwhile, the aforementioned methods generally suffer from an inability to well comprehend language, employ memory to retain and recall knowledge, and reason over complex concepts and relational paths; as indicated by their name, they involve encoding an input sequence, providing limited reasoning by transforming their hidden state given the input, and then decoding to an output.\nTherefore, the performance of generation is still far from satisfaction in many real-world scenarios. For example, in dialogue systems, conditioning on only the input text, a text generation system often produces trivial or non-committal responses of frequent words or phrases in the corpus~, such as \\emph{``Me too.''} or \\emph{``Oh my god!''} given the input text \\emph{``My skin is so dry.''} These mundane responses lack meaningful content, in contrast to human responses rich in knowledge. In comparison, humans are constantly acquiring, understanding, and storing knowledge from \\emph{broader sources} so that they can be employed to understand the current situation in communicating, reading, and writing. For example, in conversations, people often first select \\emph{concepts from related topics} (e.g., sports, food), then organize those topics into understandable content to respond; for summarization, people tend to write summaries containing \\emph{keywords} used in the input document and perform necessary modifications to ensure grammatical correctness and fluency; in question answering (QA), people use \\emph{commonsense} or \\emph{professional knowledge} pertained to the question to infer the answer. Therefore, it is often the case that knowledge beyond the input sequence is required to produce informative output text.\n\\vspace{-0.05in}", "cites": [166, 9147, 2401, 2002, 790, 38], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by connecting foundational sequence-to-sequence models (Paper 3, 5, 6) with the broader context of neural language generation (Paper 2) and deep learning (Paper 1), showing how they relate to the challenge of knowledge incorporation. It provides some abstraction by identifying the general limitation of relying only on input text for knowledge and introduces the concept of external/internal knowledge as a framework. However, critical analysis is limited to pointing out shortcomings without in-depth evaluation or comparison of specific approaches."}}
{"id": "67a93181-da5c-4a6d-95df-1dc33e363289", "title": "What is Knowledge-enhanced Text Generation?", "level": "subsection", "subsections": [], "parent_id": "130eff31-5033-4384-9c99-729ff90fe873", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Introduction"], ["subsection", "What is Knowledge-enhanced Text Generation?"]], "content": "\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=1.0\\textwidth]{figures/int-ent-knowledge-framework.pdf}\n  \\end{center}\n  \\vspace{-0.1in}\n  \\caption{We divide different knowledge sources into internal knowledge and external knowledge. Internal knowledge creation takes place within the input text(s), while external knowledge acquisition occurs when knowledge is provided from outside sources (e.g., Wikipedia, ConceptNet~).}\n  \\label{fig:int-ent-framework}\n\\vspace{-0.05in}\n\\end{figure}\nIn general, knowledge is the familiarity, awareness, or understanding that coalesces around a particular subject. \nIn NLG systems, knowledge is an awareness and understanding of the input text and its surrounding context.\nThese knowledge sources can be categorized into internal knowledge and external knowledge (see Figure~\\ref{fig:int-ent-framework}).\n\\textit{Internal knowledge} creation takes place within the input text(s), including but not limited to keyword, topic, linguistic features, and internal graph structure. \\textit{External knowledge} acquisition occurs when knowledge is provided from outside sources, including but not limited to knowledge base, external knowledge graph, and grounded text. \nThese sources provide information (e.g., commonsense triples, topic words, reviews, background documents) that can be used as knowledge through various neural representation learning methods, and then applied to enhance the process of text generation. \nIn addition, knowledge introduces interpretability for models with explicit semantics.\nThis research direction of incorporating knowledge into text generation is named as \\textit{knowledge-enhanced text generation}.\n\\vspace{-0.05in}\n\\begin{problem}[Knowledge-enhanced Text Generation] \nGiven a text generation problem where the system is given an input sequence $X$,\nand aims to generate an output sequence $Y$.\nAssume we also have access to additional knowledge denoted as $K$. Knowledge-enhanced text generation aims to incorporate the knowledge $K$ to enhance the generation of $Y$ given $X$, through leveraging the dependencies among the input text, knowledge, and output text.\n\\label{prob:seq2seq}\n\\end{problem}\nMany existing knowledge-enhanced text generation systems have demonstrated promising performance on generating informative, logical, and coherent texts. In dialogue systems, a topic-aware Seq2Seq model helped understand the semantic meaning of an input sequence and generate a more informative response such as \\emph{``Then hydrate and moisturize your skin.''} to the aforementioned example input \\emph{``My skin is so dry.''} \nIn summarization, knowledge graph produced a structured summary and highlight the proximity of relevant concepts, when complex events related with the same entity may span multiple sentences. A knowledge graph enhanced Seq2Seq model generated summaries that were able to correctly answer 10\\% more topically related questions~.\nIn question answering (QA) systems, facts stored in knowledge bases completed missing information in the question and elaborate details to facilitate answer generation .\nIn story generation, using commonsense knowledge acquired from knowledge graph facilitated understanding of the storyline and better\nnarrate following plots step by step, so each step could be reflected as a link on the knowledge graph and the whole story would be a path~.\n\\vspace{-0.05in}", "cites": [7046, 3119, 2378, 2369], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from the cited papers into a structured framework distinguishing internal and external knowledge sources, effectively connecting ideas across different NLP tasks like dialogue, summarization, and story generation. It abstracts these examples into a general definition and problem statement of knowledge-enhanced text generation. However, it lacks deeper critical evaluation, such as comparing approaches or identifying limitations, which would further enhance its insight quality."}}
{"id": "feb06be2-5673-4204-8578-9022624f9d3c", "title": "Why a Survey of Knowledge-enhanced Text Generation?", "level": "subsection", "subsections": [], "parent_id": "130eff31-5033-4384-9c99-729ff90fe873", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Introduction"], ["subsection", "Why a Survey of Knowledge-enhanced Text Generation?"]], "content": "Recent years have witnessed a surge of interests in\ndeveloping methods for incorporating knowledge in NLG beyond input text. However, there is a lack of comprehensive survey of this research topic.\nRelated surveys have laid the foundation of discussing this topic.\nFor example, Garbacea et al.~ and Gatt et al.~ reviewed model architectures for core NLG tasks but did not discuss knowledge-enhanced methods.\nJi et al.~ presented a review on\nknowledge graph techniques which could be used for enhancing NLG.\nWang et al.~ summarized how to represent structural knowledge such as knowledge base and knowledge graph for reading comprehension and retrieval.\nTo the best of our knowledge, this is the first survey that presents a comprehensive review of knowledge-enhanced text generation. It aims to provide NLG researchers a synthesis and pointer to related research. Our survey includes a detailed discussion about how NLG can benefit from recent progress in deep learning and artificial intelligence, including technologies such as graph neural network, reinforcement learning, and neural topic modeling.\n\\vspace{-0.05in}", "cites": [1165, 9147], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information by identifying relevant prior surveys and noting their limitations in covering knowledge-enhanced text generation. It abstracts to some extent by highlighting broader technologies (e.g., graph neural networks) that are relevant to the topic. However, the critical analysis is limited, as it does not deeply evaluate the shortcomings or strengths of the cited works beyond pointing out their lack of focus on the specific research area."}}
{"id": "5e9e393b-b67b-4b15-80cd-e48ae82912dc", "title": "The Basic Text Generation Models", "level": "subsection", "subsections": ["5bfe95e3-8494-41a4-b983-424c260b9239"], "parent_id": "e083ab82-0002-443e-8734-d6e90fa797b2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "The Basic Text Generation Models"]], "content": "Early encoder-decoder frameworks are often based on recurrent neural network (RNN) such as RNN-Seq2Seq~. Convolutional neural network (CNN) based encoder-decoder~ and Transformer encoder-decoder~ have been increasingly widely used. From a probabilistic perspective, the encoder-decoder frameworks learn the conditional distribution over a variable length sequence conditioned on yet another variable length sequence:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    P(Y|X) = P(y_1, \\cdots, y_m|x_1, \\cdots, x_n) = \\prod_{t=1}^{m}p(y_t|X, y_1, \\cdots, y_{t-1}).\n\\end{equation}\n\\vspace{-0.05in}", "cites": [2401, 790, 38], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of encoder-decoder models used in text generation, citing three foundational papers, but it lacks deeper synthesis or critical evaluation. It describes the evolution from RNN-based to CNN-based and Transformer-based models, yet it does not analyze their relative strengths, weaknesses, or implications for knowledge-enhanced generation. The abstraction is limited to a general probabilistic formulation without broader conceptual insights."}}
{"id": "10252dc6-a9ac-4ba2-b32d-6a80c9b8f135", "title": "Decoder.", "level": "paragraph", "subsections": [], "parent_id": "5bfe95e3-8494-41a4-b983-424c260b9239", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "The Basic Text Generation Models"], ["paragraph", "Encoder."], ["paragraph", "Decoder."]], "content": "The decoder is to decode a given fixed length vector representation into a variable length sequence~. Specially, the decoder generates an output sequence one token at each time step. At each step the model is auto-regressive, consuming the previously generated tokens as additional input when generating the next token. Formally, the decoding function is represented as:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\textbf{s}_t = \\textsc{Decoder} (\\textbf{s}_{t-1}, \\textbf{e}(y_{t-1})), \\label{eq:Seq2Seq-decoder}\n\\end{equation}\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    p(y_t|y_{t-1}, y_{t-2}, \\cdots , y_1) = \\textsc{Readout}(\\textbf{s}_t),\n\\end{equation}\nwhere $\\textsc{Readout}(\\cdot)$ is a nonlinear multi-layered function that outputs the probability of $y_t$.\n\\vspace{-0.05in}", "cites": [2401], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the decoder component in text generation models, referencing one paper but without connecting it to broader themes or alternative approaches. It lacks critical evaluation or comparison of different methods and remains focused on a general explanation of the decoding process, with minimal abstraction or synthesis of multiple sources."}}
{"id": "7100a363-05ad-41f2-80ad-599b293e84f9", "title": "Attention Mechanism", "level": "subsubsection", "subsections": ["19576d26-7a1c-47a7-9adb-7f2601cfef36"], "parent_id": "4e1ad8d3-1399-4bd1-ae45-57b3679fd0ab", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Attention Mechanism"]], "content": "}\n\\label{sec:k-attn}\nIt is useful to capture the weight of each time step in both encoder and decoder~.\nDuring the decoding phase, the context vector $\\textbf{c}_t$ is added, so the hidden state $\\textbf{s}_t$ is:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n   \\textbf{s}_t = \\textsc{Decoder} (\\textbf{s}_{t-1}, \\textbf{e}(y_{t-1}), \\textbf{c}_t).\n\\end{equation}\nUnlike Eq.(\\ref{eq:Seq2Seq-decoder}), here the probability is conditioned on the distinct context vector $\\textbf{c}_t$ for target word $y_t$, and $\\textbf{c}_t$ depends on a sequence of hidden states $\\textbf{H} = \\{\\textbf{h}_i\\}_{i=1}^{n}$ that were mapped from input sequence. \nIn RNN-Seq2Seq decoder, the $\\textbf{c}_t$ is computed as a weighted sum of $\\{\\textbf{h}_i\\}_{i=1}^{n}$:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n   \\textbf{c}_t = \\sum^{n}_{i=1} \\alpha_{ti} \\textbf{h}_i, ~\\text{where}~ \\alpha_{ti} = \\frac{\\exp (\\eta(\\textbf{s}_{t-1}, \\textbf{h}_i ))} {\\sum^{n}_{k=1} \\exp(\\eta(\\textbf{s}_{t-1}, \\textbf{h}_k)) } ,    \n   \\label{eq:atten-weight}\n\\end{equation}\nwhere $\\eta(\\cdot)$ is parametrized as a multi-layer perception to compute a soft alignment. \n$\\eta(\\cdot)$ enables the gradient of loss function to be backpropagated. There are six alternatives for the $\\eta(\\cdot)$ function (see Table 2 in ). The probability $\\alpha_{ti}$ reflects the importance of the hidden state of input sequence in presence of the previous hidden state $\\textbf{s}_{t-1}$ for deciding the next hidden state.\nIn Transformer decoder, on top of the two sub-layers in the encoder, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack $\\textbf{H}$. \nEfficient implementations of the transformer use the cached history matrix $\\textbf{S}_t$ to generate next token.\nTo compare with RNN-Seq2Seq, we summarize the Transformer decoder using recurrent notation:\n\\begin{equation}\n    \\textbf{S}_t = \\textsc{Transformer-Decoder}(\\textbf{S}_{t-1}, \\textbf{e}(y_{t-1}), \\textbf{H}), \n\\label{eq:transformer}\n\\end{equation}\nwhere $\\textbf{S}_t$ = $[(\\textbf{K}^{(1)}_t, \\textbf{V}^{(1)}_t), \\cdots, (\\textbf{K}^{(l)}_t, \\textbf{V}^{(l)}_t)]$, where $(\\textbf{K}^{(i)}_t, \\textbf{V}^{(i)}_t)$ corresponds to the key-value pairs from the $i$-th layer generated at all time-steps from $0$ to $t$.\nInstead of noting a specific name, we will use \\textsc{Encoder}($\\cdot$) and \\textsc{Decoder}($\\cdot$) to represent encoder and decoder in the following sections.\n\\begin{table*}[t]\n\\caption{NLG methods that incorporates knowledge attention ($\\S$\\ref{sec:k-attn}) and knowledge mode ($\\S$\\ref{sec:k-mode}).}\n\\vspace{-0.15in}\n\\centering\n\\scalebox{0.82}{\\begin{tabular}{|l|c|c|c|c|c|}\n\\hline\n& Topic & Keyword & Knowledge base & Knowledge graph & Grounded text \\\\\n\\hline \\hline\nKnowledge-related attention &  &  &  &  &  \\\\\nKnowledge-related mode &  &  &  &  &  \\\\\nKnowledge-related memory &  & - &   &  &  \\\\\n\\hline\n\\end{tabular}}\n\\vspace{-0.15in}\n\\label{tab:mul-source-attn}\n\\end{table*}\n\\vspace{-0.05in}", "cites": [9147, 3117, 3119, 3116, 3115, 168, 2002, 3118, 2369, 8554, 1934, 3120], "cite_extract_rate": 0.5217391304347826, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a basic description of attention mechanisms in RNN-Seq2Seq and Transformer decoder models but fails to synthesize the knowledge from the cited papers into a broader narrative. It lists equations and mentions some models, but there is minimal critical evaluation or comparison of approaches. The abstraction is limited, focusing mostly on technical details without drawing general principles or insights."}}
{"id": "19576d26-7a1c-47a7-9adb-7f2601cfef36", "title": "Knowledge-related attention", "level": "paragraph", "subsections": [], "parent_id": "7100a363-05ad-41f2-80ad-599b293e84f9", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Attention Mechanism"], ["paragraph", "Knowledge-related attention"]], "content": "Attention mechanism has been widely used to incorporate knowledge representation in recent knowledge-enhanced NLG work.\nThe general idea is to learn a knowledge-aware context vector (denoted as $\\widetilde{\\textbf{c}}_t$) by combining both hidden context vector ($\\textbf{c}_t$) and knowledge context vector (denoted as $\\textbf{c}^{K}_t$) into decoder update, such as $\\widetilde{\\textbf{c}}_t = f_{mlp}(\\textbf{c}_t \\oplus \\textbf{c}^{K}_t)$. \nThe knowledge context vector ($\\textbf{c}^{K}_t$) calculates attentions over knowledge representations (e.g., topic vectors, node vectors in knowledge graph).\nTable \\ref{tab:mul-source-attn} summarizes a variety of knowledge attentions, including\nkeyword attention~, topic attention~, knowledge base attention~, knowledge graph attention~, and grounded text attention~. \n\\vspace{-0.03in}", "cites": [2002, 3117, 1934, 2369, 3121], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of how attention mechanisms are used to integrate knowledge into NLG, citing multiple papers. While it introduces a general formula and lists different attention types, it lacks deeper synthesis of how these attention mechanisms differ or relate across the cited works. There is little critical analysis or abstraction to broader principles or trends in the field."}}
{"id": "439b4acd-a0f8-43cc-9128-c6a02aa6b48c", "title": "Copy and Pointing Mechanisms", "level": "subsubsection", "subsections": ["214fd441-539b-4d2c-9d7c-4f5552010290"], "parent_id": "4e1ad8d3-1399-4bd1-ae45-57b3679fd0ab", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Copy and Pointing Mechanisms"]], "content": "}\n\\label{sec:k-mode}\nCopyNet and Pointer-generator (PG) are used to choose subsequences in the input sequence and put them at proper places in the output sequence.\nCopyNet and PG have a differentiable network architecture~. They can be easily trained in an end-to-end manner. In CopyNet and PG, the probability of generating a target token is a combination of the probabilities of two modes, generate-mode and copy-mode. First, they represent unique tokens in the global vocabulary $\\mathcal{V}$ and the vocabulary of source sequence $\\mathcal{V}_X$. They build an extended vocabulary $\\mathcal{V}_{\\text{ext}} = \\mathcal{V} \\cup \\mathcal{V}_X \\cup \\{\\text{unk}\\}$. The difference between CopyNet and PG is the way to calculate distribution over the extended vocabulary. CopyNet calculates the distribution by\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    p(y_t)  =  p_{g}(y_t) +  p_{c}(y_t),\n\\label{eq:mode1}\n\\end{equation}\nwhere $p_{g}(\\cdot|\\cdot)$ and $p_{c}(\\cdot|\\cdot)$ stand for the probability of generate-mode and copy-mode.\nDifferently, PG explicitly calculates a switch probability $p_{m}$ between generate-mode and copy-mode. It recycles the attention distribution to serve as the copy distribution. The distribution over $\\mathcal{V}_{\\text{ext}}$ is calculated by\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    p(y_t) = p_m{(\\mathrm{g})} \\cdot p_g(y_t) + (1 - p_m{(\\mathrm{g})}) \\cdot p_c(y_t),\n\\end{equation}\nwhere $p_m(\\mathrm{g})$ indicates the probability of choosing generate-mode, which is obtained by a nonlinear multi-layered (MLP) function.\nImportantly, CopyNet and pointer-generator network have been used as the \\textit{base module} for a lot of knowledge-enhanced NLG work.\n\\vspace{-0.05in}", "cites": [1114], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of CopyNet and Pointer-generator mechanisms, their extended vocabulary, and how they calculate probabilities. It integrates information from the cited paper but does not go beyond to synthesize broader trends or critically evaluate the approaches. The abstraction is limited to the specific components of the models without generalizing to higher-level principles."}}
{"id": "214fd441-539b-4d2c-9d7c-4f5552010290", "title": "Knowledge-related mode", "level": "paragraph", "subsections": [], "parent_id": "439b4acd-a0f8-43cc-9128-c6a02aa6b48c", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Copy and Pointing Mechanisms"], ["paragraph", "Knowledge-related mode"]], "content": "A knowledge-related mode chooses subsequences in the obtained knowledge and puts them at proper places in the output sequence. It helps NLG models to generate words that are not included in the global vocabulary ($\\mathcal{V}$) and input sequence ($\\mathcal{V}_X$).\nFor example, by adding the model of knowledge base, the extended vocabulary ($\\mathcal{V}_{ext}$) adds entities and relations from the knowledge base, i.e., $\\mathcal{V}_{ext} = \\mathcal{V} + \\mathcal{V}_X + \\mathcal{V}_{KB}$. The probability of generating a target token is a combination of the probabilities of three modes: generate-mode, copy-mode and knowledge base-mode. Therefore, knowledge-related mode is not only capable of regular generation of words but also operation of producing appropriate subsequences in knowledge sources. Table \\ref{tab:mul-source-attn} summarizes different kinds of knowledge-related modes such as topic mode~, keyword mode~, knowledge base mode~, knowledge graph mode~, and background mode~.\n\\vspace{-0.03in}", "cites": [2002, 3117, 1934], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.7, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of a knowledge-related mode but merely summarizes its function without connecting the cited papers to form a deeper synthesis. It lacks critical evaluation of the approaches and only briefly mentions different modes without analyzing their strengths or weaknesses. The abstraction is minimal, offering no overarching framework or principles."}}
{"id": "a00f2dfb-145c-46fe-8e98-bb54b94cf9e9", "title": "Memory Network", "level": "subsubsection", "subsections": ["1b78bb95-fe98-499b-86a6-7a0ed257c4f3"], "parent_id": "4e1ad8d3-1399-4bd1-ae45-57b3679fd0ab", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Memory Network"]], "content": "}\nMemory networks (MemNNs) are recurrent attention models over a possibly large external memory~. They write external memories into several embedding matrices, and use query (generally speaking, the input sequence $X$) vectors to read memories repeatedly. This approach encodes long dialog history and memorize external information.\nGiven an input set $\\{m_1, \\cdots, m_i\\}$ to be stored in memory. The memories of MemNN are\nrepresented by a set of trainable embedding matrices $\\textbf{C} = \\{\\textbf{C}^1, \\cdots, \\textbf{C}^{K+1} \\}$, where each $\\textbf{C}^k$ maps tokens to vectors, and a query (i.e., input sequence) vector $\\textbf{h}_X^{k}$ is used as a reading head. The model loops over $K$ hops and it computes the attention weights at hop $k$ for each memory $m_i$ using:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\textbf{p}^k_i = \\mathrm{softmax}((\\textbf{h}^k_X)^\\top \\textbf{C}^k_i),   \n\\end{equation}\nwhere $\\textbf{C}^k_i = \\textbf{C}^k(m_i) $ is the memory content in $i$-th position, i.e., mapping $m_i$ into a memory vector. Here, $\\textbf{p}^k$ is a soft memory selector that decides the memory relevance with respect to the query vector $\\textbf{h}_X^{k}$. Then, the model reads out the memory $\\textbf{o}^k$ by the weighted sum over $\\textbf{C}^{k+1}$,\n\\begin{equation}\n\\textbf{o}^k = \\sum_i \\textbf{p}^k_i \\textbf{C}^{k+1}_i.\n\\end{equation}\nThen, the query vector is updated for the next hop by using $\\textbf{h}_X^{k+1} = \\textbf{h}_X^{k} + \\textbf{o}^k$. The result from the encoding step is the memory vector $\\textbf{o}^K$ and becomes the input for the decoding step.\n\\vspace{-0.03in}", "cites": [9109], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of memory networks, focusing on their structure and operation. It paraphrases the model from the cited paper, explaining how memory is represented and accessed, but does not synthesize this information with other approaches or provide critical analysis of its strengths and limitations. The explanation remains at a surface level with minimal abstraction or generalization."}}
{"id": "1b78bb95-fe98-499b-86a6-7a0ed257c4f3", "title": "Knowledge-related memory", "level": "paragraph", "subsections": [], "parent_id": "a00f2dfb-145c-46fe-8e98-bb54b94cf9e9", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Memory Network"], ["paragraph", "Knowledge-related memory"]], "content": "Memory augmented encoder-decoder framework has achieved promising progress for many NLG tasks. For example, MemNNs are widely used for encoding dialogue history in task-oriented dialogue systems~. Such frameworks enable a decoder to retrieve information from a memory during generation. Recent work explored to model external knowledge with memory network such as knowledge base~ and topic\n~.\n\\vspace{-0.03in}", "cites": [3116, 3122, 8554, 3115], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions memory-augmented encoder-decoder frameworks and cites several related papers, but it lacks meaningful synthesis or comparison between these works. It provides minimal critical analysis and does not abstract broader patterns or principles, focusing instead on a surface-level description of the concept of knowledge-related memory."}}
{"id": "4c9bc10b-8e87-4a1b-9b24-f82cd019c61e", "title": "Graph Network", "level": "subsubsection", "subsections": ["fc84e294-2c0a-45c6-a327-dd03985e96bf"], "parent_id": "4e1ad8d3-1399-4bd1-ae45-57b3679fd0ab", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Graph Network"]], "content": "}\n\\label{sec:graph-net}\nGraph network captures the dependence of graphs via message passing between the nodes of graphs.\nGraph neural networks (GNNs)~ and graph-to-sequence (Graph2Seq)~ potentiate to bridge up the gap between graph representation learning and text generation. \nKnowledge graph, dependency graph, and other graph structures can be integrated into text generation through various GNN algorithms. Here we denote a graph as $\\mathcal{G} = (\\mathcal{U}, \\mathcal{E})$, where $\\mathcal{U}$ is the set of entity nodes and $\\mathcal{E}$ is the set of (typed) edges.\nModern GNNs typically follow a neighborhood aggregation approach, which iteratively updates the representation of a node by aggregating information from its neighboring nodes and edges. After $k$ iterations of aggregation, a node representation captures the structural information within its $k$-hop neighborhood. Formally, the $k$-th layer of a node $u \\in \\mathcal{U}$ is:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\textbf{u}^{(k)} = \\textsc{Combine}_k (\\textbf{u}^{(k-1)},  \\textsc{Aggregate}_k(\\big{\\{} (\\textbf{u}^{(k-1)}_{i}, \\textbf{e}_{ij}^{(k-1)}, \\textbf{u}^{(k-1)}_{j}): \\forall (u_i, e_{ij}, u_j) \\in \\mathcal{N}(u)\\big{\\}})),\n\\end{equation}\nwhere $\\mathcal{N}(u)= \\{(u_i, e_{ij}, u_j) \\in \\mathcal{E} | u_i = u ~\\text{or}~ u_j = u\\}$ denotes the set of edges containing node $u$, $\\textbf{u}^{(k)}$ and $\\textbf{e}_{ij}^{(k)}$ are feature vectors of a node $u$ and the edge between $u_i$ and $u_j$ at the $k$-th iteration/layer. The choice of $\\textsc{Aggregate}(\\cdot)$ and $\\textsc{Combine}(\\cdot)$ in GNNs is crucial. A number of architectures for $\\textsc{Aggregate}(\\cdot)$ have been proposed in different GNN works such as GAT~. Meanwhile, the $\\textsc{Aggregate}(\\cdot)$ function used in labeled graphs (e.g., a knowledge graph) is often taken as those GNNs for modeling relational graphs~.\nTo obtain the representation of graph $\\mathcal{G}$ (denoted as $\\textbf{h}_{G}$), the $\\textsc{Readout}(\\cdot)$ function (either a simple permutation invariant function or sophisticated graph-level pooling function) pools node features from the final iteration $K$,\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n     \\textbf{h}_{G} =\n     \\textsc{Readout}(\\big{\\{}\\textbf{u}^{(K)}: u \\in \\mathcal{U}\\big{\\}}).\n\\end{equation}\n\\vspace{-0.05in}", "cites": [553, 259, 7011, 180], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of graph networks in knowledge-enhanced text generation, integrating definitions and formalisms from the cited papers. It mentions different GNN variants (e.g., GAT) and their roles in text generation but does not critically evaluate or compare them. Some abstraction is present in defining general GNN operations, but it remains primarily focused on explaining methods rather than offering deeper insights or identifying overarching trends."}}
{"id": "fc84e294-2c0a-45c6-a327-dd03985e96bf", "title": "Applications", "level": "paragraph", "subsections": [], "parent_id": "4c9bc10b-8e87-4a1b-9b24-f82cd019c61e", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Graph Network"], ["paragraph", "Applications"]], "content": "Graph network has been commonly used in integrating knowledge in graph structure such as knowledge graph and dependency graph. Graph attention network~ can be combined with sequence attention and jointly optimized~. \n{We will introduce different graph structure knowledge in subsequent sections such as knowledge graph (Section \\ref{sec:know-graph}), dependency graph (Section \\ref{sec:syn-graph}-\\ref{sec:sem-graph}), and open knowledge graph (OpenKG) (Section \\ref{sec:openkg}).}\n\\vspace{-0.03in}", "cites": [1934, 180], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, listing the use of graph networks in integrating knowledge and referencing two papers without connecting their contributions into a broader framework. There is no critical analysis or comparison of the methods, and the abstraction is limited to mentioning graph structures without generalizing underlying principles."}}
{"id": "b40e3380-9e7b-4d8e-af7a-5fcf7d31ad2a", "title": "Pre-trained Language Models", "level": "subsubsection", "subsections": [], "parent_id": "4e1ad8d3-1399-4bd1-ae45-57b3679fd0ab", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Model Architectures"], ["subsubsection", "Pre-trained Language Models"]], "content": "}\nPre-trained language models (PLMs) aims to learn universal language representation by conducting self-supervised training on large-scale unlabeled corpora.\nRecently, substantial PLMs such as BERT~ and T5~ have achieved remarkable performance in various NLP downstream tasks.\nHowever, these PLMs suffer from two issues when performing on knowledge-intensive tasks.\nFirst, these models struggle to grasp structured world knowledge, such as concepts and relations, which are very important in language understanding. For example, BERT cannot deliver great performance on many commonsense reasoning and QA tasks, in which many of the concepts are directly linked on commonsense knowledge graphs~.\nSecond, due to the domain discrepancy between pre-training and fine-tuning, these models do not perform well on domain-specific tasks. For example, BERT can not give full play to its value when dealing with electronic medical record analysis task in the medical field~.\nRecently, a lot of efforts have been made on investigating how to integrate knowledge into PLMs~. Specifically, we will introduce some PLMs designed for NLG tasks.\nOverall, these approaches can be grouped into two categories:\nThe first one is to explicitly inject entity representation into PLMs, where the representations is pre-computed from external sources~. For example, KG-BART encoded the graph structure of KGs with knowledge embedding algorithms like TransE~, and then took the informative entity embeddings as auxiliary input~.\nHowever, the method of explicitly injecting entity representation into PLMs has been argued that the embedding vectors of words in text and entities in KG are obtained in separate ways, making their vector-space inconsistent~.\nThe second one is to implicitly modeling knowledge information into PLMs by performing knowledge-related tasks, such as concept order recovering~, entity category prediction~.\nFor example, CALM proposed a novel contrastive objective for packing more commonsense knowledge into the parameters, and jointly pre-trained both generative and contrastive objectives for enhancing commonsense NLG tasks~.\n\\vspace{-0.05in}", "cites": [2482, 8623, 2487, 9, 3123, 3124, 3125, 7, 3126], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to present a structured analysis of how knowledge is integrated into pre-trained language models, distinguishing between explicit and implicit methods. It offers critical insights by discussing limitations, such as vector-space inconsistency and domain discrepancy, and highlights how different approaches aim to address these issues. The discussion abstracts beyond individual papers to identify broader methodological trends in knowledge enhancement for NLG."}}
{"id": "5c5bd456-c1bb-49e2-8001-2c1c0983cea3", "title": "Knowledge-enhanced Learning and Inference", "level": "subsection", "subsections": ["bad0692a-ea6d-42f6-ab72-0e4604a3f3b4", "7cc37322-8500-4abd-b9a0-cb0de0e042b5", "175121d4-807a-498b-9b55-642d5bd5480e"], "parent_id": "e083ab82-0002-443e-8734-d6e90fa797b2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Learning and Inference"]], "content": "Besides specialized model architectures, one common way of injecting knowledge to generation models is through the supervised knowledge learning. For example, one can encode knowledge into the objective function that guides the model training to acquire desired model behaviors~. Such approaches enjoy the flexibility of integrating diverse types of knowledge by expressing them as certain forms of objectives. In general, knowledge-enhanced learning is agnostic to the model architecture, and can be combined with the aforementioned architectures.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=1.0\\textwidth]{figures/knowledge-as-target.pdf}\n  \\end{center}\n  \\vspace{-0.1in}\n  \\caption{Incorporating knowledge into text generation by treating knowledge as the target. The first category of methods (left) combine knowledge-related tasks as auxiliary into the text generation task, resulting in a \\textit{multi-task learning} setting. The second category of methods (right) create \\textit{weakly-supervised} labels from knowledge, enforcing the relevancy between the knowledge and the target sequence.}\n  \\label{fig:knowledge-related-task}\n\\vspace{-0.15in}\n\\end{figure}\n\\vspace{-0.05in}", "cites": [457, 3118], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of knowledge-enhanced learning and inference in a general way, referencing two papers but without deeply connecting their ideas or contrasting their approaches. It describes a methodological category (supervised knowledge learning) and mentions how knowledge can be encoded into objectives, but lacks critical evaluation or a higher-level abstraction of principles. The inclusion of a figure and the reference to multi-task learning and weakly-supervised labels suggest some structure, but the synthesis remains superficial."}}
{"id": "264ece9d-16ec-4456-8245-928826233148", "title": "Knowledge as target", "level": "paragraph", "subsections": [], "parent_id": "bad0692a-ea6d-42f6-ab72-0e4604a3f3b4", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Learning and Inference"], ["subsubsection", "Learning with knowledge-related tasks"], ["paragraph", "Knowledge as target"]], "content": "}\nThe methods can be mainly divided into two categories as shown in Figure \\ref{fig:knowledge-related-task}.\nThe first category of knowledge-related tasks creates learning targets based on the knowledge, and the model is trained to recover the targets. These tasks can be combined as auxiliary tasks with the text generation task, resulting in a \\emph{multi-task learning} setting.\nFor example, knowledge loss is defined as the cross entropy between the predicted and true knowledge sentences, and it is combined with the standard conversation generation loss to enhance grounded conversation~. Similar tasks include keyword extraction loss~, template re-ranking loss~, \nlink prediction loss on knowledge graph~,\npath reasoning loss~,\nmode loss~, bag-of-word (BOW) loss~, etc.\nThe second category of methods directly derive the text generation targets from the knowledge, and use those (typically noisy) targets as supervisions in the standard text generation task. The approach is called \\textit{weakly-supervised} learning. Weakly-supervised learning enforces the relevancy between the knowledge and the target sequence.\nFor example, in the problem of aspect based summarization, the work  automatically creates target summaries based on external knowledge bases, which are used to train the summarization model in a supervised manner.\n\\vspace{-0.05in}", "cites": [457, 3129, 3128, 3118, 3130, 3127, 3120], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by categorizing methods into two coherent groupsâ€”those using knowledge as part of the learning targets and those using weakly-supervised learning. It abstracts these methods to highlight broader learning paradigms like multi-task and weak supervision. However, it lacks deeper critical analysis of the limitations or trade-offs of the approaches, focusing more on description than evaluation."}}
{"id": "a6831b8e-400c-426b-9942-61e91c6cce17", "title": "Knowledge as condition", "level": "paragraph", "subsections": [], "parent_id": "bad0692a-ea6d-42f6-ab72-0e4604a3f3b4", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Learning and Inference"], ["subsubsection", "Learning with knowledge-related tasks"], ["paragraph", "Knowledge as condition"]], "content": "}\nThe second way of devising knowledge-related tasks is to augment the text generation task by conditioning the generation on the knowledge. That is, the goal is to learn a function $p_{\\theta}(Y|X, K)$, where $X$ is the input sequence, $Y$ is the target text and $K$ is the knowledge. Generally, the knowledge $K$ is first given externally (e.g., style, emotion) or retrieved from external resources (e.g., facts from knowledge base, a document from Wikipedia) or extracted from the given input text (e.g., keywords, topic words). Second, a conditional text generation model is used to incorporate knowledge and generate target output sequence. In practice, knowledge is often remedied by soft enforcing algorithms such as attention mechanism~ and copy/pointing mechanism~.\nRegarding knowledge as condition is widely used in knowledge-enhanced text generation. For examples, work has been done in making personalized dialogue response by taking account of persona~ and emotion~, controlling various aspects\nof the response such as politeness~, grounding the responses in external source of knowledge~ and generating topic-coherent sequence~.\nBesides, using variational autoencoder (VAE) to enforce the generation process conditioned on knowledge is one popular approach to unsupervised NLG.\nBy manipulating latent space for certain attributes, such as topic~ and style~, the output sequence can be generated with desired attributes without supervising with parallel data.\n\\vspace{-0.05in}", "cites": [1114, 3116, 457, 1996, 2005, 168, 650, 7186, 3127, 1887, 3131], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by framing them under the concept of knowledge as a conditional input in text generation. It abstracts common practices like attention and copy mechanisms, and connects different works (e.g., emotion, politeness, persona-based dialogue) to illustrate the broad applicability of the approach. However, it offers limited critical analysis of the cited works, such as their effectiveness or limitations, and focuses more on summarizing methods than evaluating them."}}
{"id": "7cc37322-8500-4abd-b9a0-cb0de0e042b5", "title": "Learning with knowledge constraints", "level": "subsubsection", "subsections": [], "parent_id": "5c5bd456-c1bb-49e2-8001-2c1c0983cea3", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Learning and Inference"], ["subsubsection", "Learning with knowledge constraints"]], "content": "}\nInstead of creating training objectives in standalone tasks that encapsulate knowledge, another paradigm of knowledge-enhanced learning is to treat the knowledge as the \\emph{constraints} to regularize the text generation training objective. \nThe posterior regularization (PR) framework was proposed to restrict the space of the model posterior on unlabeled data as a way to guide the model towards desired behavior~. PR has been used as a principled framework to impose knowledge constraints on probabilistic models (including deep networks) in general~.\nPR augments any regular training objective $\\mathcal{L}(\\theta)$ (e.g., negative log-likelihood, as in Eq.\\eqref{eq:Seq2Seq-loss}) with a constraint term to encode relevant knowledge. Formally, \ndenote the constraint function as $f(X,Y) \\in \\mathbb{R}$ such that a higher $f(X,Y)$ value indicates a better generated sequence $Y$ that incorporates the knowledge. \nPR introduces an auxiliary distribution $q(Y|X)$, and imposes the constraint on $q$ by encouraging a large expected $f(X, Y)$ value: $\\mathbb{E}q [f(X,Y)]$. Meanwhile, the model $p_{\\theta}$ is encouraged to stay close to $q$ through a KL divergence term. The learning problem is thus a constrained optimization:\n\\begin{align}\n    \\max_{\\theta, q} &\\mathcal{L}(\\theta) - \\mathrm{KL}(q(Y|X)||p_\\theta(Y|X)) + \\xi \\\\\n    &s.t.~~ \\mathbb{E}q [f(X,Y)] > \\xi,\n\\end{align}\nwhere $\\xi$ is the slack variable. The PR framework is also related to other constraint-driven learning methods~. We refer readers to  for more discussions.\n\\vspace{-0.05in}", "cites": [3132, 3134, 3133], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear synthesis of the PR framework by integrating insights from three different papers, showing how it can be used to incorporate knowledge as constraints in text generation. It abstracts the method into a general formulation and highlights its broader applicability. While it identifies some limitations (e.g., need for constraints to be fully specified a priori), the critical analysis could be more in-depth for a higher score."}}
{"id": "175121d4-807a-498b-9b55-642d5bd5480e", "title": "Inference with knowledge constraints", "level": "subsubsection", "subsections": [], "parent_id": "5c5bd456-c1bb-49e2-8001-2c1c0983cea3", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "General Methods of Integrating Knowledge into NLG"], ["subsection", "Knowledge-enhanced Learning and Inference"], ["subsubsection", "Inference with knowledge constraints"]], "content": "}\nPre-trained language models leverage large amounts of unannotated data with a simple log-likelihood training objective. Controlling language generation by particular knowledge in a pre-trained model is difficult if we do not modify the model architecture to allow for external input knowledge or fine-tuning with specific data~. Plug and play language model (PPLM) opened up a new way to control language generation with particular knowledge during inference. At every generation step during inference, the PPLM shifts the history matrix in the direction of the sum of two gradients: one toward higher log-likelihood of the attribute\n$a$ under the conditional attribute model $p(a|Y)$ and the other toward higher log-likelihood of the unmodified pre-trained generation model $p(Y|X)$ (e.g., GPT). Specifically, the attribute model $p(a|Y)$ makes gradient based updates to $\\Delta\\textbf{S}_t$ as follows:\n\\begin{equation}\n\\Delta \\textbf{S}_t \\leftarrow \\Delta \\textbf{S}_t +  \\frac{\\nabla_{\\Delta \\textbf{S}_t} \\log p(a|\\textbf{S}_t + \\Delta\\textbf{S}_t)}{||\\nabla_{\\Delta \\textbf{S}_t} \\log p(a|\\textbf{S}_t + \\Delta\\textbf{S}_t)||^{\\gamma} },\n\\end{equation}\nwhere $\\gamma$ is the scaling coefficient for the normalization term; $\\Delta \\textbf{S}_t$ is update of history matrix $\\textbf{S}_t$ (see Eq.(\\ref{eq:transformer})) and initialized as zero. The update step is repeated multiple times. Subsequently, a forward pass through the generation model is performed to obtain the updated $\\widetilde{\\textbf{S}}_{t+1}$ as $\\widetilde{\\textbf{S}}_{t+1} = \\textsc{Decoder}((\\textbf{S}_t + \\Delta \\textbf{S}_t), \\textbf{e}(y_t), \\textbf{H})$. The perturbed $\\widetilde{\\textbf{S}}_{t+1}$ is then used to generate a new logit vector. PPLMs is efficient and flexible to combine differentiable attribute models to steer text generation~.\n\\vspace{-0.05in}", "cites": [3135], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of the PPLM method and its mechanism for inference with knowledge constraints, including the mathematical formulation. However, it is primarily descriptive, with minimal synthesis of broader research trends or integration with other works. There is no critical evaluation or comparison of PPLM with alternative methods, nor are there higher-level abstractions or conceptual generalizations."}}
{"id": "01a02363-5419-45c4-a559-242074181a80", "title": "NLG Enhanced by Topic", "level": "subsection", "subsections": ["bfcfe80d-d9d0-4072-af71-cf7604b89bc0", "23b7d35e-1fe1-4a67-9fdd-70137fbbcf04", "d6d92af5-2a1d-4ddd-b9a7-c44d12867c48", "94c4ec14-3c4f-4412-a14a-b15327803110"], "parent_id": "b426fa47-f85f-4367-a1dc-a6d9c866a437", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Topic"]], "content": "Topic, which can be considered as a representative or compressed form of text, has been often used to maintain the semantic coherence and guide the NLG process.\nTopic modeling is a powerful tool for finding the high-level content of a document collection in the form of latent topics~.\nA classical topic model, Latent Dirichlet allocation (LDA), has been widely used for inferring a low dimensional representation that captures latent semantics of words and documents~. In LDA, each topic is defined as a distribution over words and each document as a mixture distribution over topics. LDA generates words in the documents from topic distribution of document and word distribution of topic.\nRecent advances of neural techniques open a new way of learning low dimensional representations of words from the tasks of word prediction and context prediction, making neural topic models become a popular choice of finding latent topics from text~.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=0.95\\textwidth]{figures/topic-enhanced-three-methods.pdf}\n  \\end{center}\n  \\vspace{-0.15in}\n  \\caption{Three typical methodologies for incorporating topics into NLG. Detailed designs are not included.}\n  \\label{fig:topic}\n  \\vspace{-0.05in}\n\\end{figure}\nNext, we introduce popular NLG applications enhanced by topics:\n\\begin{itemize}\n    \\item \\textbf{Dialogue system.} A vanilla Seq2Seq often  generates trivial or non-committal sentences of frequent words or phrases in the corpus~. For example,\n    a chatbot may say \\textit{``I do not know''}, \\textit{``I see''} too often. Though these off-topic responses are safe to reply to many queries, they are boring with very little information. Such responses may quickly lead the conversation to an end, severely hurting user experience. Thus, on-topic response generation is highly needed.\n    \\item \\textbf{Machine translation.} Though the input and output languages are different (e.g., translating English to Chinese), the contents are the same, and globally, under the same topic. Therefore, topic can serve as an auxiliary guidance to preserve the semantics information of input text in one language into the output text in the other language. \n    \\item \\textbf{Paraphrase.} Topic information helps understand the potential meaning and determine the semantic range to a certain extent. Naturally, paraphrases concern the same topic, which can serve as an auxiliary guidance to promote the preservation of source semantic.\n\\end{itemize}\nAs shown in Figure \\ref{fig:topic}, we summarize topic-enhanced NLG methods into three methodologies: (M1) leverage topic words from generative topic models; (M2) jointly optimize generation model and CNN topic model; (M3) enhance NLG by neural topic models with variational inference.\n\\vspace{-0.05in}", "cites": [2002, 3136], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the role of topics in NLG and introduces two cited papers, but it does so in a largely descriptive manner without deeper connections or comparison between the works. It mentions the use of topic models and neural approaches but lacks critical evaluation or identification of broader trends. However, it does offer a generalized framework (M1-M3) that shows some level of abstraction."}}
{"id": "bfcfe80d-d9d0-4072-af71-cf7604b89bc0", "title": "M1: Leverage Topic Words from Generative Topic Models", "level": "subsubsection", "subsections": [], "parent_id": "01a02363-5419-45c4-a559-242074181a80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Topic"], ["subsubsection", "M1: Leverage Topic Words from Generative Topic Models"]], "content": "}\n\\label{sec:topic-m1}\nTopics help understand the semantic meaning of sentences and determine the semantic spectrum to a certain extent. \nTo enhanced text generation, an effective solution is to first discover topics using generative topic models (e.g., LDA), and then incorporate the topics representations into neural generation models, as illustrated in Figure \\ref{fig:topic}(a). In existing work, there are two mainstream methods to represent topics obtained from generative topic models. The first way is to use the generated topic distributions for each word (i.e., word distributions over topics) in the input sequence~. The second way is to assign a specific topic to the input sequence, then picks the top-$k$ words with the highest probabilities under the topic, and use word embeddings (e.g., GloVe) to represent topic words~. Explicitly making use of topic words can bring stronger guidance than topic distributions, but the guidance may deviate from the target output sequence when some generated topic words are irrelevant.\nZhang et al. proposed the first work of using a topic-informed Seq2Seq model by concatenating the topic distributions with encoder and decoder hidden states~.\nXing et al. designed a topic-aware Seq2Seq model in order to use topic words as prior knowledge to help dialogue generation~. \n\\begin{table*}[t]\n\\centering\n\\caption{Natural language generation methods that incorporate topic knowledge in text generation. Since most of the methods are tested on different tasks and datasets, we only compare the performance between ``w/o topic'' setting and ``with topic'' setting. For evaluation metrics, PPL is short for perplexity (lower is better); B-4 is short for BLEU-4 (higher is better); R-L is short for ROUGE-L (higher is better).}\n\\label{tab:topicinseq2seq}\n\\vspace{-0.1in}\n{\\scalebox{0.82}{\\begin{tabular}{|l|l|c|c|l|l|l|l|l|}\n\\hline\n\\multirow{2}*{Task} & \\multirow{2}*{Method} & \\multirow{2}*{Ref.} & \\multirow{2}*{Cat.} & \\multicolumn{2}{c|}{Framework components} & \\multicolumn{3}{c|}{Effect of topic modeling} \\\\\n\\cline{5-9}\n& & & & Seq. Enc/Dec & Topic model & Dataset & w/o topic & with topic \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Dialogue \\\\ system}} & Tp-S2S &  & M1 & RNN Seq2Seq & LDA topics & Baidu Tieba & (PPL) 147.0 & (PPL) 134.6\\\\\n& PEE &  & M3 & RNN Seq2Seq & Neural topics & PersonaChat& (B-4) 2.98 & (B-4) 3.56 \\\\ \n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Machine \\\\ translation}} & Tp-NMT &  & M1 & RNN Seq2Seq & LDA topics & NIST & (B-4) 34.76 & (B-4) 35.91 \\\\\n& BLT-NMT &  & M2 & RNN Seq2Seq & CNN topics & NIST & (B-4) 38.97 & (B-4) 40.10 \\\\\n\\hline \\hline\n\\multirow{3}*{\\makecell[l]{Summari\\\\ -zation}} & Tp-CS2S &  & M1 & CNN Seq2Seq & LDA topics & XSum & (R-L) 25.23 & (R-L) 25.75 \\\\\n& TGVAE &  & M3 & RNN with VAE & Neural topics & Gigawords & (R-L) 32.13 & (R-L) 33.02 \\\\\n& VHTM &  & M3 & RNN with VAE & Neural topics & CNN/DM & (R-L) 36.73 & (R-L) 37.18 \\\\\n\\hline \\hline\n\\multirow{2}*{Paraphrase} & TGLM &  & M2 & RNN Seq2Seq & CNNs topics & Yahoo! Ans & (PPL) 99.13 & (PPL) 88.69 \\\\\n& PTA &  & M1 & RNN Seq2Seq & LDA topics & Quora & (B-4) 28.76 & (B-4) 31.75 \\\\\n\\hline\n\\end{tabular}}}\n\\vspace{-0.1in}\n\\end{table*}\n\\vspace{-0.05in}", "cites": [2002, 3127], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by categorizing methods under M1 and comparing how topic knowledge is incorporated into different text generation tasks. However, the integration is limited, and it does not offer a novel framework or deep analytical connections. Critical analysis is minimal, with no significant evaluation of the methods' limitations or trade-offs. Abstraction is present in the categorization of topic representation approaches but does not reach a meta-level understanding."}}
{"id": "d6d92af5-2a1d-4ddd-b9a7-c44d12867c48", "title": "M3: Enhance NLG by Neural Topic Models with Variational Inference", "level": "subsubsection", "subsections": [], "parent_id": "01a02363-5419-45c4-a559-242074181a80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Topic"], ["subsubsection", "M3: Enhance NLG by Neural Topic Models with Variational Inference"]], "content": "}\n\\label{sec:topic-m3}\nNeural topic models can be trained efficiently by backpropagation~. In neural topic models, Dirichlet distributions can be employed as the prior to generate the parameters of the multinomial distribution $\\theta_d$ for each document~. The generative process of LDA is represented as: (1) $\\theta_d \\sim \\mathrm{Dirichlet} (\\alpha)$; (2) $t_i \\sim \\mathrm{Multinomial} (\\theta_d)$; (3) $w_i \\sim \\mathrm{Multinomial} (\\beta_{t_i})$,\nwhere $d$ denotes the bag-of-words representation of a document, $t_i$ represents the topic assignment for word $w_i$, and $\\beta_{t_i}$ represents the topic distribution over words given topic assignment $t_i$. \nHowever, a directed generative model comes up against the problem of establishing low variance gradient estimators. Miao et al. parameterized the multinomial distributions with neural networks and jointly learned the model parameters via variational inference~. They created neural structures for constructing topic distributions conditioned on a draw from a multivariate Gaussian distribution, represented as \n$\\theta_d\\sim\\mathrm{G} (\\mu_0, \\sigma_0^2)$,\nwhere $\\mathrm{G} (\\mu_0, \\sigma_0^2)$ is composed of a neural network conditioned on an isotropic Gaussian $\\mathrm{N} (\\mu_0, \\sigma_0^2)$. \nTaking a Gaussian prior distribution makes re-parameterization feasible to build an unbiased and low-variance gradient estimator for the variational distribution~. Without conjugacy prior, the updates of the parameters are derived directly and easily from the variational lower bound. Formally, a variational lower bound\nfor the document log-likelihood is:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\mathcal{J}_{topic} = \\mathbb{E}_{q(\\theta|d)} [\\log p(d|\\beta, \\theta)] - \\mathrm{KL} (q(\\theta|d)||p(\\theta |\\mu_0, \\sigma^2_0)),\n\\end{equation}\nwhere $q(\\theta|d)$ is the variational distribution approximating the true posterior $p(\\theta|d)$. Its lower bound is estimate by sampling $\\theta$ from $q(\\theta|d) = \\mathrm{G}(\\theta|\\mu(d), \\sigma^2(d))$. \nIn order to combine neural topic model and neural generation model, the idea is to use the Variational Auto-Encoder (VAE)~. It adopts autoregressive networks (e.g., LSTM) both as the encoder and decoder. VAE can learn latent codes $z$ of texts by reconstructing texts with its decoder. It assumes that the generation process is controlled by codes in a continuous latent space. This kind of VAE implementation considers sequential information of texts that can model the linguistic structure of texts. Wang et al. proposed topic guided variational autoencoder (TGVAE), to draw latent code $z$ from a topic-dependent Gaussian Mixture Prior in order to incorporate the topical knowledge into latent variables~. The topic-dependent Gaussian Mixture Model (GMM) is defined as:\n$p(z|\\beta, t) = \\sum^{T}_{i=1} t_i \\mathrm{N} (\\mu(\\beta_i), \\sigma^2(\\beta_i))$,\nwhere $T$ is the number of topics, $\\mu(d)$ and $\\sigma^2(d)$ are functions implemented by MLP. TGVAE uses bag-of-words as input and embeds an input document into a topic vector. The topic vector is then used to reconstruct the bag-of-words input, and the learned topic distribution over words is used to model a topic-dependent prior to generate an output sequence $Y$ from conditioned on an input sequence $X$. \nTherefore, to maximize the log-likelihood log $p(Y,d|X)$, a variational objective function is constructed as:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\mathcal{J}_{seq2seq} = \\mathbb{E}_{q(z|X)} [\\log p(Y|X, z)] - \\mathbb{E}_{q(\\theta|d)} [\\mathrm{KL} (q(z|X)||p(z|\\beta, \\theta))],\n\\end{equation}\nwhere $q(z|X)$ is variational distributions for $z$. The combined object function is given by:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\mathcal{J} = \\mathcal{J}_{topic} +  \\mathcal{J}_{seq2seq}.\n\\end{equation}", "cites": [3137, 5680], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the two cited papers by integrating neural topic modeling with variational autoencoders (VAEs) to enhance text generation. It provides a coherent explanation of the M3 framework and its variational objective function, showing some abstraction by framing the broader approach of topic-guided generation. However, it lacks deeper critical analysis or evaluation of the strengths and weaknesses of the methods, remaining more focused on explanation than critique."}}
{"id": "94c4ec14-3c4f-4412-a14a-b15327803110", "title": "Discussion and Analysis of Different Methods", "level": "subsubsection", "subsections": [], "parent_id": "01a02363-5419-45c4-a559-242074181a80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Topic"], ["subsubsection", "Discussion and Analysis of Different Methods"]], "content": "} \n\\vspace{-0.05in}\nFor \\textbf{M1}, topic models (e.g., LDA) has a strict probabilistic explanation since the semantic representations of both words and documents are combined into a unified framework. Besides, topic models can be easily used and integrated into generation frameworks. For example, topic words can be represented as word embeddings; topic embeddings can be integrated into the decoding phase through topic attention. However,\nLDA models are separated from the training process of generation, so they cannot adapt to the diversity of dependencies between input and output sequences.\nFor \\textbf{M2}, it is an end-to-end neural framework that simultaneously learns latent topic representations and generates output sequences. Convolutional neural networks (CNN) are often used to generate the latent topics through iterative convolution and pooling operations. However, theoretical analysis is missing to ensure the quality of the topics captured by the convolutions. And their interpretability is not as good as the LDA-based topic models.\nFor \\textbf{M3}, neural topic models combine the advantages of neural networks and probabilistic topic models. They enable back propagation for joint optimization, contributing to more coherent topics, and can be scaled to large data sets. Generally, neural topic models can provide better topic coherence than LDAs~. However, neural variational approaches share a same drawback that topic distribution is assumed to be an isotropic Gaussian, which makes them incapable of modeling topic correlations. Existing neural topic models assume that the documents should be i.i.d. to adopt VAE, while they are commonly correlated. The correlations are critical for topic modeling.\n\\vspace{-0.05in}", "cites": [3127], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section offers a clear analytical comparison between three different methods of incorporating topic-based internal knowledge into NLG. It synthesizes information by discussing their integration strategies, limitations, and trade-offs. It critically evaluates each method, pointing out theoretical and practical shortcomings such as lack of adaptability, interpretability, and modeling of topic correlations. The abstraction is moderate, as it identifies general patterns and principles but does not reach a higher meta-level synthesis."}}
{"id": "9faa22fc-e3f2-4757-83e5-d722d65d47c3", "title": "NLG Enhanced by Keywords", "level": "subsection", "subsections": ["3790b927-52af-44a2-84c6-8f025ac055eb", "d070fae5-6e22-45a4-95da-ee1547936825", "fcd1fe7a-8ff8-4e23-b63c-8248ac289f2a"], "parent_id": "b426fa47-f85f-4367-a1dc-a6d9c866a437", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Keywords"]], "content": "Keyword (aka., key phrase, key term) is often referred as a sequence of one or more words, providing a compact representation of the content of a document. The mainstream methods of keyword acquisition for documents can be divided into two categories~: keyword assignment and keyword extraction. Keyword assignment means that keywords are chosen from a controlled vocabulary of terms or predefined taxonomy.\nKeyword extraction selects the most representative words explicitly presented in the document, which is independent from any vocabulary. Keyword extraction techniques (e.g., TF-IDF, TextRank, PMI) have been widely used over decades. Many NLG tasks can benefit from incorporating such a condensed form of essential content in a document to maintain the semantic coherence and guide the generation process.\nNext, we introduce popular NLG applications enhanced by keywords:\n\\begin{itemize}\n    \\item \\textbf{Dialogue system.} Keywords help enlighten and drive the generated responses to be informative and avoid generating universally relevant replies which carry little semantics. Besides, \n    recent work introduced personalized information into the generation of dialogue to help deliver better dialogue response such as emotion ~, and persona~.\n    \\item \\textbf{Summarization.} \n    Vanilla Seq2Seq models often suffer when the generation process is hard to control and often misses salient information~. Making use of keywords as explicit guidance can provide significant clues of the main points about the document~. It is closer to the way that humans write summaries: make sentences to contain the keywords, and then perform necessary modifications to ensure the fluency and grammatically correctness.\n    \\item \\textbf{Question generation.} It aims to generate questions from a given answer and its relevant context. Given an answer and its associated context, it is possible to raise multiple questions with different focuses on the context and various means of expression.\n\\end{itemize}\n\\begin{table}[t]\n\\caption{Natural language generation methods that incorporate keyword in text generation.}\n\\vspace{-0.1in}\n\\label{tab:keyword}\n\\begin{subtable}[t]{0.98\\textwidth}\n\\centering\n\\caption{(M1) Descriptions and quantitative comparisons between three methods for emotional dialogue systems.}\n\\label{tab:emotion}\n\\vspace{-0.05in}\n{\\scalebox{0.82}{\\begin{tabular}{|l|l|c|l|c|c|c|}\n\\hline\n\\multirow{2}*{Task} & \\multirow{2}*{Method} & \\multirow{2}*{Ref.} & \\multirow{2}*{Assignment method} & \\multicolumn{3}{c|}{Experiments on NLPCC dataset} \\\\\n\\cline{5-7}\n& & & & BLEU & D-1/D-2 & Emotion w/s \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Dialogue \\\\ system}} & Seq2Seq &  & Seq2Seq attention \\textit{without} using keywords & 1.50 & 0.38/1.20 & 33.5/37.1 \\\\\n& E-SCBA &  & MLP classifier to 7 emotions (categories) & 1.69 & 0.54/4.84 & 72.0/51.2 \\\\\n& EmoChat &  & E-SCBA + two memory modules for decoding & 1.68 & 0.90/7.35 & 76.5/58.0 \\\\ \n& EmoDS &  & MLP classifier after decoding (discriminator) & 1.73 & 1.13/8.67 & 81.0/68.7 \\\\\n\\hline\n\\end{tabular}}}\n\\end{subtable}\n\\begin{subtable}[t]{0.98\\textwidth}\n\\centering\n\\vspace{0.1in}\n\\caption{(M2) As most methods are tested on different tasks and datasets, we only compare the performance between ``w/o keyword'' setting and ``with keyword'' setting. Besides, HM is short for human evaluation.}\n\\label{tab:hm}\n\\vspace{-0.05in}\n{\\scalebox{0.82}{\\begin{tabular}{|l|l|c|l|l|l|l|l|}\n\\hline\n\\multirow{2}*{Task} & \\multirow{2}*{Method} & \\multirow{2}*{Ref.} & Extraction & Keyword & \\multicolumn{3}{c|}{Effect of keyword} \\\\\n\\cline{6-8}\n& & & method & labels & Dataset & w/o keyword & with keyword \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Summari- \\\\ zation}} & \\multirow{2}*{\\makecell[l]{KIGN}} & \\multirow{2}*{\\makecell[l]{}} & \\multirow{2}*{\\makecell[l]{TextRank}} & \\multirow{2}*{\\makecell[l]{Unsupervised}} & CNN/DM & (R-2) 15.66 & (R-2) 17.12 \\\\ \n& & & & & Gigaword & (R-2) 23.61 & (R-2) 23.93 \\\\ \n\\cline{2-8} \n& ComGen &  & PMI and TFIDF & Unsupervised & Tencent & (HM) 5.77 & (HM) 7.19 \\\\ \n& KGAS &  & BiLSTM-Softmax & $\\text{w}(X) \\cap \\text{w}(Y)$ & Gigaword & (R-2) 23.61 & (R-2) 25.06 \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Question \\\\ generation}} & Selector &  & BiLSTM-Softmax & $\\text{w}(X) \\cap \\text{w}(Y)$ & SQuAD & (B-4) 14.72 & (B-4) 15.87 \\\\\n& Prior &  & BiLSTM-Softmax & $\\text{w}(X) \\cap \\text{w}(Y)$ & SQuAD & (B-4) 14.72 & (B-4) 15.34 \\\\\n\\hline\n\\end{tabular}}}\n\\end{subtable}\n\\vspace{-0.05in}\n\\end{table}\nResearchers have developed a great line of keyword-enhanced NLG methods. These methods can be categorized into two methodologies: (M1) Incorporate keyword assignment into text generation; (M2) Incorporate keyword extraction into text generation.\n\\vspace{-0.05in}", "cites": [3116, 168, 650, 3139, 3138], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section organizes methods into two categories (assignment and extraction) and compares their impact using tables with metrics. However, the synthesis is limited to grouping methods rather than connecting deeper ideas across sources. Critical analysis is minimal, as the section mostly reports results without discussing limitations or trade-offs. Some abstraction is present in the categorization of methods, but broader principles or trends in keyword usage for NLG are not fully generalized."}}
{"id": "3790b927-52af-44a2-84c6-8f025ac055eb", "title": "M1: Incorporate Keyword Assignment into Text Generation", "level": "subsubsection", "subsections": ["b64176b7-636a-410d-8fca-fd044a39d56d", "54e5e57c-d49c-4b0e-9f92-b34c3aa4508e"], "parent_id": "9faa22fc-e3f2-4757-83e5-d722d65d47c3", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Keywords"], ["subsubsection", "M1: Incorporate Keyword Assignment into Text Generation"]], "content": "}\n\\label{sec:keywork-m1}\nWhen assigning a keyword to an input document, the set of possible keywords is bounded by a pre-defined vocabulary~. \nThe keyword assignment is typically implemented by a classifier that maps the input document to a word in the pre-defined vocabulary~. \nUnfortunately, some NLG scenarios do not hold an appropriate pre-defined vocabulary, so keyword assignment cannot be widely used to enhance NLG tasks.\nOne applicable scenario is to use a pre-determined domain specific vocabulary to maintain relevance between the input and the output sequence~. Another scenario is to generate dialogue with specific attributes such as persona~, emotion~. \n\\vspace{-0.05in}", "cites": [3116, 3140, 3127, 3141], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the keyword assignment approach in knowledge-enhanced text generation and mentions scenarios where it is applicable. However, it does not synthesize the cited papers into a broader narrative, lacks critical analysis of their methods or limitations, and offers only minimal abstraction beyond the specific systems."}}
{"id": "b64176b7-636a-410d-8fca-fd044a39d56d", "title": "M1.1: Adding assigned keyword into the decoder", "level": "paragraph", "subsections": [], "parent_id": "3790b927-52af-44a2-84c6-8f025ac055eb", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Keywords"], ["subsubsection", "M1: Incorporate Keyword Assignment into Text Generation"], ["paragraph", "M1.1: Adding assigned keyword into the decoder"]], "content": "}\nA straightforward method of keyword assignment is to assign the words from pre-defined vocabulary and use them as the keywords~.\nSometimes, the input sequence does not have an explicit keyword, but we can find one from the pre-defined vocabulary. For example, a dialogue utterance \\emph{``If you had stopped him that day, things would have been different.''} expresses sadness but it does not have the word ``sad.''\nTo address this issue, Li et al. propose a method to predict an emotion category by fitting the sum of hidden states from encoder into a classifier~.\nThen, the response will be generated with the guidance of the emotion category. \nIn order to dynamically track how much the emotion is expressed in the generated sequence, Zhou et al. propose a memory module to capture the emotion dynamics during decoding~.\nEach category is initialized with an emotion state vector before the decoding phase starts. At each step, the emotion state decays by a certain amount. Once the decoding process is completed, the emotion state decays to zero, indicating that the emotion is completely expressed. \n\\vspace{-0.05in}", "cites": [3116, 3127, 3141], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methods that incorporate keywords into the decoder for emotion-guided text generation. It mentions two papers (Li et al. and Zhou et al.) and their approaches but does not meaningfully synthesize or integrate the ideas across the cited works. There is limited critical evaluation or comparison of the approaches, and no abstraction to broader principles or patterns in the field."}}
{"id": "d070fae5-6e22-45a4-95da-ee1547936825", "title": "M2: Incorporate Keyword Extraction into Text Generation", "level": "subsubsection", "subsections": [], "parent_id": "9faa22fc-e3f2-4757-83e5-d722d65d47c3", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Keywords"], ["subsubsection", "M2: Incorporate Keyword Extraction into Text Generation"]], "content": "}\n\\label{sec:keywork-m2}\nKeyword extraction selects salient words from input documents~.\nRecent work has used statistical keyword extraction techniques (e.g., PMI~, TextRank~), and neural-based keyword extraction techniques (e.g., BiLSTM~).\nThe process of incorporating extracted keywords into generation is much like the process discussed in Section \\ref{sec:keywork-m1}. It takes keywords as an additional input into decoder. Recent work improves encoding phase by adding another sequence encoder to represent keywords~.\nThen, the contextualized keywords representation is fed into the decoder together with input sequence representation.\nTo advance the keyword extraction, Li et al. propose to use multi-task learning for training a keyword extractor network and generating summaries~. Because both summarization and keyword extraction aim to select important information from input document, these two tasks can benefit from sharing parameters to improve the capacity of capturing the gist of the input text. \nIn practice, they take overlapping words between the input document and the ground-truth summary as keywords, and adopt a BiLSTM-Softmax as keyword extractor. Similar idea has also been used in question generation tasks~. They use overlapping words between the input answer context and the ground-truth question as keywords.", "cites": [3139], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the idea of keyword extraction and its integration into text generation, connecting it to prior methods discussed in the survey. It provides a general overview of both statistical and neural approaches, and highlights the use of multi-task learning as a strategy. While it offers some analytical perspective by identifying how keyword extraction can be advanced, it lacks deeper critical evaluation or a novel framework to unify the concepts."}}
{"id": "a4877cd1-8484-4f68-8049-88876f588173", "title": "NLG Enhanced by Linguistic Features", "level": "subsection", "subsections": ["d648911b-d037-414b-b733-5dd997fd5de0", "c418f3bb-d4d8-4dd3-87fb-212a2958082e", "4a0973d1-25ad-4c02-8191-660f39f1001c"], "parent_id": "b426fa47-f85f-4367-a1dc-a6d9c866a437", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Linguistic Features"]], "content": "Feature enriched encoder means that the encoder not only reads the input sequence, but also incorporates auxiliary hand-crafted features~. Linguistic features are the most common hand-crafted features, such as part-of-speech (POS) tags, dependency parsing, and semantic parsing.\n\\vspace{-0.1in}", "cites": [3143, 3142, 8624], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions the concept of feature-enriched encoders and gives examples of linguistic features used in text generation. However, it does not effectively synthesize or connect the insights from the cited papers. There is no critical analysis of the approaches or limitations, nor is there an abstraction to broader patterns or principles in the use of linguistic features for NLG."}}
{"id": "d648911b-d037-414b-b733-5dd997fd5de0", "title": "POS tags and NER tags", "level": "subsubsection", "subsections": [], "parent_id": "a4877cd1-8484-4f68-8049-88876f588173", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Linguistic Features"], ["subsubsection", "POS tags and NER tags"]], "content": "}\nPart-of-speech tagging (POS) assigns token tags to indicate the token's grammatical categories and part of speech such as \\textit{noun (N)}, \\textit{verb (V)}, \\textit{adjective (A)}. Named-entity recognition (NER) classifies named entities mentioned in unstructured text into pre-defined categories such as \\textit{person (P)}, \\textit{location (L)}, \\textit{organization (O)}. \nCoreNLP is the most common used tool~. In spite of homonymy and word formation processes, the same surface word form may be shared between several word types. Incorporating NER tags and POS tags can detect named entities and understand input sequence better, hence, further improve NLG~.\n\\vspace{-0.05in}", "cites": [1109, 3143, 3144], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis of the cited papers, merely stating their existence without connecting their contributions or methodologies. It lacks critical analysis and does not evaluate the effectiveness or limitations of using POS and NER tags in NLG. The content remains at a surface level, describing what POS and NER tags are, but fails to abstract broader implications or principles related to knowledge enhancement in text generation."}}
{"id": "c418f3bb-d4d8-4dd3-87fb-212a2958082e", "title": "Syntactic dependency graph", "level": "subsubsection", "subsections": [], "parent_id": "a4877cd1-8484-4f68-8049-88876f588173", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Linguistic Features"], ["subsubsection", "Syntactic dependency graph"]], "content": "}\n\\label{sec:syn-graph}\nSyntactic dependency graph is a directed acyclic graph representing syntactic relations between words~. For example, in the sentence ``The monkey eats a banana'', ``monkey'' is the subject of the predicate ``eats'', and ``banana'' is the object.\nEnhancing sequence representations by utilizing dependency information captures source long-distance dependency constraints and parent-child relation for different words~. \nIn NLG tasks, dependency information is often modeled in three different ways as follows: (i) linearized representation: linearize dependency graph and then use sequence model to obtain syntax-aware representation ~; (ii) path-based representation: calculate attention weights based on the linear distance between a word and the aligned center position, i.e., the greater distance a word to the center position on the dependency graph is, the smaller contribution of the word to the context vector is~; and (iii) graph-based representation: use GNNs to aggregate information from dependency relations~. \n\\vspace{-0.05in}", "cites": [3146, 8337, 3145], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of how syntactic dependency graphs are utilized in NLG, categorizing methods into three types with brief explanations. It mentions the use of different techniques (linearized, path-based, and graph-based) and references the cited papers to support these categories. However, it lacks deeper synthesis or comparison of the methods, and no critical evaluation of their strengths or limitations is offered."}}
{"id": "4a0973d1-25ad-4c02-8191-660f39f1001c", "title": "Semantic dependency graph", "level": "subsubsection", "subsections": [], "parent_id": "a4877cd1-8484-4f68-8049-88876f588173", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Linguistic Features"], ["subsubsection", "Semantic dependency graph"]], "content": "}\n\\label{sec:sem-graph}\nSemantic dependency graph represents \\textit{predicate-argument} relations between content words in a sentence and have various semantic representation schemes (e.g., DM) based on different annotation systems.\nNodes in a semantic dependency graph are extracted by semantic role labeling (SRL) or dependency parsing, and connected by different intra-semantic and inter-semantic relations~.\nSince semantic dependency graph introduces a higher level of information abstraction that captures commonalities between different realizations of the same underlying predicate-argument structures, it has been widely used to improve text generation~. Jin et al. propose a semantic dependency guided summarization model~. They incorporate the semantic dependency graph and the input text by stacking encoders to guide summary generation process. The stacked encoders consist of a sequence encoder and a graph encoder, in which the sentence encoder first reads the input text through stacked multi-head self-attention, and then the graph encoder captures semantic relationships and incorporates the semantic graph structure into the contextual-level representation. \n\\vspace{-0.05in}", "cites": [1052, 1088], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the use of semantic dependency graphs in Jin et al.'s summarization model. It mentions how they are constructed and used but does not synthesize or connect this approach with the other cited work or broader trends in the field. There is minimal critical evaluation or abstraction beyond the specific method described."}}
{"id": "d9dae76d-34fa-49af-b1bc-6337a2c466ae", "title": "NLG Enhanced by Open Knowledge Graphs", "level": "subsection", "subsections": [], "parent_id": "b426fa47-f85f-4367-a1dc-a6d9c866a437", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by Internal Knowledge"], ["subsection", "NLG Enhanced by Open Knowledge Graphs"]], "content": "\\label{sec:openkg}\nFor those KGs (e.g., ConceptNet) constructed based on data beyond the input text, we refer them as \\textit{external KGs}. On the contrary, an \\textit{internal KG} is defined as a KG constructed solely based on the input text. In this section, we will discuss incorporating internal KG to help NLG~.\nInternal KG plays an important role in understanding the input sequence especially when it is of great length. By constructing an internal KG intermediary, redundant information can be merged or discarded, producing a substantially compressed form to represent the input document~. Besides, representations on KGs can produce a structured summary and highlight the proximity of relevant concepts, when complex events related with the same entity may span multiple sentences~.\nOne of the mainstream methods of constructing an internal KG is using open information extraction (OpenIE).\nUnlike traditional information extraction (IE) methods, OpenIE is not limited to a small set of target entities and relations known\nin advance, but rather extracts all types of entities and relations found in input text~. In this way, OpenIE facilitates the domain independent discovery of relations extracted from text and scales to large heterogeneous corpora.\nAfter obtaining an internal KG, the next step is to learn the representation of the internal KG and integrate it into the generation model.  For example, Zhu et al. use a graph attention network (GAT) to obtain the representation of each node, and fuse that into a transformer-based encoder-decoder architecture via attention~. Their method generates abstractive summaries with higher factual correctness. Huang et al. extend by first encoding each paragraph as a sub-KG using GAT, and then connecting all sub-KGs with a Bi-LSTM~. This process models topic transitions and recurrences, which enables the identification of notable content, thus benefiting summarization. \n\\vspace{-0.05in}", "cites": [2378, 3147, 2369], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of internal knowledge graphs and connects it with OpenIE methods, as well as two specific papers (Zhu et al. and Huang et al.), to present a coherent narrative on their role in improving NLG. It abstracts the idea that internal KGs help compress input and model structured relationships. However, it lacks deeper critical analysis, such as evaluating trade-offs, limitations, or comparing the effectiveness of these methods with alternatives."}}
{"id": "c70e6955-5a55-48af-901c-b9f0878be07a", "title": "NLG Enhanced by Knowledge Base", "level": "subsection", "subsections": ["ce756bcf-0543-4240-af6d-ef9e29193c61", "bd237645-e27e-4fda-a09b-396270983466", "2013671e-3699-41ca-9035-5db949d200da"], "parent_id": "fd996682-bb73-4d48-801b-792919f81dc2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Base"]], "content": "One of the biggest challenges in NLG is to discover the dependencies of elements within a sequence and/or across input and output sequences. The dependencies are actually various types of \\emph{knowledge} such as commonsense, factual events, and semantic relationship. Knowledge base (KB) is a popular technology that collects, stores, and manages large-scale information for knowledge-based systems like search engines. It has a great number of triples composed of subjects, predicates, and objects. People also call them ``facts'' or ``factual triplets''. Recently, researchers have been designing methods to use KB as external knowledge for learning the dependencies easier, faster, and better. \nNext, we introduce popular NLG applications enhanced by knowledge base:\n\\begin{itemize}\n    \\item \\textbf{Question answering.} It is often difficult to generate proper answers only based on a given question. This is because, depending on what the question is looking for, a good answer may have different forms. It may completes the question precisely with the missing information. It may elaborate details of some part of the question. It may need reasoning and inference based on some facts and/or commonsense. So, only incorporating input question into neural generation models often fails the task due to the lack of commonsense/factual knowledge . Related structured information of commonsense and facts can be retrieved from KBs.\n    \\item \\textbf{Dialogue system.} The needs of KB in generating conversations or dialogues are relevant with QA but differ from two aspects. First, a conversation or dialogue could be open discussions when started by an open topic like ``\\emph{Do you have any recommendations?}'' Second, responding an utterance in a certain step needs to recall previous contexts to determine involved entities. KB will play an important role to recognize dependencies in the long-range contexts. \n\\end{itemize}\nTo handle different kinds of relationships between KB and input/output sequences, these methods can be categorized into two methodologies which is shown in Figure \\ref{fig:overall_kb_e}: (M1) design supervised tasks around KB for joint optimization; (M2) enhance incorporation by selecting KB or facts.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=1.0\\textwidth]{figures/kb-enhanced-two-methods.pdf}\n  \\end{center}\n  \\vspace{-0.1in}\n  \\caption{The left figure demonstrates retrieving relevant triples, then using them for generation; the right figure demonstrate using KL to measure the proximity between prior and posterior distribution.}\n  \\label{fig:overall_kb_e}\n\\vspace{-0.15in}\n\\end{figure}\n\\vspace{-0.05in}", "cites": [2366], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly introduces the use of knowledge bases in NLG and outlines two methodologies for incorporating KBs, but it lacks in-depth synthesis of multiple cited works. It provides a general overview of applications like QA and dialogue systems, referencing the cited paper but not connecting it to broader trends or contrasting it with other approaches. While it attempts some level of abstraction by categorizing methods, the analysis remains shallow and primarily descriptive."}}
{"id": "ce756bcf-0543-4240-af6d-ef9e29193c61", "title": "M1: Design Supervised Tasks around KB for Joint Optimization", "level": "subsubsection", "subsections": [], "parent_id": "c70e6955-5a55-48af-901c-b9f0878be07a", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Base"], ["subsubsection", "M1: Design Supervised Tasks around KB for Joint Optimization"]], "content": "}\nKnowledge bases (KBs) that acquire, store, and represent factual knowledge can be used to enhance\ntext generation. However, designing effective incorporation to achieve a desired enhancement is challenging because a vanilla Seq2Seq often fails to represent discrete isolated concepts though they perform well to learn smooth shared patterns (e.g., language diversity).  \nTo fully utilize the knowledge bases, {the idea is} to jointly train neural models on multiple tasks. For example, the target task is answer sequence generation, and additional tasks include question understanding and fact retrieval in the KB. Knowledge can be shared across a unified encoder-decoder framework design. Typically, question understanding and fact retrieval are relevant and useful tasks, because a question could be parsed to match (e.g., string matching, entity linking,\nnamed entity recognition) its subject and predicate with the components of a fact triple in KB, and the answer is the object of the triple. \nKBCopy was the first work to generate responses using factual knowledge bases~. During the generation, KBCopy is able to copy words from the KBs. However, the directly copying relevant words from KBs is extremely challenging.\nCoreQA used both copying and retrieving mechanisms to generate answer sequences with an end-to-end fashion~. Specifically, it had a retrieval module to understand the question and find related facts from the KB. \nThen, the question and all retrieved facts are transformed into latent representations by two separate encoders. During the decoding phase, the integrated representations are fed into the decoder by performing a joint attention on both input sequence and retrieved facts.\nFigure \\ref{fig:overall_kb_e}(a) demonstrates a general pipeline that first retrieves relevant triples from KBs, then leverages the top-ranked triples into the generation process.\n\\vspace{-0.05in}", "cites": [8625], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes a method for incorporating knowledge bases into text generation by jointly optimizing multiple tasks, referencing two papers (KBCopy and CoreQA) as examples. However, it lacks deeper synthesis of ideas, critical evaluation of the approaches, and abstraction to broader principles. It primarily functions as a factual summary with minimal insight or analysis."}}
{"id": "bd237645-e27e-4fda-a09b-396270983466", "title": "M2: Enhance Incorporation by Selecting KB or Facts in KB", "level": "subsubsection", "subsections": [], "parent_id": "c70e6955-5a55-48af-901c-b9f0878be07a", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Base"], ["subsubsection", "M2: Enhance Incorporation by Selecting KB or Facts in KB"]], "content": "}\nIdeally, the relevance of the facts is satisfactory with the input and output sequence dependencies, however, it is not always true in real cases. Lian et al. addressed the issue of selecting relevant facts from KBs based on retrieval models (e.g. semantic similarity) might not effectively achieve appropriate knowledge selection~. The reason is that different kinds of selected knowledge facts can be used to generate diverse responses for the same input utterance. Given a specific utterance and response pair, the posterior distribution over knowledge base from both the utterance and the response may provide extra guidance on knowledge selection. The challenge lies in the discrepancy between the prior and posterior distributions. Specifically, the model learns to select effective knowledge only based on the prior distribution, so it is hard to obtain the correct posterior distribution during inference.\nTo tackle this issue, the work of Lian et al.~ and Wu et al.~ (shown in Figure \\ref{fig:overall_kb_e}(b)) approximated the posterior distribution using the prior distribution in order to select appropriate knowledge even without posterior information. They introduced an auxiliary loss, called Kullback-Leibler divergence loss ({KLDivLoss}), to measure the proximity between the prior distribution and the posterior distribution. The {KLDivLoss} is defined as follows:\n\\begin{equation}\n    \\setlength\\abovedisplayskip{2pt} \n    \\setlength\\belowdisplayskip{2pt}\n    \\mathcal{L}_{\\emph{KLDiv}}(\\theta) = \\sum^{N}_{i=1} p(k = k_i|X, Y) \\log\\frac{p(k = k_i|X, Y)}{p(k = k_i|X)},\n\\end{equation}\nwhere $N$ is the number of retrieved facts. When minimizing {KLDivLoss}, the posterior distribution $p(k|X, Y)$ can be regarded as labels to apply the prior distribution $p(k|X)$ for approximating $p(k|X,Y)$. Finally, the total loss is written as the sum of the {KLDivLoss} and {NLL} (generation) loss.\n\\vspace{-0.05in}", "cites": [3128], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core idea from Lian et al. and Wu et al. by highlighting the challenge of knowledge selection and the use of KL divergence loss to bridge prior and posterior distributions. It provides critical analysis by identifying the discrepancy between training and inference in knowledge selection and explaining the rationale behind the proposed solution. The abstraction is strong, as it frames the issue in probabilistic terms and generalizes the approach to the broader problem of knowledge incorporation in NLG."}}
{"id": "2013671e-3699-41ca-9035-5db949d200da", "title": "Discussion and Analysis of Different Methods", "level": "subsubsection", "subsections": [], "parent_id": "c70e6955-5a55-48af-901c-b9f0878be07a", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Base"], ["subsubsection", "Discussion and Analysis of Different Methods"]], "content": "} \nThe relevance between triples in KBs and input sequences plays a central role in discovering knowledge for sequence generation.\nMethods in \\textbf{M1} typically follows the process that parses input sequence, retrieves relevant facts, and subsequently, a knowledge-aware output can be generated based on the input sequence and previously retrieved facts.\nEven though the improvement by modeling KB with memory network~, existing KG-enhanced methods still suffer from effectively selecting precise triples.\nMethods of \\textbf{M2} improve the selection of facts, in which the ground-truth responses used as the posterior context knowledge to supervise the training of the prior fact probability distribution.\nWu et al. used exact match and recall to measure whether the retrieved triples is used to generate the target outputs~. Table \\ref{tab:kb-quant} shows the entity recall scores of M1-based methods and M2-based methods reported in . We observe that compared to M1-based methods, M2-based methods can greatly improve the accuracy of triple retrieval, as well as the generation quality.\nThere are still remaining challenges in KB-enhanced methods.\nOne is that retrieved facts may contain noisy information, making the generation unstable~.\nThis problem is extremely harmful in NLG tasks, e.g., KB-based question answering and task-oriented dialogue system, since the information in KB is usually the expected entities in the response.\n\\begin{table*}[t]\n\\caption{M2-based methods can retrieve more precise triples, and further improve the generation performance.}\n\\vspace{-0.15in}\n\\centering\n\\setlength{\\tabcolsep}{2.5mm}\\scalebox{0.85}{\\begin{tabular}{|l|l|c|c|c|c|c||c|c|c|c|}\n\\hline\n\\multirow{3}*{Method} & \\multirow{3}*{Cat.} & \\multirow{3}*{Ref.} & \\multicolumn{4}{c||}{Chinese Weibo (large)~} & \\multicolumn{4}{c|}{Chinese Weibo (small)~} \\\\ \n\\cline{4-11}\n& & & \\multicolumn{2}{c|}{Entity score} & \\multicolumn{2}{c||}{Generation score} & \\multicolumn{2}{c|}{Entity score} & \\multicolumn{2}{c|}{Generation score} \\\\ \n\\cline{4-11}\n& & & Match & Recall & BLEU-2 & Dist-2 & Match & Recall & BLEU-2 & Dist-2 \\\\\n\\hline \\hline\nGenDS & M1 &  & 0.97 & 0.37 & 3.42 & 4.27 & 0.75 & 0.26 & 2.09 & 1.66 \\\\\nCCM & M1 &  & 1.09 & 0.37 & 4.75 & 4.87 & 0.99 & 0.28 & 3.26 & 2.59 \\\\\n\\hline\nConKADI & M2 &  & - & - & - & - & \\textbf{1.48} & \\textbf{0.38} & \\textbf{5.06} & \\textbf{23.93} \\\\\nTaFact & M2 &  & \\textbf{1.81} & \\textbf{0.47} & \\textbf{5.07} & \\textbf{23.56} & - & - & - & - \\\\\n\\hline\n\\end{tabular}}\n\\vspace{-0.05in}\n\\label{tab:kb-quant}\n\\end{table*}", "cites": [3148, 3118, 3115], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic comparison between M1 and M2 methods in KB-enhanced NLG, using a table to highlight differences in performance metrics. While it integrates results from multiple cited papers, the synthesis is limited to surface-level observations rather than deep conceptual connections. It identifies one critical challenge (noise in retrieved facts) and compares retrieval and generation scores, but lacks broader abstraction or deeper critique of the methodologies."}}
{"id": "2e94b5c0-1ff8-4d56-a733-a12ae2a57672", "title": "NLG Enhanced by Knowledge Graph", "level": "subsection", "subsections": ["f133a740-6739-4557-8b15-8ca5b4d0e2f6", "f8f4882c-1e70-477e-9035-663aad7f5f30", "0dd5d1f7-6679-4ea3-8424-34630dbe7456", "f848f8d7-023b-460a-8658-d07a5db87028", "957f66c6-436e-4306-94e5-deaddb7ee602"], "parent_id": "fd996682-bb73-4d48-801b-792919f81dc2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"]], "content": "\\label{sec:know-graph}\nKnowledge graph (KG), as a type of structured human knowledge, has attracted great attention from both academia and industry. A KG is a structured representation of facts (a.k.a. knowledge triplets) consisting of entities\\footnote{For brevity, we use â€œentitiesâ€ to denote both entities (e.g., prince) and concepts (e.g., musician) throughout the paper.}, relations, and semantic descriptions~. The terms of â€œknowledge baseâ€ and â€œknowledge graphâ€ can be interchangeably used, but they do not have to be synonymous. The knowledge graph is organized as a graph, so the connections between entities are first-class citizens in it. {In the KG, people can easily traverse links to discover how entities are interconnected to express certain knowledge.} Recent advances in artificial intelligence research have demonstrated the effectiveness of using KGs in various applications like recommendation systems~.\nNext, we introduce popular NLG applications that have been enhanced by knowledge graph:\n\\begin{itemize}\n    \\item \\textbf{Commonsense reasoning.} It aims to empower machines to capture the human commonsense from KG during generation.\n    The methods exploit both structural and semantic information of the commonsense KG and perform reasoning over multi-hop relational paths, in order to augment the limited information with chains of evidence for commonsense reasoning. Popular tasks in commonsense reasoning generation include abductive reasoning (e.g., the $\\alpha$NLG task)~, counterfactual reasoning~, and entity description generation~. \n    \\item \\noindent\\textbf{Dialogue system.} It frequently makes use of KG for the semantics in linked entities and relations~. A dialogue may shift focus from one entity to another, breaking one discourse into several segments, which can be represented as a linked path connecting the entities and their relations.\n    \\item \\noindent\\textbf{Creative writing.} This task can be found in both scientific and story-telling domains. Scientific writing aims to explain natural processes and phenomena step by step, so each step can be reflected as a link on KG and the whole explanation is a path~. In story generation, the implicit knowledge in KG can facilitate the understanding of storyline and better predict what will happen in the next plot~.\n\\end{itemize}\nCompared with separate, independent knowledge triplets, knowledge graph provides comprehensive and rich entity features and relations for models to overcome the influence of the data distribution and enhance its robustness.\nTherefore, node embedding and relational path have played important roles in various text generation tasks. The corresponding techniques are knowledge graph embedding (KGE)~ and path-based knowledge graph reasoning~. Furthermore, it has been possible to encode multi-hop and high-order relations in KGs using the emerging graph neural network (GNN)~ and graph-to-sequence (Graph2Seq) frameworks~.\n\\vspace{-0.05in}\n\\begin{definition}[Knowledge graph (KG)]\nA knowledge graph (KG) is a directed and multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges. Formally, a KG is defined as $\\mathcal{G} = (\\mathcal{U}, \\mathcal{E}, \\mathcal{R})$, where $\\mathcal{U}$ is the set of entity nodes and $\\mathcal{E} \\subseteq \\mathcal{U} \\times \\mathcal{R} \\times \\mathcal{U}$ is the set of typed edges between nodes in $\\mathcal{U}$ with a certain relation in the relation schema $\\mathcal{R}$.\n\\label{def:kg}\n\\end{definition}\nThen given the input/output sequences in the text generation task, a subgraph of the KG which is associated with the sequences can be defined as below.\n\\vspace{-0.05in}\n\\begin{definition}[Sequence-associated K-hop subgraph]\nA sequence-associated K-hop subgraph is defined as $\\mathcal{G}_{sub} = (\\mathcal{U}_{sub}, \\mathcal{E}_{sub}, \\mathcal{R})$, where $\\mathcal{U}_{sub}$ is the union of the set of entity nodes mapped through an \\emph{entity linking} function $\\psi: \\mathcal{U} \\times \\mathcal{X} \\rightarrow \\mathcal{U}_{sub}$ \\textit{and} their neighbors within \\textit{K}-hops.\nSimilarly, $\\mathcal{E}_{sub} \\subseteq \\mathcal{U}_{sub} \\times \\mathcal{R} \\times \\mathcal{U}_{sub}$ is the set of typed edges between nodes in $\\mathcal{U}_{sub}$. \n\\label{def:sks}\n\\end{definition}\nSequence-associated subgraph provides a graphical form of the task data (i.e., sequences) and thus enables the integration of KGs and the sequences into graph algorithms.\nMany methods have been proposed to learn the relationship between KG semantics and input/output sequences. They can be categorized into four methodologies as shown in Figure \\ref{fig:overall_kg}: (M1) incorporate knowledge graph embeddings into language generation; (M2) transfer knowledge into language model with triplet information; (M3) perform reasoning over knowledge graph via path finding strategies; and (M4) improve the graph embeddings with graph neural networks.\n\\vspace{-0.03in}", "cites": [7011, 8623, 3150, 3119, 3121, 553, 3152, 3151, 1165, 3153, 3149, 1934, 8626, 3126, 3120, 8627], "cite_extract_rate": 0.8421052631578947, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information well by grouping methods into four categories (M1-M4) and connecting them to relevant applications. It abstracts broader patterns, such as the role of KG embeddings and reasoning paths in text generation. However, it lacks deeper critical analysis, offering few comparisons or evaluations of limitations beyond stating what KGs enable."}}
{"id": "f133a740-6739-4557-8b15-8ca5b4d0e2f6", "title": "M1: Incorporate Knowledge Graph Embeddings into Language Generation", "level": "subsubsection", "subsections": [], "parent_id": "2e94b5c0-1ff8-4d56-a733-a12ae2a57672", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M1: Incorporate Knowledge Graph Embeddings into Language Generation"]], "content": "}\nKnowledge graph embedding (KGE) techniques learn node embedding from a KG~.\nKGE aims to capture the semantic relatedness between entity nodes from their connectivity information (i.e., different types of relations) in the KG. The primary idea is to represent entities and relations in a low-dimensional vector space $\\mathbb{R}^d$, where $d \\ll |\\mathcal{U} \\cup \\mathcal{R}|$, to reduce data dimensionality while preserving the inherent structure of the KG.\nTransE~ \nis the most widely used KGE technique. In TransE, given a KG edge $(u_i, r, u_j)$, the relation is seen as a translation vector $\\mathbf{r}$ so that the embedded entities $\\mathbf{u}_i$ and $\\mathbf{u}_j$ can be connected with low translation error, namely $\\mathbf{u}_i + \\mathbf{r} \\approx \\mathbf{u}_j$. For example, we have $\\overrightarrow{Tokyo} + \\overrightarrow{IsCapticalOf} \\approx \\overrightarrow{Japan} $ for the knowledge edge $(\\textit{Tokyo},~ \\textit{IsCapticalOf}, ~\\textit{Japan})$. \nAs shown in Figure \\ref{fig:overall_kg}(a), a common strategy of incorporating KGE into NLG is to concatenate the original word representations ($\\textbf{x}$) with the corresponding entity representations ($\\textbf{u}$) from KGE~.\n\\vspace{-0.03in}", "cites": [1934], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how knowledge graph embeddings are used in NLG, primarily focusing on the TransE model and a concatenation strategy. It includes one cited paper but does not synthesize ideas across multiple sources or offer a deeper analysis of approaches. There is minimal critical evaluation or abstraction to broader principles."}}
{"id": "f8f4882c-1e70-477e-9035-663aad7f5f30", "title": "M2: Transfer Knowledge into Language Model with Knowledge Triplet Information", "level": "subsubsection", "subsections": [], "parent_id": "2e94b5c0-1ff8-4d56-a733-a12ae2a57672", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M2: Transfer Knowledge into Language Model with Knowledge Triplet Information"]], "content": "}\nThe vector spaces of entity embeddings (from KGE) and word embeddings (from pre-trained language models) are usually inconsistent~.\nBeyond a simple concatenation,\nrecent methods have explored to fine-tune the language models directly on knowledge graph triplets.\nGuan et al. transformed the commonsense triplets (in ConceptNet and ATOMIC) into readable sentences using templates, as illustrated in Figure \\ref{fig:overall_kg}(b). And then the language model (e.g., GPT-2) is fine-tuned on the transformed sentences to learn the commonsense knowledge to improve text generation.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=1.0\\textwidth]{figures/kg-enhanced-two-methods.pdf}\n  \\end{center}\n  \\vspace{-0.15in}\n  \\caption{Four typical methodologies for incorporating KG semantics into text generation.}\n  \\label{fig:overall_kg}\n\\vspace{-0.15in}\n\\end{figure}\n\\vspace{-0.03in}", "cites": [3126], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes a method by Guan et al. that fine-tunes language models on knowledge triplet data, but it does not synthesize this with other works or provide a broader narrative. It lacks critical evaluation of the method's strengths and limitations and offers no abstract generalization or meta-level insights about the approach or its implications for knowledge-enhanced text generation."}}
{"id": "0dd5d1f7-6679-4ea3-8424-34630dbe7456", "title": "M3: Perform Reasoning over Knowledge Graph via Path Finding Strategies", "level": "subsubsection", "subsections": ["279636fc-df7d-4e34-8428-8f99d62c0fba", "842983b3-1887-468e-a149-773a115613af"], "parent_id": "2e94b5c0-1ff8-4d56-a733-a12ae2a57672", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M3: Perform Reasoning over Knowledge Graph via Path Finding Strategies"]], "content": "}\nKGE learns node representations from one-hop relations through a certain semantic relatedness (e.g. TransE). However, Xiong et al. argued that an intelligent machine is supposed to be able to conduct explicit reasoning over relational paths to make multiple inter-related decisions rather than merely embedding entities in the KGs~.\nTake the QA task an example. The machine performs reasoning over KGs to handle complex queries that do not have an obvious answer, infer potential answer-related entities, and generate the corresponding answer. So, the challenge lies in identifying a subset of desired entities and mentioning them properly in a response~. Because the connected entities usually follow natural conceptual threads, they help generate reasonable and logical answers to keep conversations engaging and meaningful. As shown in Figure \\ref{fig:overall_kg}(c), path-based methods explore various patterns of connections among entity nodes such as meta-paths and meta-graphs. Then they learn from walkable paths on KGs to provide auxiliary guidance for the generation process. The path finding based methods can be mainly divided into two categories: (1) path ranking based methods and (2) reinforcement learning (RL) based path finding methods. \n\\vspace{-0.03in}", "cites": [1169], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of knowledge graph reasoning and introduces path-based methods by connecting ideas from the cited paper (DeepPath) to broader reasoning strategies. It abstracts the approach into two categories (path ranking and RL-based methods) but does not extensively generalize to overarching principles. Critical analysis is limited, as the section mainly describes the problem and methods without evaluating limitations or contrasting multiple works in depth."}}
{"id": "279636fc-df7d-4e34-8428-8f99d62c0fba", "title": "M3.1: Path routing and ranking", "level": "paragraph", "subsections": [], "parent_id": "0dd5d1f7-6679-4ea3-8424-34630dbe7456", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M3: Perform Reasoning over Knowledge Graph via Path Finding Strategies"], ["paragraph", "M3.1: Path routing and ranking"]], "content": "}\nPath ranking algorithm (PRA) emerges as a promising method for learning and inferring paths on large KGs~. PRA uses random walks to perform multiple bounded depth-first search processes to find relational paths. Coupled with elastic-net based learning~, PRA picks plausible paths and prunes non-ideal, albeit factually correct KG paths. For example, Tuan et al. proposed a neural conversation model with PRA on dynamic knowledge graphs~. In the decoding phase, it selected an output from two networks, a general GRU decoder network and a PRA based multi-hop reasoning network, at each time step. \nBauer et al. ranked and filtered paths to ensure both the information quality and variety via a 3-step scoring strategy: initial node scoring, cumulative node scoring, and path selection~. Ji et al. heuristically pruned the noisy edges between entity nodes and proposed a path routing algorithm to propagate the edge probability along\nmulti-hop paths to the entity nodes~.\n\\vspace{-0.03in}", "cites": [3151, 3154, 3150], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the path routing and ranking methods used in knowledge graph-based NLG, with a focus on PRA and specific techniques from cited papers. It offers limited synthesis by briefly mentioning how these methods are applied in dialogue systems, but does not connect or contrast the approaches meaningfully. There is no critical analysis or abstraction to broader principles, remaining largely at the level of summarizing individual techniques."}}
{"id": "842983b3-1887-468e-a149-773a115613af", "title": "M3.2: Reinforcement learning based path finding", "level": "paragraph", "subsections": [], "parent_id": "0dd5d1f7-6679-4ea3-8424-34630dbe7456", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M3: Perform Reasoning over Knowledge Graph via Path Finding Strategies"], ["paragraph", "M3.2: Reinforcement learning based path finding"]], "content": "}\nReinforcement learning (RL) based methods make an agent to perform reasoning to find a path in a continuous space. These methods incorporate various criteria in their reward functions of path finding, making the path finding process flexible.\nXiong et al. proposed DeepPath, the first work that employed Markov decision process (MDP) and used RL based approaches to find paths in KGs~. \nLeveraging RL based path finding for NLG tasks typically consists of two stages~. First, they take a sequence as input, retrieve a starting node $u_0$ on $\\mathcal{G}$, then perform multi-hop graph reasoning, and finally arrive at a target node $u_k$ that incorporates the knowledge for output sequence generation. Second, they represent the sequence $X$ and selected path $\\Phi_k(u_0,u_k)$ through two separate encoders. They decode a sequence with multi-source attentions on the input sequence and selected path. \nPath-based knowledge graph reasoning converts the graph structure of a KG into a linear path structure that can be easily represented by sequence encoders (e.g, RNN)~. For example, Niu et al. encoded selected path and input sequence with two separate RNNs and generated sequence with a general attention-based RNN decoder~.\nTo enhance the RL process, Xu et al. proposed six reward functions for training an agent in the reinforcement learning process.\nFor example, the functions looked for accurate arrival at the target node as well as the shortest path between the start and target node, i.e., minimize the length of the selected path $\\Phi_k(u_0,u_k)$~.\n\\vspace{-0.03in}", "cites": [3149, 1169, 3150, 2378], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of reinforcement learning-based path finding methods in knowledge graph reasoning for NLG, mentioning key papers and their contributions. However, it lacks deeper synthesis of ideas across the works and offers minimal critical evaluation or abstraction to broader principles. The narrative remains largely descriptive and does not connect the cited methods into a cohesive or insightful framework."}}
{"id": "f848f8d7-023b-460a-8658-d07a5db87028", "title": "M4: Improve the Graph Embeddings with Graph Neural Networks", "level": "subsubsection", "subsections": ["12dacd0c-ccdc-43ff-bef4-2efc4f30b6a8", "6684051e-311b-4b4a-b6ea-aaa93c8e2ff6"], "parent_id": "2e94b5c0-1ff8-4d56-a733-a12ae2a57672", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M4: Improve the Graph Embeddings with Graph Neural Networks"]], "content": "}\nThe contexts surrounding relevant entities on KGs play an important role in understanding the entities and generating proper text about their interactions~.\nFor example, in scientific writing, it is important to consider the neighboring nodes of relevant concepts on a taxonomy and/or the global context of a scientific knowledge graph~.\nHowever, neither KGE nor relational path could fully represent such information. Graph-based representations aim at aggregating the context/neighboring information on graph data;\nand recent advances of GNN models demonstrate a promising advancement in graph-based representation learning~. In order to improve text generation, graph-to-sequence (Graph2Seq) models encode the structural information of the KG in a neural encoder-decoder architecture~. Since then, GNNs have been playing an important role in improving the NLG models.\nThey have been applied to both \\emph{encoding} and \\emph{decoding} phases.\n\\vspace{-0.03in}", "cites": [553, 3119, 7011, 3121], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of how graph neural networks enhance knowledge graph-based text generation but does not deeply synthesize the cited papers into a cohesive framework. It mentions the role of GNNs in encoding and decoding phases and references related works, but lacks comparative analysis or critical evaluation of the approaches. There is some abstraction by highlighting the importance of structural information, but it remains at a surface level without identifying broader principles."}}
{"id": "12dacd0c-ccdc-43ff-bef4-2efc4f30b6a8", "title": "Learning KG-aware input text representation with GNNs (Encoding).", "level": "paragraph", "subsections": [], "parent_id": "f848f8d7-023b-460a-8658-d07a5db87028", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M4: Improve the Graph Embeddings with Graph Neural Networks"], ["paragraph", "Learning KG-aware input text representation with GNNs (Encoding)."]], "content": "}\nFor encoding phase, a general process of leveraging GNNs for incorporating KG is to augment semantics of a word in the input text by combining with the vector of the corresponding entity node vector to the word on the KG~. A pre-defined entity linking function $\\psi: \\mathcal{U} \\times \\mathcal{X} \\rightarrow \\mathcal{U}_{sub}$ maps words in the input sequence to entity nodes on the KG. Given an input sequence, all the linked entities and their neighbors within $K$-hops compose a \\textit{sequence-associated K-hop subgraph} $\\mathcal{G}_{sub}$ (formally defined in Definition \\ref{def:sks}).\nFor each entity node in $\\mathcal{G}_{sub}$,\nit uses the KG structure as well as entity and edge features (e.g., semantic description if available) to learn a representation vector $\\textbf{u}$. \nSpecifically, a GNN model follows a neighborhood aggregation approach that iteratively updates the representation of a node by aggregating information from its neighboring nodes and edges. After $k$ iterations of aggregation, the node representation captures the structural information within its $k$-hop neighborhood. Formally, the $k$-th layer of a node $u \\in \\mathcal{U}_{sub}$ is:\n\\begin{equation}\n\\textbf{u}^{(k)} = \\textsc{Combine}_k (\\textbf{u}^{(k-1)},  \\textsc{Aggregate}_k(\\big{\\{} (\\textbf{u}^{(k-1)}_{i}, \\textbf{e}_{ij}^{(k-1)}, \\textbf{u}^{(k-1)}_{j}): \\forall (u_i, e_{ij}, u_j) \\in \\mathcal{N}(u)\\big{\\}})).\n\\end{equation}\nThe sub-graph representation $\\textbf{h}_{subG}$ is learned thorough a $\\textsc{Readout}(\\cdot)$ function from all entity node representations (i.e., $\n\\textbf{h}_{subG} = \\textsc{Readout}(\\big{\\{}\\textbf{u}^{(k)}, u \\in \\mathcal{U}_{sub}\\big{\\}})$.\nZhou et al. was the first to design such a knowledge graph interpreter to enrich the context representations with neighbouring concepts on ConceptNet using graph attention network (GAT)~.\n\\vspace{-0.05in}", "cites": [3119, 1934, 2369, 3155], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how GNNs are used to learn KG-aware input text representations, with a general explanation of the encoding process and a reference to a specific paper (Zhou et al.). While it mentions the use of a graph attention network (GAT), it does not synthesize or compare insights from the other cited papers, nor does it critically evaluate strengths or limitations. The abstraction is limited to a procedural explanation without deeper generalization."}}
{"id": "6684051e-311b-4b4a-b6ea-aaa93c8e2ff6", "title": "Dynamically attending KG representation (Decoding).", "level": "paragraph", "subsections": [], "parent_id": "f848f8d7-023b-460a-8658-d07a5db87028", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "M4: Improve the Graph Embeddings with Graph Neural Networks"], ["paragraph", "Dynamically attending KG representation (Decoding)."]], "content": "}\nThe sequence decoder uses attention mechanism to find useful semantics from the representation of KG as well as the hidden state of the input text, where the KG's representation is usually generated by GNNs.\nSpecially, the hidden state is augmented by subgraph representation $\\textbf{h}_{subG}$, i.e., $\\textbf{s}_0 = \\textbf{h}_n \\oplus \\textbf{h}_{subG} $~. Then, the decoder attentively reads the retrieved subgraph to obtain a graph-aware context vector. Then it uses the vector to update the decoding state~. It adaptively chooses a generic word or an entity from the retrieved subgraph to generate output words. Because graph-level attention alone might overlook fine-grained knowledge edge information, some recent methods adopted the hierarchical graph attention mechanism~. It attentively read the retrieved subgraph $\\mathcal{G}_{sub}$ and then attentively read all knowledge edges $\\mathcal{E}_{sub}$ involved in $\\mathcal{G}_{sub}$. Ji et al. added a relevance score that reflected the relevancy of the knowledge edge according to the decoding state~.\n\\begin{table*}[t]\n\\caption{Tasks, datasets and KG sources used in different KG-enhanced papers. We also compared the performance of different models before and after incorporating KG into the generation process, in which ``w/o KG'' performance comes from the best baseline method; ``with KG'' comes from the KG-enhanced method.}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.815}{\\begin{tabular}{|l|l|c|c|l|r|r|r|c|r|}\n\\hline\n\\multirow{2}*{Tasks} & \\multirow{2}*{Methods} & \\multirow{2}*{Ref.} & \\multirow{2}*{Cat.} & \\multicolumn{2}{c|}{Dataset Information} & \\multicolumn{3}{c|}{Effect of KG} & KG\\\\\n\\cline{5-9}\n& & & & Name & \\#Instance & w/o KG & with KG & $\\Delta$BLEU & source \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Common- \\\\sense \\\\ reasoning}} & KG-BART &  & M4 & CommonGen & 77,449 & 28.60 & 30.90 & +2.30 & ConceptNet \\\\ \n\\cline{2-10}\n& CE-PR &  & M3 & ComVE & 30,000 & 15.70 & 17.10 & +1.60 & ConceptNet \\\\ \n\\cline{2-10}\n& GRF &  & M4 & $\\alpha$NLG-ART & 60,709 & 9.62 & 11.62 & +2.00 & ConceptNet \\\\\n\\cline{2-10}\n& MGCN &  & M3 & EntDesc & 110,814 & 24.90 & 30.00 & +4.30 & Self-built KG \\\\ \n\\hline \\hline\n\\multirow{5}*{\\makecell[l]{Story \\\\ generation}}  & IE+MSA &  & M4 & ROCStories & \\multirow{2}*{98,162} & 8.25 & 9.36 & +1.11 & ConceptNet \\\\ \n\\cline{2-4}\\cline{7-10}\n& GRF &  & M4 & (split-1) & & 10.40 & 11.00 & +0.60 & ConceptNet \\\\ \n\\cline{2-10}\n& \\multirow{2}*{KEPM} & \\multirow{2}*{} & \\multirow{2}*{M2} & ROCStories & \\multirow{2}*{98,162} & \\multirow{2}*{14.10} & \\multirow{2}*{14.30} & \\multirow{2}*{+0.20} & ConceptNet\\\\ \n& &  & & (split-2) & & & & & \\& ATOMIC  \\\\ \n\\cline{2-10}\n& MRG &  & M3 & VisualStory & 50,000 & 3.18 & 3.23 & +0.05 & ConceptNet \\\\ \n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Scientific \\\\ writing}} & GraphWriter &  & M4 & AGENDA & 40,000 & 12.20 & 14.30 & +1.90 & Self-built KG \\\\ \n\\cline{2-10}\n& PaperRobot &  & M4 & PaperWriting & 27,001 & 9.20 & 13.00 & +3.80 & Self-built KG \\\\\n\\hline \\hline\n\\multirow{3}*{\\makecell[l]{Dialogue \\\\ system}} & ConceptFlow &\n & M4 & Reddit-10M & 3,384K & 1.62 & 2.46 & +0.84 & ConceptNet \\\\ \n\\cline{2-10}\n& AKGCM &  & M3 & EMNLP dialog & 43,192 & 32.45 & 30.84 & \\underline{-1.61} & Self-built KG \\\\ \n\\cline{2-10}\n& AKGCM &  & M3 & ICLR dialog & 21,569 & 6.74 & 6.94 & +0.20 & Self-built KG \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Question \\\\ answering}} & \\multirow{2}*{MHPGM} & \\multirow{2}*{} & \\multirow{2}*{M3} & \\multirow{2}*{NarrativeQA} & \\multirow{2}*{46,765} & \\multirow{2}*{19.79} & \\multirow{2}*{21.07} & \\multirow{2}*{+1.28} & \\multirow{2}*{Self-built KG} \\\\\n&&&&&&&&& \\\\\n \\hline\n\\end{tabular}}\n\\end{center}\n\\vspace{-0.15in}\n\\label{tab:data-kg}\n\\end{table*}\n\\begin{table*}[t]\n\\caption{Qualitative comparison between different KG-enhanced methods.}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.86}{\\begin{tabular}{|l|c|c|c|c|c|l|l|l|}\n\\hline\n{\\multirow{2}*{Methods}} & \\multirow{2}*{Ref.} & \\multicolumn{4}{c|}{Method category} & Multi-hop info. & Multi-hop path & Auxiliary (knowledge \\\\\n\\cline{3-6}\n& & M1 & M2 & M3 & M4 & aggregation & reasoning & related) task(s) \\\\\n\\hline \\hline\nTHOTH &  & $\\checkmark$ & & & & $\\times$ & $\\times$ & $\\times$ \\\\\n\\hline\nCCM &  & & & & $\\checkmark$ & $\\times$, one-hop & $\\times$ & $\\times$ \\\\\n\\hline\nKEPM &  & & $\\checkmark$ & & & $\\times$ & $\\times$ & $\\times$ \\\\\n\\hline\nAKGCM &  & & & $\\checkmark$ & & $\\times$ & $\\checkmark$, Markov decision & $\\checkmark$, Path selection \\\\\n\\hline\nIE+MSA &  & & & & $\\checkmark$ & $\\checkmark$, by GNN & $\\times$ & $\\times$ \\\\\n\\hline\nConceptFlow &  & & & & $\\checkmark$ & $\\checkmark$, by GNN & $\\times$ & $\\times$ \\\\\n\\hline\nCE-PR &  & & & $\\checkmark$ & & $\\times$ & $\\checkmark$, Path routing & $\\checkmark$, Concept selection \\\\\n\\hline\nGRF &  & & & & $\\checkmark$ & $\\checkmark$, by GNN & $\\checkmark$, Path scoring & $\\checkmark$, Link prediction \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\vspace{-0.15in}\n\\label{tab:qual-kg}\n\\end{table*}", "cites": [3151, 3154, 7011, 8623, 3121, 3119, 3152, 1934, 8627, 3156, 3120, 3126], "cite_extract_rate": 0.8, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes methods that use graph neural networks and attention mechanisms to incorporate knowledge graph representations into the decoding process. It lists several papers and their results but lacks deeper synthesis or critical evaluation. The abstraction level is limited, focusing on specific implementations rather than broader patterns or principles."}}
{"id": "87fd3061-044d-4dae-8158-e55567627c92", "title": "Pros and cons.", "level": "paragraph", "subsections": [], "parent_id": "957f66c6-436e-4306-94e5-deaddb7ee602", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "Discussion and Analysis of the Methodologies and Methods"], ["paragraph", "Pros and cons."]], "content": "}\nKnowledge graph embedding (\\textbf{M1}) was the earliest attempt to embed components of a KG including entities and relations into continuous vector spaces and use them to improve text generation. \nThose entity and relation embeddings can simply be used to enrich input text representations (e.g., concatenating embeddings), bridging connections between entity words linked from input text in latent space. \nBecause the graph projection and text generation are performed as two separate steps, the embedding vectors from knowledge graph and the hidden states from input text were in two different vector spaces. The model would have to learn to bridge the gap, which might make a negative impact on the performance of text generation.\nFine tuning pre-trained language models on the KG triplets (\\textbf{M2}) can eliminate the gap between the two vector spaces.\nNevertheless, M1 and M2 share two drawbacks.\nFirst, they only preserve information of direct (one-hop) relations in a KG, such as pair-wise proximity in M1 and KG triplet in M2, but ignore the indirect (multi-hop) relations of concepts. The indirect relations may provide plausible evidence of complex reasoning for some text generation tasks.\nSecond, from the time KGs were encoded in M1 or M2 methods, the generation models would no longer be able to access the KGs but their continuous representations. Then the models could not support reasoning like commonsense KG reasoning for downstream tasks.\nDue to these two reasons, M1 and M2 were often used to create basic KG representations upon which the KG path reasoning (M3) and GNNs (M4) could further enrich the hidden states~.\nThe path finding methods of KG reasoning (\\textbf{M3}) perform multi-hop walks on the KGs beyond one-hop relations. It enables reasoning that is needed in many text generation scenarios such as commonsense reasoning and conversational question answering.\nAt the same time, it provides better interpretability for the entire generation process, because the path selected by the KG reasoning algorithm will be explicitly used for generation.\nHowever, the selected paths might not be able to capture the full contexts of the reasoning process due to the limit of number.\nBesides, reinforcement-learning based path finding uses heuristic rewards to drive the policy search, making the model sensitive to noises and adversarial examples.\nThe algorithms of GNN and Graph2Seq (\\textbf{M4}) can effectively aggregate semantic and structural information from multi-hop neighborhoods on KGs, compared to M3 that considers multi-hop paths.\nTherefore, the wide range of relevant information can be directly embedded into the encoder/decoder hidden states. Meanwhile, M4 enables back propagation for jointly optimizing text encoder and graph encoder. \nFurthermore, the attention mechanism that has been applied in GNN and Graph2Seq (e.g., graph attention) can explain the model's output at some extent, though the multi-hop paths from M3 has better interpretability.\nM3 and M4 are able to use multi-hop relational information, compared to M1 and M2. However, they have two weak points. First, they have higher complexity than M1 and M2. In M3, the action space of path finding algorithms can be very large due to the large size and sparsity of the knowledge graph. In M4, the decoder has to attentively read both input sequence and knowledge graph. Second, the subgraphs retrieved by M3 and M4 might provide low coverage of useful concepts for generating the output.\nFor example, people use ConceptNet, a widely used commonsense KG, to retrieve the subgraph on three generative commonsense reasoning tasks. The task datasets are ComVE~, $\\alpha$-NLG~, and ROCSories~. We found 25.1\\% / 24.2\\% / 21.1\\% of concepts in the output could be found on ConceptNet, but only 11.4\\% / 8.1\\% / 5.7\\% of concepts in the output can be found on the retrieved 2-hop sequence-associated subgraph, respectively. It means that a large portion of relevant concepts on the KG are not utilized in the generation process.\n\\vspace{-0.05in}", "cites": [3119, 1934, 8626, 3120], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 4.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple methodologies (M1-M4) and connecting them to the cited papers, forming a structured narrative around knowledge graph usage in text generation. It provides critical evaluation by highlighting limitations such as coverage, computational complexity, and the inability to retain full graph access. The analysis abstracts the methods into a broader conceptual framework, discussing trends like the shift from one-hop to multi-hop reasoning and the trade-offs between interpretability and performance."}}
{"id": "f78642de-c75c-4e86-bbee-c6a8e8793d4a", "title": "Quantitative analysis.", "level": "paragraph", "subsections": [], "parent_id": "957f66c6-436e-4306-94e5-deaddb7ee602", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "Discussion and Analysis of the Methodologies and Methods"], ["paragraph", "Quantitative analysis."]], "content": "}\nTable \\ref{tab:data-kg} summarizes tasks, datasets, and KG sources used in existing KG-enhanced works. Three important things should be mentioned. First, all the datasets in the table are public, and we include their links in Table \\ref{tab:code}. CommonGen~, ComVE~ and $\\alpha$-NLG~ have a public leaderboard for competition. \nSecond, for KG sources, we observe that eight (57.1\\%) papers use ConceptNet as external resource, while six (42.9\\%) papers constructed their own KGs from domain-specific corpus. For example, Koncel et al. created a scientific knowledge graph by applying the SciIE tool (science domain information extraction)~. \nBesides, Zhao et al. compared the performance of models between using ConceptNet and using a self-built KG, and found the model with self-built KG could work better on story generation and review generation tasks~.\nThird, we observed that KG-enhanced NLG methods made the largest improvement on generative commonsense reasoning tasks, in which the average improvement is +2.55\\% in terms of $\\Delta$BLEU, while the average improvement on all different tasks is +1.32\\%.\n\\vspace{-0.05in}", "cites": [3121, 3156, 3157, 8626, 3158], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key trends in KG-enhanced text generation, such as the preference for ConceptNet and the effectiveness of self-built KGs in specific tasks. It provides a critical comparison between these two sources and identifies task-specific performance improvements, particularly in commonsense reasoning. While it abstracts some patterns, it does not propose a novel conceptual framework, limiting its abstraction level."}}
{"id": "3639fe71-9f79-46f8-a74c-c3ae7b1efe9e", "title": "Qualitative analysis.", "level": "paragraph", "subsections": [], "parent_id": "957f66c6-436e-4306-94e5-deaddb7ee602", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG Enhanced by Knowledge Graph"], ["subsubsection", "Discussion and Analysis of the Methodologies and Methods"], ["paragraph", "Qualitative analysis."]], "content": "}\nTable \\ref{tab:qual-kg} compares different KG-enhanced methods from three dimensions: multi-hop information aggregation, multi-hop path reasoning, and auxiliary knowledge graph related tasks. M3 is commonly used for multi-hop path reasoning and M4 is used for multi-hop information aggregation, except that CCM~ only aggregates one-hop neighbors. Besides, the auxiliary KG-related tasks are often used to further help the model learn knowledge from the KG. For example, ablation studies in  show that the tasks of path selection, concept selection and link prediction can further boost the generation performance. GRF~ learns these three abilities at the same time. It achieves the state-of-art performance on three generation tasks.\n\\begin{figure}[t]\n  \\begin{center}\n    \\includegraphics[width=1.0\\textwidth]{figures/kt-enhanced-two-methods.pdf}\n  \\end{center}\n  \\vspace{-0.15in}\n  \\caption{The left figure demonstrates retrieving relevant documents, then using them for generation; the right figure demonstrate reading background document to conduct conversions.}\n  \\label{fig:overall_kt}\n\\vspace{-0.1in}\n\\end{figure}", "cites": [3151, 3120], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section compares different knowledge graph (KG)-enhanced methods using specific dimensions like multi-hop information aggregation and reasoning. However, it lacks deeper synthesis of ideas across the cited papers and offers minimal critical evaluation or abstraction beyond individual techniques."}}
{"id": "c29ceb80-be3d-4dc9-8137-f14c6627e9b9", "title": "NLG enhanced by Grounded Text", "level": "subsection", "subsections": ["1dd2c8ed-bc5c-4bef-b423-37b20faf6dc4"], "parent_id": "fd996682-bb73-4d48-801b-792919f81dc2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG enhanced by Grounded Text"]], "content": "Knowledge grounded text refers to textual information that can provide additional knowledge relevant to the input sequence. The textual information may not be found in training corpora or structured databases, but can be obtained from massive textual data from online resources. These online resources include encyclopedia (e.g., Wikipedia), social media (e.g., Twitter), shopping \nwebsites (e.g., Amazon reviews). Knowledge grounded text plays an important role in understanding the input sequence and its surrounding contexts. For example, Wikipedia articles may offer textual explanations or background information for the input text. Amazon reviews may contain necessary descriptions and reviews needed to answer a product-related question. Tweets may contain people's comments and summaries towards an event.\nTherefore, knowledge grounded text is often taken as an important external knowledge source to help with a variety of NLG applications.\nNext, we introduce popular NLG applications enhanced by knowledge grounded text:\n\\begin{itemize}\n    \\item \\textbf{Dialogue system.} Building a fully data-driven dialogue system is difficult since most of the universal knowledge is not presented in the training corpora~. The lack of universal knowledge considerably limits the appeal of fully data-driven generation methods, as they are bounded to respond evasively or defectively and seldom include meaningfully factual contents. To infuse the response with factual information, an intelligent machine is expected to obtain necessary background information to produce appropriate response.\n    \\item \\noindent\\textbf{Summarization.} Seq2Seq models that purely depend on the input text tend to ``lose control'' sometimes. For example, 3\\% of summaries contain less than three words, and 4\\% of summaries repeat a word for more than 99 times as mentioned in~. Furthermore, Seq2Seq models usually focus on copying source words in their exact order, which is often sub-optimal in abstractive summarization. Therefore, leveraging summaries of documents similar as the input document as templates can provide reference for the summarization process~.\n    \\item \\noindent\\textbf{Question answering (QA).} It is often difficult to generate proper answers only based on the given question. For example, without knowing any information of an Amazon product, it is hard to deliver satisfactory answer to the user questions such as \\emph{``Does the laptop have a long battery life?''} or \\emph{``Is this refrigerator frost-free?''} So, the product description and customer reviews can be used as a reference for answering product-related questions~.\n\\end{itemize}\nTo handle different kinds of relationships between grounded text and input/output sequences, these methods can be categorized into two methodologies as shown in Figure \\ref{fig:overall_kt}: (M1) guiding generation with retrieved information; (M2) modeling background knowledge into response generation.\n\\vspace{-0.05in}", "cites": [2005, 3130], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section introduces the concept of knowledge grounded text and lists a few NLG applications enhanced by it, such as dialogue systems, summarization, and QA. However, it primarily describes these applications and their challenges in a general way, with minimal synthesis of the cited papers and no meaningful comparison or critique of the methods. It touches on abstraction slightly by grouping applications, but lacks a deeper conceptual framework or meta-level insights."}}
{"id": "1dd2c8ed-bc5c-4bef-b423-37b20faf6dc4", "title": "M1: Guiding Generation with Retrieved Information", "level": "subsubsection", "subsections": ["04bc4b0f-e214-44e6-a206-1736fc63f6a4", "1639fa52-01b7-46e4-b2f5-daee48310180", "8fa936eb-f794-41d7-8d93-89a8dd65817c"], "parent_id": "c29ceb80-be3d-4dc9-8137-f14c6627e9b9", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG enhanced by Grounded Text"], ["subsubsection", "M1: Guiding Generation with Retrieved Information"]], "content": "}\nBecause knowledge grounded text is not presented in the training corpora, an idea is to retrieve relevant textual information (e.g., a review, a relevant document, a summary template) from \\emph{external sources} based on the input text and to incorporate the retrieved grounded text into the generation process. This process is similar to designing knowledge acquisition and incorporation of KBs and KGs in text generation tasks. The difference is that ground text is unstructured and noisy. So, researchers design knowledge selection and incorporation methods to address the challenges.\nBased on the number of stages, we further divide related methods into two categories: retrieve-then-generate (also known as retrieval-augmented generation, short as RAG, in many existing papers~) methods (2-stage methods) and retrieve, rerank and rewrite methods (3-stage methods).\n\\vspace{-0.05in}", "cites": [3159, 7225, 2376], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by grouping methods into 2-stage and 3-stage approaches, and it connects this with the broader context of knowledge-based text generation. It includes some abstraction by highlighting the unstructured and noisy nature of grounded text as a challenge. However, it lacks deeper critical analysis of the cited works and does not compare their effectiveness or limitations in detail."}}
{"id": "04bc4b0f-e214-44e6-a206-1736fc63f6a4", "title": "M1.1: Retrieval-augmented generation (RAG)", "level": "paragraph", "subsections": [], "parent_id": "1dd2c8ed-bc5c-4bef-b423-37b20faf6dc4", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG enhanced by Grounded Text"], ["subsubsection", "M1: Guiding Generation with Retrieved Information"], ["paragraph", "M1.1: Retrieval-augmented generation (RAG)"]], "content": "}\nRAG follows a two-stage process: retrieval and generation. \nSpecially, as shown in Figure \\ref{fig:overall_kt}(a), a retriever $p(Z|X)$ first returns (usually top-K truncated) distributions over text passages given a query $X$, and then a generator $p(y_i|X, Z, y_{1:i-1})$ generates a current token based on a context of the previous tokens $y_{1:i-1}$, the original input $X$ and a retrieved passage $Z$.\nMethods for retrieving fact or review snippets are various, including matching from a collection of raw text entries indexed by named entities~; scoring relevant documents within a large collection by statistical approaches such as BM25~, or neural-based retrieval approaches such as dense paragraph retrieval (DPR)~.\nFor training the retriever and generator, most of existing work has jointly optimized these two components, without any direct supervision on what document should be retrieve~.\nHowever, by asking human experts to label what document should be retrieved and adding the retrieval loss (resulting in a multi-task learning setting), the generation performance can be greatly improved~, though the labelling process is an extremely time-consuming and labor-intensive task.\n\\begin{table*}[t]\n\\caption{Tasks, datasets and evidence sources used in retrieve-then-generate (M1) papers. We also include their document(d)/sentence(s) retrieval space and the number of retrieved document(d)/sentence(s).}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.83}{\\begin{tabular}{|l|l|l|c|l|r|r|r|}\n\\hline\nEvidence & \\multirow{2}*{Tasks} & \\multirow{2}*{Methods} & \\multirow{2}*{Ref.} & \\multicolumn{2}{c|}{Dataset Information} & Retrieval & \\# Retri- \\\\\n\\cline{5-6}\nsources & & & & Name & \\#Instance & space (d/s) & eved d/s \\\\\n\\hline \\hline\n\\multirow{7}*{\\makecell[l]{Wikipedia}} & \\multirow{2}*{\\makecell[l]{Dialogue \\\\ system}} & MemNet &  & \\multirow{2}*{\\makecell[l]{Wizard of \\\\ Wikipedia (WoW)}} & \\multirow{2}*{22,311} & \\multirow{2}*{5.4M/93M} & 7 \\\\\n &  & SKT &  & & & & 7 \\\\ \n\\cline{2-8}\n& \\multirow{3}*{\\makecell[l]{Question \\\\ answering}} & RAG &  & MS-MARCO & 267,287 & 21M/- & 10 \\\\ \n\\cline{3-8}\n& & BART+DPR &  & \\multirow{2}*{ELI5} & \\multirow{2}*{274,741} & 3.2M/- & - \\\\\n& & RT+C-REALM &  & & & 3.2M/- & 7 \\\\\n\\cline{2-8}\n& \\multirow{2}*{\\makecell[l]{Argument \\\\ generation}} & H\\&W &  & \\multirow{2}*{ChangeMyView} & \\multirow{2}*{287,152} & 5M/- & 10 \\\\ \n& & CANDELA &  & & & 5M/- & 10 \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Online platform \\\\ (e.g., Amazon)}} & \\multirow{2}*{\\makecell[l]{Dialogue (for \\\\ business)}} & AT2T &  & Amazon books & 937,032 & -/131K & 10 \\\\ \n& & KGNCM &  & Foursquare & 1M & -/1.1M & 10 \\\\ \n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Gigawords}} & \\multirow{2}*{\\makecell[l]{Summari- \\\\ zation}} & R$^3$Sum &  & \\multirow{2}*{Gigawords} & \\multirow{2}*{3.8M} & -/3.8M & 30 \\\\ \n& & BiSET &  & & & -/3.8M & 30 \\\\ \n\\hline\n\\end{tabular}}\n\\end{center}\n\\vspace{-0.1in}\n\\label{tab:data-kt}\n\\end{table*}\n\\begin{table*}[t]\n\\caption{Qualitative comparison between different grounded text enhanced methods.}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.895}{\\begin{tabular}{|l|c|c|c|c|l|l|l|}\n\\hline\n{\\multirow{2}*{Methods}} & \\multirow{2}*{Ref.} & \\multicolumn{3}{c|}{Method category} & \\multirow{2}*{Retrieval supervision} & Retriever  & Number \\\\\n\\cline{3-5}\n& & M1.1 & M1.2 & M2 & & pre-training & of stages \\\\\n\\hline \\hline\nMemNet &  & $\\checkmark$ & & & $\\checkmark$, Human annotated labels & $\\times$ & 2 \\\\\n\\hline\nSKT &  & $\\checkmark$ & & & $\\checkmark$, Human annotated labels & $\\times$ & 2 \\\\\n\\hline\nR$^3$Sum &  & & $\\checkmark$ & & $\\checkmark$, Pseudo labels & $\\times$ & 3, with rerank \\\\\n\\hline\nBiSET &  & & $\\checkmark$ & & $\\checkmark$, Pseudo labels & $\\times$ & 3, with rerank \\\\\n\\hline\nRefNet &  & & & $\\checkmark$ & $\\times$ & $\\times$ & 1, no retrieval \\\\\n\\hline\nGLKS &  & & & $\\checkmark$ & $\\times$ & $\\times$ & 1, no retrieval \\\\\n\\hline\nRAG &  & $\\checkmark$ & & & $\\times$ & $\\checkmark$, DPR & 2 \\\\\n\\hline\nKilt &  & $\\checkmark$ & & & $\\times$ & $\\checkmark$, DPR & 2 \\\\\n\\hline\nRT+C-REALM &  & $\\checkmark$ & & & $\\times$ & $\\checkmark$, REALM & 2 \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\vspace{-0.1in}\n\\label{tab:qual-kt}\n\\end{table*}\nGhazvininejad et al. proposed a knowledge grounded neural conversation model (KGNCM), which is the first work to retrieve review snippets from Foursquare and Twitter. Then it incorporates the snippets into dialogue response generation~. It uses an end-to-end memory network~ to generate responses based on the selected review snippets.\nLewis et al. introduced a general retrieval-augmented generation (RAG) framework by leveraging a pre-trained neural retriever and generator. It can be easily fine-tuned on downstream tasks, and it has demonstrated state-of-the-art performance on various knowledge intensive NLG tasks~. Recently, the fusion-in-decoder methods (i.e., the decoder performs attention over the concatenation of the resulting representations of all retrieved passages~) could even outperform RAG as reported in KILT benchmark~.\n\\vspace{-0.05in}", "cites": [457, 9109, 2005, 1073, 3117, 3118, 3130, 3161, 3159, 3160, 7225, 7126, 2376], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of retrieval-augmented generation (RAG) and related methods, presenting their structure and training approaches. It includes some synthesis through the tabular summaries, but the narrative is largely centered on listing and categorizing methods rather than deeply connecting or analyzing them. There is minimal critical evaluation or abstraction into broader principles."}}
{"id": "1639fa52-01b7-46e4-b2f5-daee48310180", "title": "M1.2: Retrieve, rerank and rewrite ($R^{3", "level": "paragraph", "subsections": [], "parent_id": "1dd2c8ed-bc5c-4bef-b423-37b20faf6dc4", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "NLG enhanced by Grounded Text"], ["subsubsection", "M1: Guiding Generation with Retrieved Information"], ["paragraph", "M1.2: Retrieve, rerank and rewrite ($R^{3"]], "content": "$)}}\nDifferent from RAG, a $R^{3}$-based method is expected to retrieve a most precise reference document that can be directly used for rewriting/editing.\n$R^{3}$-based method has proved successful in a number of NLG tasks such as machine translation~, and summarization~.\nIn summarization, Seq2Seq models that purely depend on the input document to generate summaries tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently~. Template-based summarization assume the golden summaries of the similar sentences (i.e., templates) can provide a reference point to guide the input sentence summarization process~. These templates are often called \\textit{soft templates} in order to distinguish from the traditional rule-based templates. Soft template-based summarization typically follows a three-step design: retrieve, rerank, and rewrite. The step of retrieval aims to return a few candidate templates from a summary collection. The reranking identifies the best template from the retrieved candidates. And the rewriting leverages both the source document and template to generate more faithful and informative summaries. \n\\vspace{-0.05in}", "cites": [3130], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the $R^{3}$ method, explaining its three-step process and noting its application in summarization. It references a specific paper (BiSET) but does not synthesize ideas across multiple sources or situate the method within a broader framework. There is minimal critical analysis or comparison with other approaches, and abstraction is limited to general references without identifying overarching principles."}}
{"id": "14a1bb60-b565-432a-af53-1ea70efa8605", "title": "M2: Modeling Background Knowledge into Response Generation", "level": "subsection", "subsections": ["8917b74f-612b-41f1-a0a4-7a391b128fa5"], "parent_id": "fd996682-bb73-4d48-801b-792919f81dc2", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "M2: Modeling Background Knowledge into Response Generation"]], "content": "Background document, with more global and comprehensive knowledge, has been often used for generating informative responses and ensuring a conversation to not deviate from its topic. Keeping a conversation grounded on a background document is referred as background based conversation (BBC)~. Background knowledge plays an important role in human-human conversations. For example, when talking about a movie, people often recall important points (e.g., a scene or review about the movie) and appropriately mention them in the conversation context. Therefore, an intelligent NLG model is expected to find an appropriate background snippet and generate response based on the snippet.\nAs shown in Figure \\ref{fig:overall_kt}(b), the task of BBC is often compared with machine reading comprehension (MRC), in which a span is extracted from the background document as a response to a question~. However, since BBC needs to generate natural and fluent responses, the challenge lies in not only locating the right semantic units in the background, but also referring to the right background information at the right time in the right place during the decoding phase.\nAs MRC models tie together multiple text segments to provide a unified and factual answer, many BBC models use the same idea to connect different pieces of information and find the appropriate background knowledge based on which the next response is to be generated~. For instance, Qin et al. proposed an end-to-end conversation model that jointly learned response generation together with on-demand machine reading~.\nThe MRC models can effectively encode the input utterance by treating it as a question in a typical QA task (e.g., SQuAD ) and encode the background document as the context. Then, they took the utterance-aware background representation as input into decoding phase.", "cites": [2046, 439, 3117, 3162], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of background-based conversation (BBC) by connecting it to machine reading comprehension (MRC) and highlights key challenges like grounding and fluency. While it integrates multiple papers and identifies the general approach (e.g., encoding utterances as questions and using background as context), it lacks deeper critical analysis of the limitations or trade-offs among the methods. The abstraction is moderate, as it identifies the role of external knowledge in NLG but does not fully generalize into a meta-framework or principle."}}
{"id": "597d7113-9a58-4a0d-81d5-cbb6890216ad", "title": "Pros and cons.", "level": "paragraph", "subsections": [], "parent_id": "8917b74f-612b-41f1-a0a4-7a391b128fa5", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "M2: Modeling Background Knowledge into Response Generation"], ["subsubsection", "Discussion and Analysis of Different Methods"], ["paragraph", "Pros and cons."]], "content": "}\nFor M1, guiding generation with retrieved information explicitly exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during language generation. \nSince retrieval-augmented generation (RAG) captures knowledge in a interpretable and modular way, it is often used for knowledge-intensive tasks such as long-form QA and argument generation.\nHowever, a knowledge retriever is expected to retrieve documents from a large-scale corpus, e.g., the entire Wikipedia, which causes significant computational challenge.\nBesides, one input often requires retrieved text whose amount is much larger than the input itself (as indicated in Table~\\ref{tab:data-kt}), leading to serious information overwhelming for the generation model. \nFor M2, background based conversations (BBCs) avoid generating generic responses in a dialogue system and are able to generate more informative responses by exploring related background information. However, existing methods still cannot solve inherent problems effectively, such as tending to break a complete semantic unit and generate shorter responses~.\n\\vspace{-0.05in}", "cites": [3117], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical comparison of two methods (M1 and M2) within knowledge-enhanced text generation, highlighting both their strengths and limitations. It integrates information from the cited paper (RefNet) and places it in the context of broader challenges in the field, such as information overwhelming and response coherence. While it offers some critical evaluation, the analysis remains fairly surface-level and does not present a novel synthesis or abstraction beyond the immediate methods discussed."}}
{"id": "caed0784-4ac3-40d6-9b15-52b07f7290c8", "title": "Qualitative analysis.", "level": "paragraph", "subsections": [], "parent_id": "8917b74f-612b-41f1-a0a4-7a391b128fa5", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "NLG enhanced by External Knowledge"], ["subsection", "M2: Modeling Background Knowledge into Response Generation"], ["subsubsection", "Discussion and Analysis of Different Methods"], ["paragraph", "Qualitative analysis."]], "content": "}\nTable \\ref{tab:data-kt} summarizes tasks, datasets and evidence sources used in existing grounded text enhanced work. Three important things should be mentioned. First, all the datasets in the table are public, and we include their links in Table \\ref{tab:code}.\nSecond, Wikipedia is the most commonly used evidence source since it is the largest free online encyclopedia. Besides, some online platforms contain plenty of product-related textural information, e.g., product reviews on Amazon, which are often used to build up task/goal oriented dialogue systems for business purpose.\nThird, the retrieval space of candidate documents are usually larger than 1 million and only 7-10 documents are selected. So, the process of retrieving relevant documents is challenging.\nTable \\ref{tab:qual-kt} compares different grounded text enhanced methods from three dimensions: retrieval supervision, pre-training of the retriever, and number of stages. First, as mentioned above, retrieving relevant documents from a large candidate set is a challenging task. To improve the retrieval accuracy, four (57.1\\%) papers added the retrieval supervision either by human annotated labels or pseudo labels, resulting in a multi-task learning setting. Besides, three (42.9\\%) papers used pre-trained language models to produce document representation for better retrieval. Though existing work has greatly improved the retrieval accuracy, the performance is still far from satisfactory in many text generation tasks~.\nHow to learn mutually enhancement between retrieval and generation is still a promising direction in the grounded text enhanced text generation systems.\n\\vspace{-0.05in}", "cites": [7225, 2376], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the field by summarizing trends in evidence sources and retrieval methods, and connects key ideas across multiple works, such as the use of retrieval supervision and pre-trained models. It provides a critical perspective by highlighting the limitations of current retrieval methods and identifying promising future directions. However, the analysis remains somewhat surface-level, with limited generalization to overarching principles or deeper theoretical insights."}}
{"id": "bb6a84ff-227f-4a65-a3fd-9b82bdf7f80f", "title": "Benchmark, Toolkit and Leaderboard Performance", "level": "section", "subsections": [], "parent_id": "5f69c7fa-bf2b-4523-bd34-303cffb73a99", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Benchmark, Toolkit and Leaderboard Performance"]], "content": "The development of general evaluation benchmarks for text generation helps to promote the development of research in related fields. Existing text generation benchmarks did not specially focus on choosing the tasks and datasets that have been widely used for knowledge-enhanced text generation. Therefore, we re-screened from the existing four text generation benchmarks, i.e., GLGE~, GEM~, KilT~, GENIE~, and determined 9 benchmark datasets for evaluating knowledge-enhanced NLG methods. Here is our criteria for selection:\n\\begin{itemize}\n    \\item We only consider benchmark datasets that have open-access downloading link.\n    \\item We focus on diverse text generation tasks, involving various applications.\n    \\item We select at most three benchmark datasets for each text generation task.\n    \\item We include a mix of internal and external knowledge focused datasets.\n    \\item We prefer multi-reference datasets for robust automatic evaluation.\n\\end{itemize}\nBased on the benchmark selection criteria, we finalize 9 knowledge-centric tasks that covers various NLG tasks, including commonsense reasoning, text summarization, question generation, generative question answering, and dialogue. The data statistics is shown in Table \\ref{tab:benchmark}. Descriptions and dataset links are listed as follows:\n\\begin{itemize}\n    \\item \\textbf{Wizard of Wikipedia (WOW):} It is an open-domain dialogue dataset, where two speakers conduct an open-ended conversion that is directly grounded with knowledge retrieved from Wikipedia. (Data link: \\url{https://parl.ai/projects/wizard\\_of\\_wikipedia/})\n    \\item \\textbf{CommonGen:} It is a generative commonsense reasoning dataset. Given a set of common concepts, the task is to generate a coherent sentence describing an everyday scenario using these concepts. (Data link: \\url{https://inklab.usc.edu/CommonGen/})\n    \\item \\textbf{$\\alpha$NLG-ART:}  It is a generative commonsense reasoning dataset. Given the incomplete observations about the world, the task it to generate a valid hypothesis about the likely explanations to partially observable past and future. (Data link: \\url{http://abductivecommonsense.xyz/})\n    \\item \\textbf{ComVE:}  It is a generative commonsense reasoning dataset. The task is to generate an explanation given a counterfactual statement for sense-making. (Data link: \\url{https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation}\n    \\item \\textbf{ELI5:} It is a dataset for long-form question answering. The task is to produce explanatory multi-sentence answers for diverse questions. Web search results are used as evidence documents to answer questions. (Data link: \\url{https://facebookresearch.github.io/ELI5/})\n    \\item \\textbf{SQuAD:} It is a dataset for answer-aware question generation. The task is to generate a question asks towards the given answer span based on a given text passage or document. (Data link: \\url{https://github.com/magic282/NQG})\n    \\item \\textbf{CNN/DailyMail (CNN/DM):} It is a dataset for summarization. Given a news aticles, the goal is to produce a summary that represents the most important or relevant information within the original content. (Data link: \\url{https://www.tensorflow.org/datasets/catalog/cnn_dailymail})\n    \\item \\textbf{Gigaword:} It is a dataset for summarization. Similar with CNN/DM, the goal is to generate a headline for a news article. (Data link: \\url{https://www.tensorflow.org/datasets/catalog/gigaword})\n    \\item \\textbf{PersonaChat:} It is an open-domain dialogue dataset.\n    It presents the task of making chit-chat more engaging by conditioning on profile information. (Data link: \\url{https://github.com/facebookresearch/ParlAI/tree/master/projects/personachat})\n\\end{itemize}\n\\begin{table*}[t]\n\\caption{We choose 9 knowledge-enhanced NLG benchmark datasets. These datasets have been included in four existing general NLG benchmarks (i.e., GLGE~, GEM~, Kilt~, GENIE~) or in SemEval tasks.}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.85}{\\begin{tabular}{|l|c|l|r|r|r|c|c|l|}\n\\hline\n\\multirow{2}*{Tasks} & \\multirow{2}*{Ref.} & \\multicolumn{4}{c|}{Dataset Information} & Leader & In which NLG & Papers including \\\\\n\\cline{3-6}\n& & Name & \\#Train & \\#Dev. & \\#Test & board & benchmark & this dataset \\\\\n\\hline \\hline\n\\multirow{3}*{\\makecell[l]{Dialogue \\\\ system}} & \\multirow{2}*{\\makecell[l]{}} & \\multirow{2}*{\\makecell[l]{Wizard of \\\\ Wikipedia}} & \\multirow{2}*{18,430} & \\multirow{2}*{1,948} & \\multirow{2}*{1,933}  & \\multirow{2}*{$\\checkmark$\\footnotemark[1]} & \\multirow{2}*{Kilt} & \\multirow{2}*{} \\\\\n&&&&&&&& \\\\\n\\cline{2-9}\n&  & PersonaChat & 122,499 & 14,602 & 14,056 & $\\times$ & GLGE &  \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Question \\\\ answering}} & \\multirow{2}*{} & \\multirow{2}*{ELI5} & \\multirow{2}*{272,634} & \\multirow{2}*{1,507} & \\multirow{2}*{600} & \\multirow{2}*{$\\checkmark$\\footnotemark[3]} & \\multirow{2}*{Kilt} & \\multirow{2}*{} \\\\\n&&&&&&&& \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Question \\\\ generation}} & \\multirow{2}*{} & \\multirow{2}*{SQuAD} & \\multirow{2}*{75,722} & \\multirow{2}*{10,570} & \\multirow{2}*{11,877} & \\multirow{2}*{$\\times$} & \\multirow{2}*{GLGE} & \\multirow{2}*{} \\\\\n&&&&&&&& \\\\\n\\hline \\hline\n\\multirow{3}*{\\makecell[l]{Commonsense \\\\ reasoning}} &  & CommonGen & 67,389 & 4,018 & 6,042 & $\\checkmark$\\footnotemark[4] & GEM &  \\\\\n\\cline{2-9}\n&  & $\\alpha$NLG-ART & 50,481 & 7,252 & 2,976 & $\\checkmark$\\footnotemark[5] & GENIE &  \\\\\n\\cline{2-9}\n&  & ComVE & 25,596 & 1,428 & 2,976 & $\\checkmark$\\footnotemark[6] & SemEval &  \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Summarization}} &  & CNN/DM & 287,226 & 13,368 & 11,490 & $\\checkmark$\\footnotemark[7] & GLGE &  \\\\ \n\\cline{2-9}\n&  & Gigaword & 3.8M & 189K & 1,951 & $\\checkmark$\\footnotemark[8] & GLGE &  \\\\ \n\\hline\n\\end{tabular}}\n\\end{center}\n\\vspace{-0.1in}\n\\label{tab:benchmark}\n\\end{table*}\n\\footnotetext[1]{\\url{https://parl.ai/projects/wizard\\_of\\_wikipedia}}\n\\footnotetext[2]{\\url{https://nikitacs16.github.io/holl-e-website/}}\n\\footnotetext[3]{\\url{https://facebookresearch.github.io/ELI5/}}\n\\footnotetext[4]{\\url{https://inklab.usc.edu/CommonGen/leaderboard.html}}\n\\footnotetext[5]{\\url{https://leaderboard.allenai.org/genie-anlg/submissions/public}}\n\\footnotetext[6]{\\url{https://competitions.codalab.org/competitions/21080\\#results}}\n\\footnotetext[7]{\\url{https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail}}\n\\footnotetext[8]{\\url{https://paperswithcode.com/sota/text-summarization-on-gigaword}}\n\\vspace{-0.05in}", "cites": [457, 439, 7186, 650, 3151, 3118, 3128, 1071, 3164, 3159, 3139, 7127, 8626, 442, 2376, 3126, 3157, 3120, 3163, 8415, 3165, 3158], "cite_extract_rate": 0.7586206896551724, "origin_cites_number": 29, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of benchmark datasets relevant to knowledge-enhanced text generation, listing each with its task, size, and inclusion in broader benchmarks. While it outlines a selection criteria and links to resources, it lacks deeper synthesis of how these datasets relate to one another or to the broader research goals. There is minimal critical analysis of the strengths or weaknesses of the datasets or cited works. Some abstraction is present in grouping datasets by task, but the section remains largely focused on factual listing rather than offering meta-level insights."}}
{"id": "4697a984-5562-45d9-8fb5-094f3d263a4f", "title": "Incorporate Knowledge into Visual-Language Generation Tasks", "level": "subsection", "subsections": [], "parent_id": "918c7c6b-1917-4b0e-a06c-ecaf632c2d80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Discussion on Future Directions"], ["subsection", "Incorporate Knowledge into Visual-Language Generation Tasks"]], "content": "Beyond text-to-text generation tasks, recent years have witnessed a growing interest in visual-language (VL) generation tasks, such as describing visual scenes~, and answering visual-related questions~.\nAlthough success has been achieved in recent years on VL generation tasks, there is still room for improvement due to the fact that image-based factual descriptions are often not enough to generate high-quality captions or answers~. External knowledge can be added in order to generate attractive image/video captions. We observed some pioneer work has attempted to utilize external knowledge to enhance the image/video captioning tasks. For example, Tran et al. proposed to detect a diverse set of visual concepts and generate captions by using an external knowledge base (i.e., Freebase), in recognizing a broad range of entities such as celebrities and landmarks~. \nZhou et al. used a commonsense knowledge graph (i.e., ConceptNet), to infer a set of terms directly or indirectly related to the words that describe the objects found in the scene by the object recognition module~.\nIn addition,\n proposed a neuro-symbolic learner for improving visual-language generation tasks (e.g., visual question answering).\nHowever, existing approaches for knowledge-enhanced visual-language generation tasks still have a lot of space for exploration. Some promising directions for future work include using other knowledge sources, such as retrieving image/text to help solve open-domain visual question answering and image/video captioning tasks; bringing structured knowledge for providing justifications for the captions that they produce, tailoring captions to different audiences and contexts, etc.\n\\vspace{-0.05in}", "cites": [3167, 3166, 3168], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes a few relevant works on knowledge-enhanced visual-language generation, connecting ideas across papers to form a narrative about the potential of external knowledge in these tasks. It provides some critical perspective by highlighting the limitations of image-based descriptions and suggesting future directions. However, it lacks deeper comparative analysis and a more abstract conceptualization of the underlying principles, remaining somewhat at the level of identifying trends and opportunities."}}
{"id": "877f3075-82a9-4733-9944-b9510faecbb3", "title": "Learning Knowledge from Broader Sources", "level": "subsection", "subsections": ["212793a3-642a-4b98-a22a-05ddf89dbf1c"], "parent_id": "918c7c6b-1917-4b0e-a06c-ecaf632c2d80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Discussion on Future Directions"], ["subsection", "Learning Knowledge from Broader Sources"]], "content": "More research efforts should be spent on learning to discover knowledge more broadly and combine multiple forms of knowledge from different sources to improve the generation process. More knowledge sources can be but not limited to network structure, dictionary and table. For examples, Yu et al.~ and An et al.~ augmented the task of scientific papers intention detection and summarization by introducing the citation graph; Yu et al. augmented the rare word representations by retrieving their descriptions from Wiktionary and feed them as additional input to a pre-trained language model~.\nBesides, structured knowledge and unstructured knowledge can play a complementary role in enhancing text generation. To improve knowledge richness, Fu et al. combined both structured (knowledge base) and unstructured knowledge (grounded text)~.\n\\vspace{-0.05in}", "cites": [3169, 3170], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers to highlight the use of external knowledge sources in enhancing text generation. It connects the idea of using citation graphs and dictionaries to show the value of incorporating structured and unstructured knowledge. However, it lacks deeper critical evaluation of the approaches, such as their limitations or trade-offs, and offers only modest abstraction by briefly mentioning complementary roles of knowledge types without fully generalizing patterns or principles."}}
{"id": "212793a3-642a-4b98-a22a-05ddf89dbf1c", "title": "Leveraging Knowledge from Pre-trained Language Models", "level": "paragraph", "subsections": [], "parent_id": "877f3075-82a9-4733-9944-b9510faecbb3", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Discussion on Future Directions"], ["subsection", "Learning Knowledge from Broader Sources"], ["paragraph", "Leveraging Knowledge from Pre-trained Language Models"]], "content": "Pre-trained language models can learn a substantial amount of in-depth knowledge from data without any access to an external memory, as a parameterized implicit knowledge base~.\nHowever, as mentioned in~, directly fine-tuning pre-trained language generation models on the story generation task still suffers from \ninsufficient knowledge by representing the input text thorough a pre-trained encoder,\nleading to repetition, logic conflicts, and lack of long-range coherence in the generated output sequence.\nTherefore, discovering knowledge from pre-trained language models can be more flexible, such as knowledge distillation, data augmentation, and using pre-trained models as external knowledge~. More efficient methods of obtaining knowledge from pre-trained language models are expected.\n\\vspace{-0.05in}", "cites": [8623, 3171, 9, 7225], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of pre-trained language models as implicit knowledge sources by connecting ideas from multiple papers, such as the limitations in fine-tuning and alternative approaches like knowledge distillation. It provides some critical evaluation by pointing out the shortcomings of fine-tuning and the potential of more efficient knowledge extraction methods, but the critique remains surface-level. The section abstracts beyond individual papers by discussing broader strategies for knowledge acquisition, though it does not fully develop overarching theoretical principles."}}
{"id": "074792d8-0bcd-4698-aece-0902f744c1a0", "title": "Learning Knowledge in a Continuous Way", "level": "subsection", "subsections": [], "parent_id": "918c7c6b-1917-4b0e-a06c-ecaf632c2d80", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Discussion on Future Directions"], ["subsection", "Learning Knowledge in a Continuous Way"]], "content": "A machine learning is expected to learn continuously, accumulate the knowledge learned in previous tasks, and use it to assist future learning. This research direction is referred as lifelong learning~. In the process, the intelligent machine becomes more and more knowledgeable and effective at learning new knowledge. To make an analogy, humans continuously acquire new knowledge and constantly update the knowledge system in the brain. However, existing knowledge-enhanced text generation systems usually do not keep updating knowledge in real time (e.g., knowledge graph expansion). \nA meaningful exploration of was discussed in~. They built a general knowledge learning engine for chatbots to enable them to continuously and interactively learn new knowledge during conversations.\nTherefore, it is a promising research direction to \ncontinuously update knowledge obtained from various information sources, empowering intelligent machines with incoming knowledge and improving the performance on new text generation tasks.\n\\vspace{-0.05in}", "cites": [3172], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section introduces the concept of lifelong learning and provides a basic synthesis by connecting human-like continuous learning to chatbot systems, referencing one paper to support the idea. While it offers some abstraction by identifying a broader principle of continuous knowledge acquisition, it lacks deep critical analysis or comparison with alternative approaches. The narrative is coherent but limited in scope and depth."}}
{"id": "f3d88e56-31ac-4e40-b641-96e8e809dcc9", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "4d48ae73-8c5b-48f8-b370-bc45fe5245eb", "prefix_titles": [["title", "A Survey of Knowledge-Enhanced Text Generation"], ["section", "Appendix"], ["subsection", "Evaluation Metrics"]], "content": "\\vspace{0.05in}\n\\noindent \\textbf{BLEU-$m$ (short as B-$m$):} BLEU is a weighted geometric mean of $n$-gram precision scores.\n\\vspace{0.05in}\n\\noindent \\textbf{ROUGE-$m$ (short as R-$m$):} ROUGE measures the overlap of n-grams between the reference and hypothesis; ROUGE-L measures the longest matched words using longest common sub-sequence.\n\\vspace{0.05in}\n\\noindent \\textbf{Distinct-$k$ (short as D-k):} Distinct measures the total number of unique $k$-grams normalized by the total number of generated $k$-gram tokens to avoid favoring long sentences.\n\\begin{table}\n\\caption{Leaderboard performance on ten knowledge-enhanced generation benchmarks.}\n\\vspace{-0.1in}\n\\begin{subtable}[t]{1.0\\textwidth}\n\\caption{Leaderboard performance on two summarization benchmark datasets with different knowledge-enhanced NLG methods. Evaluation metrics are standard n-gram based metrics: ROUGE-2 and ROUGE-L.}\n\\vspace{-0.1in}\n\\begin{center}\n\\scalebox{0.83}{\\begin{tabular}{|l|c|l|l|cc|cc|l|}\n\\hline\n{\\multirow{2}*{Methods}} & \\multirow{2}*{Ref.} & Knowledge & Method & \\multicolumn{2}{c|}{\\textbf{CNN/DM}} & \n\\multicolumn{2}{c|}{\\textbf{Gigaword}} & \\\\\n & & source & category & R-2 & R-L & R-2 & R-L & \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{9}{|c|}{Baseline methods (w/o KG)} \\\\ \\hline\nSeq2Seq &  & & & 11.81 & 28.83 & 11.32 & 26.42 & with attention mechanism \\\\\nPG &  & & & 15.66 & 33.42 & 17.63 & 33.66 & w/o coverage mechanism \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{9}{|c|}{Knowledge enhanced methods} \\\\ \\hline\nVHTM &  & Topic & M3 & 18.05 & 37.18 & - & - & -  \\\\\nSELECTOR &  & Keyword & M2 & 18.31 & - & - & - & * Improve generation diversity \\\\\nFASUM &  & OpenKG & - & 17.84 & 37.40 & - & - & * Improve factual correctness \\\\\nKIGN &  & Keyword & M1 & 17.12 & 35.68 & 17.93 & 34.44 & -  \\\\\nBottomUp &  & Keyword & M2 & 18.68 & 38.34 & 17.61 & 33.54 & - \\\\\nTGVAE &  & Topic & M3 & - & - & 17.27 & 33.02 & - \\\\\nHierDualPG &  & Keyword & M2 & - & - & 18.06 & 34.39 & - \\\\\nR$^3$Sum &  & Text & M1 & - & - & 19.03 & 34.46 & -  \\\\\nBiSET &  & Text & M1 & - & - & \\textbf{19.78} & \\textbf{36.87} & - \\\\ \nSemSUM &  & DepGraph & - & - & - & 19.75 & 36.09 & - \\\\ \nASGARD &  & OpenKG & - & \\textbf{20.37} & \\textbf{40.48} & - & - & - \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\label{tab:quant-kg}\n\\end{subtable}\n\\begin{subtable}[t]{1.0\\textwidth}\n\\begin{minipage}{0.48\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on $\\alpha$NLG-ART dataset. Both B-4 and R-L are commonly used.}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|l|c|c|} \n\\hline\nMethod & Ref. & Source & B-4 & R-L \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Baseline methods} \\\\\n\\hline  \nSeq2Seq &  & - & 2.37 & 22.30 \\\\\n\\hline  \nGPT-2 &   & - & 9.80 & 32.90 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nGPT-COMeT &  & KG & 9.62 & 32.88 \\\\\n\\hline \nGRF &  & KG & \\textbf{11.62} & \\textbf{34.62} \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\hspace{0.15in}\n\\begin{minipage}{0.47\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on ComVE dataset. Both B-4 and R-L are commonly used.}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|l|c|c|} \n\\hline\nMethod & Ref. & Source & B-4 & R-L \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Baseline methods} \\\\\n\\hline  \nSeq2Seq &  & - & 6.10 & 25.80 \\\\\n\\hline  \nGPT-2 &   & - & 15.70 & 36.50 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nCE-PR &  & KG & 17.10 & 37.90 \\\\\n\\hline \nGRF &  & KG & \\textbf{17.19} & \\textbf{38.10} \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\end{subtable}\n\\begin{subtable}[t]{1.0\\textwidth}\n\\begin{minipage}{0.48\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on CommonGen dataset. SPICE is the primary evaluation metric.}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|l|c|c|} \n\\hline\nMethod & Ref. & Source & B-4 & SPICE \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Baseline methods} \\\\\n\\hline  \nBART &  & - & 31.83 & 27.99 \\\\\n\\hline  \nT5 &   & - & 31.96 & 28.86 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nEKI-BART &  & Text & 35.95 & 29.59 \\\\\n\\hline \nKG-BART &  & KG & 33.87 & 29.63 \\\\\n\\hline \nRE-T5 &  & Text & \\textbf{40.87} & \\textbf{31.08} \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\hspace{0.15in}\n\\begin{minipage}{0.47\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on Holl-E (mix-short setting) dataset. R-L are the primary metric.}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|l|c|c|} \n\\hline\nMethod & Ref. & Source & R-L & B-4 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Baseline methods} \\\\\n\\hline  \nSeq2Seq &  & - & 21.48 & 5.26 \\\\\n\\hline  \nBiDAF &   & - & 35.09 & 27.44 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nAKGCM &  & KG & 34.72 & \\textbf{30.84} \\\\\n\\hline \nRefNet &  & Text & 36.17 & 29.38 \\\\\n\\hline \nGLKS &  & Text & \\textbf{39.63} & - \\\\\n\\hline\n\\end{tabular}}\n\\end{minipage}\n\\end{subtable}\n\\label{tab:leaderboard}\n\\end{table}\n\\begin{table}\n\\ContinuedFloat\n\\vspace{-0.1in}\n\\begin{subtable}[t]{1.0\\linewidth}\n\\begin{minipage}{0.48\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on Wizard of Wikipedia with seen (S) and unseen (UnS) test set. }\n\\vspace{-0.05in}\n\\scalebox{0.85}{\\begin{tabular}{|l|c|c|c|} \n\\hline\nMethod & Ref. & R-1/R-2 (S) & R-1/R-2 (UnS) \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Baseline methods} \\\\\n\\hline  \nTransformer &  & 17.8/ --- & 14.0/ --- \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nMemNet &  & 16.9/ --- & 14.4/ --- \\\\\n\\hline \nPostKS &  & 18.1/5.3 & 13.5/2.0 \\\\\n\\hline \nSKT &  & 19.3/6.8 & 16.1/4.2 \\\\\n\\hline \nPIPM+KDBTS &  & \\textbf{19.9}/\\textbf{7.3} & \\textbf{17.6}/\\textbf{5.4} \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\hspace{0.15in}\n\\begin{minipage}{0.47\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{State-of-the-art performance on SQuAD.}\n\\vspace{-0.05in}\n\\scalebox{0.87}{\\begin{tabular}{|l|c|l|c|} \n\\hline\nMethod & Ref. & Source & B-4 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Baseline methods} \\\\\n\\hline  \nSeq2Seq &  & - & 3.01 \\\\\n\\hline  \nTransformer &   & - & 3.09 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nNQG++ &  & LF & 13.27 \\\\\n\\hline \nSELECTOR &  & LF+Keyword & 15.87 \\\\\n\\hline \nG2S+BERT &  & LF+DepGraph & 17.49 \\\\\n\\hline \nG2S+BERT+RL &  & LF+DepGraph & \\textbf{18.30} \\\\\n\\hline\n\\end{tabular}}\n\\end{minipage}\n\\end{subtable}\n\\begin{subtable}[t]{1.0\\linewidth}\n\\begin{minipage}{0.48\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Leaderboard performance on ELI5 dataset. The Kilt R-L (KRL) is the primary evaluation metric.}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|l|c|c|} \n\\hline\nMethod & Ref. & Source & KRL & R-L \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Baseline methods} \\\\\n\\hline  \nT5 &  & - & 0.0 & 19.1 \\\\\n\\hline  \nBART &  & - & 0.0 & 20.1 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nRAG &  & Text & 1.7 & 17.4 \\\\\n\\hline \nBART+DPR &  & Text & 1.9 & 17.4 \\\\\n\\hline \nRT+\\textsc{c}-REALM &  & Text & \\textbf{2.4} & \\textbf{23.2} \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\hspace{0.15in}\n\\begin{minipage}{0.47\\linewidth}\n\\centering\n\\vspace{0.05in}\n\\caption{Some state-of-the-art performance on PersonaChat dataset (no leaderboard on this dataset).}\n\\vspace{-0.05in}\n\\scalebox{0.9}{\\begin{tabular}{|l|c|c|c|} \n\\hline\nMethod & Ref. & B-1/B-2 & D-1/D-2\\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Baseline methods} \\\\\n\\hline  \nSeq2Seq &  & 18.2/9.3 & 2.6/7.4 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{4}{|c|}{Knowledge-enhanced methods} \\\\\n\\hline \nMemNet(soft) &  & 17.7/9.1 & 3.5/9.6 \\\\\n\\hline \nMemNet(hard) &  & 18.6/9.7 & 3.7/9.9 \\\\\n\\hline \nPostKS &  & 19.0/9.8 & \\textbf{4.6}/\\textbf{13.4} \\\\\n\\hline \nPEE &  & \\textbf{23.2}/\\textbf{11.5} & - / - \\\\\n\\hline \n\\end{tabular}}\n\\end{minipage}\n\\end{subtable}\n\\end{table}\n\\begin{table*}[t]\n\\caption{A list of representative open-source knowledge-enhanced text generation systems.}\n\\vspace{-0.15in}\n\\begin{center}\n\\scalebox{0.72}{\\begin{tabular}{|l|c|l|c|l|}\n\\hline\n\\multirow{2}*{Task} & \\multirow{2}*{Ref.} &  \\multirow{2}*{Paper title and open source code/toolkit} & Programming & Venue \\\\\n& & & language & \\& Year \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Topic-enhanced methods} \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Summarization}} & \\multirow{2}*{} & Topic-Aware Convolutional Neural Networks for Extreme Summarization & \\multirow{2}*{PyTorch} & EMNLP \\\\\n& & ------ Code: \\url{https://github.com/EdinburghNLP/XSum} &  & 2018 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Friendly Topic Assistant for Transformer Based Abstractive Summarization & \\multirow{2}*{-} & EMNLP \\\\\n& & ------ Code: \\url{https://github.com/BoChenGroup/TA} & & 2020 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Keyword-enhanced methods} \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Dialogue \\\\ system}} & \\multirow{2}*{} & A Content-Introducing Approach to Generative Short-Text Conversation & \\multirow{2}*{Tensorflow} & COLING \\\\\n& & ------ Code: \\url{https://github.com/MaZhiyuanBUAA/Seq2BFforDialogueGeneration} &  & 2016 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Emotional Chatting Machine: Emotional Conversation Generation with & \\multirow{2}*{PyTorch} & AAAI \\\\\n& & Internal and External Memory ------ Code: \\url{https://github.com/loadder/ECM-tf} & & 2018 \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Summarization}} & \\multirow{2}*{} & Coherent Comment Generation with a Graph-to-Sequence Model & \\multirow{2}*{PyTorch} & ACL \\\\\n& & ------ Code: \\url{https://github.com/lancopku/Graph-to-seq-comment-generation} &  & 2018 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{KB-enhanced methods} \\\\\n\\hline \\hline\n\\multirow{10}*{\\makecell[l]{Dialogue \\\\ system}} & \\multirow{2}*{} & Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End & \\multirow{2}*{PyTorch} & ACL \\\\\n& & Dialog Systems ------ Code: \\url{https://github.com/HLTCHKUST/Mem2Seq} &  & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Global-to-local Memory Pointer Networks for Task-Oriented Dialogue & \\multirow{2}*{PyTorch} & ICLR \\\\\n& & ------ Code: \\url{https://github.com/jasonwu0731/GLMP} & & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Improving Knowledge-aware Dialogue Generation via Knowledge Base & \\multirow{2}*{PyTorch} & AAAI \\\\\n& & Question Answering ------ Code: \\url{https://github.com/siat-nlp/TransDG} & & 2020 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Diverse and Informative Dialogue Generation with Context-Specific Knowledge & \\multirow{2}*{Tensorflow} & ACL \\\\\n& & Awareness ------ Code: \\url{https://github.com/pku-sixing/ACL2020-ConKADI} & & 2020 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & TopicKA: Generating Commonsense Knowledge-Aware Dialogue Responses  & \\multirow{2}*{Tensorflow} & IJCAI \\\\\n& & ------ Code: \\url{https://github.com/pku-sixing/IJCAI2020-TopicKA} & & 2020 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{KG-enhanced methods} \\\\\n\\hline \\hline\n\\multirow{6}*{\\makecell[l]{Dialogue \\\\ system}} & \\multirow{2}*{} & Commonsense Knowledge Aware Conversation Generation with Graph & \\multirow{2}*{Tensorflow} & IJCAI \\\\\n& & Attention ------ Code: \\url{https://github.com/thu-coai/ccm} &  & 2018 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic & \\multirow{2}*{Tensorflow} & EMNLP \\\\\n& &  Knowledge Graphs ------ Code: \\url{https://github.com/Pascalson/DyKGChat} & & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Grounded Conversation Generation as Guided Traverses in Commonsense & \\multirow{2}*{PyTorch} & ACL \\\\\n& & Knowledge Graphs ------ Code: \\url{https://github.com/thunlp/ConceptFlow} & & 2020 \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Scientific \\\\ writing}} & \\multirow{2}*{} & Text Generation from Knowledge Graphs with Graph Transformers & \\multirow{2}*{PyTorch} & NAACL \\\\\n& & ------ Code: \\url{https://github.com/rikdz/GraphWriter} & & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & PaperRobot: Incremental Draft Generation of Scientific Ideas & \\multirow{2}*{PyTorch} & ACL \\\\\n& & ------ Code: \\url{https://github.com/EagleW/PaperRobot} & & 2019 \\\\\n\\hline \\hline\n\\multirow{8}*{\\makecell[l]{Commonsense \\\\ reasoning \\\\ \\&\\\\ Story \\\\ generation }} & \\multirow{2}*{} & Story Ending Generation with Incremental Encoding and Commonsense & \\multirow{2}*{Tensorflow} & AAAI \\\\\n& & Knowledge ------ Code: \\url{https://github.com/JianGuanTHU/StoryEndGen} & & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Language Generation with Multi-Hop Reasoning on Commonsense  & \\multirow{2}*{PyTorch} & EMNLP \\\\\n& & Knowledge Graph ------ Code: \\url{https://github.com/cdjhz/multigen} & & 2020 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & KG-BART: Knowledge Graph-Augmented BART for Generative   & \\multirow{2}*{PyTorch} & AAAI \\\\\n& & Commonsense Reasoning ------ Code: \\url{https://github.com/yeliu918/KG-BART} & & 2021 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & ENT-DESC: Entity Description Generation by Exploring Knowledge Graph & \\multirow{2}*{MXNet} & EMNLP \\\\\n& & ------ Code: \\url{https://github.com/LiyingCheng95/EntityDescriptionGeneration} & & 2020 \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Question \\\\ answering}} & \\multirow{2}*{} & Commonsense for Generative Multi-Hop Question Answering Tasks & \\multirow{2}*{Tensorflow} & EMNLP \\\\\n& & ------ Code: \\url{https://github.com/yicheng-w/CommonSenseMultiHopQA} & & 2018 \\\\\n\\hline \\hline\n\\rowcolor{gray!12}\\multicolumn{5}{|c|}{Ground text-enhanced methods} \\\\ \n\\hline \\hline\n\\multirow{8}*{\\makecell[l]{Dialogue \\\\ system}} & \\multirow{2}*{} & Wizard of Wikipedia: Knowledge-Powered Conversational agents & \\multirow{2}*{PyTorch} & ICLR \\\\\n& & ------ Code: \\url{https://github.com/facebookresearch/ParlAI} &  & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Conversing by Reading: Contentful Neural Conversation with On-demand & \\multirow{2}*{PyTorch} & ACL \\\\\n& & Machine Reading ------ Code: \\url{https://github.com/qkaren/converse_reading_cmr} & & 2019 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue & \\multirow{2}*{Tensorflow} & ICLR \\\\\n& & ------ Code: \\url{https://github.com/bckim92/sequential-knowledge-transformer} & & 2020 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & RefNet: A Reference-aware Network for Background Based  & \\multirow{2}*{Tensorflow} & AAAI \\\\\n& & Conversation ------ Code: \\url{https://github.com/ChuanMeng/RefNet} & & 2020 \\\\\n\\hline \\hline\n\\multirow{2}*{\\makecell[l]{Summarization}} & \\multirow{2}*{} & BiSET: Bi-directional Selective Encoding with Template for & \\multirow{2}*{PyTorch} & ACL \\\\\n& & Abstractive Summarization ------ Code: \\url{https://github.com/InitialBug/BiSET} & & 2019 \\\\\n\\hline \\hline\n\\multirow{4}*{\\makecell[l]{Question \\\\ answering}} & \\multirow{2}*{} & Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks & \\multirow{2}*{PyTorch} & Neurips \\\\\n& & ------ Code in \\url{https://github.com/huggingface/transformers} & & 2020 \\\\\n\\cline{2-5}\n& \\multirow{2}*{} & KILT: a Benchmark for Knowledge Intensive Language Tasks & \\multirow{2}*{PyTorch} & NAACL \\\\\n& & ------ Code: \\url{https://github.com/facebookresearch/KILT} & & 2021 \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\label{tab:code}\n\\end{table*}\n\\end{document}\n\\endinput", "cites": [457, 3154, 3117, 3150, 1139, 3119, 7186, 3162, 3121, 3152, 38, 3127, 3151, 1934, 3118, 2369, 3130, 3173, 2401, 3143, 8554, 3128, 3115, 1071, 3139, 3159, 8626, 3126, 9, 7225, 3120, 8627, 3163, 8415, 3165], "cite_extract_rate": 0.6862745098039216, "origin_cites_number": 51, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a list of evaluation metrics and compares the performance of various knowledge-enhanced and baseline methods on multiple datasets. However, it lacks a coherent narrative or synthesis of underlying themes, and does not critically evaluate the effectiveness or limitations of the metrics or methods. The analysis remains mostly at the level of performance comparison without broader abstraction or insight."}}
