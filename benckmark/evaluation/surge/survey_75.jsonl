{"id": "6716f171-5279-4be6-829f-92b9ec4bb350", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "36832e43-191f-4e88-aa11-6c7b91fe1bf8", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:Introduction}\nUsers are increasingly recording their daily activities, sharing interesting experiences, and expressing personal viewpoints using mobile devices on social networks, such as Twitter\\footnote{\\url{https://twitter.com}}, Facebook\\footnote{\\url{https://www.facebook.com}}, and Weibo\\footnote{\\url{https://www.weibo.com}}, \\textit{etc}. As a result, a rapidly growing volume of multimedia data (\\textit{i.e.}, image, music, and video) has been generated, as shown in Figure~\\ref{fig:multimedia}, which results in a great demand for the management, retrieval, and understanding of these data. Most existing work on multimedia analysis focus on the cognitive aspects, \\textit{i.e.}, understanding the objective content, such as object detection in images~, speaker recognition in speech~, and action recognition in videos~. Since what people feel have a direct influence on their decision making, affective computing (AC) of these multimedia data is of significant importance and has attracted increasing attention . For example, companies would like to know how customers evaluate their products and can thus improve their services~; depression and anxiety detection from social media can help understand psychological distress and thus potentially prevent suicidal actions~.\nWhile the sentiment analysis in text~ has long been a standard task, AC from other modalities, such as image and video, has just begun to be considered recently. In this article, we aim to review the existing AC technologies comprehensively for large-scale heterogeneous multimedia data, including image, music, video, and multimodal data.\nAffective computing of multimedia (ACM) aims to recognize the emotions that are expected to be evoked in viewers by a given stimuli. Similar to other supervised learning tasks, ACM is typically composed of three steps: data collection and annotation, feature extraction, and mapping learning between features and emotions~. One main challenge for ACM is the affective gap, \\textit{i.e.}, ``the lack of coincidence between the features and the expected affective state in which the user is brought by perceiving the signal''~. In the early stage, various hand-crafted features were designed to bridge this gap with traditional machine learning algorithms, while more recently researchers have focused on end-to-end deep learning from raw multimedia data to recognize emotions. Existing ACM methods mainly assign the dominant (average) emotion category (DEC) to an input stimuli, based on the assumption that different viewers have similar reactions to the same stimuli. We can usually formulate this task as a single-label learning problem.\nHowever, emotions are influenced by subjective and contextual factors, such as the educational background, cultural diversity, and social interaction~. As a result, different viewers may react differently to the same stimuli, which creates the subjective perception challenge. Therefore, the perception inconsistency makes it insufficient to simply predict the DEC for the highly subjective variable. As stated in~, we can perform two kinds of ACM tasks to deal with the subjectivity challenge: predicting personalized emotion perception for each viewer and assigning multiple emotion labels for each stimuli. For the latter one, we can either assign multiple labels to each stimuli with equal importance using multi-label learning methods, or predict the emotion distributions which tries to learn the degrees of each emotion~.\nIn this article, we concentrate on surveying the existing methods on ACM and analyzing potential research trends. Section~\\ref{sec:EmotionModels} introduces the widely-used emotion representation models from psychology. Section~\\ref{sec:Datasets} summarizes the existing available datasets for evaluating ACM tasks. Section~\\ref{sec:Image}, Section~\\ref{sec:Music}, Section~\\ref{sec:Video}, and  Section~\\ref{sec:multimodal} survey the representative methods on AC of images, music, videos, and multimodal data, respectively, including both handcrafted features-based methods and deep learning methods. Section~\\ref{sec:FutureDirections} provides some suggestions for future research, followed by conclusion in Section~\\ref{sec:Conclusion}.\nTo the best of our knowledge, this article is among the first that provide a comprehensive survey of affective computing of multimedia data from different modalities. Previous surveys mainly focus on a single modality, such as images~, speech~, music~, video~, and multimodal data~. From this survey, readers can more easily compare the correlations and differences among different AC settings. We believe that this will be instrumental in generating novel research ideas.", "cites": [1016, 1015], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The introduction section provides an analytical overview of affective computing for multimedia data, discussing key challenges such as the affective gap and subjective perception. It integrates information from cited papers to highlight the evolution from handcrafted features to deep learning, and introduces different ACM tasks (e.g., multi-label learning, emotion distribution prediction). While it does not deeply critique individual papers, it offers a structured framework for understanding the broader field and identifies research trends."}}
{"id": "31b031e5-4c1a-41d6-91e4-966f262d426b", "title": "Datasets for AC of Images", "level": "subsection", "subsections": [], "parent_id": "21f5b1a5-8481-4ed1-83d5-a0396cc43cfa", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Datasets"], ["subsection", "Datasets for AC of Images"]], "content": "\\label{sec:ImageDatasets}\nThe early datasets for AC of images mainly come from the psychology community with small-scale images. The International Affective Picture System (\\textbf{IAPS}) is an image set that is widely used in psychology to evoke emotions~. Each image that depicts complex scenes is associated with the mean and standard deviation (STD) of VAD ratings in a 9-point scale by about 100 college students. The \\textbf{IAPSa} dataset is selected from IAPS with 246 images~, which are labeled by 20 undergraduate students. The \\textbf{Abstract} dataset consists of 279 abstract paintings without contextual content. Approximately 230 people peer rated these paintings. The Artistic dataset (\\textbf{ArtPhoto}) includes 806 artistic photographs from a photo sharing site~ with emotions determined by the artist uploading the photos. The Geneva affective picture database (\\textbf{GAPED}) is composed of 520 negative, 121 positive, and 89 neutral images~. Besides, these images are also rated with valence and arousal values, ranging from 0 to 100 points. There are 500 abstract paintings in both \\textbf{MART} and \\textbf{devArt} datasets, which are collected from the Museum of Modern and Contemporary Art of Trento and Rovereto~, and the ``DeviantArt'' online social network~, respectively.\n\\begin{table*}[!t]\n\\centering\\scriptsize\n\\caption{Released and freely available datasets for AC of images, where `Ref' is short for Reference, `\\# Images' and `\\# Annotators' respectively represent the total number of images and annotators (f: female, m: male), `Labeling' represents the method to obtain labels, such as human annotation (annotation) and keyword searching (keyword), and `Labels' means the detailed labels in the dataset, such as dominant emotion category (dominant), average dimension values (average), personalized emotion (personalized), and emotion distribution (distribution).}\n\\begin{tabular}\n{cccccccc}\n\\hline\n\\textbf{Dataset} & \\textbf{Ref} & \\textbf{\\# Images} & \\textbf{Type} & \\textbf{\\# Annotators}& \\textbf{Emotion model} & \\textbf{Labeling} & \\textbf{Labels}\\\\\n\\hline\nIAPS &   & 1,182 & natural & $\\approx$100 (half f) & VAD & annotation  & average  \\\\\nIAPSa &  & 246 & natural & 20 (10f,10m) & Mikels  &  annotation  & dominant \\\\\nAbstract &  & 279 & abstract & $\\approx$230 & Mikels   &  annotation  & dominant \\\\\nArtPhoto &  & 806 & artistic & -- & Mikels &  keyword &  dominant \\\\\nGAPED &  & 730 & natural & 60  & Sentiment, VA  & annotation  & dominant, average \\\\\nMART &  & 500 & abstract & 25 (11f,14m) & Sentiment   & annotation  &  dominant\\\\\ndevArt &  & 500 & abstract & 60 (27f,33m) & Sentiment   & annotation  &  dominant\\\\\nTweet &  & 603 & social & 9 & Sentiment  & annotation  & dominant \\\\\nFlickrCC &  & $\\approx$500,000 & social & -- & Plutchik   & keyword  & dominant \\\\\nFlickr &  & 301,903 & social & 6,735 & Ekman  & keyword  & dominant  \\\\\nEmotion6 &  & 1,980 & social & 432 & Ekman+neutral  & annotation  &  distribution \\\\\nFI &  & 23,308 & social & 225 & Mikels  & annotation  &  dominant \\\\\nIESN &  & 1,012,901 & social & 118,035 & Mikels, VAD  &  keyword & personalized \\\\\nFlickrLDL &  & 10,700 & social & 11 & Mikels  & annotation &  distribution \\\\\nTwitterLDL &  & 10,045 & social & 8 & Mikels  &  annotation &  distribution \\\\\n\\hline\n\\end{tabular}\n\\label{tab:ImageDataset}\n\\end{table*}\nRecent datasets, especially the large-scale ones, are constructed using images from social networks. The Tweet dataset (\\textbf{Tweet}) consists of 470 and 113 tweets for positive and negative sentiments, respectively~. The \\textbf{FlickrCC} dataset includes about 500k Flickr creative common (CC) images which are generated based on 1,553 adjective noun pairs (ANPs)~. The images are mapped to the Plutchnik's Wheel of Emotions with 8 basic emotions, each with 3 scales. The \\textbf{Flickr} dataset contains about 300k images~ with the emotion category defined by the synonym word list which has the most same words as the adjective words of an image's tags and comments. The \\textbf{FI} dataset consists of 23,308 images which are collected from Flicker and Instagram by searching the emotion keywords~ and labeled by 225 Amazon Mechanical Turk (MTurk) workers. The number of images in each Mikels emotion category is larger than 1,000. The \\textbf{Emotion6} dataset~ consists of 1,980 images collected from Flickr with 330 images for each Ekman's emotion category. Each image was scored by 15 MTurk workers to obtain the discrete emotion distribution information. The \\textbf{IESN} dataset that is constructed for personalized emotion prediction  contains about 1M images from Flickr. Lexicon-based methods and VAD averaging~ are used to segment the text of metadata from uploaders for expected emotions and comments from viewers for personalized emotions. There are 7,723 active users with more than 50 involved images.  We can also easily obtain the DEC and emotion distribution for each image. \\textbf{FlickrLDL} and \\textbf{TwitterLDL} datasets~ are constructed for discrete emotion distribution learning. The former one is a subset of FlickrCC, which are labeled by 11 viewers. The latter one consists of 10,045 images which are collected by searching various sentiment key words from Twitter and labeled by 8 viewers. These datasets are summarized in Table~\\ref{tab:ImageDataset}.", "cites": [1017], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of image datasets for affective computing, listing their sources, sizes, labeling methods, and emotional models. It integrates some information from the cited papers but lacks deeper synthesis or comparative insights. There is minimal critical evaluation or abstraction to broader trends, keeping the narrative largely descriptive."}}
{"id": "d6fc556f-5306-4560-b61f-84603258d71a", "title": "Datasets consisting of videos only", "level": "subsubsection", "subsections": [], "parent_id": "d278bc85-2fef-47c9-b4d6-d1b6bb157aad", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Datasets"], ["subsection", "Datasets for AC of Videos"], ["subsubsection", "Datasets consisting of videos only"]], "content": "The LIRIS-ACCEDE dataset~ is one of the largest datasets in this area. Because it is collected under Creative Commons licenses, there are no copyright issues. The LIRIS-ACCEDE dataset is a living database in development. In order to fulfill requirements of different tasks, new data, features and tags are included. The LIRIS-ACCEDE dataset includes the Discrete LIRIS-ACCEDE collection and the Continuous LIRIS-ACCEDE collection in 2015 and was used for the MediaEval Emotional Impact of Movies tasks from 2015 to 2018.\nThe Discrete LIRIS-ACCEDE collection~ includes 9,800 clips, which is derived from 40 feature films and 120 short films. Specifically, the majority of the 160 films are collected from the video platform VODO. The duration of all 160 films is about 73.6 hours in total. All of the 9,800 video clips last about 27 hour in total and the duration of each clip is between 8 to 12 seconds, which is long enough for viewers to feel emotions.\nIn this collection, all the 9,800 video clips are labeled by values of valence and arousal.\nThe Continuous LIRIS-ACCEDE collection~ differs from the Discrete LIRIS-ACCEDE collection in annotation type. Roughly speaking, the annotations for movie clips in the Discrete  LIRIS-ACCEDE collection are global. It means that a whole 8 to 12 second video clip is represented by a single value of valence and arousal. This annotation type limits the possiblity for tracking emotions. To address this issue,  30 longer films are selected from the 160 films mentioned above. The total duration of all the selected films is about 7.4 hours. There are  emotional annotations according to valence and arousal of each second of the films in the collection.\nThe MediaEval Affective Impact of Movies collections between 2015 and 2018 are used for the MediaEval affective Impact of Movies tasks in each corresponding year. Specifically, the MediaEval 2015 Affective Impact of Movies~ includes two sub-tasks: affect detection and violence detection. The Discrete LIRIS-ACCEDE collection was used as the development set. And 1,100 additional video clips were extracted from 39 new movies and included. Indeed, all the new collected data were shared under Creative Commons licenses. In addition, three values were used to label the 10,900 video clips: a binary signal representing the presence of violence, a class tag of the excerpt for felt arousal and an annotation for felt valence.\nThe MediaEval 2016 Affective Impact of Movies Task~ also includes two sub-tasks: Global emotion prediction and Continuous emotion prediction. The Discrete LIRIS-ACCEDE collection and the Continuous LIRS-ACCEDE collection were used as the development sets for the first and second sub-tasks, respectively. In addition, 49 new movies were chosen as the test sets. 1,200 short video clips from the new movies were extracted for the first task, and 10 long movies were selected for the second task. For the first sub-task, the tags include scores of valence and arousal for each whole movie clip. And for the second sub-task, scores of valence and arousal for each second of the movies are evaluated.\n\\label{sec:VideoDatasets}\nThe MediaEval 2017 Affective Impact of Movies Task~ is focused on long movies for two sub-tasks: valence/arousal prediction and Fear prediction. The Continuous LIRIS-ACCEDE collection was selected as the development set, and an additional 14 new movies were collected as the test set. The annotations contain a valence value and an arousal value. In addition, there are a binary value to represent whether the segment is supposed to induce fear or not for each 10-second segment.\nThe MediaEval 2018 Affective Impact of Movies task~ is also dedicated to valence/arousal prediction and fear prediction. The Continuous LIRIS-ACCEDE collection and the test set of the MediaEval 2017 Emotional Impact of Movies task were used as the development set. In addition, 12 other movies selected from the set of the 160 movies mentioned in the Discrete LIRIS-ACCEDE part were used as test set. Specifically, for the first sub-task, there are annotations containing valence and arousal values for each second of the movies. And the beginning and ending times of each sequence in movies that induce fear are recorded for the second sub-task. \nThe VideoEmotion dataset~ is a well-designed user-generated video collection. It contains 1,101 videos downloaded from web platforms, such as YouTube and Flickr. The annotations of the videos in this dataset are based on Plutchik's wheel of emotions~.\nBoth the YF-E6 Dataset and the VideoStory-P14 Dataset are introduced in~. In order to collect the YF-E6 emotion dataset, six basic emotion types are used as keywords to search videos on YouTube and Flickr. There are 3,000 videos collected in the YF-E6 dataset totally. Then there were 10 annotators performing the labeling tasks. Only when all tags for a video clip were more than 50 percent consistent, the video clip was added to the dataset. Finally, the dataset includes 1,637 videos labeled with six basic emotion types. The VideoStory-P14 Dataset is based on the VideoStory dataset. Similar to the VideoEmotion  Dataset, the keywords in Plutchik's Wheel of Emotions were used for the search process of the construction of the VideoStory dataset. Finally, there are 626 videos in the videoStory-P14 dataset with each having a unique emotion tag.", "cites": [1018], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes various video datasets used in affective computing, such as LIRIS-ACCEDE and VideoEmotion, with some mention of their structure, labeling methods, and use in MediaEval tasks. It integrates minimal insights from the cited paper, focusing instead on factual data about the datasets. There is no critical evaluation of these datasets or their limitations, nor are broader patterns or principles abstracted from the information."}}
{"id": "7eba031f-c063-4798-bf20-621f78f3118a", "title": "Datasets including both videos and audience's reactions", "level": "subsubsection", "subsections": [], "parent_id": "d278bc85-2fef-47c9-b4d6-d1b6bb157aad", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Datasets"], ["subsection", "Datasets for AC of Videos"], ["subsubsection", "Datasets including both videos and audience's reactions"]], "content": "The DEAP dataset~ includes the EEG and peripheral physiological signals that are collected from 32 participants during watching 40 one-minute long excerpts of music videos. In addition, frontal face videos collected from 22 of the 32 participants are gathered. Annotators labeled each video according to the level of like/dislike, familiarity, arousal, valence, and dominance. Though the DEAP dataset is publicly available, it should be noted that it does not include the actual videos because of the licensing issues, but the links of videos are provided.\nThe MAHNOB-HCI~ is a multimodal dataset including multi-class information recorded in response to video affective stimuli. Particularly, speeches, face videos, and eye gazes are recorded. In addition, two experiments were conducted to record both peripheral and central nervous system physiological signals from 27 subjects. In the first experiment, subjects were assigned to report their emotional responses to 20 affective induced videos, including the level of arousal, valence and dominance, and predictability as well as emotion categories. In the second experiment, the participants evaluated whether they agreed with the displayed labels after watching short videos and images. The dataset is available for academic use through a web-interface.\n\\begin{table*}[!t]\n\\centering\\scriptsize\n    \\caption{Released and freely available datasets for video emotion recognition, where `\\#Clips' and `Hours' respectively represent the total number and hours of video clips, `Type' means the genre of the videos in the dataset, `Emotion model' represents the labeling type, `Labeling' represents the method to obtain labels, such as human annotation (annotation) and keyword searching (keyword), and `Labels' means the detailed labels in the dataset, such as dominant emotion category (dominant), average dimension values (average), personalized emotion (personalized), and emotion distribution (distribution).}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}\n{C{1.8 cm} cccccccc}\n\\hline\n\\textbf{Dataset} & \\textbf{Ref} & \\textbf{\\#Clips}& \\textbf{Hours} & \\textbf{Type} & \\textbf{\\# Annotators} & \\textbf{Emotion model} & \\textbf{Labeling} & \\textbf{Labels} \\\\\n\\hline\nDiscrete LIRIS-ACCEDE &  & 9,800 & 26.9 & film & - & VA & annotation & dominant \\\\\nContinuous LIRIS-ACCEDE &  & 30 & 7.4 & film & 10 (7f,3m) & VA & annotation & average \\\\\nMediaEval 2015 &  & 1,100  & - & film & - & 3 discrete VA values & annotation & dominant \\\\\nMediaEval 2016 &  & 1,210 & - & film & - & VA & annotation & distribution, average \\\\\nMediaEval 2017 &  & 14 & 8 & film & - & VA, fear & annotation & average \\\\\nMediaEval 2018 &  & 12 & 9 & film & - & VA, fear & annotation & average \\\\\nVideoEmotion &  & 1,101 & 32.7 & user-generated & 10 (5f,5m) & Plutchik & annotation & dominant \\\\\nYF-E6 &  & 1,637 & 50.9 & user-generated & 10(5f,5m) & Emkan & annotation & dominant \\\\\nVideoStory-P14 &  & 626 & - & user-generated & - & Plutchik & keyword & dominant \\\\\nDEAP &  & 120 & 2 & music video & - & VAD & annotation & personalized  \\\\\nMAHNOB-HCI &  & - & - & multiple types & - & VAD, Ekman+neutral & annotation & personalized \\\\\nDECAF &  & 76 & - & music video/movies & - & VAD & annotation & personalized \\\\\nAMIGOS &  & 20 & - & movies collection & - & VAD, Ekman & annotation & personalized \\\\\nASCERTAIN &  & 36 & - & movies collection & 58 (21f,37m) & VA & annotation & personalized \\\\\n\\hline\n\\end{tabular}\n}\n\\label{tab:VideoDataset}\n\\end{table*}\nThe DECAF dataset~ consists of  Infra-red facial video signals, Electrocardiogram (ECG), Magnetoencephalogram (MEG), horizontal Electrooculogram (hEOG) and Trapezius Electromyogram (tEMG), recorded from 30 participants watching 36 movie clips and 40 one-minute music videos, which are derived from the DEAP dataset~. The subjective feedback is based on valence, arousal, and dominance space. In addition, time-continuous emotion annotations for movie clips are also included in the dataset.\nThe AMIGOS dataset~ includes multi-class affective data, individual and groups of viewers' responses to both short and long videos. The EEG, ECG, GSR, frontal, and full body video were recorded in two experimental settings, \\textit{i.e.}, 40 participants watching 16 short emotional clips and 4 long clips. The duration of each selected short videos is between 51 and 150 seconds, and the duration of each long excerpt is about 20 minutes. Finally, participants annotated the affective level of valence, arousal, control, familiarity, liking, and basic emotions.\nBig-five personality scales and affective self-ratings of 58 users together with their EEG, ECG, GSR, and facial activity data were included in the ASCERTAIN dataset~ . The number of videos used as the stimulus is 36 and the length of each video clip is between 51 and 128 seconds. It is the first physiological dataset that is useful for both affective and personality recognition.\nThe publicly available datasets for video affective content analysis are summarized in Table~\\ref{tab:VideoDataset}.", "cites": [1018], "cite_extract_rate": 0.07692307692307693, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of datasets related to affective computing for videos, including their content, labeling methods, and available modalities. While it organizes the data in a table for clarity, it does not synthesize the information to highlight broader trends or connections between the datasets. There is minimal critical analysis or abstraction beyond the specific details of each dataset."}}
{"id": "72c21e7b-1086-4048-bb25-1bc86a5c6b31", "title": "Datasets for AC of Multimodal Data", "level": "subsection", "subsections": [], "parent_id": "21f5b1a5-8481-4ed1-83d5-a0396cc43cfa", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Datasets"], ["subsection", "Datasets for AC of Multimodal Data"]], "content": "\\label{sec:multimodalDatasets}\nIn addition to audiovisual content and viewers' reactions, other modalities, such as language, also contain significant information for affective understanding of multimedia content. \nVisual sentiment is the sentiment associated with the concepts depicted in images. Two datasets were developed through mining images associated with the adjective-noun pair (ANP) representations that have affective significance~. ANPs in  were generated by first using seed terms from Plutchik's Wheel of Emotion~ to query Flickr\\footnote{\\url{https://www.flickr.com}} and YouTube\\footnote{\\url{https://www.youtube.com/}}. After mining the tags associated with visual content on YouTube and Flickr, adjective and noun candidates were identified through part-of-speech tagging. Then adjective and nouns were paired to create ANP candidates which were  filtered by sentiment strength, named entities, and popularity. The  Visual Sentiment Ontology (VSO), \\footnote{\\url{https://visual-sentiment-ontology.appspot.com}}, is the results of this process. Sentibank resulted in the creation of a set of photo-tweet sentiment dataset, with both visual and textual data with polarity labels, collected on Amazon Mechanical Turk\\footnote{\\url{http://www.ee.columbia.edu/ln/dvmm/vso/download/twitter_dataset.html}}. This work was later extended to form a multilingual ANP set and its dataset, in~\\footnote{\\url{http://mvso.cs.columbia.edu}}, containing 15,630 ANPs from 12 major languages and 7.37M images~. My Reaction When (MRW) dataset contains 50,107 video-sentence pairs crawled from social media, depicting physical or emotional reactions to the situations described in sentences~. The GIFs are sourced from Giphy\\footnote{\\url{https://giphy.com/}}. Even though there is no emotional labels, the language and visual associations are mainly based on sentiment which makes this dataset an interesting resource for affective content analysis.\nCMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is a collection of multiple datasets for multimodal sentiment analysis and emotion recognition. This collection includes more than 23,500 sentence utterance videos from more than 1,000 people from YouTube~\\footnote{\\url{https://github.com/A2Zadeh/CMU-MultimodalSDK}}. All the videos are transcribed and aligned with audiovisual modalities. \nA multimodal multi-party dataset for emotion recognition in conversation (MELD) was primarily developed for emotion recognition in multiparty interaction purposes~. MELD contains visual, audio, and textual modalities and includes 13,000 utterances from 1,433 dialogues from the TV-series Friends, with each utterance labeled with emotion and sentiment. \n\\begin{table*}[!t]\n\\centering\\scriptsize\n    \\caption{Released and freely available datasets for multimodal multimodal emotion recognition. Disc. for MELD corresponds to six Ekman emotions in addition to neutral. `Labeling' represents the method to obtain labels, such as human annotation (annotation), self-reported felt emotion and keyword searching (keyword), `Labels' means the detailed labels in the dataset, such as dominant emotion category (dominant), average dimension values (average), personalized emotion (personalized), and emotion distribution (distribution).} \n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}\n{c c c c c c c c}\n\\hline\n\\textbf{Dataset} & \\textbf{Ref} & \\textbf{\\#Samples}& \\textbf{Modalities} & \\textbf{Type} & \\textbf{Emotion model} & \\textbf{Labeling}&\\textbf{Labels}\\\\\n\\hline\nSentiBank tweet &  & 603  & images, text & images & Sentiment&annotation& dominant\\\\\nMVSO &  & 7.36M  & image, metadata & photos & Sentiment &automatic & average\\\\\nCMU-MOSEI &  &  23,500  & video, audio, text & YouTube videos & Sentiment &annotation& average\\\\\nMELD &  &  13,000  & video, audio, text & TV series & Sentiment, Disc. &annotation& dominant\\\\\nCOGNIMUSE &  &  3.5h  & video, audio, text & movies & VA&annotation, self-report& dominant\\\\\nVR &  &  73  & video, audio & VR videos & VA&self-report&average \\\\\n\\hline\n\\end{tabular}\n}\n\\label{tab:MMDataset}\n\\end{table*}\nCOGNINMUSE is a collection of videos annotated with sensory and semantic saliency, events, cross-media semantics, and emotions~. A subset of 3.5h extracted from movies, including textual modality, are annotated on arousal and valence. \n\\citeauthor{Li_VR} collected a dataset of 360 degrees virtual reality videos that can elicit different emotions~. Even though the dataset consists of 73 short videos, on average 183s long, it is one of the first datasets of its kind whose content understanding stays limited. These multimodal datasets are summarized in Table~\\ref{tab:MMDataset}.", "cites": [1019, 1020, 1021, 7322], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of multimodal datasets for affective computing but lacks deeper synthesis of ideas across the cited works. It integrates some information, such as labeling methods and dataset types, but does not connect these to broader themes or trends in the field. The analysis is limited to surface-level descriptions and does not evaluate strengths, weaknesses, or implications of the datasets or their methodologies."}}
{"id": "6469fc7b-8c4d-423d-88c3-f522313e7cbb", "title": "Handcrafted Features-Based Methods for AC of Images", "level": "subsection", "subsections": [], "parent_id": "50a2810e-2777-44d0-85b6-12b160fc2d4d", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Images"], ["subsection", "Handcrafted Features-Based Methods for AC of Images"]], "content": "\\label{sec:ImageHandcrafted}\n\\textbf{Low-level Features} are difficult to be understood by viewers. These features are often directly derived from other computer vision tasks. Some widely extracted features include GIST, HOG2x2, self-similarity and geometric context color histogram features as in~, because of their individual power and distinct description of visual phenomena in a scene perspective.\nCompared with the above generic features, some specific features derived from art theory and psychology have been designed. For example, \\citeauthor{machajdik2010affective}~\\shortcite{machajdik2010affective} extracted elements-of-art features, including \\emph{color} and \\emph{texture}. The MPEG-7 visual descriptors are employed in~, which include four color-related ideas and two texture-related ideas. How shape features in natural images influence emotions is investigated in~ by modeling the concepts of roundness-angularity and simplicity-complexity. \\citeauthor{sartori2015s}~\\shortcite{sartori2015s} designed two kinds of visual features to represent different color combinations based on Itten's color wheel.\n\\textbf{Mid-level Features} contain more semantics, are more easily interpreted by viewers than low-level features, and thus are more relevant to emotions. \\citeauthor{patterson2012sun}~\\shortcite{patterson2012sun} proposed to detect 102 attributes in 5 different categories, including materials, surface properties, functions or affordances, spatial envelop attributes, and object presence. Besides these attributes, eigenfaces that may contribute to facial images are also incorporated in~. More recently, in~, SIFT features are first extracted as basic features, which are fed into bag-of-visual-words (BoVW) to represent the multi-scale blocks. Another mid-level representation is the latent topic distribution estimated by probabilistic latent semantic analysis.\nHarmonious composition is essential in an artwork. Several compositional features, such as low depth of field, are designed to analyze such characteristics of an image~. Based on the fact that figure-ground relationships, color patterns, shapes and their diverse combinations are often jointly employed by artists to express emotions in their artworks, \\citeauthor{wang2013interpretable}~\\shortcite{wang2013interpretable} proposed to extract interpretable aesthetic features.\nInspired by princiles-of-art, \\citeauthor{zhao2014exploring}~\\shortcite{zhao2014exploring} designed corresponding mid-level features, including \\emph{balance}, \\emph{emphasis}, \\emph{harmony}, \\emph{variety}, \\emph{gradation}, and \\emph{movement}. For example, Itten's color contrasts and the rate of focused attention are employed to measure \\textit{emphasis}.\n\\begin{table*}\n\\centering\\scriptsize\n\\caption{Summary of the hand-crafted features at different levels for AC of images. `\\# Feat' indicates the dimension of each feature.}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{ccc p{8cm} c}\n\\hline\n\\textbf{Feature} & \\textbf{Ref} & \\textbf{Level} & \\multicolumn{1}{c}{\\textbf{Short description}} & \\textbf{\\# Feat} \\\\\n\\hline\nLOW\\_C &  & low  & GIST, HOG2x2, self-similarity and geometric context color histogram features & 17,032\\\\\nElements &  & low  & color: mean saturation, brightness and hue, emotional coordinates, colorfulness, color names, Itten contrast, Wang's semantic descriptions of colors, area statistics; texture: Tamura, Wavelet and gray-level co-occurrence matrix & 97\\\\\nMPEG-7 &  & low  & color: layout, structure, scalable color, dominant color; texture: edge histogram, texture browsing &  $\\approx$200 \\\\\nShape &  & low & line segments, continuous lines, angles, curves &  219\\\\\nIttenColor &  & low & color co-occurrence features and patch-based color-combination features & 16,485\\\\\n\\hline\nAttributes &  & mid & scene attributes & 102 \\\\\nSentributes &  & mid & scene attributes, eigenfaces & 109 \\\\\nComposition &  & mid & level of detail, low depth of field, dynamics, rule of thirds & 45 \\\\\nAesthetics &  & mid & figure-ground relationship, color pattern, shape, composition & 13 \\\\\nPrinciples &  & mid & principles-of-art: balance, contrast, harmony, variety, gradation, movement &  165\\\\\nBoVW &  & mid & bag-of-visual-words on SIFT, latent topics & 330 \\\\\n\\hline\nFS &  & high & number of faces and skin pixels, size of the biggest face, amount of skin w.r.t. the size of faces & 4 \\\\\nANP &  & high & semantic concepts based on adjective noun pairs & 1,200 \\\\\nExpressions &  & high & automatically assessed facial expressions (anger, contempt, disgust, fear, happiness, sadness, surprise, neutral) & 8 \\\\\n\\hline\n\\end{tabular}\n}\n\\label{tab:ImageHandCraftedFeatures}\n\\end{table*}\n\\textbf{High-level Features} that represent the semantic content contained in images can be easily understood by viewers. We can also well recognize the conveyed emotions in images through these semantics. In the early years, simple semantic content including faces and skins contained in images are extracted in~. For the images that contain faces, facial expressions may directly determine the emotions. \\citeauthor{yang2010exploring}~\\shortcite{yang2010exploring} extracted 8 kinds of facial expressions as high-level features. They built compositional features of local Haar appearances by a minimum error based optimization strategy, which are embedded into an improved AdaBoost algorithm. For the images detected without faces, the experessions are simply set as \\emph{neutral}. Finally, they generated a 8 dimensional vector with each element representing the number of corresponding facial expressions.\n\\begin{table*}[!t]\n\\centering\\scriptsize\n\\caption{Representative work on AC of images using hand-crafted features, where `Fusion' indicates the fusion strategy of different features, `cla, reg, ret, cla\\_p, dis\\_d, dis\\_c' in the Task column are short for classification, regression, retrieval, personalized classification, discrete distribution learning, continuous distribution learning (the same below), respectively, `Result' is the reported best accuracy for classification, mean squared error for regression, discounted cumulative gain for retrieval, F1 for personalized classification, and KL divergence for distribution learning (the first line~ is the result on sum of squared difference) on the corresponding datasets.}\n\\begin{tabular}\n{l p{2.5cm} l p{1.5cm} p{2.5cm} l R{2.5cm}} \n\\hline\n\\textbf{Ref} & \\textbf{Feature} & \\textbf{Fusion} & \\textbf{Learning} & \\textbf{Dataset} & \\textbf{Task} & \\textbf{Result} \\\\\n\\hline\n & Elements, Composition, FS & early & NB  & IAPSa, Abstract, ArtPhoto & cla & 0.471, 0.357, 0.495\\\\\n & MPEG-7 & -- & KNN  & unreleased &  cla  & 0.827\\\\\n & Shape, Elements  & early & SVM, SVR &  IAPSa; IAPS &  cla; reg & 0.314; V-1.350, A-0.912\\\\%shape:0.299, V: 1.708, A: 0.943\\\\\n & Segmented objects  & -- & SL &  IAPS, ArtPhoto &  cla & 0.612, 0.610\\\\\n & Sentributes  & -- & SVM, LR & Tweet & cla  & 0.824\\\\\n & Aesthetics & -- & NB & Abstract, ArtPhoto & cla  & 0.726, 0.631\\\\\n & Principles  & -- & SVM, SVR & IAPSa, Abstract, ArtPhoto; IAPS &  cla; reg & 0.635, 0.605, 0.669; V-1.270, A-0.820\\\\\n & LOW\\_C, Elements, Attributes, Principles, ANP, Expressions & graph & MGL & IAPSa, Abstract, ArtPhoto, GAPED, Tweet &  ret & 0.773, 0.735, 0.658, 0.811, 0.701\\\\\n & IttenColor & -- & SL & MART, devArt & cla  & 0.751, 0.745\\\\\n & BoVW & -- & MIL & IAPSa, Abstract, ArtPhoto & cla  & 0.699, 0.636, 0.707\\\\\n & IttenColor  & -- & MC & MART, devArt & cla  & 0.728, 0.761\\\\\n\\hline\n & GIST, Elements, Attributes, Principles, ANP, Expressions & graph & RMTHG & IESN &  cla\\_p & 0.582\\\\\n\\hline\n & GIST, Elements, Principles & - & SSL  & Abstract & dis\\_d & 0.134\\\\\n & GIST, Elements, Attributes, Principles, ANP, deep features from AlexNet  & weighted & WMMSSL  & Abstract, Emotion6, IESN & dis\\_d & 0.482, 0.479, 0.478 \\\\\n & ANP, VGG16  & - & ACPNN &  Abstract, Emotion6, FlickrLDL, TwitterLDL & dis\\_d & 0.480, 0,506, 0,469, 0.555\\\\\n & GIST, Elements, Attributes, Principles, ANP, AlexNet  & weighted & WMMCPNN &  Abstract, Emotion6, IESN  & dis\\_d & 0.461, 0.464, 0.470\\\\\n & GIST, Elements, Attributes, Principles, ANP, AlexNet  & -- & MTSSR &  IESN  & dis\\_c & 0.436\\\\\n\\hline\n\\end{tabular}\n\\label{tab:ImageHandCraftedMethods}\n\\end{table*}\nMore recently, the semantic concepts are described by adjective noun pairs (ANPs)~, which are detected by SentiBank~ or DeepSentiBank~. The advantages of ANP are that it turns a neutral noun into an ANP with strong emotions and makes the concepts more detectable, compared to nouns and adjectives, individually. A 1,200 dimensional vector representing the probability of the ANPs can form a feature vector.\nTable~\\ref{tab:ImageHandCraftedFeatures} summarizes the above-mentioned hand-crafted features at different levels for AC of images. Some recent methods also extracted CNN features from pre-trained deep models, such as AlexNet~ and VGGNet~.\nTo map the extracted handcrafted features to emotions,\n\\textbf{Machine Learning Methods} are commonly employed. Some typical learning models include Naive Bayes (NB)~, support vector machine (SVM)~, $K$ nearest neighbor (KNN)~, sparse learning (SL)~, logistic regression (LR)~, multiple instance learning (MIL)~, and matrix completion (MC)~ for emotion classification , support vector regression (SVR)~ for emotion regression, and multi-graph learning (MGL)~ for emotion retrieval.\nInstead of assigning the DEC to an image, some recent methods began to focus on the perception subjectivity challenge, \\textit{i.e.}, predicting personalized emotions for each viewer or learning emotion distributions for each image. The personalized emotion perceptions of a specified user after viewing an image is predicted in~, associated with online social networks. They considered different types of factors that may contribute to emotion recognition, including the images' visual content, the social context related to the corresponding users, the emotions' temporal evolution, and the images' location information. To jointly model these factors, they proposed rolling multi-task hypergraph learning (RMTHG), which can also easily hanlde the data incompleteness issue.\nGenerally, the distribution learning task can be formulated as a regression problem, which slightly differs for different distribution categories (\\textit{i.e.}, discrete or continuous). For example, if emotion is represented by CE, the regression problem targets predicting the discrete probability of each emotion category with the sum equal to 1; if we represent emotion based on DES, the regression problem is typically transformed to the prediction of the parameters of specified continuous probability distributions. For the latter one, we usually need to firstly determine the form of continuous distributions, such as exponential distribution and Gaussian distribution. Some representative learning methods for emotion distribution learning of discrete emotions include shared sparse learning (SSL)~, weighted multimodal SSL (WMMSSL) , augmented conditional probability neural network (ACPNN)~, and weighted multi-model CPNN (WMMCPNN)~. Both SSL and WMMSSL can only model one test image each time, which is computationally inefficient. After the parameters are learned, ACPNN and WMMCPNN can easily predict the emotion distributions of a test image. Based on the assumption that the VA emotion labels can be well modeled by a mixture of 2 bidimensional Gaussian mixture models (GMMs), \\citeauthor{zhao2017continuous}~\\shortcite{zhao2017continuous} proposed to learn continuous emotion distributions in VA space by multi-task shared sparse regression (MTSSR). Specifically, the parameters of GMMs are regressed, including the mean vector and covariance matrix of the 2 Gaussian components as well as the mixing coefficients.\nTable~\\ref{tab:ImageHandCraftedMethods} summarizes some representative work based on hand-crafted features. Generally, high-level features (such as ANP) can achieve better recognition performance for images with rich semantics, mid-level features (such as Principles) are more effective for artistic photos, while low-level features (such as Elements) perform better for abstract paintings.", "cites": [1022], "cite_extract_rate": 0.043478260869565216, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of handcrafted features for affective computing in images, organizing them into low-, mid-, and high-level categories. While it does attempt basic synthesis by grouping features according to their level and purpose, and includes some contextual explanations, it lacks deeper comparative or critical analysis of the methods or their limitations. Abstraction is limited to labeling feature types, without identifying broader theoretical or methodological patterns."}}
{"id": "c7360062-0ae0-4488-a262-c17a829a8a11", "title": "Deep Learning-Based Methods for AC of Images", "level": "subsection", "subsections": [], "parent_id": "50a2810e-2777-44d0-85b6-12b160fc2d4d", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Images"], ["subsection", "Deep Learning-Based Methods for AC of Images"]], "content": "\\label{sec:ImageDeep}\nTo deal with the situation where images are weakly labeled, a potentially cleaner subset of the training instances are selected progressively~. First, they trained an initial CNN model based on the training data. Second, they selected the training samples with distinct sentiment scores between the two classes with a high probability based on the prediction score of the trained model on the training data itself. Finally, the pre-trained AlexNet on ImageNet is fine-tuned to classify emotions into 8 categories by changing the last layer of the CNN from 1000 to 8~. Besides using the fully connected layer as classifier, they also trained an SVM classifier based on the extracted features from the second to the last layer of the pre-trained AlexNet model.\nMulti-level deep representations (MldrNet) are learned in~ for image emotion classification. They segmented the input image into 3 levels of patches, which are input to 3 different CNN models, including Alexnet, aesthetics CNN (ACNN), and texture CNN (TCNN). The fused features are fed into multiple instance learning (MIL) to obtain the emotion labels. \\citeauthor{zhu2017dependency}~\\shortcite{zhu2017dependency} proposed to integrate the different levels of features by a Bidirectional GRU model (BiGRU) to exploit their dependencies based on MldrNet. They generated two features from the Bi-GRU model and concatenated them as the final feature representations. To enforce the feature vectors extracted from each pair of images from the same category to be close enough, and those from different categories to be far away, they proposed to jointly optimize a contrastive loss together with the traditional cross-entropy loss.\nMore recently, \\citeauthor{yang2018retrieving}~\\shortcite{yang2018retrieving} employed deep metric learning to explore the correlation of emotional labels with the same polarity, and proposed a multi-task deep framework to optimize both retrieval and classification tasks. By considering the relations among emotional categories in the Mikels' wheel, they jointly optimized a novel sentiment constraint with the cross-entropy loss. Extending triplet constraints to a hierarchical structure, the sentiment constraint employs a sentiment vector based on the texture information from the convolutional layer to measure the difference between affective images. In~, \\citeauthor{yang2018retrieving} proposed a weakley supervised coupled convolutional neural network to exploit the discriminability of localized regions for emotion classification. Based on the image-level labels, a sentiment map is firstly detected in one branch with the cross spatial pooling strategy. And then the holistic and localized information are jointly combined in the other branch to conduct a classification task. The detected sentiment map can easily explain which regions of an image determine the emotions.\nThe above deep methods mainly focused on the dominant emotion prediction. There are also some work on emotion distribution learning based on deep models. The very first work is a mixed bag of emotions, which trains a deep CNN regressor (CNNR) for each emotion category in Emotion6~ based on the AlexNet architecture. They changed the number of output nodes to 1 to predict a real value for each emotion category and replaced the Softmax loss with Euclidean loss. To ensure the sum of different probabilities to be 1, they normalized the predicted probabilities of all emotion categories. However, CNNR has some limitations. First, the predicted probability cannot be guaranteed to be non-negative. Second, the probability correlations among different emotions are ignored, since the regressor for each emotion category is trained independently. In~, \\citeauthor{yang2017joint} designed a multi-task deep framework based on VGG16 by jointly optimizing the cross-entropy loss for emotion classification and Kullback-Leibler (KL) divergence loss for emotion distribution learning. To match the single emotion dataset to emotion distribution learning settings, they transformed each single label to emotion distribution with emotion distances computed on Mikels' wheel~. By extending the size of training samples, this method achieves the state-of-the-art performance for discrete emotion distribution learning.\n\\begin{table*}[!t]\n\\centering\\scriptsize\n\\caption{Representative work on deep learning based AC of images, where `Pre' indicates whether the network is pre-trained using ImageNet, `\\# Feat' indicates the dimension of last feature mapping layer before the emotion output layer, `Cla' indicates the classifier used after the last feature mapping with default Softmax, `Loss' indicates the loss objectives (besides the common cross-entropy loss for classification), and `Result' is the reported best accuracy for classification, discounted cumulative gain for retrieval, and KL divergence for distribution learning on the corresponding datasets.}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}\n{c C{1.5cm} c C{1cm} cc C{3cm}  C{0.4cm} C{2cm}} \n\\hline\n\\textbf{Ref} & \\textbf{Base net} & \\textbf{Pre} & \\textbf{\\#Feat} & \\textbf{Cla} & \\textbf{Loss} & \\textbf{Dataset} & \\textbf{Task} & \\textbf{Result} \\\\\n\\hline\n & self-defined & no  & 24 & -- & -- & FlickrCC & cla & 0.781\\\\\n & AlexNet & yes  & 4,096 & SVM & -- & FI, IAPSa, Abstract, ArtPhoto &  cla  & 0.583, 0.872, 0.776, 0.737\\\\\n & AlexNet, ACNN, TCNN & yes & 4,096, 256, 4,096 & MIL & -- &  \\tiny{FI, IAPSa, Abstract, ArtPhoto, MART} &  cla & \\tiny{0.652, 0.889, 0.825, 0.834, 0.764}\\\\\n & self-defined & no & 512  & -- & contrastive &  FI, IAPSa, ArtPhoto &  cla & 0.730, 0.902, 0.855\\\\\n & GoogleNet-Inception & yes & 1,024  & -- & sentiment & FI, IAPSa, Abstract, ArtPhoto & cla; ret  & 0.676, 0.442, 0.382, 0.400; 0.780, 0.819, 0.788, 0.704\\\\\n & ResNet-101 & yes & 2,048  & -- & -- & FI, Tweet & cla  & 0.701, 0.814\\\\\n\\hline\n & AlexNet & yes & 4,096 & -- & Euclidean & Emotion6 & dis\\_d  & 0.480\\\\\n & VGG16 & yes & 4,096 & -- & KL & Emotion6, FlickrLDL, TwitterLDL & dis\\_d & 0.420, 0,530, 0,530\\\\\n\\hline\n\\end{tabular}\n}\n\\label{tab:ImageDeepMethods}\n\\end{table*}\nThe representative deep learning based methods are summarized in Table~\\ref{tab:ImageDeepMethods}. The deep representation features generally perform better than the hand-crafted ones, which are intuitively designed for specific domains based on several small-scale datasets. However, how the deep features correlate to specific emotions is unclear.", "cites": [1023, 1017, 1024], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key developments in deep learning-based affective computing for images, connecting ideas from progressively trained networks, multi-level representations, and emotion distribution learning. It provides critical analysis by pointing out limitations of methods such as CNNR, including issues with non-negative probabilities and ignored emotion correlations. The section also generalizes to highlight broader trends, such as the shift from hand-crafted to deep features and the focus on dominant versus distributed emotion modeling."}}
{"id": "6a4a2789-5122-4ca2-8207-b0cf83ceda85", "title": "Affective Computing of Music", "level": "section", "subsections": [], "parent_id": "36832e43-191f-4e88-aa11-6c7b91fe1bf8", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Music"]], "content": "\\label{sec:Music}\nMusic emotion recognition (MER) strives to identify emotion expressed by music and subsequently predict listener's felt emotion from acoustic content and music metadata, \\textit{e.g.}, lyrics, genre, \\textit{etc.} Emotional understanding of music have applications in music recommendation and is particularly useful for producing music retrieval. An analysis of search queries from creative professionals showed that 80\\% contain emotional terms, showing emotions prominence in that field~. A growing number of work have tried to address emotional understanding of music from acoustic content and metadata (see~ for earlier reviews on this topic). \nEarlier work on emotion recognition from music relied on extracting acoustic features similar to the ones used in speech analysis, such as audio energy and formants. Acoustic features describe attributes related to musical dimensions. Musical dimensions include melody, harmony, rhythm, dynamics, timbre (tone color), expressive techniques, musical texture, and musical form~, as shown in Table~\\ref{tab:MusicFeatures}. Some also add energy as a musical feature which is important for MER~. \nMelody is a linear succession of tones and can be captured by features representing key, pitch and tonality. Among others, chroma is often used to represent melodic features~. \nHarmony is how the combination of various pitches are processed during hearing. Understanding harmony involves chords or multiple notes played together. Examples of acoustic features capturing harmony include chromagram, key, mode, and chords~.\nRhythm consists of repeated patterns of musical sounds, \\textit{i.e.}, notes and pulses that can be describes in terms of tempo and meter. Higher tempo songs often induce higher arousal and fluent rhythm is associated with higher valence and firm rhythm is associated with sad songs~. Mid-level acoustic features, such as onset rate, tempo and beat histogram, can represent rhythmic characteristics of music. \nDynamics of music involve the variation in softness or loudness of notes which include change of loudness (contrast) and emphasis on individual sounds (accent)~. Dynamics of music can be captured by changes in acoustic features related to energy such as root mean square (RMS) energy.\nTimbre is the perceived sound quality of musical notes. Timbre is what differentiates different voices and instruments playing the same sound. Acoustic features capturing timbre, such as MFCC and spectrum shape, describe sound quality~.  Acoustic features describing timbre include MFCC, spectral features (centroid, contract, flatness), and zero crossing rate~.\nExpressive techniques are the way a musical piece is played including tempo and articulation~. Acoustic features, such as tempo, attack slope, and time, can be used to describe this dimension.\nMusical texture is how rhythmic, melodic, and harmonic features are combined in music production~. It is related to the range of tones played at the same time. Musical form describes how a song is structured, such as introduction verse and chorus~. \nEnergy whose dynamics are described in music dynamic features is strongly associated with arousal perception. \n\\begin{table*}[!t]\n\\centering\\scriptsize\n    \\caption{Musical dimensions and acoustic features describing them.}\n\\begin{tabular}\n{ l p{10cm} }\n\\hline\n\\textbf{Musical dimension} & \\textbf{Acoustic features} \\\\\n\\hline\nMelody & Pitch \\\\%\\hline\nHarmony & chromagram, chromagram peak, key, mode, key clarity, harmonic, change, chords \\\\%\\hline\nRhythm & tempo, beat histograms, rhythm regularity, rhythm strength, onset rate\\\\%\\hline\nDynamics and loudness & RMS energy, loudness, timpral width\\\\%\\hline\nTimbre & MFCC, spectral shapres (centroid, shape, spread, skewness, kurtosis, contrast and flatness),  brightness, rolloff frequency, zero crossing rate, spectral contrast, auditory modulation features, inharmonicity, roughness, dissonance, odd to even harmonic ratio \\\\%\\hline\nMusical form & Similarity Matrix (similarity between all possible frames) \\\\%\\hline\nTexture & attack slope, attack time\\\\\n\\hline\n\\end{tabular}\n\\label{tab:MusicFeatures}\n\\end{table*}\nThere are a number of toolboxes available for extracting acoustic features from music that can be used for music emotion recognition. \nMusic Analysis, Retrieval and Synthesis for Audio Signals (Marsyas)~ is an open source framework developed in C++ that supports extracting a large range of acoustic features with music information retrieval applications in mind, including time-domain zero-crossings, spectral centroid, rolloff, flux, and Mel-Frequency Cepstral Coefficients (MFCC) \\textit{etc.}\nMIRToolbox~ is an open source toolbox implemented in MATLAB for music information retrieval applications. MIRToolbox offers the ability to extract a comprehensive set of acoustic features at  different levels including features related to tonality, rhythm, and structures.\nSpeech and music interpretation by large-space extraction or OpenSMILE~ is an open source software developed in C++ with the ability to extract a large number of acoustic features for speech and music analysis in real-time. \nLibROSA~ is a Python package for music and audio analysis. It is mainly developed with music information retrieval application in mind and supports importing from different audio sources and extracting musical features such as onsets chroma and tempo in addition to the low-level acoustic features. \nESSENTIA~ is an open source library developed in C++ with Python interface that is developed for audio analysis. ESSENTIA contains an extensive collection of algorithms supporting audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. \nMusic emotion recognition either attempts to classify songs or excerpt into categories (classification) or estimate their expressed emotions on continuous dimensions (regression). The choice of machine learning model in music emotion recognition depends on the emotional representation used. Mood clusters~, dimensional representations such as arousal, tension and valence as well as music specific emotion representation can be used. An analysis of the methods proposed for MediaEval ``Music in Emotion'' task submissions revealed that using deep learning accounted for the superior performance for emotion recognition much more than the choice features~. Recent methods for emotion recognition in music rely on deep learning and often use spectrogram features that are converted to images~. \\citeauthor{aljanaki2018data} proposed learning musically meaningful mid-level perceptual features that can describe emotions in music~. They demonstrated that perceptual features such as  melodiousness, modality, rhythmic complexity and dissonance can describe a large portion of emotional variance in music both in dimensional representation and MIREX clusters. They also trained a deep convolutional neural network to recognize these mid-level attributes. There have been also work attempting to use lyrics in addition to acoustic content for recognizing emotion in music~. However, lyrics are copyrighted and not easily available which hinders further work in this direction.", "cites": [1025], "cite_extract_rate": 0.0625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concepts of musical dimensions and their corresponding acoustic features, integrating them into a coherent framework. It also highlights the transition from handcrafted to deep learning methods in MER, particularly referencing a key paper for mid-level perceptual features. While it provides a solid analytical perspective and identifies broader trends, its critical evaluation remains somewhat limited, focusing more on summarizing limitations (e.g., lyrics availability) than deeply critiquing methodologies."}}
{"id": "14b7a32f-91ab-4f67-baaf-fabb6149902f", "title": "Affective Computing of Videos", "level": "section", "subsections": ["0091d8de-1e85-4000-afb6-6cc1c3dcc3de", "849377a8-415a-42d7-8a8d-eb0aab34165c", "930f941a-bd9c-4939-bb1e-c81c0b2dac96", "cefa92e9-5af7-4000-8441-381e9420e9cf", "dc8e2b0f-60af-45ac-bc34-2b55fa2b0f2b"], "parent_id": "36832e43-191f-4e88-aa11-6c7b91fe1bf8", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Videos"]], "content": "\\label{sec:Video}\nCurrently, the features used in affective video content analysis are mainly from two categories~. One is considering the stimulus of video content and extracting the features reflecting the emotions conveyed by the video content itself. And the other is extracting features from the viewers. Features extracted from the video content are content-based features, and features formed from the signals of the viewers' responses are viewer-related features.", "cites": [1026], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of the two main feature categories in affective video analysis but does not engage in meaningful synthesis of the cited paper or others. There is no critical evaluation of the methodologies or limitations, nor is there abstraction to broader trends or principles in the field."}}
{"id": "688b2f4a-8d63-4f40-b57e-7f57ac458bc1", "title": "Low-level features", "level": "subsubsection", "subsections": [], "parent_id": "0091d8de-1e85-4000-afb6-6cc1c3dcc3de", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Videos"], ["subsection", "Content-related Features"], ["subsubsection", "Low-level features"]], "content": "Commonly, the low-level features are directly computed from the raw visual and audio content, and usually carry no semantic information. As for visual content, color, lighting, and tempo are important elements that can endow the video with strong emotional rendering and further give viewers direct visual stimuli. In many cases, computations are conducted over each frame of the video, and the average values of the computational results of the overall video are considered as visual features. Specifically, the color-related features often contain the histogram and variance of color~, the proportions of color~, the number of white frame and fades~, the grayness~, darkness ratio, color energy~, brightness ratio and saturation~, \\textit{etc}. In addition, the differences of dark and light can be reflected by the lighting key, which is used to evoke emotions in video and draw the attention of viewers by creating an emotional atmosphere~. As for the tempo-related features, properties of shot can reinforce the expression of video, such as shot change rate and shot length variance~ according to movie grammar. To better take advantage of the temporal information of the video, the motion vectors have been computed as features in~. Since the optical flow can characterize the influence of camera motions, the histogram of optimal flow matrix (HOF) has been computed as features in~. Additionally, \\citeauthor{yi2018multi}~\\shortcite{yi2018multi} traced motion key points at multiple spatial scales and  computed the mean motion magnitude of each frame as features.\nTo represent audio content, pitch, zero crossing rates (ZCR), Mel frequency cepstrum coefficients (MFCC), and energy are the most popular features  . In particular, the MFCC~ and its $\\Delta{MFCC}$ are used to characterize emotions in video clips frequently; while the derivatives and statistics (min, max,mean) of MFCC or $\\Delta{MFCC}$ are also explored widely. As for pitch, ~ shows that pitch of sound is associated closely with some emotions, such as anger with higher pitch and sadness with lower standard deviation of pitch. Similar situation can also occur in the energy~. For example, the total energy of anger or happiness is higher than the counterpart of unexciting emotions. ZCR~ is used to separate different types of audio signals, such as music, environmental sound and speech of human. Besides these frequent related features, audio flatness~, spectral flux~, delta spectrum magnitude, harmony~, band energy ratio, spectral centroid~, and spectral contrast~ are also utilized.\nEvidently, the aforementioned features are mostly handcrafted. With the emergence of  deep learning, features can be automatically learned through deep neural networks. Some pre-trained convolutional neural networks (CNNs) are used to learn static representations from every frame or some selected key frames, while  a Long-short term memory (LSTM) is exploited to capture dynamic  representations existing in videos.\nFor instance, in~, an AlexNet with seven fully-connected layers trained on 2600 ImageNet classes is used to learn features. A Convolutional Auto-Encoder (CAE) is designed to ensure the CNNs can extract the visual features effectively in~. \\citeauthor{ben2018deep}~ first used the pre-trained ResNet-152 to extract feature vectors. And then, these vectors are fed into an LSTM according to their temporal order to extract high-order representations. Pre-trained model, SoundNet, is utilized to learn audio features. Because the expressive emotions of video are induced and communicated by the protagonist in video in many cases, the features of protagonist are extracted from the key frame by a pre-trained CNN and used in video affective analysis in~. In addition to the protagonist, other objects in each frame of video also give insights into emotional expression of video. For example, in~, \\citeauthor{shukla2018looking} removed the non-gaze regions from video frames (Eye ROI) and built the coarse grained scene structure remaining gist information by Guassian filter with variance. After the operations above in~, the next video affective analysis may pay more attention to important information and reduce unnecessary noise.", "cites": [1018], "cite_extract_rate": 0.05, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a descriptive overview of low-level features in video affective computing, listing common visual and audio features used in the field. It briefly mentions some papers, but does not deeply integrate or synthesize their contributions into a broader narrative. There is minimal critical evaluation or identification of overarching principles or trends, keeping the discussion largely factual and surface-level."}}
{"id": "930f941a-bd9c-4939-bb1e-c81c0b2dac96", "title": "Machine Learning Methods", "level": "subsection", "subsections": [], "parent_id": "14b7a32f-91ab-4f67-baaf-fabb6149902f", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Videos"], ["subsection", "Machine Learning Methods"]], "content": "After feature extracting,  a classifier or a regressor is used to obtain emotional analysis results. For classification, there are several  frequently used classifiers, including support vector machines (SVM)~, Naive Bayes (NB)~, Linear Discriminant Analysis~, logistic regression (LR)~, and ensemble learning~, \\textit{etc}.\nRecent work show that the SVM-based methods are very popular for affective video content analysis due to its simplicity, max-margin training property, and use of kernels ~.  For example,  \\citeauthor{yi2018multi}' work ~ demonstrated that linear SVM is more suitable for classification than RBM, MLP, and LR . In~, LDA, linear SVM (LSVM), and Radial Basis SVM (RSVM) classifiers are employed in emotion recognition experiments, and the RSVM obtained the best F1 scores. In~, both Navie Bayes and SVM are used as classifiers in unimodal and multimodal conditions. In the unimodal experimental condition, NB is not better than SVM. And the fusion results showed that SVM is much better than NB in multimodal situations. However, SVM also has its shortages, such as the difficulty of selecting suitable kernel functions. Indeed, SVM is not always the best choice. In~, the results demonstrated that ensemble learning outperforms SVM in terms of classification accuracy. Ensemble learning has acquired a lot of attention in many fields because of its accuracy, simplicity, and robustness. In addition, in~, LR is adopted as the classifier for its effectiveness and simplicity. In fact, LR is used frequently in many transfer learning tasks.\nHowever, all the classifiers mentioned above are not able to capture the temporal information. Some other methods try to use temporal information. For example, \\citeauthor{gui2018implicit}~ combined SVM and LSTM to predict emotion labels. Specifically, global features and sequence features are proposed to represent the pupillary response signals. Then a SVM classifier is trained with the global features and a LSTM classifier is trained with the sequence features. Finally, a decision fusion strategy is proposed to combine these two classifiers.\n\\begin{table*}[!t]\n\\centering\\scriptsize\n\\caption{Representative work on AC of videos using kinds of features, where $P_{a}$, $P_{v}$, $MSE_{a}$, $MSE_{v}$, $Acc_{a}$, $Acc_{v}$, $Acc$ and $MAP$ indicates the Pearson correlation coefficients of arousal and valence, the mean sum error of arousal and valence, the accuracy of arousal and valence, the average accuracy and mean average precision respectively. `statistics' means (min, max, mean).}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}\n{p{0.4 cm} p{4 cm} p{0.7 cm} p{2 cm} p{2 cm} p{0.3 cm} R{2cm}} \n\\hline\n\\textbf{Ref} & \\textbf{Feature} & \\textbf{Fusion} & \\textbf{Learning} & \\textbf{Dataset} & \\textbf{Task} & \\textbf{Result} \\\\\n\\hline\n & Mel frequency spectral; MFCC, Chroma and their derivatives and statistics; Audio compressibility; Harmonicity; Shot frequency; HOF and statistics; Histogram of 3D HSV and statistics; Video compressibility; Histogram of facial area & decision & LSTM & Dataset described by Malandrakis  & reg & $P_{a}$:$0.84 \\pm 0.06$ $MSE_{a}$:$0.08 \\pm 0.04$ $P_{v}$:$0.50 \\pm 0.14$ $MSE_{v}$:$0.21 \\pm 0.06$\\\\\n & Average, standard deviation and four spectral power bands of pupil dilation ratio time-series & decision & SVM, LSTM  & MAHNOB-HCI & cla & $Acc_{a}$:0.730, $Acc_{v}$:0.780\\\\\n & Audio and visual deep features from pretrained model & feature & CNN, LSTM, SVM & PMIT & cla & $MAP$:0.0.2122\\\\\n & MFCC; Color values; HoG; Dense trajectory descriptor; CNN-learned features & decision & CNN, SVM, Ensemble & DEAP & cla & $Acc$: 0.81, 0.49 \\\\\n & HHTC features & feature & SVR & Discrete LIRIS-ACCEDE & reg & $MSE_{a}$:0.294, $MSE_{v}$: 0.290 \\\\\n & Statistical measures (such as mean, median, skewness kurtosis) for EEG data, power spectral features, ECG, GSR, Face/Head-pose  & decision & SVM/NB &  music excerpts~ & cla & F1 (v: 0.59, 0.58, a: 0.60, 0.57) \\\\\n & Time-span visual and visual features & feature & CNN Opensmile toolbox &  music excerpts~ & cla & $MSE_{a}$:0.082 $MSE_{v}:0.071$\\\\\n & tempo; pitch; zero cross; roll off; MFCCs; Saturation; Color heat; Shot length feature; General preferences; Visual excitement; Motion feature; fMRI feature & feature & DBM SVM & TRECVID & cla & - \\\\\n & Colorfulness; MFCC; CNN-learned features from the keyframes containing protagonist & decision & CNN, SVM, SVR & LIRIS-ACCEDE, PMSZU & cla/reg & - \\\\\n & Multi-frame motion vectors & decision & CNN & SumMe, TVsum, Continuous LIRIS-ACCEDE & reg & - \\\\\n &  The median of the L values in Luv space; means and variances of components in HSV space; texture feature; mean and standard deviation of motions between frames in a short; MFCC; Spectral power; mean and variance of the spectral centroids; Time domain zero crossings rate; Multi-instance sparse coding & feature & SVM & Musk1, Musk2, Elephant, Fox, Tiger & cla & $Acc$: 0.911, 0.906, 0.885, 0.627, 0.868\\\\\n & Lighting key; Color; Motion vectors; ZCR; energy; MFCC; pitch; Textual features & decision & SMO, Navie Bayes & DEAP & cla & F1:0.849 0.811 $Acc$: 0.911 0.883 \\\\\n & Key lighting; Grayness; Fast motion; Shot chanage rate; Shot length variation; MFCC; CNN-learned features; power spectral density; EEG; ECG; respiation; galvanic skin resistance & feature & SVM & DEAP & cla & $Acc_v$:0.7 0.7 0.7125, $Acc_a$:0.6876 0.7 0.8 F1 (A:0.664 0.687 0.789) \\\\\n & MFCC; ZCR; energy; pitch, color histograms; lighting key; motion vector & decision & SVM, Navie Bayes & DEAP & cla & F1: 0.869, 0.846 $Acc$: 0.925, 0.897 \\\\\n & CNN feature, low-level audio visual features, EEG & decision & LDA, LSVM, RSVM & Dataset introduced by the authors & cla & - \\\\\n & MKT; ConvNets feature; EmoLarge; IS13; MFCC; EmoBase10; DSIFT; HSH & decision & SVM, LR, RBM, MLP & MediaEval 2015, 2016 Affective Impact of Movies & cla & $Acc_a$:0.574, $Acc_v$:0.462\\\\\n & CNN feature & - & SVM, LDA & dataset in~ & cla & - \\\\\n & CNN feature & - & SVR & LIRIS-ACCEDE & reg &  $MSE_{a}$: 0.021,  $MSE_{v}$: 0.027\\\\\n\\hline\n\\end{tabular}\n}\n\\label{tab:VideoHandCraftedMethods}\n\\end{table*}\nA regressor is needed when mapping the extracted features to the continuous dimensional emotion space. Recently, one of the most popular regression method is support vector regression (SVR)~. For example,  in~, video features like audio, color, aesthetic are fed into SVR in the SVR-Standard experiment. And in the SVR-Transfer learning experiment, the pre-trained CNN is treated as a feature extractor. The CNN's outputs are used as the input to the SVR. The experimental results showed that the SVR-Transfer learning outperforms other methods. Indeed, the various kernel functions in SVR  provide a stronger adaptability.", "cites": [1028, 1026, 1027], "cite_extract_rate": 0.125, "origin_cites_number": 24, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a comparison of various machine learning methods used in affective video computing, such as SVM, NB, LR, and ensemble learning, citing several studies. It highlights the relative performance of these classifiers in different contexts but lacks deeper synthesis or a novel organizing framework. The critical analysis is limited to pointing out shortcomings like kernel selection in SVM and occasional performance comparisons, without deeper evaluation of methodological trends or broader implications."}}
{"id": "dc8e2b0f-60af-45ac-bc34-2b55fa2b0f2b", "title": "Deep Learning Methods", "level": "subsection", "subsections": [], "parent_id": "14b7a32f-91ab-4f67-baaf-fabb6149902f", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Videos"], ["subsection", "Deep Learning Methods"]], "content": "\\label{sec:Deep learning method}\nIn tradition, video emotional recognition includes two steps, \\textit{i.e.}, feature extraction step and regression or classification step. Because of the lack of consensus on the most relevant emotional features, we may not be able to extract the best features for the problem at hand. As a result, this two-step mode has hampered the development of affective video content analysis. In order to solve this problem, some methods based on end-to-end training frameworks are proposed. \\citeauthor{khorrami2016deep}  combined CNN and RNN to recognize the emotional information of videos. According to their method, a CNN is trained using frame facial images sampled from videos to extract features. Then the features are fed into a RNN to perform continuous emotion recognition. In~, a single network using ConvLSTM is proposed, where videos are input to the network and the predicted emotional information is output directly. In fact, due to the complexity of CNNs and RNNs, the training of these frameworks needs large amounts of data. However, in video affective content analysis, the samples in existing datasets are usually limited. This is the reason why end-to-end methods are still less common compared to the traditional two step methods, despite of their influential potentials.", "cites": [1029], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of deep learning methods in affective video computing by integrating the approach from Khorrami et al. with a general discussion of end-to-end frameworks versus traditional two-step methods. It identifies limitations such as the need for large datasets and explains why end-to-end methods are less common. However, the synthesis is limited to a small number of works, and the abstraction remains at a relatively basic level without deeper meta-insights into broader trends or principles."}}
{"id": "20c98c17-ea71-4ba4-a9aa-815a42eced5b", "title": "Affective Computing of Multimodal Data", "level": "section", "subsections": [], "parent_id": "36832e43-191f-4e88-aa11-6c7b91fe1bf8", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Affective Computing of Multimodal Data"]], "content": "\\label{sec:multimodal}\nIn this section, we survey the work that analyze multimodal data beyond audiovisual content. Most of the existing work on affective understanding of multimedia rely on one modality, even when additional modalities are available, for example in videos~. Earlier work on emotional understanding of multimedia used hand crafted features from different modalities that are fused at feature or decision levels~. The more recent work mainly use deep learning models~. \nLanguage is a commonly used modality in addition to vision and audio. There is a large body of work on text-based sentiment analysis~. Sentiment analysis from text is well-established and is deployed at scale in industry at a broad set of applications involving opinion mining~. With the shift toward an increasingly multimodal social web, multimodal sentiment analysis is becoming more relevant. For example, vloggers post their opinions on YouTube, and photos commonly accompany user posts on Instagram and Twitter. \nAnalyzing text for emotion recognition requires representing terms by features. Lexically-based approaches are one of the most popular methods for text-based emotion recognition. They involve using knowledge of words' affect for estimating document or content's affect. Linguistic Inquiry and Word Count (LIWC) is a well-known lexical tool that matches the terms in a document with its dictionary and generates  scores along different dimensions including affective and cognitive constructs such as ``present focus'' and ``positive emotion''~. The terms in each category or selected by experts is extensively validated on different content. AffectNet is another notable lexical resource which includes a semantic netowrk of 10,000 items with representations  for ``pleasantness'', ``attention'', ``sensitivity'', and ``aptitude''~. The continuous representations can be mapped to 24 distinct emotions. DepecheMood is a lexicon created through a data-driven method mining a news website annotated with its particular set of discrete emotions, namely, ``afraid'', ``amusemed'', ``anger'', ``annoyed'', ``don't care'', ``happy'', and ``inspired''~. DepecheMood is extended to DepecheMood++ by including Italian~.\nThe more recent development in text-based affective analysis is models powered by deep learning. Leveraging large scale data, deep neural networks are able to learn representations that are relevant for affective analysis in language. Word embeddings are one of the most common representations used to represent language. Word embeddings, such as Word2Vec~ or GloVe~, learn language context of the word by learning a representation (a vector), that can capture semantic and syntactic similarities. More recently, representation learning models that can encode the whole sequence of terms (sentences, documents) showed impressive performance in different language understanding tasks, including sentiment and emotional analysis. \nBidirectional Encoder Representations from Transformers (BERT)~ is a method for learning a language model that can be trained on large amount of data in an unsupervised manner. This pre-trained model is very effective in representing a sequence of terms as a fixed-length representation (vector). BERT architecture is a multi-layer bidirectional Transformer network that encodes the whole sequence at once. BERT representation achieves state-of-the-art results in multiple natural language understanding tasks. \nThe audiovisual features that are used for multimodal understanding of affect are similar to the ones discussed in previous sections. The main technique between miltimodal models lies in methods for multimodal fusion. Multimodal methods involve extracting features from multiple modalities, \\textit{e.g.}, audiovisual, and training joint or separate machine learning models for fusion~. Multimodal fusion can be done in model-based and model-agnostic ways. The model-agnostic fusion methods do not rely on a specific classification or regression method and include feature-level, decision-level, or hybrid fusion techniques.  Model-based methods address multimodal fusion in model construction. Examples of model-based fusion methods include Multiple Kernel Learning (MKL)~, graphical models, such as Conditional Random Fields~ and neural networks~.\n\\citeauthor{Pang2015}~ used Deep Boltzmann Machine (DBM) to learn a joint representation across text, vision, and audio to recognize expected emotions from social media videos. Each modality is separately encoded with stacking multiple Restricted Boltzmann Machines (RBM) and pathways are merged to a joint representation layer. The model was evaluated for recognizing eight emotion categories for 1,101 videos from~. \\citeauthor{muszynski2019recognizing}~ studied perceived vs induced emotion in movies. To this end, they collected additional labels on a subset of LIRIS-ACCEDE dataset~. They found that perceived and induced emotions do not always agree. Using multimodal Deep Belief Networks (DBN), they could demonstrate that fusion of  electrodermal responses with audiovisual content features improves the overall accuracy for emotion recognition~.\nIn~, authors performed regression to estimate intended arousal and valence levels (as judged by experts in~). LSTM recurrent neural networks are used for unimodal regressions and fused via early and late fusion for audiovisual estimation with late fusion achieving the best results. \\citeauthor{Tarvainen2018}~ performed an in-depth analysis on how emotions are constructed in movies. They identified scene type as a major factor in emotions in movies. They then used content features to recognize emotions along three dimensions of hedonic tone (valence), energetic arousal (awake--tired) and tense arousal (tense--calm).\nBilinear fusion is a method that is proposed to model inter- and intra- modality interaction among modalities by performing outer product between unimodal embeddings~. \\citeauthor{zadeh2017tensor}~ extended this to a Tensor Fusion Network to model intra-modality and inter-modality dynamics in multimodal sentiment analysis. The tensor fusion network includes modality embedding sub-networks, a tensor fusion layer modeling the unimodal, bimodal and trimodal interactions using a three-fold Cartesian product from modality embeddings along with a final sentiment inference sub-network conditioned on the tensor fusion layer. The main drawback of such methods is the increase in the dimensionality of the resulting multimodal representation.", "cites": [7, 1030, 1032, 1684, 1031], "cite_extract_rate": 0.18518518518518517, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section integrates multiple papers and connects different methods (e.g., lexical tools and deep learning models) to form a narrative about multimodal affective computing. It critically points out limitations, such as the dimensionality problem in tensor fusion. While it provides some abstraction by categorizing fusion techniques, it largely focuses on methodological descriptions and comparisons without deriving deeper overarching principles."}}
{"id": "5da3fa17-09f5-4d0c-9b55-5610d97ddda5", "title": "Future Directions", "level": "section", "subsections": [], "parent_id": "36832e43-191f-4e88-aa11-6c7b91fe1bf8", "prefix_titles": [["title", "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey"], ["section", "Future Directions"]], "content": "\\label{sec:FutureDirections}\nAlthough remarkable progress has been made on affective computing of multimedia (ACM) data, there are still several open issues and directions that can boost the performance of ACM.\n\\textbf{Multimedia Content Understanding.} As emotions may be directly evoked by the multimedia content in viewers, accurately understanding what is contained in multimedia data can significantly improve the performance of ACM. Sometimes it is even necessary to analyze the subtle details. For example, we may feel ``amused'' on a video with a laughing baby; but if the laugh is from a negative character, it is more possible for us to feel ``angry''. In such cases, besides the common property, such as ``laugh'', we may need to further recognize the identity, such as ``a lovely baby'' and ``an evil antagonist''.\n\\textbf{Multimedia Summarization.} Emotions can play a vital role in selection of multimedia for creation of summaries or highlights. This is an important application in entertainment and sports industries (\\textit{e.g.} movie trailers, sports highlights). There has been some recent work in this direction where affect information from audio visual cues has led to the successful creation of video summaries~. In particular, work reported in~ used audiovisual emotions in part to create an AI trailer for a $20^{th}$ Century Fox film in 2016. Similarly, AI Highlights described in~ hinged on audiovisual emotional cues and have successfully been employed to create the official highlights at Wimbledon and US Open since 2017. This is a very promising direction for affective multimedia computing which can have a direct impact on real world media applications.\n\\textbf{Contextual Knowledge Modeling.} The contextual information of a viewer watching some multimedia is very important. Similar multimedia data under different contexts may evoke totally different emotions. For example, we may feel ``happy\" when listening a song about love in a wedding; but if the same song is played when two lovers are departing, it is more likely that we feel ``sad\". The prior knowledge of viewers or multimedia data may also influence the emotion perceptions. An optimistic viewer and a pessimistic viewer may have totally different emotions about the same multimedia data.\n\\textbf{Group Emotion Clustering.} It is too generic to simply recognize the dominant emotion, while it is too specific to predict personalized emotion. It would make more sense to model emotions for groups or cliques of viewers with similar interests and backgrounds. Clustering different viewers into corresponding groups possibly based on the user profiles may provide a feasible solution to this problem.\n\\textbf{New AC Setting Adaptation.} Because of the domain shift~, the deep learning models trained on one labeled source domain may not work well on the other unlabeled or sparsely labeled target domain, which results in the models' low transferability to new domains. Exploring domain adaptation techniques that fit well on the AC tasks is worth investigating. One possible solution is to translate the source data to an intermediate domain that are indistinguishable from the target data while preserving the source labels~ using Generative Adversarial Networks~. How do deal with some practical settings, such as multiple labeled source domains and emotion models' homogeneity, is more challenging.\n\\textbf{Regions-of-Interest Selection.} The contributions of different regions of given multimedia may vary to the emotion recognition. For example, the regions that  contain the most important semantic information in images are more discriminative than background; some video frames are of no use to emotion recognition. Detecting and selecting the regions-of-interest may significantly improve the recognition performance as well as the computation efficiency.\n\\textbf{Viewer-Multimedia Interaction.}\nInstead of direct analysis of the multimedia content or implicit consideration of viewers' physiological signals (such as facial expressions, Electroencephalogram signals, \\textit{etc}.), joint modeling of both multimedia content and viewers' responses may better bridge the affective gap and result in superior performances. We should also study how to deal with missing or corrupted data. For example, some physiological signals are unavailable during the data collection stage.\n\\textbf{Affective Computing Applications.} Although AC is claimed to be important in real-world applications, few practical systems have been developed due to the relatively low performance. With the availability of larger datasets and improvements in self-supervised and semi-supervised learning, we foresee the deployment of ACM in real-world applications. For example, in media analytics, the content understanding methods will identify the emotional preferences of users and emotional nuances of social media content to better target advertising effort; in fashion recommendation, intelligent costumer service, such as customer-multimedia interaction, can provide better experience to customers; in advertisement, generating or curating multimedia that evokes strong emotions can attract more attention. We believe that an emotional artificial intelligence will become a significant component of mainstream multimedia applications.\n\\textbf{Benchmark Dataset Construction.}\nExisting studies on ACM mainly adopt small-scale datasets or construct relatively larger-scale ones using keyword searching strategy without annotation quality guaranteed. To advance the development of ACM, creating a large-scale and high-quality dataset is in urgent need. It has shown that there are three critical factors for dataset construction of ACM, \\textit{i.e.}, the context of viewer response, personal variation among viewers, and the effectiveness and efficiency of corpus creation~. In order to include a large number of samples, we may exploit online systems and crowdsourcing platforms to recruit large numbers of viewers with a representative spread of backgrounds to annotate multimedia and provide contextual information on their emotional responses. Since emotion is a subjective variable, personalized emotion annotation would make more sense, from which we can obtain the dominant emotion and emotion distribution. Further, accurate understanding of multimedia content can boost the affective computing performance. Inferring emotional labels from social media users' interaction with data, \\textit{e.g.}, likes, comments, in addition to their spontaneous responses, \\textit{e.g.}, facial expression, where possible, will provide new avenues for enriching affective datasets.", "cites": [7022, 1033], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides analytical insights into future directions for affective computing with multimedia data. It synthesizes ideas from cited papers to highlight the importance of context, domain adaptation, and dataset construction. While it offers some critique of current limitations, particularly in dataset quality and model transferability, the analysis remains more descriptive of challenges than deeply evaluative. It generalizes well to broader themes like emotional nuance and real-world application potential."}}
