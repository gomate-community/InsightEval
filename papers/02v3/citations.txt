@inproceedings{zhu-etal-2025-deepreview,
    title = "{D}eep{R}eview: Improving {LLM}-based Paper Review with Human-like Deep Thinking Process",
    author = "Zhu, Minjun  and
      Weng, Yixuan  and
      Yang, Linyi  and
      Zhang, Yue",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1420/",
    doi = "10.18653/v1/2025.acl-long.1420",
    pages = "29330--29355",
    ISBN = "979-8-89176-251-0",
    abstract = "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21{\%} and 80.20{\%} against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available."
}


@misc{taechoyotin2025remorautomatedpeerreview,
      title={REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning}, 
      author={Pawin Taechoyotin and Daniel Acuna},
      year={2025},
      eprint={2505.11718},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.11718}, 
}

@misc{goldberg2024usefulnessllmsauthorchecklist,
      title={Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment}, 
      author={Alexander Goldberg and Ihsan Ullah and Thanh Gia Hieu Khuong and Benedictus Kent Rachmat and Zhen Xu and Isabelle Guyon and Nihar B. Shah},
      year={2024},
      eprint={2411.03417},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.03417}, 
}


@misc{lu2024aiscientistfullyautomated,
      title={The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery}, 
      author={Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha},
      year={2024},
      eprint={2408.06292},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.06292}, 
}


@inproceedings{shao-etal-2024-assisting,
    title = "Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models",
    author = "Shao, Yijia  and
      Jiang, Yucheng  and
      Kanell, Theodore  and
      Xu, Peter  and
      Khattab, Omar  and
      Lam, Monica",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.347/",
    doi = "10.18653/v1/2024.naacl-long.347",
    pages = "6252--6278",
    abstract = "We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines throughRetrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM{'}s articles are deemed to be organized (by a 25{\%} absolute increase) and broad in coverage (by 10{\%}). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts."
}


@inproceedings{zheng-etal-2025-automation,
    title = "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery",
    author = "Zheng, Tianshi  and
      Deng, Zheye  and
      Tsang, Hong Ting  and
      Wang, Weiqi  and
      Bai, Jiaxin  and
      Wang, Zihao  and
      Song, Yangqiu",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.895/",
    doi = "10.18653/v1/2025.emnlp-main.895",
    pages = "17733--17750",
    ISBN = "979-8-89176-332-6",
    abstract = "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy{---}Tool, Analyst, and Scientist{---}to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement."
}

@article{Arts_2025,
   title={Beyond Citations: Measuring Novel Scientific Ideas and their Impact in Publication Text},
   ISSN={1530-9142},
   url={http://dx.doi.org/10.1162/rest_a_01561},
   DOI={10.1162/rest_a_01561},
   journal={Review of Economics and Statistics},
   publisher={MIT Press},
   author={Arts, Sam and Melluso, Nicola and Veugelers, Reinhilde},
   year={2025},
   month=jan, pages={1–33} }


@misc{mohole2025veriragpostretrievalauditingscientific,
      title={VERIRAG: A Post-Retrieval Auditing of Scientific Study Summaries}, 
      author={Shubham Mohole and Hongjun Choi and Shusen Liu and Christine Klymko and Shashank Kushwaha and Derek Shi and Wesam Sakla and Sainyam Galhotra and Ruben Glatt},
      year={2025},
      eprint={2507.17948},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2507.17948}, 
}


@inproceedings{chen-etal-2025-surveygen,
    title = "{S}urvey{G}en-{I}: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing",
    author = "Chen, Jing  and
      Yang, Zhiheng  and
      Shen, Yixian  and
      Liu, Jie  and
      Belloum, Adam  and
      Grosso, Paola  and
      Papagianni, Chrysa",
    editor = "Inui, Kentaro  and
      Sakti, Sakriani  and
      Wang, Haofen  and
      Wong, Derek F.  and
      Bhattacharyya, Pushpak  and
      Banerjee, Biplab  and
      Ekbal, Asif  and
      Chakraborty, Tanmoy  and
      Singh, Dhirendra Pratap",
    booktitle = "Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics",
    month = dec,
    year = "2025",
    address = "Mumbai, India",
    publisher = "The Asian Federation of Natural Language Processing and The Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.ijcnlp-long.193/",
    pages = "3687--3714",
    ISBN = "979-8-89176-298-5",
    abstract = "Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I performs survey-level retrieval to construct the initial outline and writing plan, then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across six scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage. The code is available at https://github.com/SurveyGens/SurveyGen-I."
}

@article{Wu_2025,
   title={Automated literature research and review-generation method based on large language models},
   volume={12},
   ISSN={2053-714X},
   url={http://dx.doi.org/10.1093/nsr/nwaf169},
   DOI={10.1093/nsr/nwaf169},
   number={6},
   journal={National Science Review},
   publisher={Oxford University Press (OUP)},
   author={Wu, Shican and Ma, Xiao and Luo, Dehui and Li, Lulu and Shi, Xiangcheng and Chang, Xin and Lin, Xiaoyun and Luo, Ran and Pei, Chunlei and Du, Changying and Zhao, Zhi-Jian and Gong, Jinlong},
   year={2025},
   month=apr }


@inproceedings{lin-etal-2025-breaking,
    title = "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks",
    author = "Lin, Tzu-Ling  and
      Chen, Wei-Chih  and
      Hsiao, Teng-Fang  and
      Liu, Hou-I  and
      Yeh, Ya-Hsin  and
      Chan, Yu-Kai  and
      Lien, Wen-Sheng  and
      Kuo, Po-Yen  and
      Yu, Philip S.  and
      Shuai, Hong-Han",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.259/",
    doi = "10.18653/v1/2025.findings-emnlp.259",
    pages = "4819--4839",
    ISBN = "979-8-89176-335-7",
    abstract = "Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication."
}

@article{beel2025evaluating,
  title={Evaluating sakana’s ai scientist for autonomous research: Wishful thinking or an emerging reality towards’ artificial research intelligence’(ari)},
  author={Beel, Joeran and Kan, Min-Yen and Baumgart, Moritz},
  journal={arXiv preprint arXiv:2502.14297},
  year={2025}
}

@misc{zhang2026opennoveltyllmpoweredagenticverifiable,
      title={OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment}, 
      author={Ming Zhang and Kexin Tan and Yueyuan Huang and Yujiong Shen and Chunchun Ma and Li Ju and Xinran Zhang and Yuhui Wang and Wenqing Jing and Jingyi Deng and Huayu Sha and Binze Hu and Jingqi Tong and Changhao Jiang and Yage Geng and Yuankai Ying and Yue Zhang and Zhangyue Yin and Zhiheng Xi and Shihan Dou and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2026},
      eprint={2601.01576},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2601.01576}, 
}


@inproceedings{ai-etal-2025-novascore,
    title = "{N}ov{AS}core: A New Automated Metric for Evaluating Document Level Novelty",
    author = "Ai, Lin  and
      Gong, Ziwei  and
      Deshpande, Harshsaiprasad  and
      Johnson, Alexander  and
      Phung, Emmy  and
      Emami, Ahmad  and
      Hirschberg, Julia",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.234/",
    pages = "3479--3494",
    abstract = "The rapid expansion of online content has intensified the issue of information redundancy, underscoring the need for solutions that can identify genuinely new information. Despite this challenge, the research community has seen a decline in focus on novelty detection, particularly with the rise of large language models (LLMs). Additionally, previous approaches have relied heavily on human annotation, which is time-consuming, costly, and particularly challenging when annotators must compare a target document against a vast number of historical documents. In this work, we introduce NovAScore (Novelty Evaluation in Atomicity Score), an automated metric for evaluating document-level novelty. NovAScore aggregates the novelty and salience scores of atomic information, providing high interpretability and a detailed analysis of a document{'}s novelty. With its dynamic weight adjustment scheme, NovAScore offers enhanced flexibility and an additional dimension to assess both the novelty level and the importance of information within a document. Our experiments show that NovAScore strongly correlates with human judgments of novelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0 dataset and a 0.920 Pearson correlation on an internal human-annotated dataset."
}


@inproceedings{shahid-etal-2025-literature,
    title = "Literature-Grounded Novelty Assessment of Scientific Ideas",
    author = "Shahid, Simra  and
      Radensky, Marissa  and
      Fok, Raymond  and
      Siangliulue, Pao  and
      Weld, Daniel S  and
      Hope, Tom",
    editor = "Ghosal, Tirthankar  and
      Mayr, Philipp  and
      Singh, Amanpreet  and
      Naik, Aakanksha  and
      Rehm, Georg  and
      Freitag, Dayne  and
      Li, Dan  and
      Schimmler, Sonja  and
      De Waard, Anita",
    booktitle = "Proceedings of the Fifth Workshop on Scholarly Document Processing (SDP 2025)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.sdp-1.9/",
    doi = "10.18653/v1/2025.sdp-1.9",
    pages = "96--113",
    ISBN = "979-8-89176-265-7",
    abstract = "Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the **Idea Novelty Checker**, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13{\%} higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation."
}

@misc{zhao2025literaturereviewliteraturereviews,
      title={A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence}, 
      author={Penghai Zhao and Xin Zhang and Jiayue Cao and Ming-Ming Cheng and Jian Yang and Xiang Li},
      year={2025},
      eprint={2402.12928},
      archivePrefix={arXiv},
      primaryClass={cs.DL},
      url={https://arxiv.org/abs/2402.12928}, 
}

@article{Koloveas2025InsightGUIDEAO,
  title={InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature},
  author={Paris Koloveas and Serafeim Chatzopoulos and Thanasis Vergoulis and Christos Tryfonopoulos},
  journal={2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)},
  year={2025},
  pages={1163-1167},
  url={https://api.semanticscholar.org/CorpusID:281525523}
}


@article{Arnaout2025IndepthRI,
  title={In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis},
  author={Hiba Arnaout and Noy Sternlicht and Tom Hope and Iryna Gurevych},
  journal={ArXiv},
  year={2025},
  volume={abs/2505.14838},
  url={https://api.semanticscholar.org/CorpusID:278782245}
}


@article{Farber2025ComparingHA,
  title={Comparing human and AI expertise in the academic peer review process: towards a hybrid approach},
  author={Shai Farber},
  journal={Higher Education Research \& Development},
  year={2025},
  volume={44},
  pages={871 - 885},
  url={https://api.semanticscholar.org/CorpusID:275792398}
}


@article{Wu2024AreTC,
  title={Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI},
  author={Wenqing Wu and Haixu Xi and Chengzhi Zhang},
  journal={Scientometrics},
  year={2024},
  volume={129},
  pages={4109 - 4135},
  url={https://api.semanticscholar.org/CorpusID:270654152}
}

@article{Vo2024AssessingSI,
  title={Assessing Scientific Inquiry: A Systematic Literature Review of Tasks, Tools and Techniques},
  author={De Van Vo and Geraldine Mooney Simmie},
  journal={International Journal of Science and Mathematics Education},
  year={2024},
  volume={23},
  pages={871 - 906},
  url={https://api.semanticscholar.org/CorpusID:272420434}
}


@article{Guo2024IdeaBenchBL,
  title={IdeaBench: Benchmarking Large Language Models for Research Idea Generation},
  author={Sikun Guo and Amir Hassan Shariatmadari and Guangzhi Xiong and Albert Huang and Eric Xie and Stefan Bekiranov and Aidong Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.02429},
  url={https://api.semanticscholar.org/CorpusID:283910439}
}