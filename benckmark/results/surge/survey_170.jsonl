{"id": "71e0cc84-d3f6-4661-8d90-58558f465f35", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "c8c28393-3555-4c6b-8dc2-feb553e2658f", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Introduction"]], "content": "Over the last decade, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems.  Availability of large multi-class annotated datasets makes it possible for deep networks to learn discriminative features that a classifier can exploit to perform recognition. In this survey we focus on the extreme case of recognition -- One-Class Classification (OCC), where data from only a single class (labeled positive) is present during training. During inference, the classifier encounters data from both the positive class and outside the positive class (sometimes referred to as the negative class). The objective of OCC is  to determine whether a query object belongs to the class observed during training.  In the absence of multiple-class data, both learning features and defining classification boundaries become more challenging in OCC compared to multi-class classification. Nevertheless, it has applications in several image-based machine learning tasks. One-class classification methods are extensively used in  abnormal image detection ,  and abnormal event detection , .  They are also used extensively  in biometrics applications such as Active Authentication ,  and anti-spoofing , , , . Therefore,  there has been a significant interest in the computer vision,  machine learning and biometrics communities in developing OCC algorithms. \n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{diff}\n\t\\vskip-10pt\\caption{Different forms of classification. Data points corresponding to different classes are shown in different color. Both multi-class classification and detection contain data from different classes during training.  In OCC, only a single class is given to learn a representation and/or a classifier.}\n\t\\label{fig:diff}\n\\end{figure}\nIn Figure~\\ref{fig:diff} we illustrate the differences among OCC and multi-class detection and classification. In multi-class classification, training data from several classes are present. The classifier learns multiple decision boundaries to separate each class from the others. On the other hand, in detection, there exists data from two classes -- a positive class and a negative class. The learned classification boundary in this case separates out positive data from the negative data. In contrast, OCC has only data from the positive class during training. The learned classifier defines a boundary that encompasses positive data with the hope that it will provide good separation from the other objects in the world.\nThere exists multiple research areas in computer vision and machine learning that are closely related to the task of OCC. First, we discuss similarities and differences between OCC and these research topics.\n\\noindent \\textbf{One-class novelty detection. } The objective of one-class novelty detection , ,  is the same as of OCC. Here, a detector is  sought that can detect objects from the observed class. Learned detector can be transformed into a classifier by selecting a suitable detection threshold.  Therefore, OCC and a one-class novelty detection solve essentially the same problem.  In this survey, we make no distinction between OCC and one class novelty detection.\n\\noindent \\textbf{Outlier detection (unsupervised anomaly detection). }  In outlier detection , , , a mixture of normal and abnormal data is presented without ground truth annotations. The objective is to separate normal data from abnormal data using unsupervised techniques. In contrast, all training data is assumed to be normal in OCC.  Therefore, OCC is considered to be a supervised learning problem whereas outlier detection is unsupervised.  One class classification solutions cannot be directly applied to outlier detection problems and vise versa.\n\\noindent \\textbf{Open-set recognition. }  Open-set recognition , , , , , ,  is an extension to multi-class classification. Given a query image, open-set recognition considers the possibility of the query not belonging to any of the classes observed during training. Therefore, open-set recognition essentially learns $C+1$ decision boundaries for a $C$-class problem, where the additional boundary separates the known classes from the novel (open-set) classes. One class classification is the extreme case of open-set recognition where $C=1$.\nSeveral surveys exist in the literature on OCC and related techniques , , , , , , .  However, some of them present generic one class techniques that are designed for low dimensional inputs and do not always generalize well to image data , . Reviews provided in , ,  only focus on the specific applications of OCC such as image-based novelty and anomaly detection. Furthermore, survey papers ,  do not include OCC methods proposed over the last few years, especially deep learning-based methods.  Whereas in this paper, we present a survey of recent advances in OCC with special emphasis on deep features and classifiers.  \nThis paper is organized as follows. In Section~\\ref{sec:taxonomy}, we present a taxonomy for OCC. In Section~\\ref{sec:features}, we discuss feature learning techniques targeting OCC. In Section~\\ref{sec:OCC_Alg}, we first present a review of classical OCC algorithms and then discuss recent  deep learning-based algorithms in detail.  Section~\\ref{sec:data_and_metrics} presents a discussion of commonly used datasets and evaluation metrics for OCC. Finally, Section~\\ref{sec:openproblems} concludes the paper with a brief summary and discussion.", "cites": [3283, 3213, 3319, 3325, 3324, 6883, 3282, 6885, 3208, 7134, 3323, 3207, 6884], "cite_extract_rate": 0.4642857142857143, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple cited works, particularly in distinguishing OCC from related tasks like outlier detection and open-set recognition. It provides a coherent narrative by connecting these ideas to form a foundational understanding of OCC. While it includes some critical evaluation, such as noting the limitations of older surveys and the need for deep learning-focused analysis, it could more explicitly highlight the trade-offs or performance comparisons between methods. The abstraction level is strong, as it generalizes across the cited works to articulate broader principles of OCC and its place in the machine learning landscape."}}
{"id": "6f600686-2a54-40fa-a853-dea60fc2d90a", "title": "Taxonomy", "level": "section", "subsections": [], "parent_id": "c8c28393-3555-4c6b-8dc2-feb553e2658f", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Taxonomy"]], "content": "\\label{sec:taxonomy}\nConsidering the research work carried out in computer vision and machine learning communities, we propose a taxonomy for the study of image-based OCC problems as shown in Figure~\\ref{fig:tax}.  Main categories identified in the taxonomy are as follows:\n\\begin{enumerate}\n\t\\item \\textbf{Data.} Learning with positive data, positive and unlabeled data and learning with positive data in the presence of labeled Out Of Distribution (OOD) data.\n\t\\item \\textbf{Features.} Handcrafted features, statistical data driven features and deep-learning based features.\n\t\\item \\textbf{Classification Algorithm.} Statistical classification methods, representation-based classification methods and deep-learning based methods.\n\\end{enumerate}\nThe taxonomy categorizes OCC methods based on training data usage, feature and classification method used. Contributions of each work described falls under one or more aforementioned categories.  In this paper, we discuss features and classifiers used in the recent literature for OCC. Unless otherwise specified, all methods are designed to train with only positively labeled data belonging to the same class.\n\\begin{figure}\n\t\\centering\n\t\\resizebox{1\\linewidth}{!}{\n\t\t\\begin{forest}\n\t\t\tfor tree={\n\t\t\t\tline width=1pt,\n\t\t\t\tdraw=linecol,\n\t\t\t\tfit=rectangle,\n\t\t\t\tedge={color=linecol, >={Triangle[]}, ->},\n\t\t\t\twhere level=0{\n\t\t\t\t\tl sep+=5pt,\n\t\t\t\t\tcalign=child,\n\t\t\t\t\tcalign child=2,\n\t\t\t\t\talign=center,\n\t\t\t\t\tmy rounded corners,\n\t\t\t\t\tfor descendants={\n\t\t\t\t\t\tcalign=first,\n\t\t\t\t\t},\n\t\t\t\t}{\n\t\t\t\t\twhere level=1{\n\t\t\t\t\t\tmy rounded corners,\n\t\t\t\t\t\talign=center,\n\t\t\t\t\t\tparent anchor=south west,\n\t\t\t\t\t\ttier=three ways,\n\t\t\t\t\t\tfor descendants={\n\t\t\t\t\t\t\tchild anchor=west,\n\t\t\t\t\t\t\tparent anchor=west,\n\t\t\t\t\t\t\talign=left,\n\t\t\t\t\t\t\tanchor=west,\n\t\t\t\t\t\t\tedge path={\n\t\t\t\t\t\t\t\t\\noexpand\\path[\\forestoption{edge}]\n\t\t\t\t\t\t\t\t(!to tier=three ways.parent anchor) |-\n\t\t\t\t\t\t\t\t(.child anchor)\\forestoption{edge label};\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t}{}\n\t\t\t\t},\n\t\t\t}\n\t\t\t[One Class Classification\n\t\t\t[Data\n\t\t\t[Positive data\n\t\t\t[Positive and unlabeled data\n\t\t\t[Positive and labeled OOD data]\n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\t\t[Features \n\t\t\t[Hand-crafted\n\t\t\t[Statistical data driven\n\t\t\t[Deep Learning-based \n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\t\t[Classification Algorithm\n\t\t\t[Statistical\n\t\t\t[Representation-based \n\t\t\t[Deep Learningbased:\\\\\n\t\t\t - Discriminative Methods\\\\\n\t\t\t - Generative Models\\\\\n     - Knowledge Distillation\n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\t\t]\n\t\\end{forest}}\n\t\\caption{A taxonomy for one class classification methods. These methods can be broadly categorized by the type of training data used, feature representations used and learning method used for modeling one-class data. \\label{fig:tax}}\n\\end{figure}\nIn Figure~\\ref{fig:time},  landmarks of OCC are illustrated with a break down of their contributions (feature learning, classifier learning, both feature and classifier learning).  We further indicate whether each method is based on a statistical learning framework or a deep learning framework in Figure~\\ref{fig:time}. Initial works on OCC primarily used statistical features and focused on developing classifiers.  Most methods since 2017 have used deep features in their frameworks. These methods either use classical classifiers on deep features or simultaneously learn both features and classifier. In Table~\\ref{tbl:sum} we summarize papers we survey in this work and specify their contributions with respect to the taxonomy we provided.\n\\begin{table*}[] \\label{tbl:sum}\n\t\\centering\n\t\t\t\\caption{Summary of works surveyed in this paper. This table indicates whether the contribution of each work is in the classifier or in the feature (or both).}\n\t\\begin{tabular}{|l|l|l|l|l|}\n\t\t\\hline\n\t\t\\textbf{Method}               & \\textbf{Features} & \\textbf{Classifier} & \\textbf{Data}  & \\textbf{Publication}                      \\\\ \\hline\\hline\n\t\tKernel PCA                    & Statistical       & Representation      & Positive       & Hoffman  2007 (Pattern Recognition)   \\\\ \\hline\n\t\tGeometric Transformations      & Deep              & -                   & Positive       & Golan and El-Yaniv 2018 (NeurIPS)               \\\\ \\hline\n\t\tDeep Metric Learning          & Deep              & -                   & Positive + OOD & Masana et al. 2018 (BMVC)                 \\\\ \\hline\n\t\tFeature Learning With OOD Data (DOC) & Deep              & -                   & Positive + OOD & Perera and Patel 2019 (TIP)                  \\\\ \\hline\n\t\tOne-class SVM                          & -                 & Statistical         & Positive       & SchÃ¶lkopf et al. 2001 (Neural Comput.)    \\\\ \\hline\n\t\tSVDD                          & -                 & Statistical         & Positive       & Tax and Duin 2004 (Mach. Learn.)           \\\\ \\hline\n\t\tOCMPM                         & -                 & Statistical         & Positive       & Lanckriet et al. 2002 (NeurIPS)           \\\\ \\hline\n\t\tDS-- OCMPM                    & -                 & Statistical         & Positive       & Perera and Patel 2018 (BTAS)                 \\\\ \\hline\n\tKNFST                       & -                 & Representation      & Positive       & Bodesheim et al. 2013 (CVPR)               \\\\ \\hline\n\t\tGODS                         & -           & Statistical                & Positive       & Wang et al. 2019 (ICCV)                \\\\ \\hline\n\t\tOCCNN                         & Deep              & Deep                & Positive + OOD      & Oza and Patel 2019 (SigPro Letters)          \\\\ \\hline\n\t\tDeep SVDD                     & Deep              & Deep                & Positive       & Ruff et al. 2018 (ICML)                   \\\\ \\hline\n\t\tAnoGAN                        & Deep              & Representation      & Positive       & Schlegl et al. 2017 (IPMI)                \\\\ \\hline\n\t\tALOCC                         & Deep              & Deep     & Positive       & Sabokrou et al. 2018 (CVPR)               \\\\ \\hline\n\t\tOCGAN                         & Deep              & Representation      & Positive       & Perera et al. 2019 (CVPR)                 \\\\ \\hline\n\t\tPGND                          & Deep              & Deep                & Positive       & Pidhorskyi et a. 2018 (NeurIPS)           \\\\ \\hline \n\t\tAND                           & Deep              & Deep                & Positive       & Abati et al. 2019 (CVPR)                  \\\\ \\hline\n\t\tICS                           & Deep              & Deep                & Positive       & Schlachter et al. 2019 (EUSIPCO)                 \\\\ \\hline\n\t\tP-KDGAN                   & Deep              & Deep                & Positive       & Zhang et al. 2020 (IJCAI)                  \\\\ \\hline\n\t\tHRN          & Deep              & Deep                & Positive       & Hu et al. 2020 (NeurIPS)           \\\\ \\hline\n\t\tUS-OCL                   & Deep              & Deep                & Positive       & Bergmann et al. 2020 (CVPR)  \\\\ \\hline\n\t\tOGN                   & Deep              & Deep                & Positive       & Zaheer et al. 2020 (CVPR)              \\\\ \\hline\n\t\\end{tabular}\t\n\\end{table*}\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{timeline5}\n\t\\vskip -2.5pt\n\t\\caption{Landmarks of one-class classification showing the evolution of methods over the years. As we can see the recent trend in one-class classification largely focuses on developing deep learning-based methods.}\n\t\\label{fig:time}\n\\end{figure*}", "cites": [3283, 3213, 6887, 6886, 6889, 3282, 3231, 6888, 3332, 8160, 3322], "cite_extract_rate": 0.5, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of a taxonomy for one-class classification methods and lists various papers under the categories of features, classifier, and data. While it organizes methods into a structured taxonomy and references their contributions, it lacks deeper synthesis of ideas across papers and offers minimal critical evaluation or abstraction to broader trends or principles."}}
{"id": "9207cc91-ae73-4f41-b858-910b0d5b8f75", "title": "Deep learning-based Features", "level": "subsection", "subsections": [], "parent_id": "3cde7825-71b6-4de6-b6e0-dc7fa26917fa", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Features for OCC"], ["subsection", "Deep learning-based Features"]], "content": "\\noindent \\textbf{Deep Auto-encoders}\\\\\t\nAn auto-encoder is a (convolutional) neural network consisting of an encoder ($\\mbox{En}$) and decoder ($\\mbox{De}$) networks as shown in  Figure~\\ref{fig:ae}.  The encoder comprises of a set of convolution layers followed by activation functions. The decoder consists of transposed convolutional layers and commonly has a structure similar to that of an inverted encoder.  An auto-encoder is trained with the objective of minimizing the distance between the input and the output of the network. In theory, any distance measure, such as:\n\\begin{equation}\n\\mathcal{L}_{mse} = \\|{x} - \\mbox{De}(\\mbox{En}({x})) \\|_2^2,\n\\end{equation}\ncan be considered to learn the parameters of the auto-encoder, where ${x}$ is the input image. \n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.7\\linewidth]{ae}\n\t\\vskip -2.5pt\n\t\\caption{Learning features with the help of a stacked auto-encoder network trained to reconstruct the input images using mean-squared error.}\n\t\\label{fig:ae}\n\\end{figure}\nIt is the usual practice to have a bottleneck latent-space in between with a dimension smaller than the input. Due to this bottleneck, auto-encoder retains only the essential information in the latent space which is required for reconstruction. These encoder features are informative and can be expected to exhibit \\textit{descriptiveness}. It has been shown in the literature that adding noise to the input can further improve the quality of the learned representation by reducing the problem of over-fitting, making it more generalizable .  When noise is added to the input, the network is referred to as a \\textit{de-noising auto-encoder} . In a de-noising auto-encoder, given a noisy image, the network is expected to reconstruct the clean version of the image.\\\\\n\\noindent \\textbf{Geometric Transformation based Self-supervision}\\\\\nSelf supervision is a machine learning technique that can be used to learn informative representations from unlabeled data. Golan \\emph{et al.} showed that self supervision can yield representations that work in favor of one class classification , . In their work, a random geometric transform chosen from a pre-determined set of transforms is first applied to every input image during training. The network is trained to correctly predict the applied transformation. Let us denote a set of $k$ geometric transformations as $\\{\\mathcal{T}_1 ,\\mathcal{T}_1 ,\\dots,\\mathcal{T}_{k}  \\}$, where for any given image ${x}$ both ${x}, \\mathcal{T}_i({x}) \\in \\mathbb{R}^{H \\times W}$ and $\\mathcal{T}_1$ denotes identity transformation. During training, for a given input, a number $0 \\geq r \\geq k$ is randomly chosen. The input image ${x}$ is transformed using the $r^{th}$ transformation to arrive at $\\mathcal{T}_r({x})$. The transformed image is passed through a convolutional neural network and the network parameters are optimized with a cross-entropy loss where $r$ is considered to be the ground truth label. The trained network produces features suitable for one class classification.\nGolan \\emph{et al.}  showed that a neural network trained in this manner produces relatively higher probability scores in the ground truth class for samples of the positive class. For a given test image ${x}_{test}$,  proposed to evaluate a normality score $s_{test}$ by considering log likelihood probability of the ground truth class with all $k$ transformations as:\n\\begin{equation}\ns_{test}({x}_{test}) = \\sum_{i=1}^{k} \\log(F(\\mathcal{T}_i({x}_{test}))),\n\\end{equation}\nwhere $F(\\cdot)$ is the softmax output of the network. Furthermore, Golan \\textit{et al.}  derived a Dirichlet distribution-based score which is more effective for one class classification. This second score was derived by assuming that each conditional distribution  takes the form of a Dirichlet distribution  $F(\\mathcal{T}_i({x}))|\\mathcal{T}_i \\sim Dir(\\mathbf{a}_i)$ where $\\mathbf{a}_i \\in \\mathbb{R}_{+}^{k}$. When estimates of Dirichlet parameters $\\hat{\\mathbf{a}}_i$ are found with the help of maximum likelihood estimation, the final normality score denoted as $u_{test}$ can be expressed as: \n\\begin{equation}\n\tu_{test}(x_{test}) = \\sum_{i=1}^{k} (\\hat{\\mathbf{a}}_i - 1) \\cdot f(\\mathcal{T}_i({x})),\n\\end{equation}\n\\noindent \\textbf{Deep Metric Learning with OOD Data} \\\\\nA contrastive loss-based metric learning method is proposed in  to learn features for OCC.  For the metric learning process,   proposes to use data from an OOD dataset. In the absence of an OOD dataset, it is artificially generated by adding random Gaussian noise to the images. Let $F$ be the network function of a deep convolutional network. For a pair of input images ${x_1}$ and ${x_1}$, the distance between the to inputs in the feature space is defined as $K(x_1, x_2) = \\| F( {x_1}) - F( {x_2})\\|_2$.  The training process incorporates two types of labels. Label $\\gamma$ indicates whether the two inputs belong to the same class $(\\gamma=0)$ or not $(\\gamma=1)$. Label $\\zeta$ denotes if both images belong to a OOD dataset $(\\zeta = 0$) or not ($\\zeta = 1$). The contrastive loss is defined as:\n\\begin{equation}\n\\begin{array}{llllll}\n\\mathcal{L} = &\\frac{1}{2} (1 - \\gamma) \\cdot \\zeta \\cdot K(x_1, x_2)^2 \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ + \\frac{1}{2} \\gamma \\cdot \\zeta \\cdot (\\max (0, m-K(x_1, x_2))^2,\n\\end{array}\n\\end{equation}\nwhere, $m$ denotes minimum margin. For positively labeled data when both images are from the same class ($\\gamma=0, \\zeta = 1$), the loss becomes  $\\frac{1}{2} K(x_1, x_2)^2$. Therefore, during training network learns a similar embedding for positive data. As a result, the compactness property is satisfied. On the other hand, when the image pair is from different classes, the embedding is encouraged to be at least $m$ distance apart in the feature space. In the case when both images are from the same class ($\\gamma=0, \\zeta = 0$), the loss becomes zero. Therefore, the learned feature embedding becomes descriptive with the ability of differentiating positively labeled and data outside the given category.\\\\\n\\noindent \\textbf{Feature Learning With OOD Data (DOC)}\\\\\nIn , authors consider the special scenario where a set of labeled Out Of Distribution (OOD) data is available during training along side positive data. These OOD data are data collected from non-overlapping problem domain. For example, in the case of a face recognition problem for one-class classification an annotated object dataset can be considered as OOD. This setting is important in practice as many real life applications of OCC can be solved in this setting. Let us consider a deep network with a feature extraction sub-network $F$ and a classification sub-network $G$. The network $G \\circ F $ is first trained using the OOD data using cross entropy loss. Let us denote the positively labeled dataset with $N$ number of training images as $\\mathcal{D}_t=\\{x_{ti}\\}_{i=1}^{N}$ and OOD dataset with $M$ number of images as $\\mathcal{D}_r=\\{x_{ri}, y_{ri}\\}_{i=1}^{M}$, where images $x_{ti}, x_{ri} \\in \\mathbb{R}^{3 \\times H \\times W}$ and $y_{ri}$ is the target label for the image $x_{ri}$. The features extracted from any image $x_{ti}$ are denoted as $F(x_{ti}) = \\mathbf{z}_{ti} \\in \\mathbb{R}^d$. Here, $d$ is the dimension of the feature space. A \\textit{compactness loss $\\mathcal{L}_{c}$} is defined that measures compactness of the learned feature with respect to positive data. The \\textit{compactness loss} is evaluated using a intra-class distance for that given batch in the feature space as:\n\\begin{equation}\\label{eq:lb}\n\\mathcal{L}_{c}(x_{ti}) = \\frac{1}{d} (\\mathbf{z}_{ti} - \\bm{\\mu}_{ti})^T(\\mathbf{z}_{ti}- \\bm{\\mu}_{ti}),\n\\end{equation}\nwhere feature extracted from each image $x_{ti}$ $\\mathbf{z}_{ti} \\in \\mathbb{R}^{d}$. The mean vector $\\bm{\\mu}_{ti} $ is defined as $\\bm{\\mu}_{ti}  = \\frac{1}{N-1}\\sum_{j \\neq i} \\mathbf{z}_{tj}$. Additionally, a \\textit{descriptiveness loss} denoted as $\\mathcal{L}_{d}$ is used to measure descriptiveness of the learned features. It is measured using cross-entropy loss  obtained by the output of the network with respect to OOD data. The network is fine-tuned by jointly optimizing over both \\textit{compactness} and \\textit{descriptiveness} loss as:\n\\begin{equation}\\label{eqn:opt2}\n\\min_{F, G} ~ \\sum_{i=1}^{M} \\mathcal{L}_{d}({x}_{ri}, y_{ri}) + \\lambda  \\sum_{i=1}^{N} \\mathcal{L}_{c}({x}_{ti}),\n\\end{equation}\nwhere, $x_{ri}$ and $x_{ti}$ denote data from the OOD dataset and the positively labeled data respectively and $\\lambda$ is a hyper-parameter. As a result, the learned feature is both descriptive and compact with respect to the positively labeled data.", "cites": [126, 3322, 3332], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the three cited papers by connecting auto-encoders, geometric transformation-based self-supervision, and deep metric learning with OOD data under the theme of deep learning for OCC feature learning. It provides a coherent narrative by highlighting how each approach contributes to the descriptiveness and compactness of features. Some critical analysis is present, particularly in discussing the effectiveness of Dirichlet-based scoring and the use of OOD data, but a deeper comparative or evaluative discussion is missing. The section begins to abstract beyond individual papers by emphasizing key principles like compactness and descriptiveness, but these are not fully generalized into a broader conceptual framework."}}
{"id": "ff91c738-3dcd-4aba-9e24-a390a37e2d43", "title": "Statistical Methods", "level": "subsection", "subsections": [], "parent_id": "7e315fe8-dd5f-4430-b1ff-fd077a871776", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "OCC Algorithms"], ["subsection", "Statistical Methods"]], "content": "\\noindent \\textbf{One Class Support Vector Machine (OCSVM)}\\\\\nOne class SVM is a special case of Support Vector Machine (SVM) formulation. In a binary SVM, the  hyper-plane that separates the two classes with that largest possible margin is found. The hyper-plane is defined by support vectors. In the case of one class classification, there exists only positively labeled data during training. In One Class SVM (OCSVM), hyperplane corresponding to negative class are set to be the origin of the coordinate system . Therefore, the objective of OCSVM boils down to finding a hyper-plane furthest away from the origin, where positively labeled data exists in the positive half space of the hyper-plane. When this constraint is relaxed using slack variables, the optimization objective can be written as:\n\\begin{equation} \\label{eq:ocsvm}\n\\begin{array}{rrclcl}\n\\displaystyle\t\\min_{\\mathbf{w}, \\bm{\\xi}, b} & \\frac{1}{2}||\\mathbf{w}||^2 +\\frac{1}{\\nu N} \\sum_i \\xi_i - b \\\\\n\\text{s.t.} & \\langle \\mathbf{w}, \\Phi(\\mathbf{x}_i ) \\rangle \\geq b - \\xi_i, \\xi_i \\geq 0,\n\\end{array}\n\\end{equation}\nwhere, the column vector $\\bm{\\xi} = [\\xi_i, \\ \\xi_2, \\ \\dots, \\ \\xi_N]$ and each $\\xi_i$ is the slack variable corresponding to the $i^{th}$ training sample (i.e. vectorized image), $\\Phi$ is a mapping function that maps $\\mathbf{x}_i$ to a kernel space where dot products are defined using a kernel function $K(\\cdot, \\cdot)$, $b$ is the bias term and $\\nu$ is a trade-off parameter, and $N$ is number of training examples. When the optimization is solved, inference on a query sample $\\mathbf{x}_{test}$ can be done using the condition $\\text{sgn} (\\langle \\mathbf{w}, \\phi(\\mathbf{x}) \\rangle - b)$.\nEq. \\ref{eq:ocsvm} can be modified with the help of Lagrange multipliers $\\alpha_i, \\beta_i \\geq 0$ as follows:\n\\begin{equation}\n\\begin{multlined}\n\\mathcal{L}(\\mathbf{w}, \\bm{\\xi}, b, \\bm{\\alpha}, \\bm{\\beta} ) = \\frac{1}{2}||\\mathbf{w}||^2 +\\frac{1}{\\nu N} \\sum_i \\xi_i - b \\\\ - \\sum_i \\alpha_i  ( \\langle \\mathbf{w}, \\Phi(\\mathbf{x}_i) \\rangle - b + \\xi_i) - \\sum_i \\beta_i \\xi_i,  \n\\end{multlined}\n\\end{equation}\nwhere the column vectors $\\bm{\\alpha} = [\\alpha_i, \\ \\alpha_2, \\ \\dots, \\ \\alpha_N]^{T}$ and $\\bm{\\beta} = [\\beta_i, \\ \\beta_2, \\ \\dots, \\ \\beta_N]^{T}$. Setting derivatives with respect to primal variables to zero, it can be shown that $\\mathbf{w} = \\sum_i \\langle \\alpha_i, \\Phi(\\mathbf{x}_i) \\rangle$, $\\alpha_i = \\frac{1}{\\nu N} - \\beta_i \\leq \\frac{1}{\\nu N}$  and  $\\sum_i \\alpha_i = 1$. By substituting these values in Eq.~\\ref{eq:ocsvm}, the dual optimization problem can be derived as:\n\\begin{equation} \\label{eq:ocsvm_dual}\n\\begin{array}{rrclcl}\n\\displaystyle\t\\min_{\\bm{\\alpha}} & \\frac{1}{2}  \\sum_i \\sum_j \\alpha_i \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j) \\\\ \\text{s.t.} & 0 \\leq \\alpha_i \\leq \\frac{1}{\\nu N}, \\sum_i \\alpha_i = 1.\n\\end{array}\n\\end{equation}\nFurthermore, it can be shown that when $ 0 \\leq \\alpha_i \\leq \\frac{1}{\\nu N}$ is satisfied the bias term can also be expressed as: \n\\begin{equation}\nb = \\langle \\mathbf{w}, \\Phi(\\mathbf{x}_i) \\rangle = \\sum_j \\alpha_j K(\\mathbf{x}_i, \\mathbf{x}_j).\n\\end{equation}\nWith the dual form of the problem, as shown in Eq.~\\ref{eq:ocsvm_dual}, the optimal values of parameters in problem shown in Eq.~\\ref{eq:ocsvm} can be found using the kernel function $K(\\cdot, \\cdot)$ without explicitly defining the mapping function $\\Phi(\\cdot)$. The decision for any test image $x_{test}$ that is vectorized as $\\mathbf{x}_{test}$ can also be expressed in terms of the kernel function using the dual variables and vectorized training images as follows:\n\\begin{equation}\n\t\\text{sgn}(\\sum_i \\alpha_i K(\\mathbf{x}_i, \\mathbf{x}_{test}) - b),\n\\end{equation}\n\\noindent \\textbf{Support Vector Data Descriptor (SVDD)} \\\\\nThe SVDD  formulation closely follows the OCSVM objective. However, instead of learning a hyper-plane to separate data from origin, Tax \\emph{et al.}  propose to find the smallest hyper-sphere that can fit given training samples. The hyper-sphere is characterized by its mean vector (or centroid of hyper-sphere) $\\mathbf{o}$ and radii $r_d>0$. The volume of hyper-sphere is minimized by minimizing $r_d \\in \\mathbb{R}$ while making sure hyper-sphere encloses most of the training samples. This objective can be written down in the form of optimization problem as:\t\n\\begin{equation} \\label{eq:svdd}\n\\begin{array}{ccclcl}\n\\displaystyle\t\\min_{\\mathbf{o}, \\bm{\\xi}, r_d} & r_d^2 + \\lambda \\sum_i \\xi_i \\\\\n\\text{s.t.} &  \\|\\mathbf{x}_i - \\mathbf{o}\\|^2 \\leq r_d^2 + \\xi_i, \\xi_i \\geq 0 ~ \\forall i. \\\\\n\\end{array}\n\\end{equation}\nParameter $\\lambda$ controls the trade-off between errors and the objective. Similar to the OCSVM, the above objective can be modified with the help of the Lagrangian multipliers and the updated optimization problem can be re-formulated as:\n\\begin{equation} \\label{eq:svdd_lag}\n\\begin{multlined}\n\\mathcal{L}(r_d, \\mathbf{o}, \\bm{\\alpha}, \\bm{\\gamma}, \\mathbf{\\xi}) = r_d^2 + \\lambda\\sum_i\\xi_i - \\sum_i\\alpha_i(r_d^2 + \\xi_i - \\\\ (\\|\\mathbf{x}_i\\|^2 - 2 \\langle \\mathbf{o}, \\mathbf{x}_i \\rangle + \\|\\mathbf{o}\\|^2)) - \\sum_i\\gamma_i\\xi_i,\n\\end{multlined}\n\\end{equation}\nwhere, the column vectors $\\bm{\\alpha} = [\\alpha_i, \\ \\alpha_2, \\ \\dots, \\ \\alpha_N]^{T}$ and $\\bm{\\gamma} = [\\gamma_i, \\ \\gamma_2, \\ \\dots, \\ \\gamma_N]^{T}$. By setting derivatives of primal variables to zero, it can be shown that $\\sum_i \\alpha_i = 1$, $\\mathbf{o} = \\sum_i \\alpha_i \\mathbf{x}_i$ and $\\lambda-\\alpha_i-\\gamma_i = 0$. By substituting to Equation~\\ref{eq:svdd_lag}, the dual form can be obtained as:\n\\begin{equation} \n\\begin{array}{rrclcl}\n\\displaystyle\t\\min_{\\bm{\\alpha}} & \\sum_i \\sum_j \\alpha_i\\alpha_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle - \\sum_i \\alpha_i\\langle \\mathbf{x}_i, \\mathbf{x}_i \\rangle\\\\\n& \\text{s.t.}  \\ \\ 0 \\leq \\alpha_i \\leq \\lambda, \\sum_i \\alpha_i = 1. \\\\\n\\end{array}\n\\end{equation}\nA given test sample $\\mathbf{x}_{test}$, is assigned a positive label if it is inside the identified hyper-sphere. More precisely, if the following condition is met:\n\\begin{equation} \\label{eq:svdd_test}\n\\begin{multlined}\n\\|\\mathbf{x}_{test} - \\mathbf{o}\\|^2 = \\langle \\mathbf{x}_{test}, \\mathbf{x} \\rangle - 2\\sum_i\\alpha_i\\langle \\mathbf{x}, \\mathbf{x}_i \\rangle \\\\ + \\sum_i\\sum_j \\alpha_i\\alpha_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle \\leq r_d^2.\n\\end{multlined}\n\\end{equation}\nSince, the dual form and the inference equation both include inner product terms of $\\mathbf{x}_i$ and $\\mathbf{x}$, SVDD can be extended to a kernel formulation by simply replacing product terms by a kernel function that corresponds to some mapping function $\\Phi$ as, $\\langle \\Phi(\\mathbf{x}_j), \\Phi(\\mathbf{x}_i) \\rangle = K(\\mathbf{x}_i, \\mathbf{x}_j)$. The kernalized version of the optimization problem of dual form then can be expressed as:\n\\begin{equation} \n\\begin{array}{rrclcl}\n\\displaystyle \\min_{\\mathbf{\\alpha} } &  \\sum_i\\sum_j \\alpha_i\\alpha_j(K(\\mathbf{x}_i, \\mathbf{x}_j) - \\sum_i\\alpha_i(K(\\mathbf{x}_i, \\mathbf{x}_i)))\\\\\n& \\text{s.t.} \\ \\ \\  0 \\leq \\alpha_i \\leq C,  \\sum_i \\alpha_i = 1.\\\\\n\\end{array}\n\\end{equation}\nAs we mentioned earlier that $\\sum_i \\alpha_i = 1$. Due to that in the case where the kernel function only depends on the difference between the two vectors, i.e., when $K(\\mathbf{x}_1,\\mathbf{x}_2)$ depends only on $\\mathbf{x}_1-\\mathbf{x}_2$, the linear term of the dual objective function becomes constant and the optimization becomes equivalent to the dual form of OCSVM in Equation~\\ref{eq:ocsvm_dual} discussed in the previous section.\\\\\n\\noindent \\textbf{One Class Mini-max Probability Machine (OCMPM)}\\\\\nSimilar to in one-class SVM, One class Mini-max Probability Machine (1-MPM) tries to maximize the distance between the origin and learned hyper-place with the objective of arriving at a tighter lower bound to the data. However, 1-MPM takes advantage of second order statistics of training data when the hyper-plane is learned. \nIn the 1-MPM algorithm, both mean $\\bm{\\mu}$ and the covariance matrix $\\mathbf{C}$ of the vectorized images is calculated. The covariance matrix can be calculated as $\\mathbf{C} = \\frac{1}{N} \\sum_{i=1}^{N} (\\mathbf{x}_i-\\bm{\\mu}) (\\mathbf{x}_i- \\bm{\\mu})^T$, where $\\bm{\\mu}$ is the mean is calculated as $\\bm{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N}\\mathbf{x}_N$. Furthermore, 1-MPM does not assume any distribution of the underlying data $\\mathbf{x}$ unlike PCA (which has inherent assumption of data belonging to Gaussian distribution). With the help of both mean and covariance information of the data, 1-MPM seeks to find a hyper-plane $(\\mathbf{w}, b)$ with $\\mathbf{w} \\in \\mathbb{R}^n \\setminus \\{0\\}, b \\in \\mathbb{R}$ such that data lies in the positive half space defined by $\\{\\mathbf{x} |\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{w}^T\\mathbf{x} \\geq b\\}$, at least with a probability of $\\tau$, even in the worst case scenario. With this background, the objective function of single class MPM can be formulated as in:\n\\begin{equation}\n\\max_{\\mathbf{w} \\neq 0, b} \\frac{b}{\\sqrt{\\mathbf{w}^T\\mathbf{C}\\mathbf{w}}} \\ \\ \\ \\text{s.t.} ~ \\inf_{(\\mathbf{x}, \\bm{\\mu}, \\mathbf{C})} P(\\mathbf{w}^T\\mathbf{x} \\geq b) \\geq \\tau,\n\\end{equation}\nwhen the distance between the origin and the hyper-plane is measured in terms of Mahalonabis distance with respect to\n$\\mathbf{C}$. Since this problem is positively homogeneous in $(\\mathbf{w}, b)$ and because $\\mathbf{w}\\neq\\mathbf{0}$ is always satisfied when $b>0$, value of $b$ is taken to be one without loss of generality. Then, a equivalent optimization problem can be obtained by minimizing the reciprocal of Mahalonobis distance as in:\t\n\\begin{equation}\\label{mpm1}\n\\min_{\\mathbf{w}} {\\sqrt{\\mathbf{w}^T\\mathbf{C}\\mathbf{w}}}  \\ \\ \\ \\text{s.t.} ~ \\inf_{(\\mathbf{x}, \\bm{\\mu}, \\mathbf{C})} \\ P(\\mathbf{w}^T\\mathbf{x} \\geq 1) \\geq \\tau,\n\\end{equation}\nUsing the core MPM theorem in , where it is stated that  $\\inf_{(\\mathbf{x}, \\bm{\\mu}, \\mathbf{C})} \\ P(\\mathbf{w}^T\\mathbf{x} \\geq b) \\geq \\tau$ is equivalent to $b-\\mathbf{w}^T\\hat{\\mathbf{x}} \\geq g(\\tau) \\sqrt{\\mathbf{w}^T \\mathbf{C}}$, where $ g(\\tau) = \\sqrt{\\frac{\\tau}{1-\\tau}}$, Equation~\\ref{mpm1} can be re-written as:\n\\begin{equation} \\label{mpm2}\t\n\\min_{\\mathbf{w}}   {\\|\\mathbf{w}^T\\mathbf{C}^{\\frac{1}{2}}\\|_2} \\ \\ \\ \\text{s.t.} \\ \\ 1-\\mathbf{w}^T\\bm{\\mu} \\geq g(\\tau) {\\|\\mathbf{w}^T \\mathbf{C}^{\\frac{1}{2}}\\|_2}.\n\\end{equation}\nHere it should be noted that for a real symmetric covariance matrix $\\mathbf{C}$, the matrix $\\mathbf{C}^{\\frac{1}{2}}$ always exists. Since optimization problem shown in Equation~\\ref{mpm2} is a second order cone program it can be efficiently solved using convex optimization tools.\nIn the robust 1-MPM formulation, it is assumed that difference between sample covariance and true covariance of the distribution will not exceed some arbitrary constant denoted as $\\rho$ and Mahalanobis distance between sample mean and true mean with respect to true covariance will not exceed an arbitrary constant denoted as $\\nu^2$ . Under these assumptions, it is shown in  that $\\mathbf{w}^T\\mathbf{C}\\mathbf{w}$ term in Equation~\\ref{mpm1} gets substituted by $\\mathbf{w}^T(\\mathbf{C}+\\rho\\mathbf{I}_n)\\mathbf{w}$, while $g(\\tau)$ term is changed into $K(\\tau)+\\nu$. \\\\\n\\noindent \\textbf{Dual slope Mini-max Probability Machine (DS--OCMPM)}\nDual slope Mini-max Probability Machine is a an extension of 1-MPM considering two decision hyper-planes and availability of sub-clusters. In , a second hyper-plane $(\\tilde{\\mathbf{w}}, \\tilde{b})$ is learned such that the data projected on the hyper-plane $\\tilde{\\mathbf{w}}$ has the largest possible variance. The optimization objectives takes the form of:\n\\begin{equation} \\label{mpm3} \n\\max_{\\tilde{\\mathbf{w}}} {\\sqrt{\\tilde{\\mathbf{w}}^T\\mathbf{C}\\tilde{\\mathbf{w}}}} \\ \\ \\ \\text{s.t.} ~ \\inf_{\\mathbf{x}, {\\bm{\\mu}},\\mathbf{C})} \\ P(\\tilde{\\mathbf{w}}^T\\mathbf{x} \\geq 1) \\geq \\tilde{\\tau}.\n\\end{equation}\nSince maximizing $\\sqrt{\\tilde{\\mathbf{w}}^T\\mathbf{C}\\tilde{\\mathbf{w}}}$ is equivalent to minimizing $\\sqrt{\\tilde{\\mathbf{w}}^T{\\mathbf{C}}^{-1}\\tilde{\\mathbf{w}}}$. Optimization problem in Equation~\\ref{mpm3} can be transformed into another second order cone program of the form:\n\\begin{equation}\\label{mpm4}\n\\min_{\\tilde{\\mathbf{w}}}   {\\|\\tilde{\\mathbf{w}}^T\\mathbf{C}^{-\\frac{1}{2}}\\|_2} \\ \\ \\ \\text{s.t.} ~ 1-\\tilde{\\mathbf{w}}^T\\bm{\\mu} \\geq g(\\tilde{\\tau}) {\\|\\tilde{\\mathbf{w}}^T\\mathbf{C}^{-\\frac{1}{2}}\\|_2}.\n\\end{equation}\nAssuming that the difference between sample covariance and true covariance of the distribution will not exceed some arbitrary constant $\\tilde{\\rho}$ for $\\mathbf{C}^{-1}$, the robust version of the optimization problem is obtained by substituting $\\tilde{\\mathbf{w}}^T{\\mathbf{C}}^{-1}\\tilde{\\mathbf{w}}$ term in Equation~\\ref{mpm3} by $\\tilde{\\mathbf{w}}^T{(\\mathbf{C}+\\tilde{\\rho}\\mathbf{I}_n)}^{-1}\\tilde{\\mathbf{w}}$.\nIn order to mitigate the effect of sub-distributions, data is clustered into $k$ clusters using the Ward's method . Global mean and variance $(\\bm{\\mu}, \\mathbf{C})$ are calculated with respect to the whole dataset along with cluster-wise statistics $(\\bm{\\mu}_i, \\mathbf{C}_{i})$ for $i^{th}$ cluster. Optimization is carried out over cumulative covariance of each individual cluster $\\sum_{i} \\sqrt{\\mathbf{w}^T\\mathbf{C}_{i}\\mathbf{w}}$ while constraints are defined with respect to global statistics $(\\bm{\\mu}, \\mathbf{C})$. \nGiven global and local clusters $(\\bm{\\mu}, \\mathbf{C})$ and $(\\bm{\\mu}_i, \\mathbf{C}_{i})$ for $i=1, 2, \\dots, k$, where $k$ is the number of clusters, the following joint-optimization problem is solved to find both $\\mathbf{w}$ and $\\tilde{\\mathbf{w}}$:\n\\begin{equation}\n\\begin{aligned}\n& \\underset{\\mathbf{w}, \\tilde{\\mathbf{w}}}{\\text{minimize}}\n& & \\sum_{i=1}^{k} ||\\mathbf{w}^T(\\mathbf{C}_{i} + \\rho\\mathbf{I}_n)^{\\frac{1}{2}}||_2 + \\|\\tilde{\\mathbf{w}}^T(\\mathbf{C}_{i} + \\tilde{\\rho}\\mathbf{I}_n)^{-\\frac{1}{2}}\\|_2 \\\\\n& \\text{subject to}\n& & (g(\\tilde{\\tau})+\\nu){||\\tilde{\\mathbf{w}}^T(\\mathbf{C}_{i} + \\tilde{\\rho}\\mathbf{I}_n)^{-\\frac{1}{2}}||_2} - 1 \\leq \\tilde{\\mathbf{w}}^T\\bm{\\mu}\\\\\n&&& (g(\\tau)+\\nu){\\|\\mathbf{w}^T(\\mathbf{C}_{i} + \\rho\\mathbf{I}_n)^{\\frac{1}{2}}\\|_2} - 1 \\leq \\mathbf{w}^T\\bm{\\mu}.\n\\end{aligned}\n\\end{equation}\nSince the product of $\\mathbf{w}$ and $\\tilde{\\mathbf{w}}$ do not appear in the optimization statement, this problem can be solved independently for $\\mathbf{w}$ and $\\tilde{\\mathbf{w}}$ using two second order cone programs. Once hyper-plane parameters are obtained, given a test sample $\\mathbf{x}_{test}$, identity of the sample would be assigned to be negative if $\\mathbf{w}^T\\mathbf{x}_{test} < 1 \\cap \\tilde{\\mathbf{w}}^T\\mathbf{x}_{test} < 1$, and positive otherwise.\\\\\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{comp4}\n\t\\caption{ Decision boundaries obtained by different statistical one class classifiers.}\n\t\\label{fig:comp}\n\\end{figure*}\n\\noindent \\textbf {Generalized One-class Discriminative Sub-spaces (GODS)}\\\\\nGODS  extends one-class SVM formulation into two separating hyper-planes. Similar to one-class SVM, the first hyper-plane $(\\mathbf{w}, b)$ is learned such that most of data points appear in the positive half space defined by the hyper-plane. In addition, the second hyper-plane $(\\tilde{\\mathbf{w}}, \\tilde{b})$ is learned such that most of the data lie in the negative space defined by the hyper-plane. A basic variant of the GODS classifier can be learned using the following optimization objective:\n\\begin{equation} \n\\begin{array}{rrclcl}\n\\displaystyle\t\\min_{(\\mathbf{w}, b), (\\tilde{\\mathbf{w}}, \\tilde{b}), \\bm{\\xi}, \\tilde{\\bm{\\xi}}, \\beta > 0} & \\frac{1}{2}||\\mathbf{w}||^2 + \\|\\tilde{\\mathbf{w}}\\|^2 \\\\  & - b - \\tilde{b} + \\lambda \\sum_i \\xi_{i} + \\tilde{\\mathbf{\\xi}_i} \\\\\n\\text{s.t.} & \\mathbf{w}^T\\mathbf{x}_i - b \\geq \\eta - \\xi_{i}, \\\\\n& \\tilde{\\mathbf{w}}^T\\mathbf{x}_i - \\tilde{b} \\leq \\eta - \\tilde{\\xi}_{i}, \\\\\n& dist((\\mathbf{w}, b), (\\tilde{\\mathbf{w}}, \\tilde{b})) \\leq \\beta,\n\\end{array}\n\\end{equation}\nwhere,  $\\lambda$ is the slack regularization constant, $\\eta$ defines the classifier margin and $dist(\\cdot, \\cdot)$ is suitable distance function between the two hyper-planes.  This loss function forces to find the most similar pair of hyper-planes that satisfies the stated objective. The additional constraint on distances prevents from finding a trivial solution. By explicitly setting $\\|\\mathbf{w}\\|_2^2 = 1$ and $\\|\\tilde{\\mathbf{w}}\\|_2^2 = 1$ and setting $dist(\\cdot, \\cdot)$ to be the Euclidean distance, this objective can be simplified as:\n\\begin{equation} \\label{eq:gods1}\n\\begin{array}{rrclcl}\n\\displaystyle & \\underset{b, \\tilde{b}, \\bm{\\xi}, \\bm{\\xi}, \\beta > 0}{\\min} \\ \\ \\ \\ \\  g(b, \\tilde{b}) - 2 \\ \\mathbf{w}^T\\tilde{\\mathbf{w}} + \\lambda \\sum_i \\xi_{i} + \\tilde{\\xi}_{i} \\\\\n& \\text{s.t.} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{i} \\lfloor \\eta - (\\mathbf{w}^T(\\mathbf{x}_i + b) - \\xi_{i} \\rfloor_+ \\\\\n& +  \\lfloor \\eta - (\\tilde{\\mathbf{w}}^T(\\mathbf{x}_i + \\tilde{b}) - \\tilde{\\xi}_{i} \\rfloor_+, \\\\\n& \\|\\mathbf{w}\\|_2^2 = 1, \\|\\tilde{\\mathbf{w}}\\|_2^2 = 1, \\\\\n\\end{array}\n\\end{equation}\nwhere $g(b, \\tilde{b}) = (b-\\tilde{b})^2-b-\\tilde{b}$ and notation $\\lfloor \\rfloor_+$ denotes hinge loss. This initial formulation can be extended to a generalized formulation by introducing sub-spaces in place of hyper-planes. Let $\\mathbf{W}, \\tilde{\\mathbf{W}} \\in S^M_n$ be orthonormal  subspace frames, where $\\mathbf{W}^T\\mathbf{W} = \\tilde{\\mathbf{W}}^T\\tilde{\\mathbf{W}} = \\mathbf{I}_M$. Here, $\\mathbb{I}_M$ is identity matrix of size $M \\times M$. Such frames  $S^M_n$ with $n$ dimensional subspaces, belong to the Stiefel manifold and each column in the matrices $\\mathbf{W}, \\tilde{\\mathbf{W}}$ of size $n \\times M$ is orthonormal to the rest. The number of hyper-planes $M$ is a hyper-parameter and can be set differently for each dataset to achieve better performance. By replacing the distance term $dist((\\mathbf{w}, b), (\\tilde{\\mathbf{w}}, \\tilde{b}))$ by the Euclidean distance of each data point from both hyper-planes, Equation~\\ref{eq:gods2} can be modified as:\n\\begin{equation} \\label{eq:gods2}\n\\begin{array}{llllll}\n\\displaystyle & \\underset{\\mathbf{W}, \\tilde{\\mathbf{W}}, \\mathbf{b}, \\tilde{\\mathbf{b}}, \\bm{\\xi} > 0}{\\min} \\ \\frac{1}{2} \\sum_{i=1}^N (\\|\\mathbf{W}^T\\mathbf{x}_i+\\mathbf{b}\\|_2^2 + \\|\\tilde{\\mathbf{W}}^T\\mathbf{x}_i+\\tilde{\\mathbf{b}}\\|_2^2) \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\ g(\\mathbf{b}, \\tilde{\\mathbf{b}}) + \\lambda \\sum_i (\\xi_{i} + \\tilde{\\xi}_{i}) \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\ \\frac{\\nu}{N} \\sum_i \\lfloor\\eta - \\min(\\mathbf{W}^T(\\mathbf{x}_i + \\mathbf{b}) - \\xi_{i}\\rfloor^2_+\\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\ \\frac{1}{2N}  \\sum_i \\lfloor\\eta + \\max(\\tilde{\\mathbf{W}}^T(\\mathbf{x}_i + \\tilde{\\mathbf{b}}) - \\tilde{\\xi}_{i} \\rfloor^2_+.\n\\end{array}\n\\end{equation}\nClassifier obtained by solving this optimization is named as the \\textit{generalized one-class discriminative subspace} (GODS) classifier.  presents a parameter initialization method and an  efficient optimization method to optimized the proposed optimization objective.\nFigure~\\ref{fig:comp} illustrates a pictorial comparison of the decision spaces obtained by different statistical one-class classifiers.\\\\", "cites": [6888], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a detailed mathematical explanation of OCSVM and SVDD, demonstrating a factual summary of their formulations. It makes a basic connection between the two methods by highlighting that SVDD can reduce to OCSVM under certain kernel conditions, indicating some synthesis. However, there is minimal critical analysis or evaluation of the cited papers' strengths and weaknesses, and abstraction is limited to general mentions of kernel methods without deeper conceptual insights."}}
{"id": "563f3500-3888-4c45-be4f-7beed6fc6c42", "title": "Discriminative Methods", "level": "subsubsection", "subsections": [], "parent_id": "ca8c7727-9d0c-4ccf-a689-aa53bf4644cb", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "OCC Algorithms"], ["subsection", "Deep Learning Methods"], ["subsubsection", "Discriminative Methods"]], "content": "The discriminative methods utilize loss functions that are typically inspired from the statistical methods like OC-SVM and SVDD or utilize regularization techniques to make traditional neural network training more compatible to one-class classification.\\\\\n\\noindent \\textbf {One Class CNN (OCCNN)}\\\\\t\nOne class CNN is a deep learning based classifier inspired by One Class SVM . It comprises of two sub-networks -- a feature extractor and a classifier as shown in Figure~\\ref{fig:ocnn}. The feature extractor network is learned using an OOD dataset in . However, in theory any One Class feature learning method  can be used for this purpose. The classifier is a fully connected network that terminates with two outputs corresponding to positive and negative classes.\nDuring training, positively labeled data are passed through the network to first obtain features. These  features are assigned a class label $y=0$. Then, a batch of random noise is sampled from the feature space using a Gaussian distribution $\\mathcal{N}(0,\\sigma^2\\textbf{I})$. This batch is assigned a class label of $y=1$. Two batches are concatenated and passed through the classifier to obtain a prediction. The network is trained using binary cross entropy loss defined as:\n\\begin{equation}\n\\mathcal{L}  = - \\frac{1}{2k} \\sum_{j=1}^{2k} y \\log(p_0) + (1-y) \\log(1-p_1),\n\\end{equation}\nwhere $k$ is the batch size, and $p_0, p_1$ are the outputs corresponding to the zeroth and first classes in the network respectively ($1-p_0 = p_1$). This work was later extended in  and  for active authentication and face presentation attack detection, respectively.\\\\\t\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.7\\linewidth]{ocnn}\n\t\\caption{ Training strategy used in One-Class CNN  is inspired from OC-SVM training. Here, the features are trained to be different compared to zero-centered Gaussian distribution, same as OC-SVM learns separating hyper-plane between one-class data from the origin.}\n\t\\label{fig:ocnn}\n\\end{figure}\n\\noindent \\textbf {Deep SVDD (DSVDD)}\\\\\t\nThe deep SVDD framework introduced in  learns a deep representation with the objective of enclosing embeddings of the positively labeled data with the smallest possible hyper-sphere.  Let $F$ be the network function of the deep network parameterized by weights $\\theta_f$ consisting of $L$ layers and $l^{th}$ layer parameters denoted as $\\theta_f^l$. When the center and the radius of the hyper-sphere is defined by $\\mathbf{o}$ and $r_d$ respectively, the objective of the network is to learn a representation that minimize the loss given as:\n\\begin{equation}\n\\begin{multlined}\n\\min_{r_d, \\theta_f} \\ \\ r_d^2 \\ + \\ \\frac{1}{\\nu N} \\sum_{i=1}^N \\max\\{0, \\|F({x}_i) - \\mathbf{o}\\|^2 - r_d^2\\} \\\\ \n+ \\frac{\\lambda}{2} \\sum_{l=1}^{L} \\|\\theta_f^l\\|_F^2,\n\\end{multlined}\n\\end{equation}\nThe loss function comprises of three loss terms. The first term encourages the network to find a hyper-plane with a smaller radii. The second term penalizes points lying outside the hyper-sphere. The third term is weight decay added to network parameters. Parameter $\\nu$ is shown to be an upper bound on the fraction of outliers and a lower bound on the fraction of samples being outside or on the boundary of the hypersphere.\nA second loss function is proposed for the case when most of the training data belongs to the positive class (which is the typical setting for One Class Classification). The second loss function is defined as:\n\\begin{equation}\n\\begin{multlined}\n\\min_{\\theta_f} \\frac{1}{N} \\sum_{i=1}^N  \\|F({x}_i)-\\mathbf{o}\\|^2 + \\frac{\\lambda}{2} \\sum_{l=1}^{L} \\|\\theta_f^l\\|_F^2,\n\\end{multlined}\n\\end{equation}\nIn this loss, the authors are merely forcing the embedding of the positive class to have a lower intra-class variance similar to . During inference, for a given query sample $x_{test}$, a positive class identity is declared if $\\|F({x}_{test})-\\mathbf{o}\\|^2$ is smaller than a selected threshold. Furthermore, Ruff \\emph{et al.}  found when $\\mathbf{o}$ is set to be a learnable parameter, network leads to a trivial solution. Therefore, they set $\\mathbf{o}$ to be the mean of outputs obtained at the  initial forward pass.  They further found that, using bias terms in the network would also lead to a trivial solutions. Therefore, they do not use any bias terms in the network architecture. Finally, authors advise against bounded activation functions in the network as it too could contribute to trivial solutions.\\\\\n\\noindent \\textbf{Holistic Approach to One Class (HRN)}\\\\\t\nHRN proposes  to train a deep classification network trained with log-likelihood loss with regularization applied on the learned features. The proposed loss is termed as holistic regularization or  H-regularization. Furthermore, HRN adds a 2-norm instance level data normalization strategy to deal with different feature scales in the data instances. Given one-class data sampled from distribution $p_{{x}}$ the training loss for HRN can be expressed as:\n\\begin{equation}\n\\mathcal{L}=\\underbrace{\\underset{{x} \\sim p_{{x}}}{\\mathbb{E}}[-\\log (\\operatorname{Sigmoid}(\\mbox{En}({x})))]}_{\\text {NLL }}+\\lambda \\underbrace{\\underset{{x} \\sim p_{{x}}}{\\mathbb{E}} [ \\ \\left\\|\\nabla_{{x}} \\mbox{En}({x})\\right\\|_{2}^{h}}_{\\text {H-regularization }} \\ ],\n\\end{equation}\nMinimizing the negative log-likelihood (NLL) helps encoder network $\\mbox{En}$ to model the one-class training data distribution. Here, the holistic regularization (or H-regularization) is added to control the output of the encoder network that might saturate the sigmoid function value and remove the feature bias introduced due to input data with high values that might lead to unimportant features. On top of having these two loss functions, HRN also adds a 2-norm instance-level data normalization that aims to solve problems caused due to different feature scales which might confuse model to have poor performance . In 2-norm instance normalization, the data ${x}$ normalized such that it has $\\|{x}\\|_2=1$. Furthermore, mean value from each feature is subtracted to make sure feature values of each instance have zero-mean.  This proposed normalization strategy encourages some parameters in the encoder network to be negative and consequently it increases the value space for probability of learning a better encoder model. The proposed normalization is shown to work really well for one-class classification compared to commonly used instance normalization .", "cites": [156, 6890, 6884], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple deep learning-based discriminative OCC methods, including OCCNN, Deep SVDD, and HRN, while integrating insights from related works (e.g., on instance normalization). It provides critical analysis by discussing limitations such as the trivial solutions in Deep SVDD and how HRN addresses issues like feature saturation and scaling. The abstraction is strong, as it generalizes the design choices and principles across methods, highlighting broader trends in representation learning and regularization for OCC."}}
{"id": "6e5b0c64-fe73-4f9f-ae10-3696a77f953c", "title": "Generative Models", "level": "subsubsection", "subsections": [], "parent_id": "ca8c7727-9d0c-4ccf-a689-aa53bf4644cb", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "OCC Algorithms"], ["subsection", "Deep Learning Methods"], ["subsubsection", "Generative Models"]], "content": "The generative models are one of the most popular methods for one-class learning. These models are based on a range of methods such as de-noising auto-encoders, generative adversarial networks, auto-regressive models, adversarial auto-encoders etc.\\\\\n\\noindent \\textbf {AnoGAN }\\\\\t\nAnoGAN proposes to learn a Generative Adversarial Network (GAN) based on positively labeled data . Generative Adversarial Network is a type of a generative model that consists of two sub-networks - a Generator ($G$) and a Discriminator ($D$). The set of images used to train a GAN is referred  as \\textit{real images}. The goal of the generator network is to use a random noise vector $\\mathbf{z}$ to generate images that closely resemble \\textit{real images}. Images generated by the generator is referred to as \\textit{fake images}. The goal of the discriminator is to differentiate between \\textit{real images} from  \\textit{fake images}. \nIn order to achieve this objective, discriminator is designed to generate a high score for \\textit{real images} and a low score for \\textit{fake images}. Therefore, discriminator parameters are learned such that  $\\log D(x)$ and $\\log (1-D(G(\\mathbf{z})))$ are maximized, where $x$ and $\\mathbf{z}$  are \\textit{real image} samples and random noise vectors, respectively. Therefore, optimization in discriminator update becomes:\n\\begin{equation}\n\\min_G \\max_D \\mathbb{E}_{{x} \\sim p_{x}} [\\log D({x})]+\\mathbb{E}_{\\mathbf{z} \\sim p_\\mathbf{z}} [\\log (1-D(G(\\mathbf{z})))],\n\\end{equation}\nGiven a query ${x}_{test}$, it is required to obtain the corresponding latent mapping $\\mathbf{z}$ for inference. In order to approximate the corresponding latent mapping, first starting from a random vector $\\mathbf{z}$, its value is updated by  back-propagating the loss as:\n\\begin{equation}\n\\begin{array}{llllll}\n\\mathcal{L}(\\mathbf{z}) & = (1-\\lambda) \\sum |{x}_{test} - G(\\mathbf{z})|  \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ + \\lambda  \\sum |Fe_D({x}_{test}) - Fe_D(G(\\mathbf{z}))|,\n\\end{array}\n\\end{equation}\nwhere $Fe_D$ is a feature extracted from the Discriminator. Here, $\\mathbf{z}$ is the approximated latent vector and $G(\\mathbf{z})$ is the corresponding fake image. The first term of the loss minimizes the difference between real and fake image in the pixel space where as the second term minimizes the difference in feature space. The total loss reduces when the approximated latent vector approaches the corresponding  latent vector of input ${x}$. After $T$ updates, the loss $\\mathcal{L}$ is treated as the novelty score of the input image. If it is lower that a selected threshold it is assigned a positive label.\\\\\n\\noindent \\textbf {Adversarially Learned One-Class Classifier (ALOCC) }\\\\\t\nThe main challenge in AnoGAN is to find the corresponding latent vector given a query input. To circumvent this challenge, a conditional GAN was used in . A conditional GAN, similar to a GAN, has a Generator and a Discriminator. However, different from a GAN, a conditional GAN accepts an image  and  a noise vector as the input. The Generator learns  to generate images (fake images) that have a close resemblance to real images. Discriminator tries to differentiate real images from fake images same as in a GAN.\nThe network architecture proposed in   is shown in Figure~\\ref{fig:alooc}. A auto-encoder is used as the Generator (denoted by $\\mathcal{R}$), and a CNN is used as the Discriminator (denoted by $\\mathcal{D}$). During training, Gaussian noise ${\\xi}$ is added to the input ${X}$ and the noisy input image ${\\hat{x}} = {X}+{\\xi}$  is given as the input to the Generator. The Generator tries to produce denoised outputs that corresponds to $\\mathbf{X}$ . The Discriminator tries to differentiate noise-less images $\\mathbf{X}$ from denoised outputs $\\mathcal{R}({X}+{\\xi}  )$. The network is trained using GAN loss defined as:\n\\begin{equation}\n\\min_\\mathcal{R} \\max_\\mathcal{D} \\mathbb{E}_{{x} \\sim p_{x}} [\\log D({x})]+\\mathbb{E}_{{\\hat{{x}}}\\sim p_{{x}+{\\xi}} }[\\log (1-D(G({\\hat{x}})))],\n\\end{equation}\nDuring inference, given a query image $\\mathbf{x}$, it is declared a positive query if $\\mathcal{D}( \\mathcal{R} ({x})  )$ is greater than a pre-determined threshold.\\\\\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.85\\linewidth]{alooc}\n\t\\vskip -2.5pt\n\t\\caption{ALOCC utilizes a de-noising auto-encoder network architecture  trained with generative adversarial training. Here, the input images are corrupted with Gaussian noise and network is expected to reconstruct input image perfectly.}\n\t\\label{fig:alooc}\n\\end{figure}\n\\noindent \\textbf {Old is Gold (OGN)}\\\\\nOld is Gold is an extension of the ALOCC framework . It consists of the same architecture as ALOCC  withe a Generator network ($\\mathcal{G}$) and a Discriminator network ($\\mathcal{D}$) as shown in Figure~\\ref{fig:oldisgold}.  The network is first pre-trained using the ALOCC method .  Then,  proposes to fine-tune the  network by providing two different types of \\textit{fake} images -- namely \\textit{bad quality examples }${x}_{low}$ and \\textit{pseudo anomaly images} ${x}_{pseudo}$.\nIn order to generate  \\textit{bad quality examples }${x}_{low}$, an older snapshot of the Generator network $\\mathcal{G}_{old}$ is used.  A \\textit{pseudo anomaly image} is generated considering the interpolation of two \\textit{bad quality examples } in the image space as:\n\\begin{equation}\n{x}_{pseudo} = \\frac{\\mathcal{G}_{old}({x}_i) + \\mathcal{G}_{old}({x}_j)}   {2},\n\\end{equation}\nwhere $i \\neq j$. Both  \\textit{bad quality examples } and \\textit{pseudo anomaly image} simulate unusual inputs. Therefore, authors argue that when the Discriminator is trained to differentiate these unusual inputs from normal inputs (${x}$ and ${\\hat{x}}$), the Discriminator can be used to effectively identify positive samples during inference. Therefore, during the fine-tuning stage, the Discriminator parameters are optimized such that the Discriminator is able to differentiate real image and fake images from \\textit{bad quality examples } and \\textit{pseudo anomaly images}. The optimization objective is defined as:\n\\begin{figure}[!b]\n\t\\centering\n\t\\includegraphics[width=0.85\\linewidth]{oldisgold}\n\t\\vskip -2.5pt\n\t\\caption{In old is gold method training, the old generator parameters are used to create a pseudo-negative images that acts as a negative data to build the one-class classifier that can identify underlying one-class data .}\n\t\\label{fig:oldisgold}\n\\end{figure}\n\\begin{equation}\n\t\\begin{multlined}\n\\max_\\mathcal{D}  \\alpha \\mathbb{E}_{{x} \\sim p_{x}} [\\log (1-D({x}))] \\\\ + (1- \\alpha) \\mathbb{E}_{\\mathbf{\\hat{{x}}}\\sim p_{{x}+{\\xi}} }[\\log (1-D(G({\\hat{x}})))] \\\\ +  \\beta \\mathbb{E}_{{x} \\sim p_{{x}_{low}}} [\\log (D({x}))] \\\\ +(1-\\beta) \t\\mathbb{E}_{{\\hat{{x}}}\\sim p_{{x}_{pseudo}} }[\\log (D({x}))],\n \t\\end{multlined}\n\\end{equation}\n\\begin{figure*}[!t]\n \t\\centering\n \t\\includegraphics[width=1\\linewidth]{nodel}\n \t\\caption{The figure shows different component of OCGAN network architecture used to learn one-class classifier . It consists of an auto-encoder network, one discriminator for image, one discriminator for latent space and one classifier to guide image discriminator update through informative sampling.}\n \t\\label{fig:ocgan}\n\\end{figure*}\nwhere $\\beta$ and $\\alpha$ are hyper-parameters. During inference, a given image ${x}$ is assigned to the positive class if $\\mathcal{D} ( \\mathcal{G}({x}))$ is smaller than a predetermined threshold.\\\\\n\\noindent \\textbf {One Class GAN (OCGAN)}\\\\\t\t\t\nOCGAN  is a representation based classifier that uses an denoising auto-encoder (with Encoder $\\mbox{En}$ and decoder $\\mbox{De}$) backbone to learn a representation. Authors point out that when the positive class is sufficiently diverse, auto-encoders learn a generic set of filters that leads to  good representations for all types of inputs. When this is the case, reconstruction based novelty detection fails. \nIn order to mitigate this effect, authors propose to first learn a representation such that the latent distribution of input images follow a uniform distribution, i.e. $\\mathcal{U}(-1,1)$. This is done adversarially using a \\textit{Latent Discriminator}. Then, latent vectors are sampled from  $\\mathcal{U}(-1,1)$ and its corresponding decoded output is forced to follow the distribution of real images. This is achieved adversarially using a  \\textit{Visual Discriminator}. The auto-encoder \nnetwork functions as the Generator during training. Once training converges, every latent code is guaranteed to produce an image from the positive class. Therefore, reconstruction error for non-positive images becomes high.\nThree types of images are used in OCGAN training. Images from the training dataset ${x}$ are called \\textit{real images}. Outputs of the denoising auto-encoder $\\mbox{De}(\\mbox{En}({x} + {n}))$ where ${n}$ is the added noise, are referred to as \\textit{reconstructed images}. Finally, generated images from random $\\mathcal{U}(-1,1)$ noise $\\mbox{De}(\\mathbf{z}), \\mathbf{z} \\sim \\mathcal{U}(-1,1)  $ are called \\textit{fake images}. OCGAN model contains four sub-networks as shown in Figure~\\ref{fig:ocgan}.\n\\begin{itemize}\n\t\\item \\textbf{Denoising auto-encoder}. The encoder is terminated with a tanh activation, forcing the latent space to dimensions to have a range $(-1,1)$. It maps a noisy input to the latent space and learns a inverse mapping back to the image space. auto-encoder is learned using the loss $$ \\mathcal{L}_{\\mbox{MSE}} = \\|{x} - \\mbox{De}(\\mbox{\\mbox{En}}({x}+{n})) \\|_2^2.$$\n\t\\item \\textbf{Latent discriminator}. Learns to discriminate between encoded real images and random samples generated from $\\mathcal{U}(-1,1)$. Parameters are learned by minimizing GAN loss \\begin{eqnarray}\n\t\\mathcal{L}_{\\mbox{latent}}  &&= -(\\mathbb{E}_{s \\sim \\mathbb{U}(-1,1) } [ \\log D_l(s) ] + \\nonumber\\\\\n\t&& \\mathbb{E}_{x \\sim p_x}[\\log (1-D_l( \\mbox{\\mbox{En}}(x+n)))]). \\nonumber\n\t\\end{eqnarray}\n\t\\item \\textbf{Visual discriminator}. Learns to discriminate between fake images and real images. \\begin{eqnarray}\n\t\\mathcal{L}_{\\mbox{visual}}  &=& -(\\mathbb{E}_{s \\sim \\mathbb{U}(-1,1) } [ \\log D_v(\\mbox{De}(s)) ] + \\nonumber\\\\\n\t&& \\mathbb{E}_{x \\sim p_l}[\\log (1-D_v( x))]). \\nonumber\n\t\\end{eqnarray}\n\t\\item \\textbf{Classifier}. Learns to recognize real images from fake images. Classifier is trained using the cross entropy loss. It is used to guide the sampling process such that informative samples are chosen during generator update.\n\\end{itemize}\n\\begin{figure}[!b]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{gpnd}\n\t\\vskip -2.5pt\n\t\\caption{GPND method follows training strategy of adversarial encoder , where the latent space is forced to be standard normal distribution through latent discriminator and auto-encoder is trained to produce high-quality images through visual discriminator.}\n\t\\label{fig:gpnd}\n\\end{figure}\nFirst, for a given input ${x}$, keeping everything else fixed, the classifier loss is evaluated and classifier weights are updated. Secondly, two discriminator losses are evaluated ($\\mathcal{L}_{\\emph{latent}}+\\mathcal{L}_{\\emph{visual}} $) and weights of the two discriminators are updated accordingly.  Then, negative mining is carried out in the latent space by using classifier output for guidance. Specifically, the latent sample is changed using back-propagation such that it may fool the learned classifier. Finally, the resulting latent vectors are used to evaluate $(\\mathcal{L}_{\\emph{latent}}+\\mathcal{L}_{\\emph{visual}}+\\lambda \\mathcal{L}_{\\emph{mse}})$ which is used to update Generator weights.\\\\\n\\noindent \\textbf {Generative Probabilistic Novelty Detection (GPND)}\\\\\nGPND proposed a probabilistic framework based solution for one class classification by modeling the data manifold of positive data. Authors first define a functional mapping between the latent space and data and then linearizes the mapping function around the data point. It is shown that the probability is factorized with respect to local coordinates of the manifold tangent space.\nIn , the  relationship between a training instance ${x}_i$ and its latent variable $\\mathbf{z}_i$ is modeled as  ${x}_i = f(\\mathbf{z}_i) + \\mathbf{\\zeta}_i$, where $\\mathbf{\\zeta}_i$ is additive noise. Let $g$ be the inverse process of $f$ defined as $f(g({x}_i)) = \\mathbf{z}_i$.\nFor a new data sample $\\mathbf{\\bar{x}}$ with $\\mathbf{\\bar{z}} = g(\\mathbf{\\bar{x}})$, assuming $f$ is smooth, linearization can be performed based on first order Taylor expansion as follows:\n\\begin{equation}\n\tf(\\mathbf{z}) =  f(\\mathbf{\\bar{z}}) + J_f (\\mathbf{\\bar{z}}) (\\mathbf{z} - \\mathbf{\\bar{z}}) + O(\\| \\mathbf{z} - \\mathbf{\\bar{z}}\\|^2),\n\\end{equation}\nwhere $J_f (\\mathbf{\\bar{z}})$ is the Jacobi matrix computed at $ \\mathbf{\\bar{z}}$. The tangent space of $f$ at ${x}^\\parallel$ is spanned by $\\mathbf{U}^\\parallel$, where $J_f(\\mathbf{\\bar{z}}) = \\mathbf{U^\\parallel S V}^T$ is the SVD decomposition. Data point ${\\bar{x}}$ can be expressed with respect to local coordinates that defines the tangent space and the space orthogonal to it as follows:\n\\begin{equation}\n\\mathbf{\\bar{w}} = \\mathbf{U}^T {\\bar{x}} = \\begin{bmatrix}\n\\mathbf{U^\\parallel}^T {\\bar{x}} \\\\\n\\mathbf{U^\\perp}^T {\\bar{x}}\n\\end{bmatrix} =  \\begin{bmatrix}\n\\mathbf{\\mathbf{\\bar{w}^\\parallel} } \\\\\n\\mathbf{\\mathbf{\\bar{w}^\\perp} } \n\\end{bmatrix},\n\\end{equation} \nAssuming  $ \\mathbf{\\mathbf{\\bar{w}^\\parallel} }$ and $\\mathbf{\\mathbf{\\bar{w}^\\perp} }  $ are independent, $p_{{x}}({{x}})$ can be written as:\n\\begin{equation}\np_{{x}}({{x}}) =  p_{\\mathbf{w}}({\\mathbf{w}}) =   p_{\\mathbf{w}^\\parallel}(\\mathbf{\\mathbf{\\bar{w}}^\\parallel} )   p_{\\mathbf{w}^\\perp}(\\mathbf{\\mathbf{\\bar{w}}^\\perp} ),\n\\end{equation}\nFor a given test data point ${x}$,  shows that$ \\mathbf{\\mathbf{\\bar{w}^\\parallel} }$ and $\\mathbf{\\mathbf{\\bar{w}^\\perp} }  $  can be approximated using following formula:\n\\begin{equation}\np_{\\mathbf{w}^\\parallel}(\\mathbf{\\mathbf{\\bar{w}}^\\parallel} ) = | det S^{-1}| p_{\\mathbf{z}}(\\mathbf{z}),\n\\end{equation}\n\\begin{equation}\np_{\\mathbf{w}^\\perp}(\\mathbf{\\mathbf{\\bar{w}}^\\perp} ) \\approx \\frac{\\Gamma(\\frac{m-k}{2})}{2 \\pi^ {\\frac{m-k}{2} }\\| \\mathbf{w}^\\perp\\|^{m-k}} p_{\\|\\mathbf{w}^\\perp\\|}(\\mathbf{\\|\\mathbf{\\bar{w}}^\\perp\\|} ),\n\\end{equation}\nwhere $k$ is the dimension in the latent space and $\\Gamma$ is the Gamma function. The distribution of norms $p_{\\|\\mathbf{w}^\\perp\\|}$ is learned offline by histogram norms of $\\mathbf{w}^\\perp =  {\\mathbf{U}^\\perp}^T  \\mathbf{\\bar{x}}$. During inference, this probability is calculated, and if the probability is greater than a pre-determined threshold it is declared to be a positive sample.\nFunctions $f$ and $g$ are learned using a adversarial auto-encoder network, where  $f$ and $g$ represents the encoder and decoder functions of the network respectively as shown in Figure~\\ref{fig:gpnd}. The proposed network has two discriminators. The first Discriminator $ D_z $ operates on  the latent space that forces encoder embeddings to follow $\\mathcal{N}(0,\\textbf{I})$ distribution. The second Discriminator  $ D_x$  operates in the image space and forces output of the auto-encoder to follow the distribution of real data. The network is trained adversarially by optimizing over:\n\\begin{equation}\n\\hat{g}, \\hat{f} = \\arg \\min_{g,f} \\max_{D_x, D_z} L_{adv-d_z} + L_{adv-d_x} + \\lambda L_{error},\n\\end{equation}\nwhere,\n\\begin{equation}\n\\mathcal{L}_{adv-d_z} = \\mathbb{E} [\\log(D_z(\\mathcal{N}(0,\\textbf{I})))] + \\mathbb{E}[\\log(1-D_z(g({x})))],\n\\end{equation}\n\\begin{equation}\n\\mathcal{L}_{adv-d_x} = \\mathbb{E}[\\log(D_x({x}))] + \\mathbb{E}[\\log(1-D_x(f(\\mathcal{N}(0,\\textbf{I}))))],\n\\end{equation}\n\\begin{equation}\n\\mathcal{L}_{error} = -\\mathbb{E}_z[\\log(p(f(g({x}))|{x}))],\n\\end{equation}\n\\noindent \\textbf {Latent Space Auto-regression (AND)}\\\\\nThe latent space auto-regression framework presented in  learns a representation and models embeddings of the positive class in the latent space simultaneously.  uses an auto-encoder structure to learn the representation. The latent embeddings are modeled using an autoregressive model. Both these objectives are trained end-to-end using the network shown in Figure~\\ref{fig:and}(a). \n\\begin{figure}[htp!]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{AND}\n\t\\vskip -2.5pt\n\t\\caption{ (a) Network architecture used in latent space auto-regression . (b) An example of how a novelty score can be used to detect abnormal events.}\n\t\\label{fig:and}\n\\end{figure}\nLet encoder and decoder sub-networks in the auto-encoder be denoted as $\\mbox{En}$ and $\\mbox{De}$. Given a input ${x}$, latent representation obtained from the encoding process is $\\mathbf{z} =\\mbox{En}({x})$. The objective of the autoregressive model $h$ is to estimate posterior probability of $\\mathbf{z}$ given the positive class $ p(\\mathbf{z})$.\t\nAutoregressive modeling is a technique that predicts sequential outputs based on previous observations. Using this assumption $p(\\mathbf{z})$ can be decomposed as:\n\\begin{equation}\np(\\mathbf{z}) = \\Pi_{i=1}^m p(\\mathbf{z}_i| \\mathbf{z}_{<i}),\n\\end{equation}\nwhere  symbol $<$ denotes  the order of random variables. In , each conditional probability is modeled using a multinomial with 100 quantization bins. In order to facilitate auto-regression prediction,  proposed a Masked Fully Connected Layer (MFC). It consists of $l$ fully connected layers connected to the latent vector with $j^{th}$ fully connected layer masked, where all rows $<j$ are replaced with zeros. Outputs of each fully connected layer is stacked together to form a $\\mathbb{R}^l$ dimensional vector which produces $p(\\mathbf{z})$.\nThe three networks, $\\mbox{En}, \\mbox{De}$ and $h$ are trained together with the objective of minimizing the reconstruction error $\\|\\mbox{De}(\\mbox{En}({x})- {x}) \\|^2$ and the negative log likelihood $- \\log(h(\\mathbf{z}))$. The full objective function of the network is defined as:\n\\begin{equation}\n\\mathcal{L} = \\mathbb{E}_{{x}} = \\|\\mbox{De}(\\mbox{En}({x})- {x}) ||^2 -  \\lambda  \\log(h(\\mathbf{z})),\n\\end{equation}\nWhen this loss is minimized, the network learns to both encode images of the positive class and to model the latent density of positive samples. During inference $\\mathcal{L}$ is used to measure the novelty of a query image. As illustrated in Figure~\\ref{fig:and}(b), $\\mathcal{L}$ can be used to quantify how novel  a given instance is compared to instances in  the positive class. When the value of $\\mathcal{L}$ is below a pre-determined threshold it is associated with the positive class. \\\\\n\\noindent \\textbf {Inter Class Splitting (ICS) }\\\\\nA representation learned on the positive class label will be able to represent majority of data (typical data) well. However, there exists a set of data samples (atypical data) that are not represented well. For an example, if an auto-encoder was chosen to be the representation method, the former set of data points will yield a low reconstruction error since they are represented well. On the other hand, latter data points will yield high reconstruction error. In , authors utilize this fact to learn a meaningful representation from one-class data. \n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{ics}\n\t\\vskip -2.5pt\n\t\\caption{ Network architecture used in to train OCC with inter-class splitting .}\n\t\\label{fig:ics}\n\\end{figure}\n\\begin{figure*}[htp!]\n\t\\centering\n\t\\includegraphics[width=1.00\\linewidth]{p_kdgan}\n\t\\vskip -12.5pt\n\t\\caption{The  flowchart  of  knowledge  distillation  with  GANs  for  one-class  classification . The training involves one teacher and one student network that are trained progressively, where student network is trained with distillation loss to improve its the one-class.}\n\t\\label{fig:p_kdgan}\n\\end{figure*}\nFirst, an auto-encoder network is trained using all positively labeled data. Then, atypical data samples are identified by considering the reconstruction quality.  used SSIM as an image quality measure for this purpose.\nThe core network presented in  has three components\tas shown in Figure~\\ref{fig:ics} -- Feature extraction sub-network, Distance sub-network and Classification sub-network. Given an input ${x}$, first a label $y$ is assigned based on whether it is a typical sample or not. For the given input  ${x}$ Feature extraction sub-network produces the corresponding latent representation $\\mathbf{z}$. The classification network performs binary classification to classify input into typical or atypical class. The classification network produces binary cross entropy loss $L_{ic}$ defined as: \n\\begin{equation}\n\\mathcal{L}_{ic} = - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log \\hat{y_i} +  (1-y_i) \\log (1-\\hat{y_i})],\n\\end{equation}\nwhere, $\\hat{y_i} $ is the prediction produced by the classification network.   Given a pair of data points $({x}_i, {x}_j)$ the distance network generates a score in the range of $[0,1]$ by considering their corresponding latent representations $(\\mathbf{z}_i, \\mathbf{z}_j)$ . The distance network is trained such that it generates a value closer to $0$ if both images are atypical and to generate a value closer to $1$ when both are typical. The distance network is trained by minimizing the \\textit{closeness loss} $L_{cls}$  and dispersion loss $L_{dis}$ defined as:\n\\begin{equation}\n\\mathcal{L}_{cls} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log (1- D(\\mathbf{z}_{typ, i},    \\mathbf{z}_{typ, j \\neq i})),\n\\end{equation}\nand,\n\\begin{equation}\n\\mathcal{L}_{dis} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log ( D(\\mathbf{z}_{atyp, i}, \\mathbf{z}_{atyp, j \\neq i})),\n\\end{equation}\nwhere $D(\\cdot)$ is a suitable distance measure and $\\mathbf{z}_i, \\mathbf{z}_i$ are the latent representations of the two input samples considered.  Three sub-networks are trained considering all three losses $ \\mathcal{L}_{ic} , \\mathcal{L}_{cls} $ and $\\mathcal{L}_{dis} $ together. This method encourages to learn an embedding that makes identifying typical samples from atypical samples possible. This is done by minimizing both cross-entropy loss and pair-wise loss in the latent space. Authors show that once the network is trained, the probability corresponding to the typical class can be used to detect positive samples effectively during inference .\t \\\\", "cites": [3283, 3213, 6887, 8160, 6886, 3282, 3231], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of deep learning-based generative models for OCC, focusing on specific methods like AnoGAN, ALOCC, OGN, and OCGAN. It integrates the core ideas of these papers into a coherent narrative about the role of GANs and auto-encoders in novelty detection. However, it lacks critical evaluation or comparison of the methods' strengths and weaknesses and does not abstract broader patterns or principles in generative modeling for OCC."}}
{"id": "03da0e69-afeb-4cde-a40d-e33c764affd6", "title": "Knowledge Distillation", "level": "subsubsection", "subsections": [], "parent_id": "ca8c7727-9d0c-4ccf-a689-aa53bf4644cb", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "OCC Algorithms"], ["subsection", "Deep Learning Methods"], ["subsubsection", "Knowledge Distillation"]], "content": "A few recent methods have utilized knowledge distillation with student-teacher model training to improve one-class learning.\\\\\n\\noindent \\textbf{Uninformed Students: One-Class Learning (US-OCL)}\nThe Uninformed Students based One-Class Learning (US-OCL)  proposes a student-teacher training strategy for one-class classification. The proposed US-OCL approach, utilizes ensemble of student networks that are trained to regress the output of the teacher network through knowledge distillation. The teacher network is a powerful generative model with an encoder ($\\mbox{En}^{teacher}$) and a decoder network ($\\mbox{De}^{teacher}$) trained on the one-class training data. The ensemble of student networks then distill the encoder representation using distillation loss expressed as:\n\\begin{equation}\n\\begin{array}{c}\n\\mathcal{L}_{distill}=\\|\\mbox{En}^{student}({x})-\\mbox{En}^{teacher}({x})\\|_{2}, \\\\\n\\end{array}\n\\end{equation}\nTo further improve the quality of the learned representations, US-OCL also introduces a metric learning loss ($\\mathcal{L}_{metric}$), i.e. triplets loss  computed for a mini-batch, and a compactness loss ($\\mathcal{L}_{comp}$), which minimizes the correlations between the samples of a mini-batch. Once the student networks are trained, the errors made by student networks compared to the teacher network prediction is used as scoring function to perform one-class classification.\\\\\n\\noindent \\textbf{Progressive Knowledge Distillation (P-KDGAN)}\\\\\t\nProgressive knowledge distillation (P-KDGAN)  proposes one-class method designed to distill knowledge from teacher network and transfer it to a student network through a distillation loss. For this purpose, P-KDGAN trains two GAN networks on given one-class training data, namely student and teacher networks. First, both teacher and student GANs are initialized with the same architecture, with only difference being both architecture have different number of channels. Both the networks are trained with the reconstruction loss $S_{con}$, latent space loss $S_{enc}$ and adversarial loss $S_{adv}$ defined as follows:\n\\begin{equation}\n\\begin{array}{c}\nS_{con}=\\|{x}-\\hat{{x}}\\|_{1} \\\\\nS_{enc}=\\left\\|\\mathbf{z}_{1}-\\mathbf{z}_{2}\\right\\|_{2} \\\\\nS_{adv}=\\|\\mbox{En}({x})-\\mbox{En}(\\hat{{x}})\\|_{2} \\\\\nL^S_{g}=w_{con} S_{con}+w_{enc} S_{enc}+w_{adv} S_{adv},\n\\end{array}\n\\end{equation}\nwhere, $w_{con}$, $w_{enc}$ and $w_{adv}$ are weights for the corresponding loss functions. The same loss is used for training the teacher network denoted as $L^T_{g}$. To distill the knowledge from the student network, distillation losses $K_1$, $K_x$ and $K_2$ as shown in the Fig.~\\ref{fig:p_kdgan}. The losses $K_1$ and $K_2$ minimize the distance between student and teacher representations learned by an encoder and discriminator network through L2 error and $K_x$ minimizes the L1 error in the reconstructed input images by student and teacher network. Both student and teacher networks are trained using a progressive training strategy with two steps. In the first step, the student network is trained with distillation losses and a generator loss with fixed teacher parameters. In second step, both student and teacher networks are trained with generator loss and student network is fine-tuned with a distillation loss as well.", "cites": [6886, 6889], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of two deep learning-based OCC methods (US-OCL and P-KDGAN), including their architectures and loss functions. While it connects the common theme of knowledge distillation between the two works, it lacks deeper synthesis, critical evaluation of their approaches, and broader abstraction of principles. The section remains largely descriptive without offering comparative or analytical insights."}}
{"id": "5be0c5c9-61d0-4c66-b5db-110e400e6323", "title": "Multi-class Benchmark Datasets", "level": "subsection", "subsections": [], "parent_id": "fb2611e7-5582-46c4-a2c8-60b4675a7deb", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Datasets and Evaluation Protocols"], ["subsection", "Multi-class Benchmark Datasets"]], "content": "Image based OCC performance is commonly benchmarked using standard multi-class datasets. During evaluation, each class present in the dataset is considered one at a time as the positive class. The model is trained using the training split of the positive class. The trained model is tested against the test split of all classes. Here, all other classes are considered to be negative during testing. We list datasets used commonly in recent literature below.\\\\ \n\\noindent  \\textbf{COIL100 :} COIL100  is a multi-class dataset where each object class data is captured in  different poses. There are 100 image classes in the dataset with a few images per class (typically less than hundred).\\\\\n\\noindent  \\textbf{MNIST :} The MNIST  dataset  contains $28\\times28$ hand-written digits from 0-9.  \\\\\n\\noindent  \\textbf{fMNIST :} fMNIST  is intended to be a replacement for MNIST, where the dataset comprises of 28$\\times$28 images of fashion apparels/accessories. \\\\\n\\noindent  \\textbf{CIFAR10 :} CIFAR10  is an object recognition dataset that consists of images from 10 classes. It contains images from four vehicle classes and six animal classes. All images are centered and cropped.\\\\", "cites": [652], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of commonly used multi-class datasets for benchmarking OCC methods, including COIL100, MNIST, fMNIST, and CIFAR10. It integrates minimal information from the cited paper (only fMNIST is cited), and does not critically analyze or evaluate the suitability or limitations of these datasets in the context of OCC. The content remains concrete and lacks broader abstraction or insightful synthesis of the dataset usage across studies."}}
{"id": "acf6b03c-18eb-4a9b-adb3-c14fe3aea6c8", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "fb2611e7-5582-46c4-a2c8-60b4675a7deb", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Datasets and Evaluation Protocols"], ["subsection", "Evaluation Metrics"]], "content": "One class classifiers are evaluated using a test dataset that comprises of positive data and negative data. Therefore, testing procedure of one class classifiers are analogous to binary classifiers / detectors. Majority of previous works in the literature , , , ,   have used Receiver Operating Characteristics (ROC) curve to report performance of one class classification. The ROC curve represents the relationship between the false positive rate (FPR) and the true positive rate (TPR) obtained by a classifier. They are defined as:\n\\begin{equation}\nTPR = \\frac{\\text{Number of correctly classified positive samples}}{\\text{Number of positive samples}},\n\\end{equation}\nand\n\\begin{equation}\nFPR = \\frac{\\text{Number of misclassified negative samples}}{\\text{Number of negative samples}},\n\\end{equation}\nThe following metrics can be derived based on the ROC curve:\\\\\n\\noindent \\textbf{AUC-ROC curve}. Area under the ROC curve is a commonly used metric to evaluate the effectiveness of OCC methods. Ideally, the area should account for 1.0. In the worst case, prediction will be equal to random guessing when the AUC-ROC is 0.5. AUC-ROC does not demand of using any specific operating points when the metric is evaluated. Therefore, it captures the effectiveness of the method independent of an operating point. \\\\\n\\noindent \\textbf{FPR @ TPR}. This metric reports FPR at a selected TPR value from the ROC curve. Equal error rate (EER) is a special case of this metric. EER is the FPR value reported at the point in the ROC curve when both FPR and TPR are equal.\\\\\n\\noindent \\textbf{Half Total Error Rate (HTER) @ TPR}. Half total error rate quantifies the probability of obtaining a wrong prediction given probability of observing a positive and a negative query is equal. This is quantified as:\n\\begin{equation}\nHTER = \\frac{FPR+(1-TPR)}{2},\n\\end{equation}\nThis metric is commonly used in biometrics applications. It is the usual practice to report HTER when FPR and TPR are equal.", "cites": [3213, 3282, 3332], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of evaluation metrics for one-class classification, such as AUC-ROC, FPR @ TPR, and HTER. While it connects these metrics to the general evaluation framework of OCC, it does not synthesize or integrate deeper insights from the cited papers. There is little critical analysis or abstraction beyond the basic definitions and uses of the metrics."}}
{"id": "e8b8ac7c-5a85-43bb-b5cd-2756e6f6c63c", "title": "Discussion and future research directions", "level": "section", "subsections": [], "parent_id": "c8c28393-3555-4c6b-8dc2-feb553e2658f", "prefix_titles": [["title", "One-Class Classification: A Survey"], ["section", "Discussion and future research directions"]], "content": "\\label{sec:openproblems}\nThis article attempted to provide an overview of recent developments in  OCC  for  visual recognition. One class classification promises  to  be  an  active  area  of  research.  Based on the analysis and results of various methods and the trend of other developments in computer vision, we believe that deep networks will dominate further research in the field of OCC.  We make the following observations regarding future trends in research in OCC.\\\\\n\\noindent \\textbf{OCC in complex visual object datasets.} The majority of previous works in the literature (in exception of )  have carried out evaluations on relatively simpler datasets that contain low amount of details and variations. Based on the reported performance it can be seen that although the existing methods perform very well in simpler datasets (such as MNIST, COIL100), they fail to generalize well to more complex datasets (such as CIFAR10). In multiple-class classification CIFAR10 is no longer considered as a challenging dataset. \nOne of the open problems in OCC is to create sophisticated solutions that generalize well to more complex datasets.\\\\\n\\noindent \\textbf{Semi-supervised OCC.} Conventionally OCC methods assume the availability of only positively labeled data during training. However, in practical applications, this limitation does not always hold. Semi-supervised OCC studies how to find solutions in the presence of additional annotated data. Semi-supervised OCC has been formulated in a couple of ways.  In the first case, the availability of labeled data from an OOD dataset is assumed , , . In the other case, the availability of a few annotated data from positive and negative classes are assumed   , .  More semi-supervised OCC methods are needed that can leverage additional datasets to improve the performance.  \\\\\n\\noindent \\textbf{One-Class Adversarial Attacks.} Another limitation of current OCC methods is that they can not deal with adversarial data.  It is a well-known fact that carefully designed imperceptible perturbations can be used to fool a deep learning-based model to make incorrect predictions , , .   These types of adversarial attacks are easy to deploy and may be encountered in real-world applications .  However, most of the work studying adversarial attacks and defense strategies focus on multi-class classification models , , , .  Recent work  showed that a deep network-based OCC approach is also vulnerable to adversarial attacks.  However more is needed.   Extensive study in understanding how adversarial attacks affect one-class models will help in creating more secure models for real-world applications. \\\\\n\\noindent \\textbf{Domain generalizable OCC.} When a one class classifier is trained, it is done with the assumption that the query samples observed by the classifier during inference would follow the same distribution as the training images. However, this assumption may not hold in practice. A considerable amount of interest has emerged in recent multi-class classification literature in producing classifiers that generalize well across different image distributions (domains) , , , . However, this area of research is largely unexplored for OCC , . Therefore, developing techniques that produce  generalizable OCC  across different image distributions remains an open problem.\\\\\n\\noindent \\textbf{One-Class Neural Architecture Search.} In recent years, there has been a growing interest in automating architecture design/engineering using Neural architecture search (NAS) methods .  Given different basic convolution operations, the idea is to learn the connections in feature search cell using NAS methods to obtain an optimal network for one-class classification. More is needed to explore the possibility of using NAS for one-class classification.\\\\\n\\noindent \\textbf{Explainable OCC.} In past decade, the majority of computer vision algorithms have transitioned to deep convolutional neural network (DCNN) based models. However, due to the complexity of DCNN models, the decisions they make are hard to interpret . Hence, explaining a DCNN is essential towards developing transparent models that not only make decisions, but can also explain why the decision was made. Many works have contributed in developing more transparent models, but they all focus on recognition tasks , , , , . Since all recently proposed OCC models also work on deep convolutional neural networks, a special focus on explaining OCC models is critical. There are few papers addressing this issue for OCC models , , , providing first steps towards developing more transparent OCC models. However, explainable OCC models are still largely unexplored and more work in this direction is needed.\\\\\n\\noindent \\textbf{Federated learning for OCC.} Training computer vision models with the help of decentralized data has been an interesting problem that is gaining attention in recent years. Federated learning  was proposed to specifically address this issue to train object recognition models with the help of decentralized data. Most of the federated learning methods are proposed only for multi-class classification models , , . However, these multi-class federated learning approaches are not directly applicable to train one-class classification methods with decentralized data. This is due to the additional challenges posed by the nature of one-class classification problem. For example, in federated multi-class classification, homogeneous distribution of multiple-category across individual data centers is assumed, which is not possible for the federated one-class setting. Currently there are no methods proposed to train one-class classifiers with decentralized data, making it an interesting direction for future work.\n{\\small\n\t\\bibliographystyle{ieee}\n\t\\bibliography{egbib}\n}\n\\end{document}", "cites": [953, 1356, 314, 2731, 6894, 602, 9082, 499, 917, 6891, 872, 7621, 892, 3332, 890, 8158, 596, 2734, 6895, 623, 3322, 582, 6893, 6892], "cite_extract_rate": 0.75, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers to highlight key open problems in OCC, such as generalization, adversarial robustness, and federated learning. It critically evaluates the limitations of current methods and identifies underexplored research directions. The discussion abstracts beyond specific papers to frame broader challenges and opportunities in the field."}}
