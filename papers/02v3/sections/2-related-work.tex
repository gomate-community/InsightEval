\section{Related Work}
\label{sec:related}

Our work intersects with three lines of research: LLM-based automated peer review, automated scientific writing and literature review, and novelty and insight assessment for scholarly documents. We briefly survey each area and highlight how InsightEval differs from and complements existing approaches.

\subsection{LLM-based Automated Peer Review}

The growing volume of scientific submissions has motivated the development of automated peer review systems powered by large language models. DeepReview~\cite{zhu-etal-2025-deepreview} proposes a multi-stage framework that emulates expert reviewers through structured analysis, literature retrieval, and evidence-based argumentation. REMOR~\cite{taechoyotin2025remorautomatedpeerreview} combines LLM reasoning with multi-objective reinforcement learning to generate comprehensive reviews. Beyond generation quality, researchers have investigated the reliability and robustness of LLM-based reviewers: Lin et al.~\cite{lin-etal-2025-breaking} reveal significant vulnerabilities in LLM reviewers under textual adversarial attacks, while Farber~\cite{Farber2025ComparingHA} presents a comparative analysis of human and AI expertise in the peer review process. Wu et al.~\cite{Wu2024AreTC} further examine whether reviewer confidence scores are consistent with review content in top AI conferences. At the meta-evaluation level, Goldberg et al.~\cite{goldberg2024usefulnessllmsauthorchecklist} investigate the usefulness of LLMs as author checklist assistants at NeurIPS.

While these systems focus on generating holistic quality assessments of papers (e.g., soundness, clarity, and significance), they do not specifically evaluate the \textit{insightfulness} of a paper's claims relative to its cited references. InsightEval addresses this gap by providing a fine-grained, evidence-grounded evaluation of the intellectual contribution a paper makes beyond its references.

\subsection{Automated Scientific Writing and Literature Review}

A parallel line of work leverages LLMs for automating scientific writing and literature synthesis. Lu et al.~\cite{lu2024aiscientistfullyautomated} propose the AI Scientist, a system for fully automated open-ended scientific discovery, which has been evaluated by Beel et al.~\cite{beel2025evaluating} for its practical viability. Zheng et al.~\cite{zheng-etal-2025-automation} provide a comprehensive survey on LLMs in scientific discovery, charting the evolution from task-specific automation to increasingly autonomous research agents. For structured long-form generation, STORM~\cite{shao-etal-2024-assisting} introduces a multi-perspective question-asking approach for writing Wikipedia-like articles grounded in retrieved sources. SurveyGen-I~\cite{chen-etal-2025-surveygen} addresses the challenge of consistent survey generation through evolving plans and memory-guided writing. Wu et al.~\cite{Wu_2025} present an automated literature research and review generation method, while Zhao et al.~\cite{zhao2025literaturereviewliteraturereviews} offer a meta-level literature review of survey papers in pattern analysis and machine intelligence. VERIRAG~\cite{mohole2025veriragpostretrievalauditingscientific} tackles the post-retrieval auditing of scientific study summaries to ensure factual accuracy.

These systems primarily focus on the \textit{generation} and \textit{organization} of scientific text but do not assess the quality of insights produced. InsightEval complements these tools by providing a principled framework to evaluate whether generated or human-written content achieves genuine intellectual advancement beyond its source materials.

\subsection{Novelty and Insight Assessment}

The most closely related line of work concerns automated assessment of novelty and insight in scholarly documents. Arts et al.~\cite{Arts_2025} pioneer the measurement of novel scientific ideas directly from publication text, moving beyond citation-based metrics. NovAScore~\cite{ai-etal-2025-novascore} proposes an automated metric for evaluating document-level novelty by aggregating novelty and salience scores of atomic information units. OpenNovelty~\cite{zhang2026opennoveltyllmpoweredagenticverifiable} introduces an LLM-powered agentic system for verifiable scholarly novelty assessment through literature-grounded comparison. Shahid et al.~\cite{shahid-etal-2025-literature} further develop a literature-grounded framework for assessing the novelty of scientific ideas using retrieval-augmented generation with faceted re-ranking. IdeaBench~\cite{Guo2024IdeaBenchBL} provides a benchmark for evaluating LLMs' ability to generate novel research ideas. For impact-oriented analysis, Arnaout et al.~\cite{Arnaout2025IndepthRI} propose fine-grained temporal citation analysis for in-depth research impact summarization. InsightGUIDE~\cite{Koloveas2025InsightGUIDEAO} offers an opinionated AI assistant for guided critical reading of scientific literature, while Vo and Simmie~\cite{Vo2024AssessingSI} provide a systematic review of tasks, tools, and techniques for assessing scientific inquiry.

While novelty assessment evaluates the \textit{newness} of ideas relative to prior work, it does not measure the \textit{depth of understanding} a paper achieves. A paper may introduce no novel methods yet still provide deep, integrative insight by synthesizing, reinterpreting, or abstracting known findings. InsightEval distinguishes itself by jointly evaluating information gain and the level of understanding---characterized through depth, breadth, and height---thereby capturing a complementary dimension that existing novelty metrics do not address.
