{"id": "c96a642a-f1a1-4750-b2fc-8ab4bc01d8e4", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "973dad38-1208-442a-ab84-0b97a29f24a4", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nSoftware Vulnerabilities (SVs) can negatively affect the confidentiality, integrity and availability of software systems~.\nThe exploitation of these SVs such as the Heartbleed attack\\footnote{\\url{https://heartbleed.com}} can damage the operations and reputation of millions of systems and organizations globally, resulting in huge financial losses as well. Therefore, it is important to remediate critical SVs as promptly as possible.\nVulnerability assessment is required to prioritize the remediation of critical SVs among a large and increasing number of SVs each year~ (e.g., more than 20,000 SVs were reported on National Vulnerability Database (NVD)~ in 2021). SV assessment includes tasks that determine various characteristics such as the types, exploitability, impact and severity levels of SVs~. Such characteristics help understand and select high-priority SVs to resolve early given the limited effort and resources. For example, an identified cross-site scripting (XSS) or SQL injection vulnerability in a web application will likely require an urgent remediation plan. These two types of SVs are well-known and can be easily exploited by attackers to gain unauthorized access and compromise sensitive data/information. On the other hand, an SV that requires admin access or happens only in a local network will probably have a lower priority since only a few people can initiate an attack.\nThere has been an active research area to assess and prioritize SVs using increasingly large data from multiple sources.\nMany studies in this area have proposed different Natural Language Processing (NLP), Machine Learning (ML) and Deep Learning (DL) techniques to leverage such data to automate various tasks such as predicting the Common Vulnerability Scoring System (CVSS)~ metrics (e.g.,~) or public exploits (e.g.,~).\nThese prediction models can learn the patterns automatically from vast SV data, which would be otherwise impossible to do manually. Such patterns are utilized to speed up the assessment and prioritization processes of ever-increasing and more complex SVs, significantly reducing practitioners' effort.\nDespite the rising research interest in data-driven SV assessment and prioritization, to the best of our knowledge, there has been no comprehensive survey on the state-of-the-art methods and existing challenges in this area.\n\\begin{table}[!t]\n\\fontsize{8}{9}\\selectfont\n\\caption{Comparison of contributions between our survey and the existing related surveys/reviews.}\n\\label{tab:survey_comparison}\n\\centering\n\\begin{tabular}{|l|P{3cm}|P{3cm}|P{3cm}|}\n\\hline\n\\multicolumn{1}{|l|}{\\diagbox[height=1.3cm, width=3.6cm]{\\raisebox{2\\height}{\\enspace \\textbf{Study}}}{\\raisebox{-1\\height}{\\enspace \\textbf{Contribution}}}} &\n\\multicolumn{1}{P{2.9cm}|}{\\centering\\textbf{Focus on SV assessment \\& prioritization}} & \\multicolumn{1}{P{2.9cm}|}{\\centering\\textbf{Analysis of SV\\\\data sources}} & \\multicolumn{1}{P{2.9cm}|}{\\centering\\textbf{Analysis of data-\\\\driven approaches (NLP/ML/DL)}}\n\\\\\n\\hline\n\\makecell[l]{Ghaffarian et al. 2017~} & \\multicolumn{1}{c|}{--} & \\multicolumn{1}{c|}{--} & \\makecell[c]{\\checkmark (Mostly ML)} \\\\\n\\hline\n\\makecell[l]{Lin et al. 2020~\\\\ Semasaba et al. 2020~\\\\ Singh et al. 2020~\\\\ Zeng et al. 2020~} & \\multicolumn{1}{c|}{--} & \\multicolumn{1}{c|}{--} & \\makecell[c]{\\checkmark (Mostly DL)} \\\\\n\\hline\n\\multicolumn{1}{|p{2.8cm}|}{Pastor et al. 2020~} & \\multicolumn{1}{c|}{--} & \\makecell[c]{\\checkmark (OSINT)} & \\multicolumn{1}{c|}{--} \\\\\n\\hline\n\\makecell[l]{Sun et al. 2018~\\\\ Evangelista et al. 2020~} & \\multicolumn{1}{c|}{--} & \\makecell[c]{\\checkmark (OSINT)} & \\multicolumn{1}{c|}{\\checkmark} \\\\\n\\hline\n\\makecell[l]{Khan et al. 2018~} & \\makecell[c]{\\checkmark (Rule-based methods)} & \\multicolumn{1}{c|}{--} & \\multicolumn{1}{c|}{--} \\\\\n\\hline\n\\multicolumn{1}{|p{2.8cm}|}{Kritikos et al. 2019~} & \\makecell[c]{\\checkmark (Static analysis)} & \\multicolumn{1}{c|}{\\checkmark} & \\multicolumn{1}{c|}{--} \\\\\n\\hline\n\\multicolumn{1}{|p{3.2cm}|}{Dissanayake et al. 2020~} & \\multicolumn{1}{c|}{\\checkmark (Socio-technical aspects)} & \\multicolumn{1}{c|}{--} & \\multicolumn{1}{c|}{--} \\\\\n\\hline\\hline\n\\rowcolor{lightgray}\n\\multicolumn{1}{|l|}{\\textbf{Our survey}} & \\multicolumn{1}{c|}{\\textbf{\\checkmark}} & \\multicolumn{1}{c|}{\\textbf{\\checkmark}} & \\multicolumn{1}{c|}{\\textbf{\\checkmark}} \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\\noindent \\textbf{Our Contributions}. \\circled{1} We are the first to review in-depth the research studies that automate \\textit{data-driven SV assessment and prioritization} tasks leveraging SV data and NLP/ML/DL techniques.\n\\circled{2} We categorize and describe the key tasks performed in relevant primary studies.\n\\circled{3} We synthesize and discuss the pros and cons of data, features, models, evaluation methods and metrics commonly used in the reviewed studies.\n\\circled{4} We highlight the challenges with the current practices and propose potential solutions moving forward.\nOur findings can provide useful guidelines for researchers and practitioners to effectively utilize data to perform SV assessment and prioritization.\nAn online and up-to-date (by accepting external contributions) version of the survey can be found at \\textcolor{blue}{\\url{https://github.com/lhmtriet/awesome-vulnerability-assessment}}.\n\\noindent \\textbf{Related Work}.\nThere have been several existing surveys/reviews on SV analysis and prediction, but they are fundamentally different from ours (see Table~\\ref{tab:survey_comparison}).\nGhaffarian et al.~ conducted a seminal survey on ML-based SV analysis and discovery.\nSubsequently, several studies~ reviewed DL techniques for detecting vulnerable code.\nHowever, these prior reviews did not describe how ML/DL techniques can be used to assess and prioritize the detected SVs.\nThere have been other relevant reviews on using Open Source Intelligence (OSINT) (e.g., phishing or malicious emails/URLs/IPs) to make informed security decisions~. However, these OSINT reviews did not explicitly discuss the use of SV data and how such data can be leveraged to automate the assessment and prioritization processes. Moreover, most of the reviews on SV assessment and prioritization have focused on either static analysis tools~ or rule-based approaches (e.g., expert systems or ontologies)~.\nThese methods rely on pre-defined patterns and struggle to work with new types and different data sources of SVs compared to contemporary ML or DL approaches presented in our survey~.\nRecently, Dissanayake et al.~ reviewed the socio-technical challenges and solutions for security patch management that involves SV assessment and prioritization after SV patches are identified.\nUnlike~, we focus on the challenges, solutions and practices of automating various SV assessment and prioritization tasks with data-driven techniques.\nWe also consider all types of SV assessment/prioritization regardless of the patch availability.\n\\noindent \\textbf{Paper Outline}. The rest of the paper is organized as follows. Section~\\ref{sec:survey_overview} presents the scope, methodology and taxonomy covered in this survey. Sections~\\ref{sec:exploit_prediction},~\\ref{sec:impact_prediction},~\\ref{sec:severity_prediction},~\\ref{sec:type_prediction} and ~\\ref{sec:miscellaneous} review the studies in each theme of the taxonomy. Section~\\ref{sec:elements_analysis} identifies and discusses the common practices and respective implications for data-driven SV assessment and prioritization.\nSection~\\ref{sec:challenges} discusses the open challenges and proposes some future directions of this research area. Finally, section~\\ref{sec:conclusions} concludes the survey.", "cites": [4878, 4880, 166, 4879], "cite_extract_rate": 0.17391304347826086, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from cited papers by highlighting the use of NLP, ML, and DL in automating SV assessment and prioritization. It critically compares existing surveys, pointing out their limitations in addressing data-driven approaches and patch availability. The abstraction is strong as it identifies broader trends and differentiates between traditional and contemporary methods, positioning the survey as a novel contribution to the field."}}
{"id": "3d219d68-81a8-4a46-a437-6ae5ff78b3fd", "title": "Taxonomy of Data-driven Software Vulnerability Assessment and Prioritization", "level": "subsection", "subsections": [], "parent_id": "1d525d61-ae81-40d7-8503-982ce3ccc730", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Survey Overview"], ["subsection", "Taxonomy of Data-driven Software Vulnerability Assessment and Prioritization"]], "content": "\\label{subsec:taxonomy}\nBased on the scope in section~\\ref{subsec:scope} and the methodology in section~\\ref{subsec:methodology}, we identified five main themes of the relevant studies in the area of data-driven SV assessment and prioritization (see Figure~\\ref{fig:taxonomy}). Specifically, we extracted the themes by grouping related SV assessment or prioritization tasks that the surveyed studies aim to automate/predict using data-driven models. Note that a paper is categorized into more than one theme if that paper develops models for multiple cross-theme tasks.\nWe acknowledge that there can be other ways to categorize the studies. However, we assert the reliability of our taxonomy as all of our themes (except theme 5) align with the security standards used in practice. For example, Common Vulnerability Scoring System (CVSS)~ provides a framework to characterize exploitability, impact and severity of SVs (themes 1-3), while Common Weakness Enumeration (CWE)~ includes many vulnerability types (theme 4). Hence, we believe our taxonomy can help identify and bridge the knowledge gap between the academic literature and industrial practices, making it relevant and potentially beneficial for both researchers and practitioners. Details of each theme in our taxonomy are covered in subsequent sections.\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Exploit Likelihood} sub-theme of the \\textit{Exploitation} theme.\n\\textbf{Note}: The nature of task of this sub-theme is binary classification of existence/possibility of proof-of-concept and/or real-world exploits.\n}\n\\label{tab:exploit_studies_likelihood}\n\\begin{tabular}[!t]{|p{1.5cm}|p{6.2cm}|p{5cm}|}\n  \\hline \\textbf{Study} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n  Bozorgi et al. 2010~ & CVE, Open Source Vulnerability Database (OSVDB) & Linear Support Vector Machine (SVM)\\\\\\hline\n  Sabottke et al. 2015~ & NVD, Twitter, OSVDB, ExploitDB, Symantec security advisories, private Microsoft security advisories & Linear SVM\\\\\\hline\n  Edkrantz et al. 2015~ & NVD, Recorded Future security advisories, ExploitDB & Na\\\"ive Bayes, Linear SVM, Random forest\\\\\\hline\n  Bullough et al. 2017~ & NVD, Twitter, ExploitDB & Linear SVM\\\\\\hline\n  Almukaynizi et al.~ & NVD, ExploitDB, Zero Day Initiative security advisories \\& Darkweb forums/markets & SVM, Random forest, Na\\\"ive Bayes, Bayesian network, Decision tree, Logistic regression\\\\\\hline\n  Xiao et al. 2018~ & NVD, SecurityFocus security advisories, Symantec \\newline Spam/malicious activities based on daily blacklists from abuseat.org, spamhaus.org, spamcop.net, uceprotect.net, wpbl.info \\& list of unpatched SVs in hosts & Identification of malicious activity groups with community detection algorithms + Random forest for exploit prediction\\\\\\hline\n  Tavabi et al. 2018~ & NVD, 200 sites on Darkweb, ExploitDB, Symantec, Metasploit & Paragraph embedding + Radial basis function kernel SVM\\\\\\hline\n  de Sousa et al. 2020~ & NVD, Twitter, ExploitDB, Symantec\\newline Avast, ESET, Trend Micro security advisories & Linear SVM, Logistic regression, XGBoost, Light Gradient Boosting Machine (LGBM)\\\\\\hline\n  Fang et al. 2020~ & NVD, ExploitDB, SecurityFocus, Symantec & fastText + LGBM\\\\\\hline\n  Huang et al. 2020~ & NVD, CVE Details, Twitter, ExploitDB, Symantec security advisories & Random forest\\\\\\hline\n  Jacobs et al. 2020~ & NVD, Kenna Security \\newline Exploit sources: Exploit DB, Metasploit, FortiGuard Labs, SANS Internet Storm Center, Securewords CTU, Alienvault OSSIM, Canvas/D2 Security's Elliot Exploitation Frameworks, Contagio, Reversing Labs\n  & XGBoost\\\\\\hline\n  Yin et al. 2020~ & NVD, ExploitDB, General text: Book Corpus \\& Wikipedia for pretraining BERT models & Fine-tuning BERT models pretrained on general text\\\\\\hline\n  Bhatt et al. 2021~ & NVD, ExploitDB & Features augmented by SV types + Decision tree, Random forest, Na\\\"ive Bayes, Logistic regression, SVM\\\\\\hline\n  Suciu et al. 2021~ & NVD, Vulners database, Twitter, Symantec, SecurityFocus, IBM X-Force Threat Intelligence \\newline Exploit sources: ExploitDB, Metasploit, Canvas, D2 Security's Elliot, Tenable, Skybox, AlienVault, Contagio & Multi-layer perceptron\\\\\\hline\\hline\n  Younis et al. 2014~ & Vulnerable functions from NVD (Apache HTTP Server project), ExploitDB, OSVDB & SVM\\\\\\hline\n  Yan et al. 2017~ & Executables (binary code) of 100 Linux applications & Combining ML (Decision tree) output \\& fuzzing with a Bayesian network\\\\\\hline\n  Tripathi et al. 2017~ & Program crashes from VDiscovery~ \\& LAVA~ datasets & Static/Dynamic analysis features + Linear/Radial basis function kernel SVM\\\\\\hline\n  Zhang et al. 2018~ & Program crashes from VDiscovery~ dataset & $n$-grams of system calls from execution traces + Online passive-aggressive classifier\\\\\\hline\n\\end{tabular}\n\\end{table}", "cites": [4881, 4879], "cite_extract_rate": 0.08, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the taxonomy and includes a detailed table of studies under the 'Exploit Likelihood' sub-theme. However, it lacks synthesis of broader themes across the papers, minimal critical analysis of their strengths or weaknesses, and offers limited abstraction beyond the specific methods and datasets used."}}
{"id": "4540c81f-59e1-42df-ab62-0d5028697e5e", "title": "Exploit Likelihood", "level": "subsubsection", "subsections": [], "parent_id": "b09d7b5f-afaf-4088-be33-56a77c576464", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Exploitation Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Exploit Likelihood"]], "content": "\\label{subsubsec:exploit_likelihood}\nThe first sub-theme is \\textit{exploit likelihood} that predicts whether SVs would be exploited in the wild or PoC exploits would be released publicly (see Table~\\ref{tab:exploit_studies_likelihood}). In 2010, Bozorgi et al.~ were the first to use SV descriptions on Common Vulnerabilities and Exposures (CVE)~ and Open Source Vulnerability Database (OSVDB)\\footnote{\\url{http://osvdb.org}. Note that this database has been discontinued since 2016.} to predict exploit existence based on the labels on OSVDB. In 2015, Sabottke et al.~ conducted a seminal study that used Linear SVM and SV information on Twitter to predict PoC exploits on ExploitDB~ as well as real-world exploits on OSVDB, Symantec's attack signatures~ and private Microsoft's security advisories~. These authors urged to explicitly consider real-world exploits as \\textit{not} all PoC exploits would result in exploitation in practice.\nThey also showed SV-related information on Twitter\\footnote{\\url{https://twitter.com}} can enable earlier detection of exploits than using expert-verified SV sources (e.g., NVD).\nBuilt upon these two foundational studies~, the literature has mainly aimed to improve the performance and applicability of exploit prediction models by leveraging more exploit sources and/or better data-driven techniques/practices.\nMany researchers~ increased the amount of ground-truth exploits using extensive sources other than ExploitDB and Symantec in~. The sources were security advisories such as Zero Day Initiative~, Metasploit~, SecurityFocus~, Recorded Future~, Kenna Security~, Avast\\footnote{\\url{https://avast.com/exploit-protection.php}. This link was provided by de Sousa et al.~, but it is no longer available.}, ESET~, Trend Micro~, malicious activities in hosts based on traffic of spam/malicious IP addresses~ and Darkweb sites/forums/markets~. In addition to enriching exploit sources, better data-driven models and practices for exploit prediction were also studied. Ensemble models (e.g., Random forest, eXtreme Gradient Boosting (XGBoost)~, Light Gradient Boosting Machine (LGBM)~) were shown to outperform single-model baselines (e.g., Na\\\"ive Bayes, SVM, Logistic regression and Decision tree) for exploit prediction~.\nAdditionally, Bullough et al.~ identified and addressed several issues with exploit prediction models, e.g., time sensitivity of SV data, already-exploited SVs before disclosure and training data imbalance, helping to improve the practical application of such models. Recently, Yin et al.~ demonstrated that transfer learning is an alternative solution for improving the performance of exploit prediction with scarcely labeled exploits. Specifically, these authors pre-trained a DL model, BERT~, on massive non-SV sources (e.g., text on Book Corpus~ and Wikipedia~) and then fine-tuned this pre-trained model on SV data using additional pooling and dense layers. Bhatt et al.~ also suggested that incorporating the types of SVs (e.g., SQL injection) into ML models can further enhance the predictive effectiveness. Suciu et al.~ empirically showed that unifying SV-related sources used in prior work (e.g., SV databases~, social media~, SV-related discussions~ and PoC code in ExploitDB~) supports more effective and timely prediction of \\textit{functional} exploits~.\nBesides using SV descriptions as input for exploit prediction, several studies in this sub-theme have also predicted exploits on the code level.\nYounis et al.~ predicted the exploitability of vulnerable functions in the Apache HTTP Server project. Specifically, these authors used an SVM model with features extracted from the dangerous system calls~ in entry points/functions~ and the reachability from any of these entry points to vulnerable functions~.\nMoving from high-level to binary code, Yan et al.~ first used a Decision tree to obtain prior beliefs about SV types in 100 Linux applications using static features (e.g., \\textit{hexdump}) extracted from executables. Subsequently, they applied various fuzzing tools (i.e., Basic Fuzzing Framework~ and OFuzz~) to detect SVs with the ML-predicted types. They finally updated the posterior beliefs about the exploitability based on the outputs of the ML model and fuzzers using a Bayesian network. The proposed method outperformed \\textit{!exploitable},\\footnote{\\url{https://microsoft.com/security/blog/2013/06/13/exploitable-crash-analyzer-version-1-6}} a static crash analyzer provided by Microsoft.\nTripathi et al.~ also predicted SV exploitability from crashes (i.e., VDiscovery~ and LAVA~ datasets) using an SVM model and static features from core dumps and dynamic features generated by the Last Branch Record hardware debugging utility.\nZhang et al.~ proposed two improvements to Tripathi et al.~'s approach. These authors first replaced the hardware utility in~ that may not be available for resource-constrained devices (e.g., IoT) with sequence/$n$-grams of system calls extracted from execution traces. They also used an online passive-aggressive classifier~ to enable online/incremental learning of exploitability for new crash batches on-the-fly.\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Exploit Time} sub-theme of the \\textit{Exploitation} theme.}\n\\label{tab:exploit_studies_time}\n\\begin{tabular}[!t]{|p{1.5cm}|p{4cm}|p{3.5cm}|p{3cm}|}\n  \\hline \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n  Bozorgi et al. 2010~ & \\textit{Binary classification}: Likelihood that SVs would be exploited within 2 to 30 days after disclosure & CVE, OSVDB & Linear SVM\\\\\\hline\n  Edkrantz 2015~ & \\multirowcell{2}[0ex][l]{\\textit{Binary classification}: Likelihood of\\\\SV exploits within 12 months after\\\\disclosure} & NVD, ExploitDB, Recorded Future security advisories & SVM, K-Nearest Neighbors (KNN), Na\\\"ive Bayes, Random forest\\\\\\hhline{-~*{2}{-}}\n  Jacobs et al. 2019~ & & NVD, Kenna Security \\newline Exploit sources: Exploit DB, Metasploit, D2 Security's Elliot \\& Canvas Exploitation Frameworks, Fortinet, Proofpoint, AlienVault \\& GreyNoise & Logistic regression\\\\\\hline\n  Chen et al. 2019~ & \\textit{Binary classification}: Likelihood that SVs would be exploited within 1/3/6/9/12 months after disclosure \\newline \\textit{Regression}: number of days until SV exploits after disclosure & CVE, Twitter, ExploitDB, Symantec security advisories & Graph neural network embedding + Linear regression, Bayes, Random forest, XGBoost, Lasso/Ridge regression\\\\\\hline\n\\end{tabular}\n\\end{table}", "cites": [4882, 4884, 7, 4881, 4883, 4879], "cite_extract_rate": 0.12, "origin_cites_number": 50, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple studies by tracing the evolution from foundational work (e.g., Bozorgi and Sabottke) to advanced methods using diverse exploit sources and improved ML techniques. It critically evaluates model limitations (e.g., time sensitivity, data imbalance) and highlights practical advancements such as transfer learning and Bayesian updating. The abstraction level is strong, as it identifies broader trends in data sources, modeling approaches, and application contexts (e.g., code-level analysis)."}}
{"id": "3fc89654-b4b1-4b93-a7a5-252546c98938", "title": "Exploit Time", "level": "subsubsection", "subsections": [], "parent_id": "b09d7b5f-afaf-4088-be33-56a77c576464", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Exploitation Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Exploit Time"]], "content": "\\label{subsubsec:exploit_time}\nAfter predicting the likelihood of SV exploits in the previous sub-theme, this sub-theme provides more fine-grained information about \\textit{exploit time} (see Table~\\ref{tab:exploit_studies_time}).\nBesides performing binary classification of exploits, Bozorgi et al.~ and Edkrantz~ also predicted the time frame (2-30 days in~ and 12 months in~) within which exploits would happen after the disclosure of SVs. Jacobs et al.~ then leveraged multiple sources containing both PoC and real-world exploits, as given in Table~\\ref{tab:exploit_studies_time}, to improve the number of labeled exploits, enhancing the prediction of exploit appearance within 12 months. Chen et al.~ predicted whether SVs would be exploited within 1-12 months and the exploit time (number of days) after SV disclosure using Twitter data. The authors proposed a novel regression model whose feature embedding was a multi-layer graph neural network~ capturing the content and relationships among tweets, respective tweets' authors and SVs. The proposed model outperformed many baselines and was integrated into the VEST system~ to provide timely SV assessment information for practitioners. To the best of our knowledge, at the time of writing, Chen et al.~ have been the only ones pinpointing the exact exploit time of SVs rather than large/uncertain time-frames (e.g., months) in other studies, helping practitioners to devise much more fine-grained remediation plans.", "cites": [4885, 4884], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple studies effectively, highlighting their distinct contributions to the sub-theme of exploit time. It identifies Chen et al. as unique in predicting exact exploit time, adding a critical perspective on the limitations of other studies. While it provides some abstraction by noting the shift from coarse to fine-grained predictions, it could offer a deeper meta-level analysis of the underlying trends in data usage or model design."}}
{"id": "67957933-fe5e-4807-b036-01dcd219c710", "title": "Exploit Characteristics", "level": "subsubsection", "subsections": [], "parent_id": "b09d7b5f-afaf-4088-be33-56a77c576464", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Exploitation Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Exploit Characteristics"]], "content": "\\label{subsubsec:exploit_prop}\n\\textit{Exploit characteristics} is the final sub-theme that reveals various requirements/means of exploits (see Table~\\ref{tab:exploit_studies_prop}), informing the potential scale of SVs; e.g., remote exploits likely affect more systems than local ones.\nThe commonly used outputs are the Exploitability metrics provided by versions 2~ and 3~ of Common Vulnerability Scoring System (CVSS).\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Exploit Characteristics} sub-theme of the \\textit{Exploitation} theme.}\n\\label{tab:exploit_studies_prop}\n\\begin{tabular}[!t]{|p{1.5cm}|p{4.5cm}|p{3.2cm}|p{3cm}|}\n  \\hline \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n  Yamamoto et al. 2015~ & \\multirowcell{3}[0ex][l]{\\textit{Multi-class classification}: CVSS v2\\\\(Access Vector \\& Access Complexity\\\\ metrics)\\\\\\\\ \\textit{Binary classification}: CVSS v2\\\\(Authentication metric)} & NVD & Supervised Latent Dirichlet Allocation (LDA)\\\\\\hhline{-~*{2}{-}}\n  Wen et al. 2015~ & & NVD, OSVDB, SecurityFocus, IBM X-Force & Radial basis function kernel SVM\\\\\\hhline{-~*{2}{-}}\n  Le et al. 2019~ & & NVD & Concept-drift-aware models with Na\\\"ive Bayes, KNN, Linear SVM, Random forest, XGBoost, LGBM\\\\\\hline\n  Toloudis et al. 2016~ & \\textit{Correlation analysis}: CVSS v2 & NVD & Principal component analysis \\& Spearman correlation coefficient\\\\\\hline\n  Ognawala et al. 2018~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: CVSS v3\\\\(Attack Vector, Attack Complexity \\&\\\\ Privileges Required metrics)\\\\\\\\ \\textit{Binary classification}: CVSS v3 (User\\\\ Interaction metric)} & NVD (buffer overflow SVs) \\& Source code of vulnerable software/components & Combining static analysis tool (Macke~) \\& ML classifiers (Na\\\"ive Bayes \\& Random forest)\\\\\\hhline{-~*{2}{-}}\n  Chen et al. 2019~ & & CVE, NVD, Twitter & Graph convolutional network\\\\\\hline\n  Elbaz et al. 2020~ & \\textit{Multi-class/Binary classification}: CVSS v2/v3 & NVD & Mapping outputs of Linear regression to CVSS metrics with closest values\\\\\\hhline{-~*{2}{-}}\n  Jiang et al. 2020~ & & NVD, ICS Cert, Vendor websites (Resolve inconsistencies with a majority vote) & Logistic regression\\\\\\hline\n  Gawron et al. 2017~ & \\textit{Multi-target classification}: CVSS v2 & NVD & Na\\\"ive Bayes, Multi-layer Perceptron (MLP)\\\\\\hhline{-~*{2}{-}}\n  Spanos et al. 2018~ & & NVD & Random forest, boosting model, Decision tree\\\\\\hline\n  Gong et al. 2019~ & \\textit{Multi-task classification}: CVSS v2 & NVD & Bi-LSTM with attention mechanism\\\\\\hline\\hline\n  Chen et al. 2010~ & \\textit{Multi-class classification}: Platform-specific vulnerability locations (Local, Remote, Local area network) \\& vulnerability causes (e.g., Access/Input/Origin validation error) & NVD, Secunia vulnerability database, SecurityFocus, IBM X-Force & Linear SVM\\\\\\hline\n  Ruohonen et al. 2017~ & \\textit{Binary classification}: Web-related exploits or not & ExploitDB & LDA + Random forest\\\\\\hline\n  Aksu et al. 2018~ & \\textit{Multi-class classification}: author-defined pre-/post-condition privileges (None, OS (Admin/User), App (Admin/User)) & NVD & RBF network, Linear SVM, NEAT~, MLP\\\\\\hhline{-~*{2}{-}}\n  Liu et al. 2019~ & & NVD & Information gain + Convolutional neural network\\\\\\hline\n  Kanakogi et al. 2021~ & \\textit{Multi-class classification}: Common Attack Pattern Enumeration and Classification (CAPEC) & NVD, CAPEC & Doc2vec/tf-idf with cosine similarity\\\\\\hline\n\\end{tabular}\n\\end{table}\nMany studies have focused on predicting and analyzing version 2 of CVSS exploitability metrics (i.e., Access Vector, Access Complexity and Authentication). Yamamoto et al.~ were the first one to leverage descriptions of SVs on NVD together with a supervised Latent Dirichlet Allocation topic model~ to predict these CVSS metrics.\nSubsequently, Wen et al.~ used  Radial Basis Function (RBF)-kernel SVM and various SV databases/advisories other than NVD (e.g., SecurityFocus, OSVDB and IBM X-Force~) to predict the metrics.\nLe et al.~ later showed that the prediction of CVSS metrics suffered from the \\textit{concept drift} issue; i.e., descriptions of new SVs may contain Out-of-Vocabulary terms for prediction models.\nThey proposed to combine sub-word features with traditional Bag-of-Word (BoW) features to infer the semantics of novel terms/words from existing ones, helping assessment models be more robust against concept drift. Besides prediction, Toloudis et al.~ used principal component analysis~ and Spearman's $\\rho$ correlation coefficient to reveal the predictive contribution of each word in SV descriptions to each CVSS metric. However, this technique does not directly produce the value of each metric.\nRecently, several studies have started to predict CVSS version 3 exploitability metrics including the new Privileges and User Interactions.\nOgnawala et al.~ fed the features generated by a static analysis tool, Macke~, to a Random forest model to predict these CVSS version 3 metrics for vulnerable software/components.\nLater, Chen et al.~ found that many SVs were disclosed on Twitter before on NVD. Therefore, these authors developed a system built on top of a Graph Convolutional Network~ capturing the content and relationships of related Twitter posts about SVs to enable more timely prediction of the CVSS version 3 metrics.\nElbaz et al.~ developed a linear regression model to predict the numerical output of each metric and then obtained the respective categorical value with the numerical value closest to the predicted value. For example, a predicted value of 0.8 for Attack Vector CVSS v3 is mapped to \\textit{Network} (0.85)~.\nTo prepare a clean dataset to predict these CVSS metrics, Jiang et al.~ replaced inconsistent CVSS values in various SV sources (i.e., NVD, ICS CERT and vendor websites) with the most frequent value.\nInstead of building a separate model for each CVSS metric, there has been another family of approaches predicting these metrics using a single model to increase efficiency. Gawron et al.~ and Spanos et al.~ predicted multiple CVSS metrics as a unique string instead of individual values. The output of each metric is then extracted from the concatenated string.\nLater, Gong et al.~ adopted the idea of a unified model from the DL perspective by using the multi-task learning paradigm~ to predict CVSS metrics simultaneously. The model has a feature extraction module (based on a Bi-LSTM model with attention mechanism~) shared among all the CVSS metrics/tasks, yet specific prediction head/layer for each metric/task. This model outperformed single-task counterparts while requiring much less time to (re-)train.\nAlthough CVSS exploitability metrics were most commonly used, several studies used other schemes for characterizing exploitation. Chen et al.~ used Linear SVM and SV descriptions to predict multiple SV characteristics, including three \\textit{SV locations} (i.e., Local, LAN and Remote) on SecurityFocus~ and Secunia~ databases as well as 11 \\textit{SV causes}\\footnote{Access/Input/Origin validation error, Atomicity/Configuration/Design/Environment/Serialization error, Boundary condition error, Failure on exceptions, Race condition error} on SecurityFocus.\nRegarding the exploit types, Rouhonen et al.~ used LDA~ and Random forest to classify whether an exploit would affect a web application. This study can help find relevant exploits in components/sub-systems of a large system.\nFor privileges, Aksu et al.~ extended the Privileges Required metric of CVSS by incorporating the context (i.e., Operating system or Application) to which privileges are applied (see Table~\\ref{tab:exploit_studies_prop}).\nThey found MLP~ to be the best-performing model for obtaining these privileges from SV descriptions. They also utilized the predicted privileges to generate attack graphs (sequence of attacks from source to sink nodes).\nLiu et al.~ advanced this task by combining information gain for feature selection and Convolutional Neural Network (CNN)~ for feature extraction.\nRegarding attack patterns, Kanakogi et al.~ found Doc2vec~ to be more effective than term-frequency inverse document frequency (tf-idf) when combined with cosine similarity to find the most relevant Common Attack Pattern Enumeration and Classification (CAPEC)~ for a given SV on NVD. Such attack patterns can manifest how identified SVs can be exploited by adversaries, assisting the selection of suitable countermeasures.", "cites": [168, 4887, 8850, 4878, 4886, 7265, 326], "cite_extract_rate": 0.21875, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers, tracing the evolution of approaches for predicting exploit characteristics (e.g., CVSS v2 to v3, handling concept drift). It integrates techniques like supervised LDA, SVM, and multi-task learning into a cohesive narrative. Critical analysis is present in identifying limitations such as concept drift and data inconsistency, and in comparing the efficiency of multi-task models. It abstracts by highlighting broader trends, such as the shift towards more robust models and the use of diverse data sources for timely predictions."}}
{"id": "3ed37742-2003-4733-bf8e-168b79f7b641", "title": "Impact Prediction", "level": "section", "subsections": ["1e9f7fe3-8fc5-4617-a4a8-7feff990ba94", "0c26ece9-8dc1-4812-8c8d-681d1bfd65e4"], "parent_id": "973dad38-1208-442a-ab84-0b97a29f24a4", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Impact Prediction"]], "content": "\\label{sec:impact_prediction}\nThis section describes the \\textit{Impact} theme that determines the (negative) effects that SVs have on a system of interest if such SVs are exploited. There are five key tasks that the papers in this theme have automated/predicted: (\\textit{i}) Confidentiality impact, (\\textit{ii}) Integrity impact, (\\textit{iii}) Availability impact, (\\textit{iv}) Scope and (\\textit{v}) Custom vulnerability consequences (see Table~\\ref{tab:impact_studies}). \n\\begin{table}[!t]\n\\fontsize{8}{9}\\selectfont\n\\caption{List of the surveyed papers in the \\textit{Impact} theme. \\textbf{Note}: We grouped the first four sub-themes as they were mostly predicted together.}\n\\label{tab:impact_studies}\n\\centering\n\\begin{tabular}{|p{1.5cm}|p{3.8cm}|p{3.2cm}|p{3.8cm}|}\n \\hline \\multicolumn{1}{|c|}{\\textbf{Study}} & \\multicolumn{1}{c|}{\\textbf{Nature of task}} & \\multicolumn{1}{c|}{\\textbf{Data source}} & \\multicolumn{1}{c|}{\\textbf{Data-driven technique}}\\\\\\hline\n \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-themes: 1. Confidentiality, 2. Integrity, 3. Availability \\& 4. Scope (only in CVSS v3)}}}\\\\\\hline\n Yamamoto et al. 2015~ & \\textit{Multi-class classification}: CVSS v2 & NVD & Supervised Latent Dirichlet Allocation\\\\\\hhline{-~*{2}{-}}\n Wen et al. 2015~ & & NVD, OSVDB, SecurityFocus, IBM X-Force & Radial basis function kernel SVM\\\\\\hhline{-~*{2}{-}}\n Le et al. 2019~ & & NVD & Concept-drift-aware models with Na\\\"ive Bayes, KNN, Linear SVM, Random forest, XGBoost, LGBM\\\\\\hline\n  Toloudis et al. 2016~ & \\textit{Correlation analysis}: CVSS v2 & NVD & Principal component analysis \\& Spearman correlation coefficient\\\\\\hline\n  Ognawala et al. 2018~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: CVSS v3\\\\\\\\ \\textit{Binary classification}: Scope in\\\\ CVSS v3} & NVD (buffer overflow SVs) \\& Source code of vulnerable software/components & Combining static analysis tool (Macke~) \\& ML classifiers (Na\\\"ive Bayes \\& Random forest)\\\\\\hhline{-~*{2}{-}}\n  Chen et al. 2019~ & & CVE, NVD, Twitter & Graph convolutional network\\\\\\hline\n Elbaz et al. 2020~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: CVSS\\\\ v2/v3\\\\\\\\ \\textit{Binary classification}: Scope in\\\\ CVSS v3} & NVD & Mapping outputs of Linear regression outputs to CVSS metrics with closest values\\\\\\hhline{-~*{2}{-}}\n Jiang et al. 2020~ & & NVD, ICS Cert, Vendor websites (Resolve inconsistencies with a majority vote) & Logistic regression\\\\\\hline\n Gawron et al. 2017~ & \\textit{Multi-target classification}: CVSS v2 & NVD & Na\\\"ive Bayes, MLP\\\\\\hhline{-~*{2}{-}}\n Spanos et al. 2018~ & & NVD & Random forest, boosting model, Decision tree\\\\\\hline\n Gong et al. 2019~ & \\textit{Multi-task classification}: CVSS v2 & NVD & Bi-LSTM with attention mechanism\\\\\\hline\n \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 5. Custom Vulnerability Consequences}}}\\\\\\arrayrulecolor{black}\\hline\n Chen et al. 2010~ & \\textit{Multi-label classification}: Platform-specific impacts (e.g., Gain system access) & NVD, Secunia vulnerability database, SecurityFocus, IBM X-Force & Linear SVM\\\\\\hline\n\\end{tabular}\n\\vspace{-3pt}\n\\end{table}", "cites": [4887, 4878], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the Impact theme by listing tasks, data sources, and techniques used across various studies. While it organizes the cited works into a tabular format, it lacks deeper synthesis of ideas or trends across papers. There is minimal critical analysis or abstraction to broader principles or frameworks, focusing instead on surface-level categorization."}}
{"id": "c101bd31-27cb-49bc-a47d-76c6337599ff", "title": "Confidentiality, Integrity, Availability, and Scope", "level": "subsubsection", "subsections": [], "parent_id": "1e9f7fe3-8fc5-4617-a4a8-7feff990ba94", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Impact Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Confidentiality, Integrity, Availability, and Scope"]], "content": "\\label{subsubsec:cvss_impact}\nA majority of the papers have focused on the impact metrics provided by CVSS, including versions 2~ and 3~.\nVersions 2 and 3 share three impact metrics \\textit{Confidentiality}, \\textit{Integrity} and \\textit{Availability}. Version 3 also has a new metric, \\textit{Scope}, that specifies whether an exploited SV would affect only the system that contains the SV. For example, \\textit{Scope} changes when an SV occurring in a virtual machine affects the whole host machine, in turn increasing individual impacts.\nThe studies that predicted the CVSS impact metrics are mostly the same as the ones predicting the CVSS exploitability metrics in section~\\ref{sec:exploit_prediction}.\nGiven the overlap, we hereby only describe the main directions and techniques of the \\textit{Impact}-related tasks rather than iterating the details of each study.\nOverall, a majority of the work has focused on classifying CVSS impact metrics (versions 2 and 3) using three main learning paradigms: single-task~, multi-target~ and multi-task~ learning. Instead of developing a separate prediction model for each metric like the single-task approach, multi-target and multi-task approaches only need a single model for all tasks. Multi-target learning predicts concatenated output; whereas, multi-task learning uses shared feature extraction for all tasks and task-specific softmax layers to determine the output of each task. These three learning paradigms were powered by applying and/or customizing a wide range of data-driven methods. The first method was to use single ML classifiers like supervised Latent Dirichlet Allocation~, Principal component analysis~, Na\\\"ive Bayes~, Logistic regression~, Kernel-based SVM~, Linear SVM~, KNN~ and Decision tree~. Other studies employed ensemble models combining the strength of multiple single models such as Random forest~, boosting model~ and XGBoost/LGBM~. Recently, more studies moved towards more sophisticated DL architectures such as MLP~, attention-based (Bi-)LSTM~ and graph neural network~. Ensemble and DL models usually beat the single ones, but there is a lack of direct comparisons between these two emerging model types.", "cites": [4887, 4878], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers by focusing on the shared use of CVSS impact metrics and learning paradigms in vulnerability prediction, and connects this to broader trends in machine learning and deep learning applications. It generalizes well by identifying three main learning approaches and their respective techniques. However, the critical analysis is limited, as it mainly states that ensemble and DL models perform better without exploring the reasons or implications in depth."}}
{"id": "31305a1c-b386-42bd-99ce-c1ce7c4a30ea", "title": "Severity Levels", "level": "subsubsection", "subsections": [], "parent_id": "5587387b-27d7-4a6e-ad3c-d854609029b5", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Severity Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Severity Levels"]], "content": "\\label{subsec:severity_levels}\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Severity Levels} sub-theme of the \\textit{Severity} theme.}\n\\label{tab:severity_level_studies}\n\\begin{tabular}{|p{1.5cm}|p{4cm}|p{3cm}|p{3.8cm}|}\n \\hline  \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n Spanos et al. 2017~ & \\textit{Multi-class classification}: NVD severity levels based on CVSS v2 \\& WIVSS (High, Medium, Low) & NVD & Decision tree, SVM, MLP\\\\\\hline\n Wang et al. 2019~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: NVD\\\\severity levels based on CVSS v2\\\\(High, Medium, Low)} & NVD (XSS attacks) & XGBoost, Logistic regression, SVM, Random forest\\\\\\hhline{-~*{2}{-}}\n Le et al. 2019~ & & NVD & Concept-drift-aware models with Na\\\"ive Bayes, KNN, Linear SVM, Random forest, XGBoost, LGBM\\\\\\hhline{-~*{2}{-}}\n Liu et al. 2019~ & & NVD, China National Vulnerability Database (XSS attacks) & Recurrent Convolutional Neural Network (RCNN), Convolutional Neural Network (CNN), Long-Short Term Memory (LSTM)\\\\\\hhline{-~*{2}{-}}\n Sharma et al. 2020~ & & CVE Details & CNN\\\\\\hline\n Han et al. 2017~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: Atlassian\\\\categories of CVSS severity score\\\\(Critical, High, Medium, Low)} & CVE Details & 1-layer CNN, 2-layer CNN, CNN-LSTM, Linear SVM\\\\\\hhline{-~*{2}{-}}\n Sahin et al. 2019~ & & NVD & 1-layer CNN, LSTM, XGBoost, Linear SVM\\\\\\hhline{-~*{2}{-}}\n Nakagawa et al. 2019~ & & CVE Details & Character-level CNN vs. Word-based CNN + Linear SVM\\\\\\hline\n Gong et al. 2019~ & \\textit{Multi-task classification}: Atlassian categories of CVSS severity score (Critical, High, Medium, Low) & CVE Details & Bi-LSTM with attention mechanism\\\\\\hline\n Chen et al. 2010~ & \\textit{Multi-class classification}: severity levels of Secunia (Extremely/highly/ moderately/less/non- critical) & CVE, Secunia vulnerability database, SecurityFocus, IBM X-Force & Linear SVM\\\\\\hline\n Zhang et al. 2020~ & \\textit{Multi-class classification}: Platform-specific levels (High/Medium/Low) & China National Vulnerability Database & Logistic regression, Linear discriminant analysis, KNN, CART, SVM, bagging/boosting models\\\\\\hline\n Khazaei et al. 2016~ & \\textit{Multi-class classification}: 10 severity score bins (one unit/bin) & CVE \\& OSVDB & Linear SVM, Random forest, Fuzzy system\\\\\\hline\n\\end{tabular}\n\\end{table}\nRather than just performing binary classification of whether an SV is severe, several studies have identified one among multiple \\textit{severity levels} that an SV belongs to (see Table~\\ref{tab:severity_level_studies}). This setting can be considered as multi-class classification. Spanos et al.~ were to first one to show the applicability of ML to classify SVs into one of the three severity levels using SV descriptions. These three levels are provided by NVD and based on the severity score of CVSS version 2~ and WIVSS~, i.e., Low (0.0 -- 3.9), Medium (4.0 -- 6.9), High (7.0 -- 10.0). Note that WIVSS assigns different weights for the Confidentiality, Integrity and Availability impact metrics of CVSS, enhancing the ability to capture varied contributions of these impacts to the final severity score. Later, Wang et al.~ showed that XGBoost~ performed the best among the investigated ML classifiers for predicting these three NVD-based severity levels. Le et al.~ also confirmed that ensemble methods (e.g., XGBoost~, LGBM~ and Random forest) outperformed single models (e.g., Na\\\"ive Bayes, KNN and SVM) for this task. Predicting severity levels has also been tackled with DL techniques~ such as Recurrent Convolutional Neural Network (RCNN)~, Convolutional Neural Network (CNN)~, Long-Short Term Memory (LSTM)~. These studies showed potential performance gain of DL models compared to traditional ML counterparts. Han et al.~ showed that DL techniques (i.e., 1-layer CNN) also achieved promising results for predicting a different severity categorization, namely Atlassian's levels.\\footnote{\\url{https://www.atlassian.com/trust/security/security-severity-levels}} Such findings were successfully replicated by Sahin et al.~. Nakagawa et al.~ further enhanced the DL model performance for the same task by incorporating the character-level features into a CNN model~. Complementary to performance enhancement, Gong et al.~ proposed to predict these severity levels concurrently with other CVSS metrics in a single model using multi-task learning~ powered by an attention-based Bi-LSTM shared feature extraction model. The unified model was demonstrated to increase both the prediction effectiveness and efficiency.\nBesides Atlassian's categories, several studies applied ML models to predict severity levels on other platforms such as Secunia~ and China National Vulnerability Database\\footnote{\\url{https://www.cnvd.org.cn}}~. Instead of using textual categories, Khazaei et al.~ divided the CVSS severity score into 10 bins with 10 increments each (e.g., values of 0 -- 0.9 are in one bin) and obtained decent results (86-88\\% Accuracy) using Linear SVM, Random forest and Fuzzy system.", "cites": [4878, 4883, 326, 1096], "cite_extract_rate": 0.19047619047619047, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a factual summary of the surveyed papers by listing their tasks, data sources, and techniques. It includes some basic synthesis, such as noting that ensemble methods outperformed single models and that DL techniques showed potential gains over traditional ML. However, it lacks deeper critical evaluation or abstraction into broader principles, remaining largely descriptive in nature."}}
{"id": "e19a620c-76fa-4c1c-8fc7-f947f1cc3bf5", "title": "Severity Score", "level": "subsubsection", "subsections": [], "parent_id": "5587387b-27d7-4a6e-ad3c-d854609029b5", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Severity Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Severity Score"]], "content": "\\label{subsec:severity_score}\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Severity Score} sub-theme of the \\textit{Severity} theme. \\textbf{Notes}: \\textsuperscript{\\textdagger}denotes that the severity score is computed from ML-predicted base metrics using the formula provided by an assessment framework (CVSS and/or WIVSS).}\n\\label{tab:severity_score_studies}\n\\begin{tabular}{|p{1.5cm}|p{3cm}|p{3.2cm}|p{4.5cm}|}\n \\hline \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n Sahin et al. 2019~ & \\textit{Regression}: CVSS v2 (0-10) & NVD & 1-layer CNN, LSTM, XGBoost regressor, Linear regression\\\\\\hhline{-~*{2}{-}}\n Wen et al. 2015~ & & OSVDB, SecurityFocus, IBM X-Force & Radial basis function kernel SVM\\textsuperscript{\\textdagger}\\\\\\hline\n Ognawala et al. 2018~ & \\textit{Regression}: CVSS v3 (0-10) & NVD (buffer overflow SVs) & Combining a static analysis tool (Macke~) \\& ML classifiers (Na\\\"ive Bayes \\& Random forest)\\textsuperscript{\\textdagger}\\\\\\hhline{-~*{2}{-}}\n Chen et al. 2019~ & & CVE, NVD, Twitter & Graph convolutional network\\\\\\hhline{-~*{2}{-}}\n Anwar et al. 2020~ & & NVD & Linear regression, Support vector regression, CNN, MLP\\\\\\hline\n Elbaz et al. 2020~ & \\textit{Regression}: CVSS v2/v3 (0-10) & NVD & Mapping outputs of Linear regression to CVSS metrics with closest values\\textsuperscript{\\textdagger}\\\\\\hhline{-~*{2}{-}}\n Jiang et al. 2020~ & & NVD, ICS Cert, Vendor websites (Resolve inconsistencies with a majority vote) & Logistic regression\\textsuperscript{\\textdagger}\\\\\\hline\n Spanos et al. 2018~ & \\textit{Regression}: CVSS v2 \\& WIVSS (0-10) & NVD & Random forest, boosting model, Decision tree\\textsuperscript{\\textdagger}\\\\\\hline\n Toloudis et al. 2016~ & \\textit{Correlation analysis}: CVSS v2 \\& WIVSS (0-10) & NVD & Principal component analysis \\& Spearman correlation coefficient\\\\\\hline\n\\end{tabular}\n\\end{table}\nTo provide even more fine-grained severity value than the categories, the last sub-theme has predicted the \\textit{severity score} (see Table~\\ref{tab:severity_score_studies}).\nUsing SV descriptions on NVD, Sahin et al.~ compared the performance of ML-based regressors (e.g., XGBoost~ and Linear regression) and DL-based ones (e.g., CNN~ and LSTM~) for predicting the severity score of CVSS version 2~. These authors showed that DL-based approaches generally outperformed the ML-based counterparts. For CVSS version 3~, Chen et al.~ and Anwar et al.~ also reported the strong performance of DL-based models (e.g., CNN and graph convolutional neural network~). Some other studies did not directly predict severity score from SV descriptions, instead they aggregated the predicted values of the CVSS Exploitability (see section~\\ref{sec:exploit_prediction}) and Impact metrics (see section~\\ref{sec:impact_prediction}) using the formulas of CVSS version 2~, version 3~ and WIVSS~.\nWe noticed the papers predicting both versions (e.g., CVSS versions 2 vs. 3 or CVSS version 2 vs. WIVSS) usually obtained better performance for version 3 and WIVSS than version 2~. These findings may suggest that the improvements made by experts in version 3 and WIVSS compared to version 2 help make the patterns in severity score clearer and easier for ML models to capture.\nIn addition to predicting severity score, Toloudis et al.~ examined the correlation between words in descriptions of SVs and the severity values of such SVs, aiming to shed light on words that increase or decrease the severity score of SVs.", "cites": [4883, 4887, 4888], "cite_extract_rate": 0.1875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple studies by categorizing them and highlighting trends, such as the performance of deep learning models over traditional ML approaches. It offers a moderate level of critical analysis by noting that newer scoring frameworks (CVSS v3, WIVSS) appear to improve model performance, suggesting clearer patterns. Abstraction is evident in the identification of broader trends in severity score prediction techniques and data sources."}}
{"id": "6c43e491-72ad-4145-bd16-8fece1cd4250", "title": "Theme Discussion", "level": "subsection", "subsections": [], "parent_id": "3c12fb9c-f597-4d15-a64b-7fafa14d7d3a", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Severity Prediction"], ["subsection", "Theme Discussion"]], "content": "In the \\textit{Severity} theme, predicting the severity levels is the most prevalent task, followed by severity score prediction and then binary classification of the severity.\nIn practice, severity score gives more fine-grained information (fewer SVs per value) for practitioners to rank/prioritize SVs than categorical/binary levels. However, predicting continuous score values is usually challenging and requires more robust models as this task involves higher uncertainty to learn inherent patterns from data than classifying fixed/discrete levels. We observed that DL models such as graph neural networks~\\mbox{}, LSTM~\\mbox{} and CNN~\\mbox{} have been shown to be better than traditional ML models for predicting severity score. However, most of these studies did not evaluate their models in a continuous deployment setting to investigate how the models will cope with changing patterns of new SVs over time. We distill recommendations for future work on real-world application and evaluation of these models in section~\\mbox{\\ref{subsec:app_and_eval}}.", "cites": [4888], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section offers an analytical overview of the severity prediction theme by comparing the prevalence of different tasks and discussing the relative effectiveness of DL models over traditional ML. It integrates the cited work by highlighting a broader trend in model preference and identifies a significant limitation (lack of continuous deployment evaluation). While it provides useful generalizations about severity prediction approaches, the synthesis could be deeper by explicitly referencing more papers and their contributions."}}
{"id": "0cc2e312-c7ec-48e3-87d5-d63a4456e1f5", "title": "Type Prediction", "level": "section", "subsections": ["f42ddb25-de12-4c89-9eab-a666962ee271", "9f494f6c-70b2-4429-bb58-d421678db09f"], "parent_id": "973dad38-1208-442a-ab84-0b97a29f24a4", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Type Prediction"]], "content": "\\label{sec:type_prediction}\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Type} theme.}\\label{tab:type_studies}\n\\vspace{-2pt}\n\\begin{tabular}{|p{1.5cm}|p{3.9cm}|p{2.3cm}|p{4.5cm}|}\n \\hline \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n    \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 1. Common Weakness Enumeration (CWE)}}}\\\\\\hline\n    Wang et al. 2010~ & \\textit{Multi-class classification}: CWE classes & NVD, CVSS & Na\\\"ive Bayes\\\\\\hhline{-~*{2}{-}}\n    Shuai et al. 2013~ & & NVD & SVM\\\\\\hhline{-~*{2}{-}}\n    Na et al. 2016 ~ & & NVD & Na\\\"ive Bayes\\\\\\hhline{-~*{2}{-}}\n    Ruohonen et al. 2018~ & & NVD, CWE, Snyk & tf-idf with 1/2/3-grams and cosine similarity\\\\\\hhline{-~*{2}{-}}\n    Huang et al. 2019~ & & NVD, CWE & MLP, Linear SVM, Na\\\"ive Bayes, KNN\\\\\\hhline{-~*{2}{-}}\n    Aota et al. 2020~ & & NVD & Random forest, Linear SVM, Logistic regression, Decision tree, Extremely randomized trees, LGBM\\\\\\hhline{-~*{2}{-}}\n    Aghaei et al. 2020~ & & NVD, CVE & Adaptive fully-connected neural network with one hidden layer\\\\\\hhline{-~*{2}{-}}\n    Das et al. 2021~ & & NVD, CWE & BERT, Deep Siamese network\\\\\\hhline{-~*{2}{-}}\n    \\noalign{\\vskip\\doublerulesep\\vskip-\\arrayrulewidth}\\hhline{-~*{2}{-}}\n    Zou et al. 2019~ & & NVD \\& Software Assurance Reference Dataset (SARD) & Three Bi-LSTM models for extracting and combining global and local features from code functions\\\\\\hline\\hline\n    Murtaza et al. 2016~ & \\textit{Unsupervised learning}: sequence mining of SV types (over time) & NVD (CWE \\& CPE) & 2/3/4/5-grams of CWEs\\\\\\hline\n    Lin et al. 2017~ & \\textit{Unsupervised learning}: association rule mining of CWE-related aspects (prog. language, time of introduction \\& consequence scope) & CWE & FP-growth association rule mining algorithm\\\\\\hline\n     Han et al. 2018~ & \\textit{Binary/Multi-class classification}: CWE relationships (CWE links, link types \\& CWE consequences) & CWE & Deep knowledge graph embedding of CWE entities\\\\\\hline\n    \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 2. Custom Vulnerability Types}}}\\\\\\hline\n    Venter et al. 2008~  & \\textit{Unsupervised learning}: clustering & CVE & Self-organizing map\\\\\\hline\n    Neuhaus et al. 2010~  & \\textit{Unsupervised learning}: topic modeling & CVE & Latent Dirichlet Allocation (LDA)\\\\\\hhline{-~*{2}{-}}\n    Mounika et al.~ & & CVE, Open Web Application Security Project (OWASP) & LDA\\\\\\hhline{-~*{2}{-}}\n    Aljedaani et al. 2020~ & & SV reports (Chromium project) & LDA\\\\\\hline\\hline\n    Williams et al.~ & \\multirowcell{2}[0ex][l]{\\textit{Multi-class classification}: manually\\\\coded SV types} & NVD & Supervised Topical Evolution Model \\& Diffusion-based storytelling technique\\\\\\hhline{-~*{2}{-}}\n    Russo et al. 2019~ & & NVD & Bayesian network, J48 tree, Logistic regression, Na\\\"ive Bayes, Random forest\\\\\\hhline{-~*{2}{-}}\n    Yan et al. 2017~ & & Executables of 100 Linux applications & Decision tree\\\\\\hline\n    Zhang et al. 2020~ & \\textit{Multi-class classification}: platform-specific vulnerability types & China National Vulnerability Database & Logistic regression, Linear discriminant analysis, KNN, CART, SVM, bagging/boosting models\\\\\\hline\n\\end{tabular}\n\\end{table}\n\\vspace{-3pt}\nThis section reports the work done in the \\textit{Type} theme. Type groups SVs with similar characteristics, e.g., causes, attack patterns and impacts, and thus facilitating the reuse of known prioritization and remediation strategies employed for prior SVs of the same types.\nTwo key prediction outputs are: (\\textit{i}) Common Weakness Enumeration (CWE) and (\\textit{ii}) Custom vulnerability types (see Table ~\\ref{tab:type_studies}).", "cites": [4889, 7860], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section primarily lists papers related to vulnerability type prediction with minimal synthesis of ideas or connections between them. There is no critical evaluation of the cited works or their methodologies, and no abstraction to broader trends or principles in the field. The content remains largely descriptive with a focus on categorizing approaches."}}
{"id": "137bcfd9-c624-491d-b814-b3a9f49c35df", "title": "Common Weakness Enumeration (CWE)", "level": "subsubsection", "subsections": [], "parent_id": "f42ddb25-de12-4c89-9eab-a666962ee271", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Type Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Common Weakness Enumeration (CWE)"]], "content": "\\label{subsubsec:cwe}\nThe first sub-theme determines and analyzes the patterns of the SV types provided by \\textit{CWE}~.\nCWE is currently the standard for SV types with more than 900 entries.\nThe first group of studies has focused on multi-class classification of these CWEs. Wang et al.~ were the first to tackle this problem with a Na\\\"ive Bayes model using the CVSS metrics (version 2)~ and product names. Later, Shuai et al.~ used LDA~ with a location-aware weighting to extract important features from SV descriptions for building an effective SVM-based CWE classifier. Na et al.~ also showed that features extracted from SV descriptions can improve the Na\\\"ive Bayes model in~.\nRuohonen et al.~ studied an information retrieval method, i.e., term-frequency inverse document frequency (tf-idf) and cosine similarity, to detect the CWE-ID with a description most similar to that of a given SV collected from NVD and Snyk.\\footnote{\\url{https://snyk.io/vuln}} This method performed well for CWEs without clear patterns/keywords in SV descriptions.\nAota et al.~ utilized the Boruta feature selection algorithm~ and Random forest to improve the performance of \\textit{base} CWE classification.\nBase CWEs give more fine-grained information for SV remediation than categorical CWEs used in~.\nThere has been a recent rise in using neural network/DL based models for CWE classification. Huang et al.~ implemented a deep neural network with tf-idf and information gain for the task and obtained better performance than SVM, Na\\\"ive Bayes and KNN. Aghaei et al.~ improved upon~ for both categorical (coarse-grained) and base (fine-grained) CWE classification with an adaptive hierarchical neural network to determine sequences of less to more fine-grained CWEs. To capture the hierarchical structure and rare classes of CWEs, Das et al.~ matched SV and CWE descriptions instead of predicting CWEs directly.\nThey presented a deep Siamese network with a BERT-based~ shared feature extractor that outperformed many baselines even for rare/unseen CWE classes. Recently, Zou et al.~ pioneered the multi-class classification of CWE in vulnerable functions curated from Software Assurance Reference Dataset (SARD)~ and NVD. They achieved high performance ($\\sim$95\\% F1-score) with DL (Bi-LSTM) models. The strength of their model came from combining global (semantically related statements) and local (variables/statements affecting function calls) features. Note that this model currently only works for functions in C/C++ and 40 selected classes of CWE.\nAnother group of studies has considered unsupervised learning methods to extract CWE sequences, patterns and relationships.\nSequences of SV types over time were identified by Murtaza et al.~ using an $n$-gram model.\nThis model sheds light on both co-occurring and upcoming CWEs (grams), raising awareness of potential cascading attacks. Lin et al.~ applied an association rule mining algorithm, FP-growth~, to extract the rules/patterns of various CWEs aspects including types, programming language, time of introduction and consequence scope. For example, buffer overflow (CWE type) usually appears during the implementation phase (time of introduction) in C/C++ (programming language) and affects the availability (consequence scope). Lately, Han et al.~ developed a deep knowledge graph embedding technique to mine the relationships among CWE types, assisting in finding relevant SV types with similar properties.", "cites": [4889, 7, 7860], "cite_extract_rate": 0.15789473684210525, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple papers to present a coherent narrative around different approaches to CWE classification, including traditional ML, DL, and unsupervised learning. It critically evaluates the performance and limitations of these methods, particularly noting their applicability to specific languages or CWE subsets. The abstraction is strong, as it identifies broader trends such as the shift from manual to automated classification and the use of hierarchical or contextual modeling for improved generalization."}}
{"id": "e7ee33f2-6ae4-4314-b8e8-5df9a735c2b1", "title": "Custom Vulnerability Types", "level": "subsubsection", "subsections": [], "parent_id": "f42ddb25-de12-4c89-9eab-a666962ee271", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Type Prediction"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Custom Vulnerability Types"]], "content": "\\label{subsubsec:custom_types}\nThe second sub-theme is about \\textit{custom vulnerability types} other than CWE.\nVenter et al.~ used Self-organizing map~, an unsupervised clustering algorithm, to group SVs with similar descriptions on CVE. This was one of the earliest studies that automated SV type classification.\nTopic modeling is another popular unsupervised learning model~ to categorize SVs without an existing taxonomy. Neuhaus et al.~ applied LDA~ on SV descriptions to identify 28 prevalent SV types and then analyzed the trends of such types over time. The identified SV topics/types had considerable overlaps (up to 98\\% precision and 95\\% recall) with CWEs. Mounika et al.~ extended~ to map the LDA topics with the top-10 OWASP~. However, the LDA topics/keywords did not agree well ($<$ 40\\%) with the OWASP descriptions, probably because 10 topics did not cover all the underlying patterns of SV descriptions.\nAljedaani et al.~ again used LDA to identify 10 types of SVs reported in the bug tracking system of Chromium\\footnote{\\url{https://bugs.chromium.org/p/chromium/issues/list}} and found memory-related issues were the most prevalent topics.\nAnother group of studies has classified manually defined/selected SV types rather than CWE as some SV types are encountered more often in practice and require more attention. Williams et al.~ applied a supervised topical evolution model~ to identify the features that best described the 10 pre-defined SV types\\footnote{1. Buffer errors, 2. Cross-site scripting, 3. Path traversal, 4. Permissions and Privileges, 5. Input validation, 6. SQL injection, 7. Information disclosure, 8. Resources Error, 9. Cryptographic issues, 10. Code injection.} prevalent in the wild.\nThese authors then used a diffusion-based storytelling technique~ to show the evolution of a particular topic of SVs over time; e.g., increasing API-related SVs requires hardening the APIs used in a product. To support user-friendly SV assessment using ever-increasing unstructured SV data, Russo et al.~ used Bayesian network to predict 10 pre-defined SV types.\\footnote{1. Authentication bypass or Improper Authorization, 2. Cross-site scripting or HTML injection, 3. Denial of service, 4. Directory Traversal, 5. Local/Remote file include and Arbitrary file upload, 6. Information disclosure and/or Arbitrary file read, 7. Buffer/stack/heap/integer overflow, 8. Remote code execution, 9. SQL injection, 10. Unspecified vulnerability}\nBesides predicting manually defined SV types using SV natural language descriptions, Yan et al.~ used a decision tree to predict 22 SV types prevalent in the executables of Linux applications. The predicted type was then combined with fuzzers' outputs to predict SV exploitability (see section~\\ref{subsubsec:exploit_likelihood}).\nBesides author-defined types, custom SV types also come from specific SV platforms.\nZhang et al.~ designed an ML-based framework to predict the SV types collected from China National Vulnerability Database. Ensemble models (bagging and boosting models) achieved, on average, the highest performance for this task.", "cites": [4890], "cite_extract_rate": 0.06666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple studies on custom vulnerability types effectively, grouping them by methodology (e.g., unsupervised vs. supervised learning) and context (e.g., platform-specific types). It also provides some critical evaluation, such as the limitations of LDA topic mapping with OWASP. While it identifies patterns in the use of ML techniques for non-CWE type prediction, it could offer a more generalized framework or deeper abstraction for further insight."}}
{"id": "389d11dc-7a78-4470-876b-d55cd2b8a19c", "title": "Miscellaneous Tasks", "level": "section", "subsections": ["0ff69fe0-3881-497f-9da2-20012f6e0831", "014e00bc-993c-42fb-8430-73f0c6ebd66e"], "parent_id": "973dad38-1208-442a-ab84-0b97a29f24a4", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Miscellaneous Tasks"]], "content": "\\label{sec:miscellaneous}\nThe last theme is \\textit{Miscellaneous Tasks} covering the studies that are representative yet do not fit into the four previous themes.\nThis theme has three main sub-themes/tasks: (\\textit{i}) Vulnerability information retrieval, (\\textit{ii}) Cross-source vulnerability patterns and (\\textit{iii}) Vulnerability fixing effort (see Table ~\\ref{tab:misc_studies}).\n\\vspace{-3pt}\n\\begin{table}\n\\fontsize{8}{9}\\selectfont\n\\centering\n\\caption{List of the surveyed papers in the \\textit{Miscellaneous Tasks} theme.}\n\\label{tab:misc_studies}\n\\begin{tabular}{|p{1.5cm}|p{4.7cm}|p{3cm}|p{3cm}|}\n    \\hline \\textbf{Study} & \\textbf{Nature of task} & \\textbf{Data source} & \\textbf{Data-driven technique}\\\\\\hline\n    \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 1. Vulnerability Information Retrieval}}}\\\\\\hline\n    Weeraward-hana et al. 2014~ & \\textit{Multi-class classification}: Extraction of entities (software name/version, impact, attacker/user actions) from SV descriptions & NVD (210 randomly selected and manually labeled SVs) & Stanford Named Entity Recognizer implementing a CRF classifier\\\\\\hline\n    Dong et al. 2019~ & \\textit{Multi-class classification}: Vulnerable software names/versions & CVE Details, NVD, ExploitDB, SecurityFocus, SecurityFocus Forum, SecurityTracker, Openwall & Word-level and character-level Bi-LSTM with attention mechanism\\\\\\hline\n    Gonzalez et al. 2019~ & \\textit{Multi-class classification}: Extraction of 19 Vulnerability Description Ontology~ classes from SV descriptions & NVD & Na\\\"ive Bayes, Decision tree, SVM, Random forest, Majority voting model\\\\\\hline\n    Binyamini et al. 2020~ & Multi-class classification: Extraction of entities (attack vector/means/technique, privilege, impact, vulnerable platform/version/OS, network protocol/port) from SV descriptions to generate MulVal~ interaction rules & NVD & Bi-LSTM with various feature extractors: word2vec, ELMo, BERT (pre-trained or trained from scratch)\\\\\\hline\n    Guo et al. 2020~ & \\textit{Multi-class classification}: Extraction of entities (SV type, root cause, attack type, attack vector) from SV descriptions & NVD, SecurityFocus & CNN, Bi-LSTM (with or without attention mechanism)\\\\\\hline\n    Waareus et al. 2020~ & \\textit{Multi-class classification}: Common Product Enumeration (CPE) & NVD & Word-level and character-level Bi-LSTM\\\\\\hline\n    Yitagesu et al. 2021~ & \\textit{Multi-class classification}: Part-of-speech tagging of SV descriptions & NVD, CVE, CWE, CAPEC, CPE, Twitter, PTB corpus~ & Bi-LSTM\\\\\\hline\n    Sun et al. 2021~ & \\textit{Multi-class classification}: Extraction of entities (vulnerable product/version/component, type, attack type, root cause, attack vector, impact) from ExploitDB to generate SV descriptions & NVD, ExploitDB & BERT models\\\\\\hline\n    \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 2. Cross-source Vulnerability Patterns}}}\\\\\\hline\n    Horawalavith-ana et al. 2019~ & \\textit{Regression}: Number of software development activities on GitHub after disclosure of SVs & Twitter, Reddit, GitHub & MLP, LSTM\\\\\\hline\n    Xiao et al. 2019~ & \\textit{Knowledge-graph reasoning}: modeling the relationships among SVs, its types and attack patterns & CVE, CWE, CAPEC (Linux project) & Translation-based knowledge-graph embedding\\\\\\hline\n    \\multicolumn{4}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{4}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Sub-theme: 3. Vulnerability Fixing Effort}}}\\\\\\hline\n    Othmane et al. 2017~ & \\textit{Regression}: time (days) to fix SVs & Proprietary SV data collected at the SAP company & Linear/Tree-based/Neural network regression\\\\\\hline\n\\end{tabular}\n\\end{table}", "cites": [8851, 4891, 4892], "cite_extract_rate": 0.21428571428571427, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily lists studies under the 'Miscellaneous Tasks' theme with minimal synthesis or integration of their ideas. It organizes the papers by sub-themes and outlines their tasks, data sources, and techniques, but does not provide a coherent narrative or connect the works in a meaningful way. There is little critical evaluation or abstraction beyond the surface-level descriptions of individual papers."}}
{"id": "19754f29-e923-419c-8977-93703ebf62a7", "title": "Vulnerability Information Retrieval", "level": "subsubsection", "subsections": [], "parent_id": "0ff69fe0-3881-497f-9da2-20012f6e0831", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Miscellaneous Tasks"], ["subsection", "Summary of Primary Studies"], ["subsubsection", "Vulnerability Information Retrieval"]], "content": "\\label{subsec:information_retrieval}\nThe first and major sub-theme is \\textit{vulnerability information retrieval} that studies data-driven methods to extract different SV-related entities (e.g., affected products/versions) and their relationships from SV data. The current sub-theme extracts assessment information appearing explicitly in SV data (e.g., SV descriptions on NVD) rather than predicting implicit properties as done in prior sub-themes. For instance, CWE-119, i.e., ``Improper Restriction of Write Operations within the Bounds of a Memory Buffer'', can be retrieved directly from CVE-2020-28022,\nbut not from CVE-2021-2122.\\footnote{\\url{https://nvd.nist.gov/vuln/detail/CVE-2020-28022} \\& \\url{https://nvd.nist.gov/vuln/detail/CVE-2021-21220}} The latter case requires techniques from section~\\ref{subsubsec:cwe}.\nMost of the retrieval methods in this sub-theme have been formulated under the multi-class classification setting. One of the earliest works was conducted by Weerawardhana et al.~. This study extracted software names/versions, impacts and attacker's/user's action from SV descriptions on NVD using Stanford Named Entity Recognition (NER) technique, a.k.a. CRF classifier~. Later, Dong et al.~ proposed to use a word/character-level Bi-LSTM to improve the performance of extracting vulnerable software names and versions from SV descriptions available on NVD and other SV databases/advisories (e.g., CVE Details~, ExploitDB~, SecurityFocus~, SecurityTracker~ and Openwall~). Based on the extracted entities, these authors also highlighted the inconsistencies in vulnerable software names and versions across different SV sources. Besides version products/names of SVs, Gonzalez et al.~ used a majority vote of different ML models (e.g., SVM and Random forest) to extract the 19 entities of Vulnerability Description Ontology (VDO)~ from SV descriptions to check the consistency of these descriptions based on the guidelines of VDO.\nSince 2020, there has been a trend in using DL models (e.g., Bi-LSTM, CNNs or BERT~/ELMo~) to extract different information from SV descriptions including required elements for generating MulVal~ attack rules~ or SV types/root cause, attack type/vector~, Common Product Enumeration (CPE)~ for standardizing names of vulnerable vendors/products/versions~, part-of-speech~ and relevant entities (e.g., vulnerable products, attack type, root cause) from ExploitDB to generate SV descriptions~. BERT models~, pre-trained on general text (e.g., Wikipedia pages~ or PTB corpus~) and fine-tuned on SV text, have also been increasingly used to address the data scarcity/imbalance for the retrieval tasks.", "cites": [4892, 8851, 8385, 4891, 7], "cite_extract_rate": 0.23809523809523808, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from several papers to present a coherent narrative on vulnerability information retrieval methods, highlighting trends such as the shift from traditional NER to deep learning models. It offers some critical insights by discussing inconsistencies across SV sources and the use of BERT to address data imbalance. However, it could provide deeper comparative evaluation and more overarching theoretical abstraction to elevate its insight level."}}
{"id": "014e00bc-993c-42fb-8430-73f0c6ebd66e", "title": "Theme Discussion", "level": "subsection", "subsections": [], "parent_id": "389d11dc-7a78-4470-876b-d55cd2b8a19c", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Miscellaneous Tasks"], ["subsection", "Theme Discussion"]], "content": "In the \\textit{Miscellaneous Tasks} theme, the key focus is on retrieving SV-related entities and characteristics from SV descriptions. The retrieval tasks are usually formulated as Named Entity Recognition from SV descriptions. However, we observed that NVD descriptions do not follow a consistent template~\\mbox{}, posing significant challenges in labeling the entities for retrieval. The affected versions and vendor/product names of SVs also contain inconsistencies~\\mbox{}, making the retrieval tasks difficult. We recommend that data normalization and cleaning should be performed before labeling entities and building respective retrieval models to ensure the reliability of results.\nBesides information retrieval, other tasks such as linking multi-sources, extracting cross-source patterns or estimating fixing effort are also useful to obtain richer SV information for assessment and prioritization, yet these tasks are still in early stages. Linking multiple sources and their patterns is the first step towards building an SV knowledge graph to answer different queries regarding a particular SV (e.g., what systems are affected, exploitation status, how to fix, or what SVs are similar). In the future, such a knowledge graph can be extended to capture artifacts of SVs in emerging software types like AI-based systems (see section~\\mbox{\\ref{subsec:sv_ap_data_driven_systems}}). Moreover, to advance SV fixing effort prediction, future work can consider adapting/customizing the existing practices/techniques used to predict fixing effort for general bugs~\\mbox{}.", "cites": [4888], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper on NVD normalization but does not connect it to other broader works in the field. It offers a critical view of the inconsistencies in NVD data and suggests data cleaning as a prerequisite for reliable entity retrieval. The abstraction level is moderate, as it identifies broader implications for building knowledge graphs and future work on SV fixing effort, but does not offer a comprehensive framework or meta-level analysis."}}
{"id": "0c35245d-ebaa-47dc-b398-eecdb5efcae0", "title": "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization", "level": "section", "subsections": ["94d2a07b-5f93-4610-bf3a-024bf6a5e14e", "9de214da-9fbb-456d-a6e5-4d0c6e6812be", "2bb867c1-af93-41c1-be0c-ea049171b1a0", "743710b7-553e-4857-93ef-27c64af06665", "83358df6-779e-4f0c-8673-85ea422516f1"], "parent_id": "973dad38-1208-442a-ab84-0b97a29f24a4", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization"]], "content": "\\label{sec:elements_analysis}\n\\begin{table}\n\\fontsize{5.5}{6}\\selectfont\n\\centering\n\\caption{The frequent data sources, features, models, evaluation techniques and evaluation metrics used for the five identified SV assessment and prioritization themes. \\textbf{Notes}: The values are organized based on their overall frequency across the five themes. For the Prediction Model and Evaluation Metric elements, the values are first organized by categories (ML then DL for Prediction Model and classification then regression for Evaluation Metric) and then by frequencies. k-CV stands for k-fold cross-validation.\nThe full list of values and their appearance frequencies for the five elements in the five themes can be found at~\\mbox{}.}\n\\label{tab:data_driven_elements}\n\\begin{tabular}{|p{3cm}|p{4.55cm}|p{5.2cm}|}\n    \\hline \\textbf{Source/Technique/Metric} & \\textbf{Strengths} & \\textbf{Weaknesses}\\\\\\hline\n    \\multicolumn{3}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{3}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Element: Data Source}}}\\\\\\hline\n    \\makecell[l]{NVD/CVE/CVE Details (deprecated\\\\ OSVDB)} &\n    \\makecell[l]{\\tabitem Report expert-verified information (with CVE-ID)\\\\ \\tabitem Contain CWE and CVSS entries for each SV\\\\ \\tabitem Link to external sources (official fixes or vendors' info)} & \\makecell[l]{\\tabitem Missing/incomplete links to vulnerable code/fixes\\\\ \\tabitem Inconsistencies due to human errors\\\\ \\tabitem Delayed SV reporting and assignment of CVSS metrics}\\\\\\hline\n    ExploitDB & \n    \\makecell[l]{\\tabitem Report PoC exploits of SVs (with links to CVE-ID)} & \\makecell[l]{\\tabitem May not lead to real exploits in the wild}\\\\\\hline\n    \\makecell[l]{Other security advisories (e.g.,\\\\ SecurityFocus, Symantec or X-Force)} & \n    \\makecell[l]{\\tabitem Report real-world exploits of SVs\\\\ \\tabitem Cover a wide range of SVs (including ones w/o CVE-ID)} & \\makecell[l]{\\tabitem Some exploits may not have links to CVE entries for\\\\ mapping with other assessment metrics}\\\\\\hline\n    \\makecell[l]{Informal sources (e.g., Twitter\\\\ and darkweb)} & \n    \\makecell[l]{\\tabitem Early reporting of SVs (maybe even earlier than NVD)\\\\ \\tabitem Contain non-technical SV information (e.g., financial\\\\ damage or socio-technical challenges in addressing SVs)} & \\makecell[l]{\\tabitem Contain non-verified and even misleading information\\\\ \\tabitem May cause adversarial attacks to assessment models}\\\\\\hline\n    \\multicolumn{3}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{3}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Element: Model Feature}}}\\\\\\hline\n    BoW/tf-idf/n-grams & \n    \\makecell[l]{\\tabitem Simple to implement\\\\ \\tabitem Strong baseline for text-based inputs (e.g., SV descrip-\\\\tions in security databases/advisories)} & \\makecell[l]{\\tabitem May suffer from vocabulary explosion (e.g., many new descrip-\\\\tion words for new SVs)\\\\ \\tabitem No consideration of word context/order (maybe needed for\\\\ code-based SV analysis)\\\\ \\tabitem Cannot handle Out-of-Vocabulary (OoV) words (can be resolved\\\\ with subwords~)}\\\\\\hline\n    Word2vec & \n    \\makecell[l]{\\tabitem Capture nearby context of each word\\\\ \\tabitem Can reuse existing pre-trained model(s)} & \\makecell[l]{\\tabitem Cannot handle OoV words (can be resolved with fastText~)\\\\ \\tabitem No consideration of word order}\\\\\\hline\n    \\makecell[l]{DL model end-to-end trainable\\\\ features} & \n    \\makecell[l]{\\tabitem Produce SV task-specific features} & \\makecell[l]{\\tabitem May not produce high-quality representation for tasks\\\\ with limited data (e.g., real-world exploit prediction)}\\\\\\hline\n    \\makecell[l]{Bidirectional Encoder Representations\\\\ from Transformers (BERT)} & \n    \\makecell[l]{\\tabitem Capture contextual representation of text (i.e., the\\\\ feature vector of a word is specific to each input)\\\\ \\tabitem Capture word order in an input\\\\ \\tabitem Can handle OoV words} & \\makecell[l]{\\tabitem May require GPU to speed up feature inference\\\\ \\tabitem May be too computationally expensive and require too\\\\ much data to train a strong model from scratch\\\\ \\tabitem May require fine-tuning to work well for a source task}\\\\\\hline\n    \\makecell[l]{Source/expert-defined metadata\\\\ features} & \n    \\makecell[l]{\\tabitem Lightweight\\\\ \\tabitem Human interpretable for a task of interest} & \\makecell[l]{\\tabitem Require SV expertise to define relevant features\\\\ \\tabitem Hard to generalize to new tasks}\\\\\\hline\n    \\multicolumn{3}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{3}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Element: Prediction Model}}}\\\\\\hline\n    \\makecell[l]{Single ML models (e.g., Linear SVM,\\\\ Logistic regression, Nave Bayes)} & \n    \\makecell[l]{\\tabitem Simple to implement\\\\ \\tabitem Efficient to (re-)train on large data (e.g., using the entire\\\\ NVD database)} & \\makecell[l]{\\tabitem May be prone to overfitting\\\\ \\tabitem Usually do not perform as well as ensemble/DL models}\\\\\\hline\n    \\makecell[l]{Ensemble ML models (e.g., Ran-\\\\dom forest, XGBoost, LGBM)} & \n    \\makecell[l]{\\tabitem Strong baseline (usually stronger than single models)\\\\ \\tabitem Less prone to overfitting} & \\makecell[l]{\\tabitem Take longer to train than single models}\\\\\\hline\n    \\makecell[l]{Latent Dirichlet Allocation (LDA --\\\\ topic modeling)} & \n    \\makecell[l]{\\tabitem Require no labeled data for training\\\\ \\tabitem Can provide features for supervised learning models} & \\makecell[l]{\\tabitem Require SV expertise to manually label generated topics\\\\ \\tabitem May generate human non-interpretable topics}\\\\\\hline\n    \\makecell[l]{Deep Multi-Layer Perceptron\\\\ (MLP)} & \n    \\makecell[l]{\\tabitem Work readily with tabular data (e.g., manually defined\\\\ features or BoW/tf-idf/n-grams)} & \\makecell[l]{\\tabitem Perform comparably yet are more costly compared to ensemble\\\\ ML models\\\\ \\tabitem Less effective for unstructured data (e.g., SV descriptions)}\\\\\\hline\n    \\makecell[l]{Deep Convolutional Neural\\\\ Networks (CNN)} & \n    \\makecell[l]{\\tabitem Capture local and hierarchical patterns of inputs\\\\ \\tabitem Usually perform better than MLP for text-based data} & \\makecell[l]{\\tabitem Cannot effectively capture sequential order of inputs (maybe\\\\ needed for code-based SV analysis)}\\\\\\hline\n    \\makecell[l]{Deep recurrent neural networks\\\\ (e.g., LSTM or Bi-LSTM)} & \n    \\makecell[l]{\\tabitem Capture short-/long-term dependencies from inputs\\\\ \\tabitem Usually perform better than MLP for text-based data} & \\makecell[l]{\\tabitem May suffer from the information bottleneck issue (can be\\\\ resolved with attention mechanism~)\\\\ \\tabitem Usually take longer to train than CNNs}\\\\\\hline\n    \\makecell[l]{Deep graph neural networks\\\\ (e.g., Graph convolutional network)} & \n    \\makecell[l]{\\tabitem Capture directed relationships among multiple SV\\\\ entities and sources} & \\makecell[l]{\\tabitem Require graph-based inputs to work\\\\ \\tabitem More computationally expensive than other DL models}\\\\\\hline\n    \\makecell[l]{Deep transfer learning with\\\\ fine-tuning (e.g., BERT with task-\\\\specific classification layer(s))} & \n    \\makecell[l]{\\tabitem Can improve the performance for tasks with small data\\\\ (e.g., real-world exploit prediction)} & \\makecell[l]{\\tabitem Require target task to have similar nature as source task}\\\\\\hline\n    \\makecell[l]{Deep constrastive learning\\\\ (e.g., Siamese neural networks)} & \n    \\makecell[l]{\\tabitem Can improve performance for tasks with small data\\\\ \\tabitem Robust to class imbalance (e.g., CWE classes)} & \\makecell[l]{\\tabitem Computationally expensive (two inputs instead of one)\\\\ \\tabitem Do not directly produce class-wise probabilities}\\\\\\hline\n    \\makecell[l]{Deep multi-task learning} & \n    \\makecell[l]{\\tabitem Can share features for predicting multiple tasks (e.g.,\\\\ CVSS metrics) simultaneously\\\\ \\tabitem Reduce training/maintenance cost} & \\makecell[l]{\\tabitem Require predicted tasks to be related\\\\ \\tabitem Hard to tune the prediction/performance of individual tasks}\\\\\\hline\n    \\multicolumn{3}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{3}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Element: Evaluation Technique}}}\\\\\\hline\n    \\makecell[l]{Single k-CV without test} & \n    \\makecell[l]{\\tabitem Easy to implement\\\\ \\tabitem Reduce the randomness of results with multiple folds} & \\makecell[l]{\\tabitem Do not have a separate test set for validating optimized models\\\\ (can be resolved with separate test set(s))\\\\ \\tabitem Maybe infeasible for expensive DL models\\\\ \\tabitem Use future data/SVs for training, maybe leading to biased results}\\\\\\hline\n    \\makecell[l]{Single/multiple random train/test\\\\ with/without val (using val to tune\\\\ hyperparameters)} & \n    \\makecell[l]{\\tabitem Easy to implement\\\\ \\tabitem Reduce the randomness of results (the multiple version)} & \\makecell[l]{\\tabitem May produce unstable results (the single version)\\\\ \\tabitem Maybe infeasible for expensive DL models (the multiple version)\\\\ \\tabitem Use future data/SVs for training, maybe leading to biased results}\\\\\\hline\n    \\makecell[l]{Single/multiple time-based train/test\\\\ with/without val (using val to tune\\\\ hyperparameters)} & \n    \\makecell[l]{\\tabitem Consider the temporal properties of SVs, simulating the\\\\ realistic evaluation of ever-increasing SVs in practice\\\\ \\tabitem Reduce the randomness of results (the multiple version)} & \\makecell[l]{\\tabitem Similar drawbacks for the single \\& multiple versions as the\\\\ random counterparts\\\\ \\tabitem May result in uneven/small splits (e.g., many SVs in a year)}\\\\\\hline\n    \\multicolumn{3}{|c|}{\\cellcolor[HTML]{C0C0C0}}\\\\\n    \\multicolumn{3}{|c|}{\\multirow{-2}{*}{\\cellcolor[HTML]{C0C0C0} \\textbf{Element: Evaluation Metric}}}\\\\\\hline\n    \\makecell[l]{F1-score/Precision/Recall\\\\ (classification)} & \n    \\makecell[l]{\\tabitem Suitable for imbalanced data (common in SV assessment\\\\ and prioritization tasks)} & \\makecell[l]{\\tabitem Do not consider True Negatives in a confusion matrix (can be\\\\ resolved with Matthews Correlation Coefficient (MCC))}\\\\\\hline\n    \\makecell[l]{Accuracy (classification)} & \n    \\makecell[l]{\\tabitem Consider all the cells in a confusion matrix} & \\makecell[l]{\\tabitem Unsuitable for imbalanced data (can be resolved with MCC)}\\\\\\hline\n    \\makecell[l]{Area Under the Curve (AUC)\\\\ (classification)} & \n    \\makecell[l]{\\tabitem Independent of prediction thresholds} & \\makecell[l]{\\tabitem May not represent real-world settings (i.e., as models in practice\\\\ mostly use fixed classification thresholds)\\\\ \\tabitem ROC-AUC may not be suitable for imbalanced data (can be\\\\ resolved with Precision-Recall-AUC)}\\\\\\hline\n    \\makecell[l]{Mean absolute (percentage) error/\\\\ Root mean squared error (regression)} & \n    \\makecell[l]{\\tabitem Show absolute performance of models} & \\makecell[l]{\\tabitem Maybe hard to interpret a value on its own without domain\\\\ knowledge (i.e., whether an error of $x$ is sufficiently effective)}\\\\\\hline\n    \\makecell[l]{Correlation coefficient ($r$)/ Coef.\\\\ of determination ($R^2$) (regression)} & \n    \\makecell[l]{\\tabitem Show relative performance of models (0 -- 1), where 0 is\\\\ worst \\& 1 is best} & \\makecell[l]{\\tabitem $R^2$ always increases when adding any new feature (can be\\\\ resolved with adjusted $R^2$)}\\\\\\hline\n\\end{tabular}\n\\end{table}\nWe extract and analyze the five key elements for data-driven SV assessment and prioritization: (\\textit{i}) Data sources, (\\textit{ii}) Model features, (\\textit{iii}) Prediction models, (\\textit{iv}) Evaluation techniques and (\\textit{v}) Evaluation metrics.\nThese elements correspond to the four main steps in building data-driven models: data collection (data sources), feature engineering (model features), model training (prediction models) and model evaluation (evaluation techniques/metrics)~.\nWe present the most common practices for each element in Table~\\mbox{\\ref{tab:data_driven_elements}}.", "cites": [8369, 168, 4878], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong insight by synthesizing multiple papers and integrating their findings into a structured discussion of data sources, model features, and prediction models. It critically evaluates the strengths and weaknesses of each approach, often referencing relevant papers (e.g., word2vec, BERT, and contrastive learning). The abstraction level is high as the section generalizes across works to identify broader patterns and challenges in the field of data-driven SV assessment and prioritization."}}
{"id": "94d2a07b-5f93-4610-bf3a-024bf6a5e14e", "title": "Data sources", "level": "subsection", "subsections": [], "parent_id": "0c35245d-ebaa-47dc-b398-eecdb5efcae0", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization"], ["subsection", "Data sources"]], "content": "\\label{subsec:data_source}\nIdentifying and collecting rich and reliable SV-related data are the first tasks to build data-driven models for automating SV assessment and prioritization tasks. As shown in Table~\\ref{tab:data_driven_elements}, a wide variety of data sources have been considered to accomplish the five identified themes.\nAcross the five themes, NVD~ and CVE~ have been the most prevalently used data sources.\nThe popularity of NVD/CVE is mainly because they publish expert-verified SV information that can be used to develop prediction models.\nFirstly, many studies have considered SV descriptions on NVD/CVE as model inputs. Secondly, the SV characteristics on NVD have been heavily used as assessment outputs in all the themes, e.g., CVSS Exploitability metrics for \\textit{Exploitation}, CVSS Impact/Scope metrics for \\textit{Impact}, CVSS severity score/levels for \\textit{Severity}, CWE for \\textit{Type}, CWE/CPE for \\textit{Miscellaneous tasks}. Thirdly, external sources on NVD/CVE have enabled many studies to obtain richer SV information (e.g., exploitation availability/time~ or vulnerable code/crashes~) and extract relationships among multiple SV sources to develop a knowledge graph of SVs (e.g.,~).\nHowever, NVD/CVE still suffer from information inconsistencies~ and missing relevant external sources (e.g., SV fixing code)~.\nSuch issues motivate future work to validate/clean NVD data and utilize more sources for code-based SV assessment and prioritization (see section~\\ref{subsubsec:developer_sources}).\nTo enrich the SV information on NVD/CVE, many other security advisories and SV databases have been commonly leveraged by the reviewed studies, notably ExploitDB~, Symantec~, SecurityFocus~, CVE Details~ and OSVDB. Most of these sources disclose PoC (ExploitDB and OSVDB) and/or real-world (Symantec and Security Focus) exploits. However, real-world exploits are much rarer and different compared to PoC ones~.\nIt is recommended that future work should explore more data sources (other than the ones in Table~\\ref{tab:exploit_studies_likelihood}) and better methods to retrieve real-world exploits (see section~\\ref{subsec:data_avail}).\nAdditionally, CVE Details and OSVDB are SV databases like NVD yet with a few key differences. CVE Details explicitly monitors Exploit-DB entries that may be missed on NVD and provides a more user-friendly interface to view/search SVs. OSVDB also reports SVs that do not appear on NVD (without CVE-ID), but this site was discontinued in 2016.\nBesides official/expert-verified data sources, we have seen an increasing interest in mining SV information from informal sources that also contain non-expert generated content such as social media (e.g., Twitter) and darkweb. Especially, Twitter has been widely used for predicting exploits as this platform has been shown to contain many SV disclosures even before official databases like NVD~. Recently, darkweb forums/sites/markets have also gained traction as SV mentions on these sites have a strong correlation with their exploits in the wild~. However, SV-related data on these informal sources are much noisier because they neither follow any pre-defined structure nor have any verification and they are even prone to fake news~. Thus, the data integrity of these sources should be checked,\npotentially by checking the reputation of posters, to avoid inputting unreliable data to prediction models and potentially producing misleading findings.", "cites": [4888, 4893], "cite_extract_rate": 0.1, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from cited papers and other sources to present a coherent narrative on the use and limitations of various SV data sources. It critically discusses issues such as information inconsistency and data noise, particularly in informal sources like Twitter and the dark web. The section abstracts broader patterns, such as the trade-off between data richness and reliability across formal and informal sources, and offers recommendations for future research."}}
{"id": "9de214da-9fbb-456d-a6e5-4d0c6e6812be", "title": "Model features", "level": "subsection", "subsections": [], "parent_id": "0c35245d-ebaa-47dc-b398-eecdb5efcae0", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization"], ["subsection", "Model features"]], "content": "\\label{subsec:model_feature}\nCollected raw data need to be represented by suitable features for training prediction models.\nThere are three key types of feature representation methods in this area: term frequency (e.g., BoW, tf-idf and n-grams), DL learned features (e.g., BERT and word2vec) and source/expert-defined metadata (e.g., CVSS metrics and CPE on NVD or tweet properties on Twitter), as summarized in Table~\\ref{tab:data_driven_elements}.\nRegarding the term-frequency based methods, BoW has been the most popular one. Its popularity is probably because it is one of the simplest ways to extract features from natural language descriptions of SVs and directly compatible with popular ML models (e.g., Linear SVM, Logistic regression and Random forest) in section~\\ref{subsec:prediction_model}. Besides plain term count/frequency, other studies have also considered different weighting mechanisms such as inverse document frequency weighting (tf-idf) or tf-igm~ inverse gravity moment weighting (tf-igm). Tf-igm has been shown to work better than BoW and tf-idf at classifying severity~.\nFuture work is still needed to evaluate the applicability and generalizability of tf-igm for other SV assessment and prioritization tasks.\nRecently, Neural Network (NN) or DL based features such as word2vec~ and BERT~ have been increasingly used to improve the performance of predicting CVSS exploitation/impact/severity metrics~, CWE types~ and SV information retrieval~. Compared to BoW and its variants, NN and DL can extract more efficient and context-aware features from vast SV data~. NN/DL techniques rely on distributed representation to encode SV-related words using fixed-length vectors much smaller than a vocabulary size. Moreover, these techniques capture the sequential order and context (nearby words) to enable better SV-related text comprehension (e.g., SV vs. general \\textit{exploit}). Importantly, these NN/DL learned features can be first trained in a non-SV domain with abundant data (e.g., Wikipedia pages~) and then transferred/fine-tuned in the SV domain to address limited/imbalanced SV data~. The main concern with these sophisticated NN/DL features is their limited interpretability, which is an exciting research area (see section~\\mbox{\\ref{subsubsec:interpretability}}).\nThe metadata about SVs can also complement the missing information in descriptions or code for SV assessment and prioritization. For example, prediction of exploits and their characteristics have been enhanced using CVSS metrics~, CPE~ and SV types~ on NVD. Additionally, Twitter-related statistics (e.g., number of followers, likes and retweets) have been shown to increase the performance of predicting SV exploitation, impact and severity~. Recently, alongside features extracted from vulnerable code, the information about a software development process and involved developers have also been extracted to predict SV fixing effort~. Currently, metadata-based and text-based features have been mainly integrated by concatenating their respective feature vectors (e.g.,~\\mbox{}). An alternative yet unexplored way is to build separate models for each feature type and then combine these models using meta-learning (e.g., model stacking~\\mbox{}).", "cites": [4894, 1684, 4891, 7], "cite_extract_rate": 0.17391304347826086, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers by grouping feature representation methods into three coherent categories and explaining their relevance and application in SV assessment. It offers critical analysis by highlighting limitations such as interpretability issues with DL methods and the need for further evaluation of tf-igm. The abstraction level is strong, identifying broader patterns such as transfer learning and proposing meta-learning as a novel integration approach."}}
{"id": "2bb867c1-af93-41c1-be0c-ea049171b1a0", "title": "Prediction models", "level": "subsection", "subsections": [], "parent_id": "0c35245d-ebaa-47dc-b398-eecdb5efcae0", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization"], ["subsection", "Prediction models"]], "content": "\\label{subsec:prediction_model}\nThe extracted features enter a wide variety of ML/DL-based prediction models shown in Table~\\ref{tab:data_driven_elements} to automate various SV assessment and prioritization tasks.\nClassification techniques have the largest proportion, while regression and unsupervised techniques are less common.\nLinear SVM~ has been the most frequently used classifier, especially in the Exploitation, Impact and Severity themes.\nThis popularity is expected as Linear SVM works well with the commonly used features, i.e., BoW and tf-idf.\nBesides Linear SVM, Random forest, Na\\\"ive Bayes and Logistic regression have also been common classification models.\nIn recent years, advanced boosting models (e.g., XGBoost~ and LGBM~), and more lately, DL techniques (e.g., CNN~ and (Bi-)LSTM with attention~) have been increasingly utilized and shown better results than simple ML models like Linear SVM or Logistic regression.\nIn this area, some DL models are essential for certain tasks, e.g., building SV knowledge graph from multiple sources with graph neural networks~\\mbox{}. DL models also offer solutions to data-related issues such as addressing class imbalance (e.g., deep Siamese network~\\mbox{}) or improving data efficiency (e.g., deep multi-task learning~\\mbox{}).\nWhenever applicable, it is recommended that future work should still consider simple baselines alongside sophisticated ones as simple methods can perform on par with advanced ones~\\mbox{}.\nBesides classification, various prediction models have also been investigated for regression (e.g., predicting exploit time, severity score and fixing time).\nLinear SVM has again been the most commonly used regressor as SV descriptions have usually been the regression input.\nNotably, many studies in the Severity theme did not build regression models to directly obtain the severity score (e.g.,~). Instead, they used the formulas defined by assessment frameworks (e.g., CVSS versions 2/3~ or WIVSS~) to compute the severity score from the base metrics predicted by respective classification models.\nWe argue that more effort should be invested in determining the severity score directly from SV data as these severity formulas can be subjective~\\mbox{}. We also observe that there is still limited use of DL models for regression compared to classification.\nIn addition to supervised (classification/regression) techniques, unsupervised learning has also been considered for extracting underlying patterns of SV data, especially in the Type theme. Latent Dirichlet Allocation (LDA)~ has been the most commonly used topic model to identify latent topics/types of SVs without relying on a labeled taxonomy. The identified topics were mapped to the existing SV taxonomies such as CWE~ and OWASP~.\nThe topics generated by topic models like LDA can also be used as features for classification/regression models~ or building topic-wise models to capture local SV patterns~.\nHowever, definite interpretations for unsupervised outputs are challenging to obtain as they usually rely on human judgement~\\mbox{}.", "cites": [4896, 168, 4887, 4886, 4068, 4895, 4883, 326], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers to provide a coherent overview of prediction models in vulnerability assessment. It critically evaluates trends (e.g., preference for Linear SVM, shift to boosting and DL), identifies limitations (e.g., DL underutilized in regression, unsupervised methods' interpretability challenges), and abstracts broader patterns such as the trade-off between model complexity and performance. While not creating a novel framework, it offers a structured analytical perspective on model usage and potential improvements."}}
{"id": "743710b7-553e-4857-93ef-27c64af06665", "title": "Evaluation techniques", "level": "subsection", "subsections": [], "parent_id": "0c35245d-ebaa-47dc-b398-eecdb5efcae0", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Analysis of Data-driven Approaches for Software Vulnerability Assessment and Prioritization"], ["subsection", "Evaluation techniques"]], "content": "\\label{subsec:evaluation_technique}\nIt is important to evaluate a trained model to ensure the model meets certain requirements (e.g., advancing the state-of-the-art).\nThe evaluation generally needs to be conducted on a different set of data other than the training set to avoid overfitting and objectively estimate model generalizability~. The commonly used evaluation techniques are summarized in Table~\\ref{tab:data_driven_elements}.\nThe reviewed studies have mostly used one or multiple validation and/or test sets\\footnote{Validation set(s) helps optimize/tune a model (finding the best task/data-specific hyperparameters), and test set(s) evaluates the optimized/tuned model. Using only validation set(s) means evaluating a model with default/pre-defined hyperparameters.} to evaluate their models, in which each validation/test set has been either randomly or time-based selected. Specifically, k-fold cross-validation has been one of the most commonly used techniques. The number of folds has usually been 5 or 10, but less standard values like 4~ have also been used. However, k-fold cross-validation uses all parts of data at least once for training; thus, there is no hidden test set to evaluate the optimal model with the highest (cross-)validation performance.\nTo address the lack of hidden test set(s), a common practice in the studied papers has been to split a dataset into single training and test sets, sometimes with an additional validation set for tuning hyperparameters to obtain an optimal model. Recently, data has been increasingly split based on the published time of SVs to better reflect the changing nature of ever-increasing SVs~. There have been various ratios for random (e.g., 80:20, 75:25 or 67:33) and time-based (e.g., week/month/year-wise) splits. However, the results reported using single validation/test sets may be unstable (i.e., unreproducible results using different set(s))~.\nTo ensure both the time order and reduce the result randomness, we recommend using multiple splits of training and test sets in combination with time-based validation in each training set. Statistical analyses (e.g., hypothesis testing and effect size) should also be conducted to confirm the reliability of findings with respect to the randomization of models/data in multiple runs~.", "cites": [4897, 4878, 4879], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key evaluation practices from multiple studies, connecting ideas about k-fold cross-validation, random and time-based splits, and the issue of concept drift. It critically addresses limitations of these methods, such as the absence of a hidden test set and potential instability. The section abstracts beyond individual papers by recommending a broader, more robust evaluation framework that integrates time-based validation and statistical analysis."}}
{"id": "667df1c3-8e60-496d-870d-f28e55835f64", "title": "Utilization of developer Q\\&A platforms and version control systems", "level": "subsubsection", "subsections": [], "parent_id": "cb027b7a-aef7-4ae6-83f1-cb887649e9c9", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "SV Data Availability"], ["subsubsection", "Utilization of developer Q\\&A platforms and version control systems"]], "content": "\\label{subsubsec:developer_sources}\n\\textit{Developer Question \\& Answer (Q\\&A) platforms} like Stack Overflow and Security StackExchange\\footnote{Stack Overflow: \\url{https://stackoverflow.com} \\& Security StackExchange: \\url{https://security.stackexchange.com}} contain tens of thousands of posts about challenges and solutions shared by millions of developers when tackling known SVs in real-world scenarios~.\nOne of the key insights of Le et al.~'s study is that the top SV types that developers usually struggle with are not always the same as those reported on SV databases (CWE~ or OWASP~). Thus, future work should also consider real-world development-related issues discussed on developer Q\\&A platforms for automatically assessing and prioritizing SVs. For example, the fixing effort of SVs may depend on the technical difficulty of implementing the respective mitigation strategies in a language or system of interest.\n\\textit{Version control systems} like GitHub\\footnote{\\url{https://github.com}} provide details about how developers addressed past SVs in real-world projects. Shrestha et al.~ found developers sometimes discuss/disclose SV-related information on GitHub discussions even before the studied social media such as Twitter or Reddit. These findings show the potential of using GitHub discussions to complement the current sources for earlier SV assessment and prioritization. GitHub can also provide vulnerable code for performing assessment and prioritization for SVs rooted in source code~, which is important yet has received limited attention from the community so far.\nMoreover, Walden~ demonstrated the impact of a major SV (i.e., Heartbleed) on the characteristics (e.g., code complexity/style, contributors and development practices) of a single project (i.e., OpenSSL). Based on Walden's findings, future work can study whether the impact of an SV would be similar or different in multiple affected projects. Such investigation would give insights into the possibility of leveraging data from large projects to perform SV assessment and prioritization in smaller projects with the same/similar SVs.", "cites": [4899, 4898], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes insights from two cited papers to highlight how developer Q&A platforms and version control systems can provide unique and valuable data for SV assessment and prioritization. It offers some critical perspectives by pointing out limitations in current data sources and proposes future research directions. The abstraction is moderate, as it generalizes the importance of development-related data beyond specific examples but does not fully develop a meta-level framework."}}
{"id": "71cb02c5-84be-442b-a882-d74cc68efe9d", "title": "Relaxation of fully-supervised learning", "level": "subsubsection", "subsections": [], "parent_id": "cb027b7a-aef7-4ae6-83f1-cb887649e9c9", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "SV Data Availability"], ["subsubsection", "Relaxation of fully-supervised learning"]], "content": "\\label{subsubsec:relax_supervised}\nSupervised learning models of many tasks in the five themes (see section~\\ref{subsec:prediction_model}) require fully labeled data, but the data of some tasks are quite limited. To address the data-hungriness of these fully-supervised learning models, future studies can approach the SV assessment and prioritization tasks with \\textit{low-shot learning} and/or \\textit{semi-supervised learning}.\n\\textit{Low-shot learning} a.k.a. \\textit{few-shot learning} is designed to perform supervised learning using only a few examples per class, significantly reducing the labeling effort~.\nSo far, only one study utilized low-shot learning with a deep Siamese network~ (i.e., a shared feature model with similarity learning) to effectively predict SV types (CWE) and even generalize to unseen classes (i.e., zero-shot learning). There are still many opportunities for investigating different few-shot learning techniques for other SV assessment and prioritization tasks. Note that the shared features in few-shot learning can also be enhanced with pretrained models (e.g., BERT~) on another domain/task/project with more labeled data than the current task/project in the SV domain.\n\\textit{Semi-supervised learning} enables training models with limited labeled data yet a large amount of unlabeled data~, potentially leveraging hidden/unlabeled SVs in the wild. Recently, we have seen an increasing interest in using different techniques of this learning paradigm in the SV domain such as collecting SV patches using multi-view co-training~ or retrieving SV discussions on developer Q\\&A sites using positive-unlabeled learning~. However, it is still little known about the effectiveness of semi-supervised learning for SV assessment and prioritization.", "cites": [3624, 7, 4901, 4900], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates ideas from multiple cited papers to discuss how low-shot and semi-supervised learning can address data scarcity in SV assessment. It provides some connections between methods and their applications, such as using BERT for feature enhancement and PU learning for mining security discussions. However, it lacks deeper comparative or evaluative analysis and could offer more meta-level generalizations about the broader implications of relaxing supervised learning in the SV domain."}}
{"id": "1a7d6048-96c9-47d4-9c90-450c7fc8d58e", "title": "More timely and fine-grained prediction", "level": "subsubsection", "subsections": [], "parent_id": "0de5a083-1b00-4ffb-9841-c434a74dde51", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Real-world Application and Evaluation"], ["subsubsection", "More timely and fine-grained prediction"]], "content": "\\label{subsubsec:timely_finegrained_prediction}\nAlthough SV descriptions have been commonly used as model inputs (see section~\\ref{subsec:data_source}), these descriptions are usually published long after SVs introduced/discovered in codebases~. One potential solution to this issue is to perform assessment and prioritization of SVs in code commits. Code commits contain changes made by developers to fix a bug/SV, implement a new feature or refactor code, and new SVs may be introduced in such changes~. Commit-level prediction would allow just-in-time SV assessment and prioritization as soon as SVs are introduced, reducing the waiting time for SV information to be verified and published on security advisories/databases~. It should be noted that report-level prediction is still important for assessing and prioritizing third-party libraries/software, especially the ones without available code (commits), and/or SVs missed by commit-level prediction.\nCVSS~ has been most frequently used for assessing the exploitability, impact and severity levels/score of SVs (see sections~\\ref{sec:exploit_prediction},~\\ref{sec:impact_prediction} and~\\ref{sec:severity_prediction}), but there are increasing concerns that CVSS outputs are still generic. Specifically, Spring et al.~ argued that CVSS tends to provide one-size-fits-all assessment metrics regardless of the context of SVs; i.e., the same SVs in different domains/environments are assigned the same metric values. For instance, banking systems may consider the confidentiality and integrity of databases more important than the availability of web/app interfaces. In the future, alongside CVSS, prediction models should also incorporate the domain/business knowledge to customize the assessment of SVs to a system of interest (e.g., the impact of SVs on critical component(s) and/or the readiness of developers/solutions for mitigating such SVs in the current system). Future case studies with practitioners are also desired to correlate the quantitative performance of models and their usability/usefulness in real-world systems (e.g., reducing more critical SVs yet using fewer resources).", "cites": [4902], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the idea of using code commits for timely vulnerability assessment, drawing on the concept from Paper 1 (DeepCVA), and integrates it with the broader context of SV data sources. It critically evaluates the limitations of CVSS, particularly its generic nature, but does not compare multiple models or engage in deep critique of the cited work. The abstraction level is moderate as it identifies the need for domain-specific knowledge and outlines a general direction for improving SV assessment, though it does not offer a novel framework."}}
{"id": "355b557f-5bbd-4fab-91fe-59fb1301ef34", "title": "Enhancing model interpretability", "level": "subsubsection", "subsections": [], "parent_id": "0de5a083-1b00-4ffb-9841-c434a74dde51", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Real-world Application and Evaluation"], ["subsubsection", "Enhancing model interpretability"]], "content": "\\label{subsubsec:interpretability}\nModel interpretability is important to increase the transparency of the predictions made by a model, allowing practitioners to adjust the model/data to meet certain requirements~. Unfortunately, very few reviewed papers (e.g.,~) explicitly discussed important features and/or explained why/when their models worked/failed for a task.\nSV assessment and prioritization can draw inspiration from the related SV detection area where the interpretability of (DL-based) prediction models has been actively explored mainly by using (\\textit{i}) specific model architectures/parameters or (\\textit{ii}) external interpretation models/techniques~. In the first approach, prior studies successfully used the feature activation maps in a CNN model~ or leveraged attention-based neural network~ to highlight and visualize the important code tokens that contribute to SVs. The second approach uses separate interpretation models on top of trained SV detectors. The interpretation models are either domain/model-agnostic~, domain-agnostic yet specific to a model type (graph neural network~) or SV-specific~. The aforementioned approaches produce local/sample-wise interpretation, which can be aggregated to obtain global/task-wise interpretation. The global interpretation is similar to the feature importance of traditional ML models~ such as the weights of linear models (e.g., Logistic regression) or the (im)purity of nodes split by each feature in tree-based models (e.g., Random forest). However, it is still unclear about the applicability/effectiveness of these approaches for interpreting ML/DL-based SV assessment and prioritization models, requiring further investigations.\n\\vspace{-3pt}", "cites": [4905, 4904, 4903, 4906], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple cited papers to present a coherent discussion on model interpretability in vulnerability assessment. It categorizes approaches into two main types, drawing from both vulnerability detection and broader ML literature, and raises critical questions about their applicability to SV prioritization. The abstraction is strong as it generalizes from specific methods to broader interpretability frameworks and identifies open research questions."}}
{"id": "633fe90a-3912-45b9-bd65-65106ba4badb", "title": "Realistic evaluation settings", "level": "subsubsection", "subsections": [], "parent_id": "0de5a083-1b00-4ffb-9841-c434a74dde51", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Real-world Application and Evaluation"], ["subsubsection", "Realistic evaluation settings"]], "content": "\\label{subsubsec:realistic_eval}\nMost of the reviewed studies have evaluated their prediction models without capturing many factors encountered during the deployment of such models to production.\nSpecifically, the models used in practice would require to handle new data and be robust against adversarial data from informal sources such as social media or darkweb.\nThere are concerns with both predicting and integrating new SV data.\nRegarding the prediction, Out-of-Vocabulary words in new data need to be properly accommodated to avoid performance degradation of prediction models~.\nRegarding the new data integration, online/incremental training on new data can be considered instead of batch training on the whole dataset to reduce computational cost~. The time-based splits should be used rather than random splits for evaluating online training to avoid leaking unseen (future) patterns to the model training (see section~\\ref{subsec:evaluation_technique}).\nRegarding the model robustness, only three reviewed studies considered adversarial attacks as part of their evaluation~. However, a recent survey shows the prevalence of adversarial attacks targeted models in cybersecurity~. Thus, there is certainly a need for more evaluation of adversarial robustness for SV assessment and prioritization models, especially DL-based ones.\n\\vspace{-3pt}", "cites": [4907, 4878], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from two relevant papers to highlight key evaluation challenges in data-driven SV assessment and prioritization. It critically identifies gaps in handling new data and adversarial robustness, while abstracting these issues into broader concerns such as model deployment in real-world settings and the need for time-based evaluation splits. The analysis connects the dots between data evolution and security threats, offering a principled critique and forward-looking perspective."}}
{"id": "bc38b5de-0920-4df8-96f2-23829630fb4c", "title": "Data-driven SV Assessment and Prioritization of Data-driven Systems", "level": "subsection", "subsections": ["19a0ad3d-7fb5-4653-bb54-9fc9e63c4de1", "24f8a2d4-e666-4e10-9699-001c6c6a08d7", "8220c39f-e82b-44b8-a4bc-1488b7c1be69"], "parent_id": "da6030f9-9171-47ad-8fc5-765c930225d6", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Data-driven SV Assessment and Prioritization of Data-driven Systems"]], "content": "\\label{subsec:sv_ap_data_driven_systems}\nCompared to other systems, reporting/analyzing SVs of data-driven/Artificial Intelligence (AI)-based systems is still in its infancy~.\nData-driven systems (e.g., smart recommender systems, chatbots, robots, and autonomous cars) are an emerging breed of systems whose cores are powered by AI technologies, e.g., ML and DL models built on data, rather than human-defined instructions as in traditional systems.\nWe discuss three key challenges of SV assessment and prioritization of data-driven systems compared to traditional systems and suggest potential solutions. Firstly, the current SV assessment frameworks need customizations to better reflect the nature of SVs in data-driven systems (section~\\ref{subsubsec:new_assessment_framework}). Secondly, there is a lack of SVs collected from real-world data-driven systems, limiting the potential of data-driven SV assessment and prioritization (section~\\ref{subsubsec:svs_datadriven_systems}). Thirdly, the current models require redesign, especially in the SV representation, to capture unique characteristics and artifacts of data-driven systems (section~\\ref{subsubsec:datadriven_sv_representation}).\n\\vspace{-3pt}", "cites": [4908], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section identifies three key challenges in assessing and prioritizing vulnerabilities in data-driven systems and references a single paper (4908) related to adversarial ML and legal risks. While it provides a general direction, it lacks synthesis of multiple sources or deeper integration of concepts. The critical analysis is limited, as the paper is mentioned in the context of real-world systems but not evaluated or contrasted with others. The section begins to generalize about the need for new frameworks and representations, but these ideas are not fully developed into overarching principles."}}
{"id": "19a0ad3d-7fb5-4653-bb54-9fc9e63c4de1", "title": "Designing a compatible SV assessment framework", "level": "subsubsection", "subsections": [], "parent_id": "bc38b5de-0920-4df8-96f2-23829630fb4c", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Data-driven SV Assessment and Prioritization of Data-driven Systems"], ["subsubsection", "Designing a compatible SV assessment framework"]], "content": "\\label{subsubsec:new_assessment_framework}\nCVSS~ is currently the most popular SV assessment framework for traditional systems, but its compatibility with data-driven systems still requires more investigation. The current CVSS documentation lacks instructions on how to assign metrics/score for SVs in data-driven systems. For example, it is unclear how to assign static CVSS metrics to systems with automatically updated data-driven models~ because adversarial examples for exploitation would likely change after the models are updated.\nSuch ambiguities should be clarified/resolved in future CVSS versions as data-driven systems become more prevalent.\nThe types of SVs in ML/DL models in data-driven systems are also mostly different from the ones provided by CWE~. The difference is mainly because these new SVs do not only emerge from configurations/code as in traditional systems, but also from training data and/or trained models~. Thus, we recommend that a new category of these SVs should be studied and potentially incorporated into CWE, similar to the newly added category for architectural SVs.\\footnote{\\url{https://cwe.mitre.org/data/definitions/1008.html}}", "cites": [4907], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by identifying limitations in existing frameworks (CVSS and CWE) for assessing vulnerabilities in data-driven systems. It synthesizes the implications of adversarial machine learning, as discussed in the cited paper, and connects them to the need for a new SV assessment category. However, it does not integrate multiple papers or present a novel framework, and the critique remains at a general level without deeper comparative or meta-level analysis."}}
{"id": "24f8a2d4-e666-4e10-9699-001c6c6a08d7", "title": "Collection of SVs in data-driven systems", "level": "subsubsection", "subsections": [], "parent_id": "bc38b5de-0920-4df8-96f2-23829630fb4c", "prefix_titles": [["title", "A Survey on Data-driven Software Vulnerability Assessment and Prioritization"], ["section", "Open Research Challenges and Future Directions of Data-driven Software Vulnerability Assessment and Prioritization"], ["subsection", "Data-driven SV Assessment and Prioritization of Data-driven Systems"], ["subsubsection", "Collection of SVs in data-driven systems"]], "content": "\\label{subsubsec:svs_datadriven_systems}\nTo the best of our knowledge, there has been no existing large-scale dataset of SVs in ML/DL models deployed in real-world data-driven systems. Very few of such SVs have been reported in the wild, one of which is CVE-2019-20634.\\footnote{\\url{https://nvd.nist.gov/vuln/detail/CVE-2019-20634}} More of these SVs are required to help develop sufficiently effective SV assessment and prioritization models. One potential way to build such a dataset is to first match the (pre-trained) ML/DL models proposed in the literature or released on model repositories (e.g., Tensorflow Hub\\footnote{\\url{https://www.tensorflow.org/hub}}) with the ones used in real-world systems either on version control systems or in mobile apps~. The matched models would then be tested against known adversarial attacks to identify corresponding SVs. Notably, significant effort is still required to define/label assessment outputs of these SVs (see section~\\ref{subsubsec:new_assessment_framework}).", "cites": [4909], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying a key limitation in the availability of real-world SV datasets for ML/DL models and proposing a method to address it. It integrates a single cited paper to discuss on-device models in Android apps but does not synthesize ideas across multiple sources. It critically highlights the need for more labeled SVs and connects this to broader challenges in assessment and prioritization, showing some abstraction."}}
