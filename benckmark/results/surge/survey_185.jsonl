{"id": "64415a0a-1e56-4c60-a042-addd3d8b68cf", "title": "Reinforcement Learning", "level": "paragraph", "subsections": [], "parent_id": "18022e96-23f2-4664-a5de-de4277dc0b3e", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "Background"], ["subsection", "Sequential Decision Making"], ["paragraph", "Probabilistic Planning"], ["paragraph", "Reinforcement Learning"]], "content": "Agents frequently have to make sequential decisions with an incomplete model of domain dynamics (e.g., without $R$, $T$, or both), making it infeasible to use classical PP methods. Under such circumstances, RL algorithms can be used by the agent to explore the effects of executing different actions, learning a policy (mapping states to actions) that maximizes the expected cumulative reward as the agent tries to achieve a goal~. The underlying formulation is that of an MDP or a formulation that reduces to an MDP under certain constraints. \nThere are at least two broad classes of RL methods: \\textbf{model-based} and \\textbf{model-free}. Model-based RL methods enable an agent to learn a model of the domain, e.g., $R(s,a)$ and $T(s,a, s')$ in an MDP, from the experiences obtained by the agent by trying out different actions in different states. Once a model of the domain is learned, the agent can use PP methods to compute an action policy. Model-free RL methods, on the other hand, do not learn an explicit model of the domain; the policy is instead directly computed from the experiences gathered by the agent. The standard approach to incrementally update the value of each state is the Bellman equation:\n$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} pr(s', r| s,a)[r+\\gamma v_k(s')], \\forall s\\in \\mathcal{S}$$\nwhere $v(s)$ is the value of state $s$, and $\\gamma$ is a discount factor. It is also possible to compute the values of state-action pairs, i.e., $Q(s, a)$, from which a policy can be computed.\nMany algorithms have been developed for model-based and model-free RL; for more details, please see~. More recent work has also explored the integration of deep neural networks (DNNs) with RL, e.g., to approximate the value function~, and deep policy-based methods, e.g.,~. This survey focuses on the interplay between SDM (including RL) and RDK methods; the properties of individual RL methods (or SDM methods) are out of scope.", "cites": [2219, 1391], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of reinforcement learning, distinguishing between model-based and model-free approaches, and briefly mentions the Bellman equation and deep learning integration. However, it lacks synthesis of the cited papers into a cohesive narrative and offers minimal critical analysis or abstraction beyond the surface-level description of RL methods."}}
{"id": "4be41793-2f83-4877-81e9-0b4ad24c9a4a", "title": "Factor 5: State or Belief State in SDM", "level": "paragraph", "subsections": [], "parent_id": "d805730d-9c37-4c24-a6d9-762746fb6ab9", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "Characteristic Factors"], ["subsection", "Reasoning Factors"], ["paragraph", "Factor 3: Dynamics in RDK"], ["paragraph", "Factor 5: State or Belief State in SDM"]], "content": "SDM methods involve an agent making decisions based on observing and estimating the state of the world. A key distinction here is whether this state is fully observable or partially observable, or equivalently, whether the observations are assumed to be complete and correct. The \\underline{\\textit{fifth factor}} categorizes the SDM methods based on whether they reason assuming full knowledge of state after action execution, or assume that the true state is unknown and reason with \\textit{belief states}, e.g., probability distributions over the underlying states~, and implicit representations computed with neural networks~. Among the SDM formulations considered in this paper, MDPs map to the former category, whereas POMDPs map to the latter category. \n\\medskip\nNote that there are other distinguishing characteristics of reasoning systems (of RDK-for-SDM systems) that we do not explore in this paper. For instance, reasoning in such systems often includes a combination of active and reactive processes, e.g., actively planning and executing a sequence of actions to achieve a particular goal in the RDK component, and computing a probabilistic policy that is then used reactively for action selection in the SDM component.", "cites": [3590], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the state and belief state distinction in SDM, primarily referencing MDPs and POMDPs. It mentions the use of neural networks but does not delve into how cited works like DRQN contribute to this discussion. There is minimal synthesis, critical evaluation, or abstraction beyond surface-level categorization."}}
{"id": "459dfec2-054b-4783-9392-03b4a5557034", "title": "Unified RDK-for-SDM Representations", "level": "subsubsection", "subsections": [], "parent_id": "2363a548-c72c-40e1-92ff-d018b089ec70", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "RDK-for-SDM Methods"], ["subsection", "Representation-focused Systems"], ["subsubsection", "Unified RDK-for-SDM Representations"]], "content": "Developing a unified representation for RDK and SDM maps to developing a unified representation for logical and probabilistic reasoning, which has been a fundamental problem in robotics and AI for decades. \nFrameworks and methods based on unified representations provide significant expressive power, but they also impose a significant computational burden despite the ongoing work on developing more efficient (and often approximate) reasoning algorithms for such unified paradigms. \n\\smallskip\n\\noindent\n\\textbf{Statistical Relational AI} Some of the foundational work in this area has built on work in statistical relational learning/AI. These RDK-for-SDM methods typically use unified representations and  differ based on the underlying design choices. For instance, Markov Logic Networks (MLNs) combine probabilistic graphical models and first order logic, assigning weights to logic formulas~; these have been extended to Markov logic decision networks by associating logic formulas with utilities in addition to weights~.\nIn a similar manner, Probabilistic Logic (ProbLog) programming annotates facts in logic programs with probabilities and supports efficient inference and learning using weighted Boolean formulas~. This includes an extension of the basic ProbLog system, called Decision-Theoretic (DT)ProbLog, in which the utility of a particular choice of actions is defined as the expected reward for its execution in the presence of probabilistic effects~. Another example of an elegant (unified) formalism for dealing with degrees of belief and their evolution in the presence of noisy sensing and acting, extends situation calculus by assigning weights to possible worlds and embedding a theory of action and sensing~. This formalism has been extended to deal with decision making in the continuous domains seen in many robotics applications~. Others have developed frameworks based on unified representations specifically for decision theoretic reasoning, e.g., first-order relational POMDPs that leverage symbolic programming for the specification of POMDPs with first-order abstractions~. \n\\smallskip\n\\noindent\n\\textbf{Classical Planning} RDK-for-SDM systems based on unified representations have also built on tools and methods in classical planning. Examples include PPDDL, a probabilistic extension of the action language PDDL, which retains the capabilities of PDDL and provides a semantics for planning problems as MDPs~, and Relational Dynamic Influence Diagram Language (RDDL) that was developed to formulate factored MDPs and POMDPs~. In comparison with PPDDL, RDDL provides better support for modeling concurrent actions and for representing rewards and uncertainty quantitatively. \n\\smallskip\n\\noindent\n\\textbf{Logic Programming} RDK-for-SDM systems with a unified representation have also been built based on logic programming frameworks. One example is P-log, a probabilistic extension of ASP that encodes probabilistic facts and rules to compute probabilities of different possible worlds represented as answer sets~. P-log has been used to specify MDPs for SDM tasks, e.g., for robot grasping~. More recent work has introduced a coherence condition that facilitates the construction of P-log programs and proofs of correctness~. One limitation of P-log, from the SDM perspective, is that it requires the horizon to be provided as part of the input. The use of P-log for probabilistic planning with infinite horizons requires a significant engineering effort.", "cites": [6781], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple works on unified representations for RDK-for-SDM by connecting them under the broader theme of combining logical and probabilistic reasoning. It provides critical evaluation of the limitations and strengths of different frameworks, such as the need for approximate algorithms, support for concurrent actions, and constraints on modeling horizons. The analysis abstracts these systems into a coherent conceptual framework, highlighting overarching design principles and application contexts."}}
{"id": "30f401b3-a193-4cf1-9673-94a1ed0b7f21", "title": "Linked RDK-for-SDM Representations", "level": "subsubsection", "subsections": [], "parent_id": "2363a548-c72c-40e1-92ff-d018b089ec70", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "RDK-for-SDM Methods"], ["subsection", "Representation-focused Systems"], ["subsubsection", "Linked RDK-for-SDM Representations"]], "content": "As stated earlier in the context of Factor 1 in Section~\\ref{sec:factors-represent}, RDK-for-SDM systems with linked (hybrid) representations trade expressivity or correctness guarantees for computational speed, an important consideration if an agent has to respond to dynamic changes in complex domains. These methods often also use different levels of abstraction and link rather than unify the corresponding descriptions of knowledge and uncertainty. This raises interesting questions about the choice of domain variables in each representation, and the transfer of knowledge and control between the different reasoning mechanisms. For instance, a robot delivering objects in an office building may plan at an abstract level, reasoning logically with rich commonsense domain knowledge (e.g., about rooms, objects, and exogenous agents) and cognitive theories. The abstract actions can be implemented by reasoning probabilistically at a finer resolution about relevant domain variables (e.g., regions in specific rooms, parts of objects, and agent actions).\n\\smallskip\n\\noindent\n\\textbf{Switching Systems} The simplest option for methods based on linked representations is to switch between reasoning mechanisms based on different representations for different tasks. One example is the \\textit{switching planner} that uses either a classical first-order logic planner or a probabilistic (decision-theoretic) planner for action selection~. This method used a combination of the Fast-Downward~ and PPDDL~ representations. Another approach uses ASP for planning and diagnostics at a coarser level of abstraction, switches to using probabilistic algorithms for executing each abstract action, and adds statements to the ASP program's history to denote success or failure of action execution; this approach has been used for multiple robots in scenarios that mimic manufacturing in toy factories~.  \n\\smallskip\n\\noindent\n\\textbf{Tightly-Coupled Systems}\nThere has been some work on generic RDK-for-SDM frameworks that represent and reason with knowledge and beliefs at different abstractions, and ``\\emph{tightly couple}'' the different representations and reasoning mechanisms by formally establishing the links between and the attributes of the different representations. These methods are often based on the \\textit{principle of refinement}~. \nThis principle has also been explored in fields such as software engineering and programming languages~, but without any theories of actions and change that are important in robotics and AI. One approach examined the refinement of agent action theories represented using situation calculus at two different levels. This approach makes a strong assumption of the existence of a bisimulation relation between the action theories for a given refinement mapping between these theories at the high-level and the low-level~. The principle of refinement has also been used to construct abstractions of ASP programs, with the objective of shrinking the domain size while preserving the structure of the rules~.\nAn example of tightly-coupled systems in robotics is the refinement-based architecture (REBA) that considers transition diagrams of any given domain at two different resolutions, with the fine-resolution diagrams defined formally as a refinement of the coarse-resolution diagram~. Non-monotonic logical reasoning with limited commonsense domain knowledge at the coarse-resolution provides a sequence of abstract actions to achieve any given goal. Each abstract action is implemented as a sequence of concrete actions by automatically zooming to and reasoning probabilistically with automatically-constructed models (e.g., POMDPs) of the relevant part of the fine-resolution diagram, adding relevant observations and outcomes to the coarse-resolution history. The formal definition of refinement, zooming, and the connections between the transition diagrams enables smooth transfer of relevant information and control, and improves scalability. It also enables the robot to represent and reason with sophisticated cognitive theories in the coarse resolution, e.g., with an adaptive theory of intentions~.\n\\smallskip\n\\noindent\n\\textbf{Cognitive Architectures} Systems such as ACT-R~, SOAR~, ICARUS~ and DIRAC~ can represent and draw inferences based on declarative knowledge, often using first-order logic. These architectures typically support SDM through a linked representation, but some architectures have pursued a unified representation for use in robotics by attaching a quantitative measure of uncertainty to logic statements~.\n\\smallskip\n\\noindent\nThere are many other RDK-for-SDM systems based on hybrid representations. In these systems, the focus is not on developing new representations; they instead adapt or combine existing representations to support interesting reasoning and learning capabilities, as described below.", "cites": [9072, 9071], "cite_extract_rate": 0.13333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes concepts from the cited papers by integrating discussions of switching and tightly-coupled systems, and by linking the principle of refinement to both AI planning and robotics. It offers a critical perspective by highlighting the trade-offs between expressivity, correctness, and computational efficiency in hybrid representations. The abstraction level is strong, as it identifies overarching patterns such as the role of refinement and the use of multi-resolution reasoning in enabling scalable and cognitively rich decision-making."}}
{"id": "425c6e00-c8f7-4429-8379-5cf05059662a", "title": "Reasoning-focused Systems", "level": "subsection", "subsections": [], "parent_id": "ef4e809f-85f5-4bb7-a4f2-933ad534e3b9", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "RDK-for-SDM Methods"], ["subsection", "Reasoning-focused Systems"]], "content": "\\label{sec:rdkforsdm-reason}\nNext, we discuss some other representative RDK-for-SDM systems in which the primary focus is on addressing related reasoning challenges. \n\\smallskip\n\\noindent\n\\textbf{RDK for State Estimation}\nRDK methods can be used for estimating the current world state in order to guide SDM. Although practical domains often include many objects with different attributes, and multiple relationships between these objects, only a small subset of these objects, attributes, and relationships may be relevant to any particular task that an agent has to perform. Researchers have therefore used RDK methods to identify the task-relevant information to guide state estimation and SDM. For instance, the state of the world has been represented using knowledge predicates and assumptive predicates, which were then used for planning based on declarative action knowledge and probabilistic rules~. This approach, embedded within a three-layered architecture, was used for applications such as object search, and semantic mapping.  Another example is the CORPP system that uses P-log~ to reason with probabilistic declarative knowledge in order to generate informative priors for POMDP planning~. In a human-robot dialog domain, CORPP demonstrated that commonsense knowledge, such as ``people like coffee in the mornings'' and ``office doors are closed over weekends'', is useful for guiding dialog actions. Other researchers have exploited factored state spaces to develop algorithms that use probabilistic declarative knowledge to efficiently compute informative priors for POMDPs~. In particular, they developed an efficient belief state representation that dynamically selects an appropriate factoring to guide SDM, and demonstrated its effectiveness in robot cooking tasks. These methods separate the variables modeled at different levels and (manually) link relevant variables between the levels, improving scalability and dynamic response. These links enable the flow of information between the different reasoning mechanisms, often at different abstractions, but they typically do not focus on developing (or extending) the underlying representations or on establishing the properties of the connections between the representations. \n\\smallskip\n\\noindent\n\\textbf{Dynamics Models for SDM}\nIn some RDK-for-SDM systems, the focus is on RDK guiding the construction or adaptation of the world models used for SDM.\nOne example is the extension of~ that seeks to automatically determine the variables to be modeled in the different representations~. Another example is the use of logical smoothing to refine past beliefs in light of new observations; the refined beliefs can then be used for diagnostics and to reduce the state space for planning~. There is also work on an action language called pBC+, which supports the definition of MDPs and POMDPs over finite and infinite horizons~. \nIn some RDK-for-SDM systems, RDK and prior experiences of executing actions in the domain are used to construct domain models and guide SDM. For instance, symbolic planning has been combined with hierarchical RL to guide the agent's interactions with the world, resulting in reliable world models and SDM~. \nIn other work, each symbolic transition is mapped (manually) to options, i.e., temporally-extended MDP actions; RDK helps compute the MDP models and policies, and the outcomes of executing the corresponding primitive actions help revise the values of state action combinations in the symbolic reasoner~. These systems use a linked representation, and reason about dynamics in RDK and states and world models in SDM. Other systems reason without explicit world models in SDM, e.g., the use of deep RL methods to compute the policies in the options corresponding to each symbolic transition in the context of game domains~. \n\\begin{table*}[t]\n\\renewcommand*{\\arraystretch}{1.4}\n\\caption {A subset of the surveyed RDK-for-SDM algorithms from the literature mapped to the space defined by the characteristic factors discussed in Section~\\ref{sec:factors}. \nEach column corresponds to one characteristic factor (except for the last one); if a factor's range includes multiple values, this table shows the most typical value. \n\\textbf{Uni. Rep.:} unified representation for both RDK and SDM (Factor 1). \n\\textbf{Abs. Rep.:} abstract representations for RDK and SDM that are linked together (Factor 2). \n\\textbf{Dyn. RDK:} declarative knowledge includes action knowledge and can be used for task planning (Factor 3). \n\\textbf{RL SDM:} world models are not provided to SDM, rendering RL necessary (Factor 4). \n\\textbf{Par. Obs.:} current world  states are partially observable (Factor 5). \n\\textbf{On. Acq.:} online knowledge acquisition is enabled (Factor 6). \n\\textbf{ML RDK:} at least part of the knowledge base is learned by the agents, where the opposite is human developing the entire knowledge base (Factor 7). \n\\textbf{Rew. RDK:} RDK is used for reward shaping. \n} \n\\label{tab1} \n\\begin{center}\n\\tiny\n\\begin{tabular}{|l| l | c c c c c c c c|} \n\\hline\n  & & Uni. Rep. &  Abs. Rep.&  Dyn. RDK &  RL SDM   & Par. Obs. &  On. Acq.     & ML RDK    & Rew. RDK \\\\\\hline \n \\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=l]{90}{Representation}}} \n &            & \\pief     & \\piee\t    & \\pie      & \\piee     & \\piee     & \\piee         & \\piee     & \\piee\t\\\\\\cline{2-10}\n &        & \\pief     & \\piee\t    & \\pie      & \\piee     & \\pieh     & \\piee         & \\piee     & \\pief\t\\\\\\cline{2-10}\n &                & \\pief     & \\piee     & \\pie      & \\piee     & \\pieh     & \\piee         & \\piee     & \\piee \\\\\\cline{2-10} \n &            & \\pief     & \\piee\t    & \\pie      & \\piee     & \\pieh     & \\pief         & \\pief     & \\pief\t\\\\\\cline{2-10} \n &        & \\pief     & \\piee\t    & \\piee     & \\piee     & \\pieh     & \\pief         & \\piee     & \\pief\t\\\\ \\hline \\hline \n \\parbox[t]{2mm}{\\multirow{15}{*}{\\rotatebox[origin=l]{90}{Reasoning}}} \n &           & \\piee     & \\pief\t    & \\pief     & \\piee     & \\pief     & \\pief         & \\pief     & \\piee\t\\\\\\cline{2-10} \n &         & \\piee     & \\pief\t    & \\pief     & \\pief     & \\piee     & \\piee         & \\piee     & \\piee \\\\\\cline{2-10} \n &   & \\piee     & \\pief\t    & \\pief     & \\pief     & \\piee     & \\pief         & \\piee     & \\piee\t\\\\\\cline{2-10} \n &        & \\piee     & \\pief\t    & \\pief     & \\pief     & \\piee     & \\pief         & \\pief     & \\pief\t\\\\\\cline{2-10} \n &    & \\piee     & \\piee\t    & \\pief     & \\piee     & \\pief     & \\piee         & \\piee     & \\piee\t\\\\\\cline{2-10} \n &\n                                    & \\piee     & \\piee\t    & \\piee     & \\pief     & \\piee     & \\pief         & \\pief     & \\piee\t\\\\\\cline{2-10}\n &\n                                    & \\piee     & \\piee\t    & \\piee     & \\piee     & \\pief     & \\pief         & \\piee     & \\piee\t\\\\\\cline{2-10} \n & & \\piee   & \\piee     & \\piee     & \\piee     & \\pief     & \\piee         & \\piee     & \\piee\t\\\\\\cline{2-10} \n &           & \\piee     & \\piee\t    & \\piee     & \\piee     & \\pief     & \\pief         & \\pief     & \\piee\t\\\\\\cline{2-10} \n &        & \\piee     & \\pief\t    & \\pief     & \\pief     & \\piee     & \\piee         & \\piee     & \\pief\t\\\\\\cline{2-10} \n &      & \\piee     & \\piee\t    & \\piee     & \\piee     & \\pief     & \\pief         & \\piee     & \\pief\t\\\\\\cline{2-10}\n &              & \\piee     & \\piee\t    & \\pief     & \\pief     & \\piee     & \\piee         & \\piee     & \\pief\t\\\\\\cline{2-10}\n &             & \\piee     & \\piee\t    & \\piee     & \\pief     & \\piee     & \\piee         & \\piee     & \\piee\t\\\\\\cline{2-10}\n &       & \\piee     & \\piee\t    & \\pief     & \\pief     & \\piee     & \\pief         & \\piee     & \\piee\t\\\\\\cline{2-10}\n &         & \\piee     & \\pief\t    & \\pief     & \\pief     & \\piee     & \\pief         & \\pief     & \\piee\t\\\\\\hline \\hline \n \\parbox[t]{2mm}{\\multirow{5}{*}{\\rotatebox[origin=l]{90}{Acquisition}}} \n &            \n                                    & \\piee     & \\pief\t    & \\pief     & \\piee     & \\piee     & \\pief         & \\pief     & \\piee\t\\\\\\cline{2-10} \n &\n                                    & \\piee     & \\piee\t    & \\piee     & \\piee     & \\pief     & \\pief         & \\piee     & \\piee\t\\\\\\cline{2-10} \n &          & \\piee     & \\piee\t    & \\pief     & \\pief     & \\piee     & \\pief         & \\pief     & \\piee\t\\\\\\cline{2-10} \n &      & \\piee     & \\piee\t    & \\pief     & \\piee     & \\piee     & \\pief         & \\piee     & \\piee\t\\\\\\cline{2-10} \n &             & \\piee     & \\piee\t    & \\piee     & \\piee     & \\piee     & \\pief         & \\pief     & \\piee\t\\\\\\hline\n\\end{tabular}\n\\end{center}\n\\end{table*}\n\\smallskip\n\\noindent\n\\textbf{Credit Assignment and Reward Shaping} When MDPs or POMDPs are used for SDM in complex domains, rewards are sparse and typically obtained only on task completion, e.g., after executing a plan or at the end of a board game. As a special case of learning and using world models in SDM, researchers have leveraged RDK methods to model and shape the rewards to improve the agent's decision-making. For instance, declarative action knowledge has been used to compute action sequences, using the action sequences to compute a potential function and for reward shaping in game domains~. In this work, RL methods such as Q-learning, SARSA, and Dyna-Q were combined with a STRIPS planner, with the planner shaping the reward function used by the agents to compute the optimal policy. These systems perform RDK with domain dynamics, and reason about states but no explicit world models in SDM. \nIn some cases, the reward specification is obtained from statistics and/or contextual knowledge provided by humans.\nFor example, the iCORPP algorithm enables a robot to reason with contextual knowledge using P-log to automatically determine the rewards (and transition functions) of a POMDP used for planning~. \nAnother system, called LPPGI, enables robots to leverage human expertise for POMDP-based planning under uncertainty in the context of task specification and execution~. RDK in this system is rather limited; domain dynamics are not considered and the system is limited to maximizing the expected probability of satisfying logic objectives in the context fo a robot arm stacking boxes. \nThere has also been work on ``reward machines'' that uses Linear Temporal Logic to represent and reason with declarative knowledge, especially temporal constraints implied by phrases such as ``until'' and ``eventually,'' \nin order to automatically generate additional rewards for RL that are potentially non-Markovian~. \n\\smallskip\n\\noindent\n\\textbf{Guiding SDM-based Exploration} \nWhen the main objective of SDM is exploration or discovery of particular aspects of the domain, RDK can be used to inform and guide the trade-off between exploration and exploitation, and to avoid poor-quality exploration behaviors in SDM. For instance, the DARLING algorithm uses RL to explore and compute action sequences that lead to long-term goals under uncertainty, with RDK being used to filter out unreasonable actions from exploration~; this approach has been evaluated on real robots navigating office environments to locate people of interest. \nAn algorithm called GDQ uses action knowledge to generate artificial, ``oppotimistic'' experience to give RL agents a warm-up learning experience before letting them interact with the real world~. \nAnother similar approach uses RDK to guide an agent's exploration behavior (formulated as SDM) in non-stationary environments~, and to learn constraints that prevent risky behaviors in video games~. There is also work on non-monotonic logical reasoning with commonsense knowledge to automatically determine the state space for relational RL-based exploration of previously unknown action capabilities~.", "cites": [6788, 9075, 6790, 6782, 6786, 9074, 6787, 9073, 6783, 6785, 6789, 6784], "cite_extract_rate": 0.3, "origin_cites_number": 40, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured synthesis of several RDK-for-SDM methods, particularly focusing on reasoning-centric approaches. It connects different papers by identifying common themes like belief state representation, integration of declarative knowledge, and the role of abstraction. However, the critical analysis is limited, mostly describing methods without evaluating their strengths or weaknesses. The section offers some abstraction by highlighting general design patterns like layered architectures and dynamic factorization, but it falls short of delivering a novel, unifying framework."}}
{"id": "1c6663a0-433e-4260-bb07-2f2a2427f345", "title": "Knowledge Acquisition-focused Systems", "level": "subsection", "subsections": [], "parent_id": "ef4e809f-85f5-4bb7-a4f2-933ad534e3b9", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "RDK-for-SDM Methods"], ["subsection", "Knowledge Acquisition-focused Systems"]], "content": "\\label{sec:rdkforsdm-learn}\nNext, we discuss some RDK-for-SDM systems whose main contribution is the acquisition (and revision) of domain knowledge used for RDK. This knowledge can be obtained through manual encoding and/or automated acquisition from different sources (Web, corpora, sensor inputs). \n\\smallskip\n\\noindent\n\\textbf{Knowledge Acquisition while Acting} Some RDK-for-SDM systems allow the agent to acquire knowledge while also simultaneously reasoning and executing actions in dynamic domains. Such systems can often support online and offline knowledge acquisition, with active and reactive aspects. For example, ASP-based non-monotonic logical reasoning has been used to guide relational RL (i.e., SDM) and decision-tree induction in order to learn previously unknown actions and domain axioms; this knowledge is subsequently used for RDK~. This system supports reactive knowledge acquisition, with reasoning used to trigger and guide learning only when some unexpected outcomes are observed (e.g., to acquire knowledge of previously unknown constraints), as well as active, online knowledge acquisition, with the robot acquiring previously unknown knowledge based on explicit exploration (e.g., of the potential effects of new actions). \n\\smallskip\n\\noindent\n\\textbf{Knowledge Acquisition from Experience} There is a well established literature of RDK-for-SDM systems, including many described above, acquiring or revising knowledge of domain dynamics in a  supervised or semi-supervised \\emph{training} phase. The robot could, for instance, be asked to execute different actions and observe the corresponding outcomes in scenarios with known ground truth information~. More recently, some RDK-for-SDM systems have built on recent developments in data-driven methods (e.g., deep learning and RL) to acquire knowledge. For instance, the symbols needed for task planning have been extracted from the replay buffers of multiple trials of deep RL, with similar states (in the replay buffers) being grouped to form the search space for symbolic planning~. In robotics domains, a small number of real-world trials have been used to enable a robot to learn the symbolic representations of the preconditions and effects of a door-opening action~. Knowledge acquisition in these systems is often offline (i.e., batch of data collected from the robot is processed offline to extract knowledge); this acquisition can be achieved by targeted exploration (i.e., active) or reactive. Researchers have also enabled robots to simultaneously acquire latent space symbols and language groundings based on prior demonstration trajectories paired with natural language instructions~; in this case, knowledge acquisition is active and offline, and requires significantly fewer training samples compared to end-to-end systems. In another RDK-for-SDM system, non-monotonic logical reasoning is used to guide deep network learning and active acquisition of previously unknown axioms describing the behavior of these networks~.\n\\smallskip\n\\noindent\n\\textbf{Knowledge Acquisition from Humans, Web, and other sources} For some RDK-for-SDM systems, researchers have developed a dialog-based interactive approach for situated task specification, with the robot learning new actions and their preconditions through verbal instructions~. In a related approach, SDM has been used to manage human-robot dialog, which helps a robot acquire knowledge of synonyms (e.g., ``java\" and ``coffee\") that are used for RDK~. Building on this work, other researchers have developed methods to add new object entities to the declarative knowledge in RDK-for-SDM systems~. In other work, human (verbal) descriptions of observed robot behavior have been used to extract knowledge of previously unknown actions and action effects, which is merged with existing knowledge in the RDK component~. More recent work in the context of a system enabling an agent to respond to a human's questions about its decisions and evolution of beliefs, has also enabled the agent to interactively construct questions to resolve ambiguities in the human's questions~.\nSome researchers have equipped their RDK-for-SDM systems with the ability to acquire domain knowledge using data available on the Web~. Information (to be encoded in first-order logic) about the likely location of paper would, for instance, be found by analyzing the results of a web search for ``kitchen\" and ``office\".", "cites": [6784, 6790], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of RDK-for-SDM systems focused on knowledge acquisition from different sources, including acting, experience, and human/Web input. It integrates basic ideas from cited papers, particularly connecting methods like deep RL and dialog-based knowledge augmentation. However, it lacks deeper critical analysis or a clear framework that elevates the discussion to an analytical level."}}
{"id": "055d07c1-97ec-4b52-8cc6-7c672608e8ae", "title": "Combining Reasoning, Learning, and Control:", "level": "paragraph", "subsections": [], "parent_id": "aa6e32ff-93d8-4eeb-9cd5-5db7acc73d99", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "Challenges and Opportunities"], ["paragraph", "Representational Choices:"], ["paragraph", "Interactive Learning:"], ["paragraph", "Combining Reasoning, Learning, and Control:"]], "content": "As discussed in this paper, many methods that integrate RDK and SDM focus on decision making (or reasoning) tasks. There are also some methods that include a learning component and some that focus on robot control and manipulation tasks. However, robots that sense and interact with the real world often require a system that combines reasoning, learning, and control capabilities~. Similar to the combination of reasoning and learning (as mentioned above), tightly coupling reasoning, learning, and control presents unique advantages and unique open problems in the context of integrated RDK and SDM. \n    For instance, reasoning with predictive models and learning can be used to identify (on demand) and revise the relevant variables in the control laws for the tasks at hand~. At the same time, real world control tasks often require a very different representation of domain attributes, e.g., reasoning to move a manipulator arm may be performed in a discrete, coarser-granularity space of states and actions whereas the actual manipulation tasks being reasoned about need to be performed in a continuous, finer-granularity space. There is thus a need for systems that integrate RDK and SDM, and suitably combine reasoning, learning, and control by carefully exploring the effect of different representational choices and the methods being used for reasoning and learning.", "cites": [6791], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by discussing the integration of reasoning, learning, and control in the context of knowledge-based sequential decision making. It synthesizes the cited paper (TAMP) to highlight the need for combining these components but does not deeply connect multiple sources or present a novel framework. The critical perspective is limited, as it points out representational challenges without evaluating specific methods or their shortcomings. It does generalize to some extent by emphasizing the importance of representational choices and the need for integration."}}
{"id": "92c19f78-3921-42e1-ab15-3829a18db4b1", "title": "Evaluation Measures and Benchmarks:", "level": "paragraph", "subsections": [], "parent_id": "aa6e32ff-93d8-4eeb-9cd5-5db7acc73d99", "prefix_titles": [["title", "A Survey of Knowledge-based Sequential Decision Making under Uncertainty"], ["section", "Challenges and Opportunities"], ["paragraph", "Representational Choices:"], ["paragraph", "Interactive Learning:"], ["paragraph", "Evaluation Measures and Benchmarks:"]], "content": "The complexity of the components of RDK-for-SDM systems, and the connections between of these components, make it rather challenging to isolate and evaluate the impact of the underlying representation, reasoning methods, and learning methods. Often, the observed performance of a particular algorithm (e.g., for planning) is influenced by the design of this algorithm and the connections between this algorithm and other methods in the system. A key direction for further research is the definition of common measures and tasks for the evaluation of such architectures; doing so would provide deeper insights into the development and use of such architectures. The evaluation measures will need to go beyond measuring the accuracy and computational efficiency of individual components (e.g., planning and task completion accuracy, learning rate, execution time) to examine the effects of the links between the components. These measures could, for instance, explore scalability to more complex domains and tasks. Here, complexity could refer to the type and amount of knowledge encoded in the system; the type, duration, and number of operations to be performed by the robot; and the number and duration of interactions between the different components (of the system) required to complete the task. In addition, evaluation could consider qualitative measures of performance, e.g., the ability to complete different tasks, the ability to provide interactive explanations, or the satisfaction of humans interacting with the system. \n    The benchmarks used for evaluation should not be limited to providing datasets or scenarios for evaluating individual algorithms. Similar to the evaluation measures, the benchmarks should instead challenge the robot to explore and use the interplay between the different components of the system being evaluated, e.g., use reasoning to guide knowledge acquisition, and use the learned knowledge to inform reasoning. In this context, many different domains hold promise in terms of being suitable for evaluation of such RDK-for-SDM systems; these include \\textit{games}~, \\textit{interactive dialog}~, \\textit{robot navigation and exploration}~, and \\textit{scene understanding}~.\n\\section*{Acknowledgments}\n\\noindent\nRelated work in the Autonomous Intelligent Robotics (AIR) group at SUNY Binghamton was supported in part by grants from NSF (NRI-1925044), Ford Motor Company (URP Awards), OPPO (Faculty Research Award), and SUNY RF. Related work in the Intelligent Robotics Lab (IRLab) at the University of Birmingham was supported in part by the U.S. Office of Naval Research Science of Autonomy Awards N00014-17-1-2434 and N00014-20-1-2390, the Asian Office of Aerospace Research and Development award FA2386-16-1-4071, and the UK Engineering and Physical Sciences Research Council award EP/S032487/1. The authors thank collaborators on research projects that led to the development of the ideas described in this paper.\n{\n\\bibliographystyle{named}\n\\bibliography{ref}\n}\n\\section*{Autobiographical Sketch and Photograph}\n\\begin{wrapfigure}{rb}{.12\\textwidth}\n\\vspace{-.5em}\n\\begin{center}\n  \\includegraphics[width=.12\\textwidth]{images/zhang.jpg}\n\\end{center}\n\\vspace{-1em}\n\\end{wrapfigure}\n\\textbf{Dr.~Shiqi Zhang} is an Assistant Professor of Computer Science, at the State University of New York (SUNY) at Binghamton (USA). \nHe was a Postdoctoral Fellow at The University of Texas at Austin (USA) from 2014 to 2016, and  received his Ph.D. in Computer Science (2013) from Texas Tech University (USA). Before that, he received his Master's and B.S. from Harbin Institute of Technology in China. Dr. Zhang's research lies at the intersection of artificial intelligence and robotics. \n\\vspace{1em}\n\\begin{wrapfigure}{rb}{.12\\textwidth}\n\\vspace{-1.5em}\n\\begin{center}\n  \\includegraphics[width=.12\\textwidth]{images/mohan.jpg}\n\\end{center}\n\\vspace{-1em}\n\\end{wrapfigure}\n\\noindent\n\\textbf{Dr. Mohan Sridharan} is a Reader in Cognitive Robot Systems in the School of Computer Science at the University of Birmingham (UK). Prior to his current appointment, he held faculty positions at Texas Tech University (USA) and The University of Auckland (NZ). He received his Ph.D. in Electrical and Computer Engineering from The University of Texas at Austin (USA). Dr.~Sridharan's research interests include cognitive systems, knowledge representation and reasoning, machine learning, and computational vision in the context of human-robot and human-agent collaboration. \n\\end{document}", "cites": [9074, 6785, 6784], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the need for better evaluation measures and benchmarks for RDK-for-SDM systems, highlighting the interplay between components and the limitations of current practices. It synthesizes ideas from the cited papers by connecting them to the broader theme of integrating knowledge, reasoning, and learning. While it does not deeply critique specific works or offer a novel framework, it identifies general challenges and suggests promising evaluation domains."}}
