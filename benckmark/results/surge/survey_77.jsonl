{"id": "92ac6101-b828-4042-8708-2e0386e701bb", "title": "Pruning", "level": "section", "subsections": ["2362b1c7-732c-4073-87eb-1e3f045414b0", "56b21370-ac94-4d19-9ed7-985faad42cc8", "49a2e5f4-1aec-4e81-ac60-4671c95948a6", "10a33a73-e952-454b-b5ce-09086a71de83", "80f88ea7-37a5-4100-9fe4-121a616869c9"], "parent_id": "3efda6e0-cf0a-4f26-8cb1-3dde0497c32c", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"]], "content": "\\label{sec:pruning} \nThe first proposed methods for model compression were based on pruning. Pruning can be done in two ways: structured versus unstructured. In unstructured pruning, individual weight connections are removed from a network by setting them to 0. One can prune away weights from a weight matrix depending on various criteria (e.g., prune away low magnitude weights). Such unstructured pruning methods lead to sparse matrices and need special sparse matrix manipulation libraries at inference time. Hence, various structured pruning methods have also been proposed which aim to prune away structures like neurons, weight matrix blocks, attention heads or layers. Fig.~\\ref{fig:pruning} provides a broad overview of various pruning styles. Fig~\\ref{fig:pruning}(B) depicts unstructured pruning. Fig~\\ref{fig:pruning}(C)-(G) shows various structured pruning methods. In this section, we discuss unstructured weight pruning methods in Section~\\ref{subsec:PruningWeights} and structured pruning methods in Sections~\\ref{subsec:PruningNeurons}-\\ref{subsec:PruningHeadsLayers}.\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{pruning}\n    \\caption{Different Types of Pruning: (A) represents no pruning. Filled cells represent pruned entries. (B), (H) and (I) are unstructured pruning methods discussed in Section~\\ref{subsec:PruningWeights}. Remaining are structured pruning methods: (C) is discussed in Section~\\ref{subsec:PruningNeurons}, (D) in Section~\\ref{subsec:PruningBlock}, (E), (F) and (G) in Section~\\ref{subsec:PruningHeadsLayers}.}\n    \\label{fig:pruning}\n\\end{figure*}\nIn pruning, the main idea is to grow a large model and then prune away weights to end up with a much smaller but effective model. This is inspired by the following biological observation. Trillions of synapses are generated in the human brain during the first few months of birth. At one year old, synapse count peaks at 1000 trillion. And then natural pruning begins to occur. A ten year old child has nearly 500 trillion synapses. This `pruning' mechanism removes redundant connections in the brain~. \nOne natural question is should you prune large networks or build small dense networks? Pruning involves extra processing plus sparse matrices need special handling. Can we avoid it by training smaller models? Zhu et al.~ experimented with models of various sizes with/ without pruning of stacked LSTMs models for language modeling, and seq2seq models for Neural Machine Translation (NMT). They found that large-sparse models consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.", "cites": [844], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent overview of pruning methods, distinguishing between unstructured and structured approaches and referencing relevant experimental findings. It integrates the cited paper by Zhu et al. to address a key question about the trade-off between large-sparse and small-dense models. The analysis is somewhat limited, as it does not deeply critique the limitations or assumptions of the works but does offer some comparative insight and generalizes the concept of pruning beyond specific implementations."}}
{"id": "db3c2850-1885-440e-a725-4d53c91cb9a3", "title": "Magnitude Pruning Methods", "level": "subsubsection", "subsections": [], "parent_id": "2362b1c7-732c-4073-87eb-1e3f045414b0", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Weights"], ["subsubsection", "Magnitude Pruning Methods"]], "content": "A more computationally feasible method for pruning connections and relearning weights based solely on the magnitude of the original weights is to simply prune away weights with small magnitudes. The idea was first proposed by Han et al.~. Pruning away low magnitude weights makes the matrix sparse. Sparse matrices can be stored in Compressed Sparse Row/Column (CSR/CSC) formats. Further space can be saved by storing the index difference instead of the absolute position, and encoding this difference using small fixed number of bits. See et al.~ experimented with these pruning methods on encoder-decoder LSTM NMT (neural machine translation) models. They perform magnitude pruning on all weight matrices of a 4-layer LSTM. They find that higher layers, attention and softmax weights are the most important, while lower layers and the embedding weights hold a lot of redundancy. At the lower layers the parameters for the input are most crucial, but at higher layers the parameters for the gates also become important. These methods typically have a target pruning percent as a hyper-parameter and pruning is either performed statically (after training the full model) or dynamically (while training itself). Retraining the sparse pruned network helps in improving accuracy.\nIn a typical encoder-decoder LSTM model, there are these weight classes: source embedding weights, target embedding weights, source layer weights, target layer weights, attention weights and softmax weights. An important consideration related to magnitude pruning is how do we distribute the pruning over these different weight classes of a model, given a target $x$\\% pruning? Three ways suggested by See et al.~ include class-blind, class-uniform and class-distribution. In the class-blind way, we take all parameters, sort them by magnitude and prune the $x$\\% with smallest magnitude, regardless of the weight class. So some classes are pruned proportionally more than others. In the class-uniform way, Within each class, we sort the weights by magnitude and prune the x\\% with smallest magnitude. So all classes have exactly x\\% of their parameters pruned. In the class-distribution scheme, for each class $c$, weights with magnitude less than $\\lambda\\sigma_c$ are pruned. Here, $\\sigma_c$ is the standard deviation of that class and $\\lambda$ is a universal parameter chosen such that in total, $x$\\% of all parameters are pruned. Class-blind pruning is the simplest and adheres to the principle that pruning weights (or equivalently, setting them to zero) is least damaging when those weights are small, regardless of their locations in the architecture. Class-uniform pruning and class-distribution pruning both seek to prune proportionally within each weight class, either absolutely, or relative to the standard deviation of that class. They observe that class-blind pruning outperforms both other schemes.", "cites": [4478], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from the cited paper, particularly the different pruning schemes and their application to encoder-decoder LSTM NMT models. It provides a critical comparison between the class-blind, class-uniform, and class-distribution approaches, noting their relative merits and the observed superiority of class-blind pruning. The content also abstracts the discussion by framing the pruning strategies as generalizable principles, making it insightful and conceptually rich."}}
{"id": "bdf39c62-744f-439a-a336-cc2914c01b9e", "title": "Iterative Magnitude Pruning Methods", "level": "subsubsection", "subsections": [], "parent_id": "2362b1c7-732c-4073-87eb-1e3f045414b0", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Weights"], ["subsubsection", "Iterative Magnitude Pruning Methods"]], "content": "Typically, it has been seen that rather than pruning in one-shot, it is a good idea to prune gradually over epochs. This way of pruning is called iterative or gradual pruning (see Fig.~\\ref{fig:pruning}(H)). For starting proportion $x$\\% and ending proportion $y$\\%,  iterative magnitude pruning procedure prunes $x$\\% of each of the weights, does re-training, and then prunes $(y-x)/T$\\% of the weights every $K$ iterations. $T$ is the number of times, pruning is done. Sometimes, pruning is started after few warmup iterations have already been performed. Magnitude pruning has been seen to be very effective with regularization ($L_1/L_2$) while training. Dropouts also help in effective pruning. \nIn some pruning methods, a weight once pruned cannot be a part of the network in future iterations. On the other hand, other methods do not modify the gradients of a pruned weight in the back-propagation step. In that case, it is possible for the updates of a pruned weight to be larger than the threshold of that layer, and then the weight will be involved in the forward pass again. Also, in every pruning iteration, we could either use a fixed threshold~ or monotonically increase it~. \nIn case of gradual pruning~, where pruning threshold $\\epsilon$ is monotonically increased, $\\epsilon$ is determined as follows in every iteration $i$. Let $f$ be the number of iterations after which $\\epsilon$ is updated. Also, after a few warmup iterations, weights are sorted using absolute values and we pick the weight corresponding to the $90^{th}$ percentile as $q$. Pruning threshold $\\epsilon$ is increased in two stages. In the first stage (which starts at start iteration $s$ and continues until ramp iteration $r$, we use $\\theta$ as the initial slope to prune weights. In the second stage (which starts at ramp iteration $r$ and continues until end iteration $e$), we use $\\phi$ as the ramp slope to change the rate of pruning. Typically, $\\phi$ is set to 1.5$\\theta$ where $\\theta$ is calculated as follows.\n\\begin{eqnarray}\n    \\theta=\\frac{2qf}{2(r-s)+3(e-r)}\n\\end{eqnarray}\nThus, from iteration $s$ to $r$, we set the pruning threshold as follows.\n\\begin{eqnarray}\n\\epsilon=\\frac{\\theta(i-s+1)}{f}    \n\\end{eqnarray}\nFrom iterations $r+1$ to $e$, we set the pruning threshold as follows.\n\\begin{eqnarray}\n\\epsilon=\\frac{\\theta(r-s+1)+\\phi(i-r+1)}{f}\n\\end{eqnarray}\nTypically when pruning, biases are not pruned since they are much fewer in number. Overall, RNN/LSTM model size can be reduced by 90\\% and speed-up is around 2x to 7x using gradual pruning with no deterioration in accuracy. Also, layers closer to input are pruned more aggressively compared to the final layers.\nAnother way of performing iterative pruning is to set a pruning target per iteration~. In this scheme, we start with an initial sparsity value $s_0$. To achieve a final sparsity value of $s_f$ after $n$ pruning steps with pruning frequency $f$, pruning target in iteration $i$ can be computed as follows.\n\\begin{eqnarray}\ns_i=s_f+(s_0-s_f)\\left(1-\\frac{i}{nf}\\right)^3\n\\end{eqnarray}\nThus, the sparsity of the network is gradually increased while allowing the network training steps to recover from any pruning-induced loss in accuracy. We prune the network rapidly in the initial phase when the redundant connections are abundant and gradually reduce the number of weights being pruned each time as there are fewer and fewer weights remaining in the network.\nCheong et al.~ found that iterative pruning leads to poor results when pruning Transformer models like BERT. Guo et al.~ found that there are two problems with pruning especially when done with regularization. \n\\begin{itemize}\n    \\item The larger weights $w_j$ are penalized more heavily than smaller weights $w_i$ in $L_1$ regularization, which violates the original intention of weight pruning, ``removing the unimportant connections''. \n    \\item Direct optimization of a regularization penalty term causes divergence from the original loss function and has negative effect on the effectiveness of gradient-based update.\n\\end{itemize}\nThey propose to perform reweighted $L_1$ minimization where $\\alpha_i>0$ are inversely proportional to magnitude of corresponding weights $|w_i|$. Thus, they solve the following optimization problem\n\\begin{eqnarray}\n    \\min_w f(w)+\\gamma \\sum_i \\alpha_i |w_i|\n\\end{eqnarray}\n\\noindent where $f(w)$ is the original loss function for the network. This optimization is solved using a reweighted proximal pruning (RPP) method (which depends on proximal operators). RPP decouples the goals of high sparsity from minimizing loss, and hence leads to improved accuracy even with high levels of pruning for BERT.", "cites": [4331, 4479, 844], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple papers, including pruning techniques and optimization approaches, to present a coherent overview of iterative magnitude pruning. It critically evaluates the effectiveness and limitations of pruning in different architectures, such as the poor results for Transformers noted by Cheong et al. and the issues with regularization by Guo et al. While it abstracts some general patterns (e.g., pruning strategies and sparsity progression), it focuses more on specific technical insights and solutions, preventing full meta-level generalization."}}
{"id": "8ead8d95-de3b-4322-8928-4d2d5dd4066f", "title": "Iterative Magnitude Pruning and Densification", "level": "subsubsection", "subsections": [], "parent_id": "2362b1c7-732c-4073-87eb-1e3f045414b0", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Weights"], ["subsubsection", "Iterative Magnitude Pruning and Densification"]], "content": "Further, the effectiveness of pruning can be improved by performing pruning and densification~ alternately across multiple iterations (see Fig.~\\ref{fig:pruning}(I)). There are two ways of doing this. In the first method~, in each iteration, either pruning is performed or densification. The sparse training regularizes the model, and the dense training restores the pruned weights, increasing the model capacity without overfitting. Sparsification helps the optimizer escape saddle points, and leads to regularized training which converges to a significantly better minima. In the second method~, in every iteration some dormant weights can reappear in the network while other active ones can get pruned out. A dormant $w\\in W$ is activated iff $|w.grad|$ is larger than the $(100\\alpha)^{th}$ percentile of all elements in $|W.grad|$. A $w\\in W$ is removed iff $|w|$ is smaller than the $(100\\beta)^{th}$ percentile of all elements in $|W|$. $\\alpha$ and $\\beta$ refer to growth ratio, and pruning ratio, respectively.", "cites": [4480], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of iterative magnitude pruning and densification, distinguishing two methods and their operational criteria. It synthesizes the core idea from the cited paper by introducing a structured framework for alternating pruning and densification. While it offers some abstraction by describing general conditions for weight activation and removal, it lacks deeper critical evaluation or comparison with alternative approaches."}}
{"id": "60bed51b-08a1-4c61-9777-2265a58630d5", "title": "Removing Low Importance Neurons", "level": "subsubsection", "subsections": [], "parent_id": "56b21370-ac94-4d19-9ed7-985faad42cc8", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Neurons"], ["subsubsection", "Removing Low Importance Neurons"]], "content": "He et al.~ proposed three node importance functions to determine importance score for neurons. \n\\begin{itemize}\n    \\item Entropy: For a neuron $i$, let $a_i$ ($d_i$) be the \\#instances with node output $> (or\\leq)$ 0.5 for binary classification with a sigmoid activation. Then entropy for node $i$ can be written as follows.\n    \\begin{eqnarray}\n    \\text{Entropy}(i)=\\frac{d_i}{a_i+d_i}\\log_2 \\frac{d_i}{a_i+d_i}+\\frac{a_i}{a_i+d_i}\\log_2 \\frac{a_i}{a_i+d_i}  \n    \\end{eqnarray}\n    The intuition is that if one node's outputs are almost identical on all training data, these outputs do not generate variations to later layers and consequently the node may not be useful. \n    \\item Output-weights Norm (onorm): average $L_1$-norm of the weights of its outgoing links.\n    \\item Input-weights norm (inorm): average $L_1$-norm of the weights of its incoming links.\n\\end{itemize}\nAll the neurons are sorted by their scores and nodes with less importance values are removed. In most cases, onorm has been found to be the best among these importance functions. \nSpecial regularizers have also been proposed to force neurons to push either all incoming or outgoing connection weights towards zero~. Specifically, for handling incoming connections, the following two regularizers are popular.\n\\begin{itemize}\n    \\item $L_2$ norm on weight matrix $W$ defined as follows.\n    \\begin{eqnarray}\n        \\sum_i ||W_{i:}||_2=\\sum_i \\left(\\sum_j W^2_{ij}\\right)^{1/2}\n    \\end{eqnarray}\n    This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero. \n    \\item $L_\\infty$ norm on weight matrix $W$ defined as follows.\n    \\begin{eqnarray}\n        \\sum_i ||W_{i:}||_\\infty=\\sum_i \\max_j |W_{ij}|\n    \\end{eqnarray}\n    This puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s).\n\\end{itemize}\nSimilar regularizers can easily be defined for outgoing connections as well.", "cites": [4481, 4482], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key concepts from the two cited papers, particularly the neuron importance functions and regularization techniques. It provides some critical evaluation by noting that 'onorm has been found to be the best among these importance functions,' but lacks deeper comparative or evaluative analysis. The content abstracts some general principles of neuron pruning and regularization, though not to the level of identifying broader frameworks or meta-patterns."}}
{"id": "e7c0d686-3915-4133-8281-2cebccd6dd52", "title": "Removing Redundant Neurons", "level": "subsubsection", "subsections": [], "parent_id": "56b21370-ac94-4d19-9ed7-985faad42cc8", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Neurons"], ["subsubsection", "Removing Redundant Neurons"]], "content": "Consider a simple network with one hidden layer with $n$ neurons. Thus, the output can be computed as follows.\n\\begin{eqnarray}\nz=a_1 h(W_1^TX)+a_2 h(W_2^TX)+...+a_n h(W_n^TX)\n\\end{eqnarray}\n\\noindent where $a_i$ and $W_i$ indicate weights. In case $W_1==W_2$, $h(w_1^TX)=h(w_2^TX)$. Thus, we can compute output as follows.\n\\begin{eqnarray}\nz=(a_1+a_2) h(W_1^TX)+0. h(W_2^TX)+...+a_n h(W_n^TX)\n\\end{eqnarray} \nIn general, whenever two weight sets ($W_1$, $W_2$) are equal, one of them can effectively be removed. This should be done with a surgery step, i.e., we need to alter the co-efficient $a_1$ to $a_1+a_2$. Of course, for many pairs of weight sets (i.e., neurons), $W_1$ and $W_2$ are not exactly the same. Hence, Srinivas et al.~ proposed this 3 step method for redundant neuron identification and removal. \n\\begin{itemize}\n    \\item Compute saliency $s_{ij}$ for all possible neuron pairs (i, j) as follows.\n    \\begin{eqnarray}\n        s_{ij}=\\langle a_j^2 \\rangle||\\epsilon_{ij}||_2^2\n    \\end{eqnarray} \n    \\noindent where $\\langle a_j^2 \\rangle$ denotes the average of the quantity over all output neurons. Let $S$ be the matrix with all $s_{ij}$ values. \\item Pick the indices $(i',j')$ corresponding to the minimum $s_{ij}$. Delete the $j'$ neuron, and update $a_i'$ as follows.\n    \\begin{eqnarray}\n    a_i'\\leftarrow a_i'+a_j'    \n    \\end{eqnarray}\n    \\item Update $S$ by removing the $j'^{th}$ column and row, and updating the $i'^{th}$ column (to account for the updated $a_i'$).\n\\end{itemize}", "cites": [4483], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the method proposed by Srinivas et al. for identifying and removing redundant neurons, using mathematical expressions to explain the approach. It lacks critical evaluation or comparison with other pruning techniques, and while it attempts to generalize the idea of redundancy, it remains tied to a specific method without broader conceptual abstraction."}}
{"id": "49a2e5f4-1aec-4e81-ac60-4671c95948a6", "title": "Pruning Blocks", "level": "subsection", "subsections": [], "parent_id": "92ac6101-b828-4042-8708-2e0386e701bb", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Blocks"]], "content": "\\label{subsec:PruningBlock}\nIn weight pruning, irregularity of sparse matrices limits the maximum performance and energy efficiency achievable on hardware accelerators. Pruning neurons avoids sparse matrix issues but is limited in term of overall pruning possible. Hence, block based pruning methods were introduced  (see Fig.~\\ref{fig:pruning}(D)).\nBlock-sparse formats store blocks contiguously in memory reducing irregular memory accesses. If the maximum magnitude weight of a block is below the current threshold, we set all the weights in that block to zeros. Similar to iterative weight pruning, block pruning~ can also be done iteratively using a monotonically growing threshold. Any blocks that had been zeroed out are held at zero even after pruning has ended resulting in a sparse model at the end of training. Just like weight pruning (as discussed in Section~\\ref{subsec:PruningWeights}), the start slope $\\theta$ and ramp slope $\\phi$ determine the rate at which the threshold increases. For block pruning, we need to modify the start slope to account for the number of elements in a block ($N_b$). Thus, the start slope for block pruning is typically set as follows.\n\\begin{eqnarray}\n\\theta_b=\\theta\\times\\sqrt[\\leftroot{-2}\\uproot{2}4]{N_b}    \n\\end{eqnarray}\nFurther, to enable effective removal of blocks, Narang et al.~ propose group Lasso regularization method. Group lasso is a type of weight regularization that works on groups of weights and can zero out all the weights in a group. For each block, we add a loss term proportional to the $L_2$ norm of the block. Thus, we optimize for the following.\n\\begin{eqnarray}\n\\min_w f(w)+\\lambda_g \\sum_{g=1}^{G} ||w^{(g)}||_2    \n\\end{eqnarray}\nWhen we combine group lasso with block pruning,  group lasso guides the selection of blocks to prune. Group lasso regularization is applied to coincide with the pruning schedule, i.e., we turn off regularization when the pruning schedule ends. Typically, inducing block sparsity with 4x4 blocks in vanilla RNNs and GRUs works well, compared to larger block sizes. Larger blocks require lower sparsity to maintain similar accuracy. \nUnfortunately, it becomes challenging to maintain the same model accuracy when block sparsity is applied. Also, block sizes (i.e., pruning granularity) are application-sensitive, making it another hyper-parameter to tune. To avoid these problems, Cao et al.~ proposed a new method called Bank-Balanced Sparsity (BBS). BBS splits each weight matrix row into multiple equal-sized banks, and adopts fine-grained pruning to each bank independently to obtain identical sparsity among banks. Each bank has the same number of non-zero values. For example, retaining top two weights in each bank of size 4 implies a sparsity of 50\\%. We apply the BBS pruning method iteratively to a pre-trained network, and fine-tune the network after each pruning iteration to restore the model accuracy. BBS achieves almost the same model accuracy as unstructured sparsity and significantly outperforms block sparsity when pruning weights at the same sparsity level. BBS is also amenable to FPGA (Field Programmable Gate Arrays) acceleration because it inherently provides a balanced matrix partitioning for parallel computing.", "cites": [7826], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from relevant works, particularly connecting block pruning techniques to the challenges of sparse matrix handling on hardware. It also introduces and explains the BBS method critically, highlighting its advantages over block sparsity. The abstraction level is strong, as it generalizes the concept of block sparsity and discusses implications for hardware acceleration and sparsity granularity."}}
{"id": "bc2b6e76-0a87-4b66-8b3c-2bc732d0a821", "title": "Pruning Attention Heads", "level": "subsubsection", "subsections": [], "parent_id": "10a33a73-e952-454b-b5ce-09086a71de83", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Heads and Layers"], ["subsubsection", "Pruning Attention Heads"]], "content": "BERT-base model consists of 12 layers each with 12 attention heads. Similarly, a typical NMT encoder-decoder Transformer with 6 layers each for encoder as well as decoder contains 16 attention heads per layer. Michel et al.~ found that majority of attention heads can be removed without deviating too much from the original score. Surprisingly, in some cases removing an attention head  (see Fig.~\\ref{fig:pruning}(E)) results in an increase in accuracy. When these heads are removed individually, only 8 (out of 96) heads in 6-layer WMT NMT Transformer (16 heads/layer) cause a statistically significant change in performance when they are removed from the model, half of which actually result in a higher BLEU score. For most layers, one head is indeed sufficient at test time, even though the network was trained with 12 (BERT) or 16 (WMT Transformer) attention heads. One can also do iterative pruning of multiple heads (rather than just one at a time) across layers. For iterative pruning, head importance score is defined using the expected sensitivity of the model to the mask variables $\\xi_h$ as follows.\n\\begin{equation}\n    I_h=E_{x\\sim X}\\left|\\frac{\\partial L(x)}{\\partial \\xi_h}\\right|=E_{x\\sim X}\\left|\\text{Att}_h(x)^T\\frac{\\partial L(x)}{\\partial \\text{Att}_h(x)}\\right|\n\\end{equation}\n\\noindent where $X$ is the data distribution, $L(x)$ is the loss on sample $x$, and $Att_h(x)$ is the output of the attention head $h$ for instance $x$, . Intuitively, if $I_h$ has a high value then changing $\\xi_h$ is liable to have a large effect on the model. Hence, in every iteration heads with low $I_h$ values are pruned out. Michel et al.~ observed that pruning up to 20\\% and 40\\% of heads from NMT and BERT models respectively, did not lead to any noticeable negative impact on accuracy.\nVoita et al.~ used two other head importance scores to prune attention heads from the NMT model. The two scoring methods were: (1) Layer-wise relevance propagation (LRP)~. LRP is a method for computing the relative contribution of neurons at one point in a network to neurons at another. (2) ``confidence'' of a head which is computed as the average of its maximum attention weight excluding the end of sentence symbol, where the average is taken over tokens in a set of sentences used for evaluation. For pruning the heads, they propose a method based on stochastic gates and a differentiable relaxation of the $L_0$ penalty. $L_0$ norm equals the number of non-zero components and pushes the model to switch off less important heads. They find that only a small subset of heads are important for translation. On the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.", "cites": [4485, 4484], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key findings from both Michel et al. and Voita et al. on pruning attention heads, connecting their methodologies and results. It offers some critical analysis by explaining the rationale behind different pruning criteria and showing how they impact performance. The abstraction is moderate, as it identifies a general trend that only a subset of attention heads is necessary for performance but does not fully generalize to broader model compression principles."}}
{"id": "c4355812-9f8a-4da4-86ff-8739c39b93e8", "title": "Pruning Layers", "level": "subsubsection", "subsections": [], "parent_id": "10a33a73-e952-454b-b5ce-09086a71de83", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Heads and Layers"], ["subsubsection", "Pruning Layers"]], "content": "Note that dropping attention heads does not reduce runtime as they are usually computed in parallel. While one can prune weights, neurons or attention heads, how can we design a scheme to prune away layers  (see Fig.~\\ref{fig:pruning}(G))? The LayerDrop idea proposed in~ is inspired by DropConnect. DropConnect randomly drops weights while training on a batch. LayerDrop does structured dropout: it drops groups of weights, heads, feed forward network (FFN) matrices, or layers. The layers to be pruned can be decided using one of these ways: \n\\begin{itemize}\n    \\item Every Other: Prune every other layer (with rate $p$), e.g., every $3^{rd}$ layer in a 12-layer BERT model. \n    \\item Search on Validation: Search for a set of layers to be pruned by checking their impact on a validation set. This entails trying various combinations. \n    \\item Data Driven Pruning: Learn the drop rate $p_d$ of each layer in a data driven manner.\n\\end{itemize}\nGiven a target drop rate $p$, we learn an individual drop rate $p_d$ for the layer at depth $d$ such that the average rate over layers is equal to $p$. At inference time, we forward only the fixed top-$k$ highest scoring layers based on the softmax output. Across the three methods, ``Every Other'' strategy works surprisingly well across many tasks and configurations. ``Search on Validation'' and ``Data Driven Pruning'' only offer marginal gains.", "cites": [4486], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of LayerDrop from the cited paper and explains its structured dropout mechanism in relation to other pruning strategies. It provides a critical evaluation by comparing the performance of different methods and noting that 'Every Other' is often sufficient. While it introduces a useful abstraction by categorizing approaches, it does not fully generalize to broader principles or frameworks beyond the specific methods discussed."}}
{"id": "7dc1a143-838c-41ca-90a0-ebe43039cc74", "title": "Pruning General Structures", "level": "subsubsection", "subsections": [], "parent_id": "10a33a73-e952-454b-b5ce-09086a71de83", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Pruning Heads and Layers"], ["subsubsection", "Pruning General Structures"]], "content": "Lastly, Prasanna et al.~ experiment with pruning both the FFN layers  (see Fig.~\\ref{fig:pruning}(F)) as well as attention heads  (see Fig.~\\ref{fig:pruning}(E)) in a BERT network. Just like~, they assign a mask variable to each of these structures. To decide which structures to prune, we look at the expected sensitivity of the model to the mask variables. High sensitivity implies large impact on the model output and hence corresponding structures should be retained. They find that it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and similarly-sized subnetworks sampled from the rest of the model perform worse. \n\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|l|l|l|l|l|l|l|l|}\n    \\hline\nTask&Dataset&Model&Method&Size (Pruned; Orig)&Eval. (Pruned; Orig)&Metric\\\\\n\\hline\n\\hline\nLanguage modeling&Europarl v7 English &2-layer MLP&Prune Neurons~&5.06M; 5.1M&57; 100&Perplexity (L)\\\\\n\\hline\nLanguage modeling&PTB&2-layer LSTM&Iter. Mag.~&6.6M; 66M&80.24; 78.45&Perplexity (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Iter. Mag.~&20M; 66M&78.5; 78.8&Perplexity (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Block Sparsity~&20M; 66M&83; 78.8&Perplexity (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&BBS~&20M; 66M&78.5; 78.8&Perplexity (L)\\\\\n\\hline\nLanguage modeling&Wikitext-103&Transformer&LayerDrop~&22M; 44M&19.5; 18.2&Perplexity (L)\\\\\n\\hline\nLanguage modeling&AFP from English Gigaword&2-layer MLP&Prune Neurons~&5.07M; 5.1M&107; 100&Perplexity (L)\\\\\n\\hline\nLinguistic acceptability&CoLA&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&82.8/76.3; 81.5&Matthews (H)\\\\\n\\hline\nMachine reading comp.&MRPC&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&88.1/83.5; 89.3&Acc (H)\\\\\n\\hline\nMachine reading comp.&MRPC&BERT-base&LayerDrop~&66M; 110M&85.3; 88.9&Acc (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de) &WMT14&Multi-layer LSTM&Mag.~&43M; 216M&20.91; 20.5&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de) &WMT16&4-layer LSTM&Iter. Mag.~&23M; 211M&26.19; 26.77&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de) &WMT16 &Transformer&LayerDrop~&22M; 44M&29; 29&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de) &WMT17&Transformer&Iter. Mag.~&22M; 44M&26.4; 28.09&BLEU (H)\\\\\n\\hline\nParaphrasing&QQP&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&91.2/85.1; 91.2&Acc (H)\\\\\n\\hline\nQuestion answering&SQuAD 1.1&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&90.23/85.3; 90.9&Acc (H)\\\\\n\\hline\nQuestion answering&SQuAD 2.0&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&81.3/75.3; 81.9&Acc (H)\\\\\n\\hline\nSentiment analysis&SST-2&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&92.4/91.3; 93.2&Acc (H)\\\\\n\\hline\nSentiment analysis&SST-2&BERT-base&LayerDrop~&66M; 110M&92.5; 93.5&Acc (H)\\\\\n\\hline\nSpeech recognition&2100 hours English Speech&2 CONV+7 BiRNNs+CTC&Iter. Mag.~&11.1M; 67M&10.59; 10.67&CER (L) on dev\\\\\n\\hline\nSpeech recognition&2100 hours English Speech&2 CONV+7 BiGRUs+CTC&Iter. Mag.~&17.8M; 115M&9.76; 9.55&CER (L) on dev\\\\\n\\hline\nSpeech recognition&2100 hours English speech&2 CONV+7 BiRNNs+CTC&Block Sparsity~&25.8M; 67M&15.66; 15.36&CER (L) on test\\\\\n\\hline\nSpeech recognition&2100 hours English speech&2 CONV+7 BiRNNs+CTC&Block Sparsity+Group Lasso~&12.9M; 67M&15.89; 15.36&CER (L) on test\\\\\n\\hline\nSpeech recognition&2100 hours English speech&2 CONV+3 BiGRUs+CTC&Block Sparsity~&25.6M; 115M&16.23; 15.42&CER (L) on test\\\\\n\\hline\nSpeech recognition&2100 hours English speech&2 CONV+3 BiGRUs+CTC&Block Sparsity+Group Lasso~&10.8M; 115M&16.78; 15.42&CER (L) on test\\\\\n\\hline\nSpeech recognition&AN4&2 CONV+3 HLSTMs+CTC&Grow and Prune~&2.6M; 44.72M&10.37; 8.92&WER (L)\\\\\n\\hline\nSpeech recognition&Switchboard (swb/fsh)&7-layer MLP&Prune Neurons~&12.2M; 32.19M&25.5/28.8; 25.7/28.8&WER (L)\\\\\n\\hline\nSpeech recognition&TIMIT&5-layer MLP&Prune Neurons~&3.5M; 5.76M&20.7; 20.79&PER (L)\\\\\n\\hline\nSpeech recognition&TIMIT&LSTM&Iter. Mag.~&0.32M; 3.2M&23.5; 23.5&PER (L)\\\\\n\\hline\nSpeech recognition&TIMIT&LSTM&Block Sparsity~&0.32M; 3.2M&26.5; 23.5&PER (L)\\\\\n\\hline\nSpeech recognition&TIMIT&LSTM&BBS~&0.32M; 3.2M&23.5; 23.5&PER (L)\\\\\n\\hline\nSpeech recognition&WSJ 92&1 CONV+3 FC+1 BiRNN&DSD~&4.07M; 8.14M&27.9; 27.45&WER (L)\\\\\n\\hline\nSpeech recognition&WSJ 92&2 CONV+7 BiRNNs+CTC&DSD~&33.5M; 67M&10.65; 9.02&WER (L)\\\\\n\\hline\nSpeech recognition&WSJ 93&1 CONV+3 FC+1 BiRNN&DSD~&4.07M; 8.14M&32.99; 31.6&WER (L)\\\\\n\\hline\nSpeech recognition&WSJ 93&2 CONV+7 BiRNNs+CTC&DSD~&33.5M; 67M&14.84; 13.44&WER (L)\\\\\n\\hline\nSummarization&CNN-Dailymail&Transformer&LayerDrop~&22M; 44M&39; 40&ROUGE (H)\\\\\n\\hline\nTextual entailment&MNLI&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&86.1/77; 86.1&Acc (H)\\\\\n\\hline\nTextual entailment&MNLI-m&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&85.7/82.5; 85.9&Acc (H)\\\\\n\\hline\nTextual entailment&MNLI-m&BERT-base&LayerDrop~&66M; 110M&82.9; 84.6&Acc (H)\\\\\n\\hline\nTextual entailment&QNLI&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&92.3/90.2; 91.3&Acc (H)\\\\\n\\hline\nTextual entailment&QNLI&BERT-base&LayerDrop~&66M; 110M&89.4; 90.5&Acc (H)\\\\\n\\hline\nTextual entailment&RTE&BERT-large&RPP/Iter. Mag.~&138M/170M; 340M&70.1/68.6; 70.1&Acc (H)\\\\\n\\hline\n    \\end{tabular}\n    \\caption{Comparison of various pruning methods (sorted by Task and then Dataset). CONV=Convolution. CTC=Connectionist temporal classification. FC=Fully connected. HLSTM=hidden-layer LSTM~. In the metric column, H means high is better while L means low is better. PER/CER/WER=Phone/Character/Word error rate. For~, embedding weights have not been considered when computing model size in the table. Block sparsity methods use block size of 4x4. BBS uses 64 banks.}\n    \\label{tab:pruningSummary}\n\\end{table}", "cites": [4331, 4479, 4487, 4478, 4486, 844, 4482, 4480, 4485, 7826], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section presents a table comparing various pruning methods across different models and tasks, which provides a basic level of comparison. However, it lacks synthesis by not connecting the findings across papers into a broader narrative and offers minimal critical analysis or abstraction beyond the specific results."}}
{"id": "80f88ea7-37a5-4100-9fe4-121a616869c9", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "92ac6101-b828-4042-8708-2e0386e701bb", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Pruning"], ["subsection", "Summary"]], "content": "Table~\\ref{tab:pruningSummary} compares various pruning methods across different tasks and datasets. Size and accuracy of both the original and the pruned model are shown. The papers report multiple (model size, model accuracy) pairs but we carefully chose the pair such that accuracy is typically within 10\\% of the original or the best pruned model accuracy reported. For language modeling, most popular datasets include PTB, Europarl v7 English, Wikitext-103 and AFP from English Gigaword. On PTB using LSTMs, we observe that Bank-Balanced Sparsity method~ leads to lowest perplexity. For the Linguistic Acceptability CoLA task, RPP~ resulted into a smaller and a more accurate model compared to iterative magnitude pruning. As expected in some senses, the pruned model provides better accuracy than the unpruned one since pruning causes regularization. For the machine reading comprehension, question answering and paraphrasing tasks also, RPP seems to work better than iterative magnitude pruning. For the machine translation (NMT) task, on English-German WMT datasets, pruned Transformer models provide better accuracy than pruned LSTM with comparable number of parameters. For the sentiment analysis task, on SST-2, although RPP leads to a better pruned model compared to iterative pruning, LayerDrop~ improves further on it with a model less than half of the RPP-pruned model. For the speech recognition task, experiments have been reported on 2100 hours English speech data, TIMIT, WSJ, Switchboard and AN4 datasets. On 2100 hours English speech data, Block Sparsity+Group Lasso is better than Block sparsity without regularization. Also, it is better than plain iterative magnitude pruning. On TIMIT, block sparsity~ leads to a more accurate 90\\% pruned LSTM model compared to  the unpruned one. For the summarization task, LayerDrop~ can compress the model to half without any noticeable accuracy change. Finally, for the textual entailment task, experiments have been done on GLUE~ datasets: MNLI, MNLI-m, QNLI and RTE. Models pruned from BERT-large perform better than models pruned from BERT-base; RPP performs better than iterative magnitude pruning.\nWhile older methods~ claimed that Hessian based methods were more effective than magnitude based pruning, almost all recent methods have been based on magnitude based pruning. See et al.~ proposed three pruning schemes. They make the following observations: (1) Class-blind pruning outperforms both other schemes. Further, the overall performance loss is caused disproportionately by a few classes: softmax weights, source and target embedding weights. (2) It seems that higher layers are more important than lower layers, and that attention and softmax weights are crucial in LSTMs. (3) After retraining the pruned NMT models, baseline performance (20.48 BLEU) is both recovered and improved upon, up to 80\\% pruning (20.91 BLEU), with only a small performance loss at 90\\% pruning (20.13 BLEU). (4) In LSTMs, the parameters corresponding to the less common words are more dispensable. Weights connecting to the input are most crucial, followed by the input gate, then the output gate, then the forget gate. This is particularly true of the lower layers, which focus primarily on the input. However for higher layers, especially on the target side, weights connecting to the gates are as important as those connecting to the input. \nNarang et al.~ observe that for approximately same number of parameters, gradual/iterative pruning is 7\\% to 9\\% better than hard pruning. They also conclude that the initial layers are pruned more aggressively compared to the final layers. Zhu et al.~ advise that in order to get the best-performing sparse model of a certain size, we should train a dense model that is 5x-10x larger and then prune to the desired number of parameters rather than taking the largest and best-performing dense model and pruning this model by 20x or more to the desired number of parameters. Guo et al.~ find that RPP is much better than typical iterative pruning. In their experiments with BERT they find that for both original BERT and BERT pruned with RPP, the low-dimensional manifolds of the language representations are similar, showing the similar projection. This implies that the BERT applied with RPP keeps most of the language representation information similar to that from the original BERT.\nFor block pruning, Narang et al.~ make the following observations: (1) We can create block-sparse RNNs with sparsity ranging from 80\\% to 90\\% with small loss in accuracy. This allows us to reduce the model size by roughly 10×. Block sparsity works with a variety of block sizes up to 32×32. (2) For block size 4×4, models with sparsity greater 90\\% yield a relative accuracy loss of 30\\% or higher. Similarly, for blocks of 16×16, models with sparsity greater than 86\\% have 30\\% or more accuracy loss. A similar trend is observed for block size 32×32. This indicates that there is a tradeoff between sparsity, block size and accuracy of the model. (3) For both block pruning and weight pruning, we see that the initial layers are pruned more aggressively compared to the final layers. Increasing sparsity in the layers closer to the output results in poor accuracy. Additionally, the variance in sparsity across the layers increases with the block size. Further, Cao et al.~ make the following observations comparing block sparsity with BBS: (1) BBS achieves almost the same model accuracy regardless of the change of bank size. For block sparsity, however, increasing the block size adversely affects model accuracy. \nFor pruning of attention heads, Michel et al.~ observe that one can prune up to 20\\% and 40\\% of heads from 6-layer NMT Transformer and BERT resp., without incurring any noticeable negative impact. When trying to remove a head at a time, only 8 (out of 96) heads in 6-layer NMT Transformer (16 heads/layer) cause a statistically significant change in performance when they are removed from the model, half of which actually result in a higher BLEU score. Further Voita et al.~ find that on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU. \nOverall, to summarize, pruning has been the most popular method for model compression. Pruning methods can be unstructured (prune weights) or structured (prune neurons, blocks, attention heads, layers). While weight pruning theoretically leads to pruning to a large extent, practical implementation of sparse data structures is difficult. Pruning and regularization need to be done together carefully. Also, it is critical to define the importance functions for various structures carefully. Among weight pruning methods, while iterative magnitude pruning with regularization works well for RNNs and LSTMs, RPP performs better for Transformer based models. Pruning blocks using BBS is better than pruning neurons. For Transformer models, pruning just the heads do not provide much model compression, but dropping a combination of attention heads and layers is better.", "cites": [4331, 4484, 4479, 4478, 4486, 844, 1568, 4485, 7826], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes findings from multiple pruning studies, drawing coherent comparisons across RNNs, LSTMs, and Transformer-based models. It critically analyzes the performance trade-offs of different pruning methods and highlights limitations, such as the difficulty in implementing sparse structures. The section abstracts insights into broader principles, such as the importance of regularization, structured vs. unstructured pruning, and the impact of layer position and block size on model compression effectiveness."}}
{"id": "f998a362-cba7-42b7-aa49-83d293cf4b58", "title": "Deterministic Binarization", "level": "subsubsection", "subsections": [], "parent_id": "11228467-1126-4614-9475-560634f98a9b", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Binarized Networks"], ["subsubsection", "Deterministic Binarization"]], "content": "Simplest way of binary quantization is to set the weight as 1 for non-negative weights, and to -1 for negative weights. This leads to 32x compression. Also, the matrix multiplication for binary matrices is $\\sim$7x faster~ leading to faster model inference. In the forward pass, binary networks drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which leads to great increases of power efficiency. Also, in the simplest version, binarization can be performed in a static manner, i.e., after the training is done. However, this method leads to large loss in accuracy. \nA variant of this simple method is to set the weight to a constant $c_1$ for non-negative weights, and to another constant $c_2$ for negative weights. Binary Scheme (BS)-Fixed method~ stores the original weights and during the forward pass replaces the values with a masked value of $c_1$ or $c_2$, where $c_1$ and $c_2$ are fixed and chosen with hyperparameter tuning. Full precision weights are used during training. At the end of training, the weights are replaced with the index of its masked value. Choosing the values of $c_1$ and $c_2$ can be difficult and time-consuming in BS-Fixed. Thus, in the BS-flexible method~, we initialize $c_1$ and $c_2$ using KMeans with two centroids over the weights, and then update $c_1$ and $c_2$ using back-propagation. Also, in the BS-Flexible method, weights are quantized as follows.\n\\begin{equation}\n    w_b= \\begin{cases}\nc_1 &\\text{if } w \\geq (c_1+c_2)/2\\\\\nc_2 &\\text{if } w < (c_1+c_2)/2\n\\end{cases}\n\\end{equation}\nNote that $w$ is the original weight value while $w_b$ is the binarized weight value. These changes eliminate the need for hyper-parameter tuning.", "cites": [4351], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of deterministic binarization techniques and mentions a few variations (BS-Fixed and BS-Flexible). While it integrates some concepts from the cited paper, it does so in a largely descriptive and surface-level manner. There is minimal critical evaluation or abstraction into broader principles, suggesting limited insight beyond summarizing individual methods."}}
{"id": "3c274eec-1522-4b1c-b25f-1f3d927c3167", "title": "Stochastic Binarization", "level": "subsubsection", "subsections": [], "parent_id": "11228467-1126-4614-9475-560634f98a9b", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Binarized Networks"], ["subsubsection", "Stochastic Binarization"]], "content": "Stochastic~ binarization is performed as follows. \n\\begin{equation}\n    w_b= \\begin{cases}\n+1 &\\text{with probability } p=\\sigma(w)\\\\\n-1 &\\text{with probability } 1-p\n\\end{cases}\n\\end{equation}\n\\noindent where\n\\begin{eqnarray}\n    \\sigma(w)=\\text{clip}\\left(\\frac{w+1}{2},0,1\\right)=\\max\\left(0, \\min\\left(1, \\frac{w+1}{2}\\right)\\right)\n\\end{eqnarray}\nWe only binarize the weights during the forward and backward propagations but not during the parameter update. Keeping good precision weights during the updates is necessary for Stochastic Gradient Descent (SGD). This is possible using something called as ``Straight Through Estimator (STE) trick''~. As per STE, as the quantized value is an approximation of the original value, we can substitute the gradient with respect to the quantized value for the gradient of original value. The trick allows the inclusion of quantization into the computation graph of back-propagation and allows QNNs to represent parameters, activations and gradients with low bitwidth numbers. For test-time inference, there are three options using such a quantization method: \n\\begin{itemize}\n    \\item Use the resulting binary weights $w_b$ (this makes most sense with the deterministic binarization). \n    \\item In the stochastic case, many different networks can be sampled by sampling a $w_b$ for each weight. The ensemble output of these networks can then be obtained by averaging the outputs from individual networks. \n    \\item Use original weights. But this does not reduce model size. \n\\end{itemize}\nBesides this, there have been further  efforts that make train/test faster but do not reduce model size. For example, Lin et al.~ convert multiplications in the backward pass into bit-shifts by restricting the activations to be power-of-two integers. Hubara et al.~ binarize weights and activations, at the inference phase and the entire training phase of a deep network.", "cites": [685, 4488, 8375, 2670], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual explanation of stochastic binarization and its implementation using the Straight Through Estimator (STE) trick. It integrates a few key concepts from the cited papers but does so in a straightforward manner without offering a novel synthesis or deeper analytical insight. Limited critical evaluation or abstraction beyond specific methods is present."}}
{"id": "bb389108-4900-443c-bee1-d687f6265cf4", "title": "Loss Aware Binarization (LAB)", "level": "subsubsection", "subsections": [], "parent_id": "11228467-1126-4614-9475-560634f98a9b", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Binarized Networks"], ["subsubsection", "Loss Aware Binarization (LAB)"]], "content": "\\label{subsubsec:lossAwareBin}\nThe na\\\"ive binary quantization methods divide the real number line into two parts and each part was mapped to a quantized weight value. Can we decide per weight value which of the two weights it should be quantized to? Thus the idea behind Binary Weight Networks (BWN)~ is to approximate the weight vector $W\\in R^n$ using a binary vector $B\\in\\{+1,-1\\}^n$ and a scaling factor $\\alpha\\in R^+$ such that $W\\approx\\alpha B$. This can be expressed as an optimization problem as follows.\n\\begin{eqnarray}\n    \\alpha^*, B^*=\\argmin_{\\alpha, B}||W-\\alpha B||^2\n\\end{eqnarray}\nWe can expand and write this as follows.\n\\begin{eqnarray}\n    ||W-\\alpha B||^2=\\alpha^2B^TB-2\\alpha W^TB+W^TW\n\\end{eqnarray}\nSince $B\\in{+1,-1}^n$, $B^TB=n$. Also $W^TW$ is a constant. Thus $B^*=\\argmax_B W^B$ such that $B\\in{+1,-1}^n$. This optimization can be solved by simply assigning $B_i=+1$ when $W_i\\geq 0$, and $B_i=-1$ otherwise. To compute $\\alpha^*$, we set the derivative of $||W-\\alpha B||^2$ wrt $\\alpha$ to 0 and get the solution as follows.\n\\begin{eqnarray}\n    \\alpha^*=\\frac{\\sum|W_i|}{n}\n\\end{eqnarray}\nThus, besides the binarized weight matrix, a scaling parameter is also learned in BWN.\nTo take this idea further, can we learn $\\alpha$ and $B$ to minimize the overall network's loss function? Thus, now, the Weight binarization can be formulated as the following optimization problem.\n\\begin{eqnarray}\n    &&\\min_{\\hat{w}} \\text{loss}(\\hat{w})\\\\\n    &&\\text{such that } \\hat{w}_l=\\alpha_l b_l; \\alpha_l>0; b_l\\in\\{+1,-1\\}^{n_l}; l=1,..., L\n\\end{eqnarray}\n\\noindent where $L$ is the number of layers, $n_l$ is the number of weights in layer $l$. This loss aware binarization~ problem can be solved using proximal Newton algorithm~ to find the best $\\alpha_l$ and $B_l$.", "cites": [4489, 4490, 851], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of the Loss Aware Binarization (LAB) approach, building on foundational ideas from binary weight networks and connecting them to the broader goal of minimizing the overall loss function. It synthesizes concepts from the cited works but does not deeply compare or critique them. The abstraction level is moderate, as it generalizes the binarization idea but stops short of offering overarching principles across methods."}}
{"id": "60a24ca4-58df-4063-9d44-f538e8a26b98", "title": "Ternarized Networks", "level": "subsection", "subsections": ["8b4d9559-79ce-43e9-8312-ccc718a576a7", "05681eea-3f45-4e85-b114-d5a86ce4f547", "3a480858-aec9-47ce-85a6-135a6bfa320a"], "parent_id": "c0c0e7e6-55dc-426b-8669-a8adfb8fe243", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Ternarized Networks"]], "content": "\\label{subsec:ternaryQuant}\nUnfortunately, binary quantization of the recurrent weights in RNNs/LSTMs never worked~. When the true value of a weight is near zero, its quantized value is either set to -1 or 1. This results into an artificial increase in the magnitude of the weights and the vanishing/exploding gradients problem becomes more severe. Hence, another popular form of quantization is ternary quantization  (see Fig.~\\ref{fig:quantization}(B)). Ternary quantization can help achieve a min of 16x compression (up to 32x compression if hardware allows to avoid storing zeros). In this section, we discuss different variants of ternary quantization from the simplest ternary connect networks to hybrid ternary networks like HitNets.", "cites": [4491], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of ternary quantization, mentioning its advantages and addressing the limitations of binary quantization for RNNs/LSTMs as discussed in the cited paper. However, it lacks deeper synthesis, critical evaluation, or abstraction beyond the immediate context of the cited work, and does not explore broader implications or compare multiple approaches in detail."}}
{"id": "8b4d9559-79ce-43e9-8312-ccc718a576a7", "title": "Ternary Weight Networks", "level": "subsubsection", "subsections": [], "parent_id": "60a24ca4-58df-4063-9d44-f538e8a26b98", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Ternarized Networks"], ["subsubsection", "Ternary Weight Networks"]], "content": "The simplest method for ternary quantization is ternary connect~ whose deterministic form is as follows.\n\\begin{equation}\n    w_t= \\begin{cases}\n+1 &\\text{if } w > 0.5\\\\\n0 &\\text{if } -0.5 < w \\leq 0.5\\\\\n-1 &\\text{if } w \\leq -0.5\\\\\n\\end{cases}\n\\end{equation}\nNote that $w$ is the original weight value while $w_t$ is the ternarized weight value. Like binary connect, ternary connect also eliminates all multiplications in the forward pass. In the stochastic form, assuming original weights have been normalized to be in the range [-1,1], ternary quantization is done as follows.\n\\begin{equation}\n    w_t= \\begin{cases}\n+1 &\\text{with prob } w \\text{ if } w\\in (0,1]\\\\\n0 &\\text{with prob } 1-w \\text{ if } w\\in (0,1]\\\\\n0 &\\text{with prob } 1+w \\text{ if } w\\in[-1,0]\\\\\n-1 &\\text{with prob } -w \\text{ if } w \\in [-1,0]\\\\\n\\end{cases}\n\\end{equation}\nA slightly related way called as Bernoulli Ternary Quantization where $w_t$ is set to +1 (or -1) with prob $p$ if $w>0$ (or) $<0$, and set to 0 with prob 1-p where $p\\sim$Bernoulli($|x|$). Yet another way to set the boundaries for the three ranges is to use Gaussian based ternary weights~ as follows.\n\\begin{equation}\n    w_t= \\begin{cases}\n+1 &\\text{if } w > -(\\mu+\\sigma/2)\\\\\n0 &\\text{if } -(\\mu+\\sigma/2)<w\\leq (\\mu+\\sigma/2)\\\\\n-1 &\\text{if } w \\leq -(\\mu+\\sigma/2)\\\\\n\\end{cases}\n\\end{equation}\n\\noindent where $\\mu$ and $\\sigma$ are the mean and standard deviation of the weight matrix being quantized.", "cites": [4492, 4488], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of ternary weight networks, outlining specific quantization methods such as Ternary Connect and Bernoulli Ternary Quantization. It integrates basic definitions and equations from the cited papers but does not critically evaluate or compare these methods. There is minimal abstraction or generalization to broader principles in model compression."}}
{"id": "05681eea-3f45-4e85-b114-d5a86ce4f547", "title": "Trained Ternary Quantization", "level": "subsubsection", "subsections": [], "parent_id": "60a24ca4-58df-4063-9d44-f538e8a26b98", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Ternarized Networks"], ["subsubsection", "Trained Ternary Quantization"]], "content": "Rather than using the rules for ternary quantization as mentioned above, one can learn the boundary ranges or the quantized values for individual weights. One way of learning the right ternary representation per weight value is to minimize the Euclidean distance between full precision weights $W$ and the ternary weights $T$ along with a scaling factor~. This can be expressed as the following optimization problem.\n\\begin{eqnarray}\n\\alpha^*, T^*=\\argmin_{\\alpha,T}||W-\\alpha T||_2^2\\\\\n\\text{such that } \\alpha\\geq 0; T_i\\in \\{-1,0,1\\}; i=1,2,..., n\n\\end{eqnarray}\nNote that this is equivalent to the BWN method~. This does not lead to a closed form solution. Hence, we approximate the solution with threshold-based ternary function. \n\\begin{equation}\n    w_t= \\begin{cases}\n+1 &\\text{if } w > \\Delta\\\\\n0 &\\text{if } -\\Delta < w \\leq \\Delta\\\\\n-1 &\\text{if } w \\leq -\\Delta\\\\\n\\end{cases}\n\\label{eq:eq2}\n\\end{equation}\nThe approximation works when we set $\\Delta$ as follows.\n\\begin{eqnarray}\n\\Delta^*=\\argmax_{\\Delta>0}\\frac{1}{|I_\\Delta|}\\left(\\sum_{i\\in I_\\Delta}|W_i|\\right)^2\n\\end{eqnarray}\n\\noindent where $I_\\Delta$ is the number of weights with magnitude$>\\Delta$. Again, this has no straightforward solution, unless we assume that original weights $W_i$'s are generated from uniform or normal distribution. When $W_i$'s are uniformly distributed in $[-a, a]$ and $\\Delta$ lies in $(0, a]$, the approximated $\\Delta^*$ is $a/3$, which equals to $\\frac{2}{3}E(W)$. When $W_i$'s are generated from normal distributions $N(0,\\sigma^2)$, the approximated $\\Delta^*$ is 0.6$\\sigma$ which equals to 0.75$E(|W|)$. Thus, we can use the following rule of thumb for fast and easy computation.\n\\begin{eqnarray}\n\\Delta^*\\approx 0.7 E(W)=\\frac{0.7}{n}\\sum_{i=1}^n |W_i|\n\\end{eqnarray}\nAnother way to learn the quantization step size $\\Delta$ in Eq.~\\ref{eq:eq2} is to learn in a loss-aware manner~, i.e., tuning it to minimize the overall network loss. Given a multi-layered network, we need to perform such quantization layer by layer in a greedy manner. We first train the network with full precision weights. We quantize all input data and signals of hidden layers. Next, we start with the weight quantizer between the input layer and the first hidden layer, try several step sizes around the initial step size and measure the output error of the network with the training set. The initial step size is determined using Lloyd-Max algorithm~. Choose the step size that minimizes the output error and quantize the weights. Further, we perform these steps for the next few layers until the output layer. Finally, the quantized neural network is retrained.\nYet another way of training ternary quantization~ is to quantize weights to one of {$-W_l^n$, 0, $W_l^p$} for each layer $l$, where $W_l^n$ and $W_l^p$ are trainable parameters, learned using back-propagation. First, we normalize the full-precision weights to the range [-1, +1] by dividing each weight by the maximum weight. During SGD, we back propagate the gradient to both $W_l^n$ and $W_l^p$ and to the latent full-precision weights. This makes it possible to adjust the ternary assignment (i.e. which of the three values a weight is assigned). To decide the quantization step size $\\Delta_l$ for a layer $l$, two heuristics can be used: (1) set $\\Delta_l=t\\times \\max(|w_l|)$ where $t$ is a constant and $w_l$ are the full precision weights in layer $l$. (2) maintain a constant sparsity $r$ for all layers throughout training. By adjusting the hyper-parameter $r$ we can obtain ternary weight networks with various sparsities.", "cites": [852, 4493, 851], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers, particularly Paper 1 and Paper 2, to explain the concept of Trained Ternary Quantization. It integrates mathematical formulations and describes different approaches, such as threshold-based approximation and loss-aware learning. While it provides some critical context by explaining why certain approximations are used and the conditions under which they are valid, it does not deeply evaluate or contrast the limitations of the methods. The abstraction is moderate, as it generalizes the process across layers and introduces a rule of thumb for computation."}}
{"id": "f96f7375-a19b-4b18-9a52-99c14e0b21ea", "title": "Uniform Quantization", "level": "subsubsection", "subsections": [], "parent_id": "be929a80-0b3b-4005-bf19-e84fee8ab111", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "General Quantized Networks"], ["subsubsection", "Uniform Quantization"]], "content": "Uniform $k$-bit Quantization simply splits the range of original weights into $2^k-1$ equal size intervals~. Refer Fig.~\\ref{fig:quantization}(C).\nIf original weights are in range [-1,1], they can be quantized as follows.\n\\begin{equation}\n    q_k(x)=2\\left(\\frac{\\text{round}[(2^k-1)\\left(\\frac{x+1}{2}\\right)]}{2^k-1}-\\frac{1}{2}\\right)\n    \\label{eq:uq}\n\\end{equation}\nSimilarly, if entries are in range [0,1], we could use the following formula.\n\\begin{eqnarray}\n    q_k(x)=\\frac{1}{2^k-1}\\lfloor(2^k-1)x+\\frac{1}{2}\\rfloor\n\\end{eqnarray}\nWhen the weights in matrix $X$ are not in the range [0,1], we can first scale weights as follows.\n\\begin{eqnarray}\n   \\tilde{X}=\\frac{X-\\beta}{\\alpha}\n\\end{eqnarray}\n\\noindent where $\\alpha=\\max(X)-\\min(X)$ and $\\beta=\\min(X)$. After quantization, we can apply a reverse transform to approximate the original values. Overall, the quantized result can be written as follows.\n\\begin{eqnarray}\n   Q(X)=\\alpha q_k(\\tilde{X})+\\beta\n\\end{eqnarray}\nGiven any quantization function $q_k(x)$, one can use it for quantizing weight matrices of various recurrent models like RNNs, GRUs and LSTMs~. Typical inference equations for a GRU can be written as follows.\n\\begin{eqnarray}\n    z_t=\\sigma(W_z.[h_{t-1},x_t]); r_t=\\sigma(W_r.[h_{t-1},x_t])\\\\\n    \\tilde{h_t}=\\text{tanh}(W.[r_t\\times h_{t-1},x_t]); h_t=(1-z_t)h_{t-1}+z_t \\tilde{h_t}\n\\end{eqnarray}\nBesides the matrix multiplications needed to compute $z_t$, $r_t$ and $\\tilde{h_t}$, the gate structure of $\\tilde{h_t}$ and $h_t$ brings in the need for element-wise multiplication. \nAs $\\tilde{h_t}$ and $h_t$ are also the inputs to computations at the next timestamp, and noting that a quantized value multiplied by a quantized value will have a larger bit-width, we need to insert additional quantization steps after element-wise multiplications.\nAnother problem with quantization of GRU structure lies in the different value range of gates. The range of tanh is [-1, 1], which is different from the value range [0, 1] of $z_t$ and $r_t$. Keeping in mind these observations, the equations for a quantized GRU can be written as follows, after the weights $W_z$, $W_r$ and $W$ and input $x_t$ have already been quantized to [-1,1].\n\\begin{eqnarray}\n    z_t&=&\\sigma(W_z.[h_{t-1},x_t])\\\\\n    r_t&=&\\sigma(W_r.[h_{t-1},x_t])\\\\\n    \\tilde{h_t}&=&tanh\\left(W.\\left[2q_k\\left(\\frac{1}{2}(r_t h_{t-1})+\\frac{1}{2}\\right)-1,x_t\\right]\\right)\\\\\n    h_t&=&2q_k\\left(\\frac{1}{2}((1-z_t) h_{t-1}+z_t  \\tilde{h_t})+\\frac{1}{2}\\right)-1\n\\end{eqnarray}\nFollowing a similar method, we can also quantize LSTM networks.", "cites": [4491, 8797, 4351, 851], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts from multiple papers on uniform quantization, particularly focusing on the implications for recurrent models such as GRUs and LSTMs. It integrates mathematical formulations from the cited works and explains necessary modifications for quantization in these models. While it does offer some analytical depth, especially in identifying challenges like value range mismatches and bit-width increases, it lacks deeper comparative analysis or a broader theoretical framework to elevate the abstraction level."}}
{"id": "6c78c9b7-5b2e-4d63-90b9-bd800235109c", "title": "Balanced Quantization", "level": "subsubsection", "subsections": [], "parent_id": "be929a80-0b3b-4005-bf19-e84fee8ab111", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "General Quantized Networks"], ["subsubsection", "Balanced Quantization"]], "content": "Uniform quantization is easy to implement but far from optimum when quantizing non-uniform data, which is believed to be the trained weights and activations of deep neural network. One way of performing non-uniform quantization is exponential quantization~. It quantizes the weight values to an integer power of 2. If we let \n\\begin{eqnarray}\n    p=\\frac{|W|}{2^{\\lfloor \\log_2|W|\\rfloor}}-1\n\\end{eqnarray}\ndeterministic exponential quantization can be written as follows.\n\\begin{equation}\n    \\log_2 W_q= \\begin{cases}\n\\lceil \\log_2|W| \\rceil &\\text{if } p>0.5\\\\\n\\lfloor \\log_2|W| \\rfloor  &\\text{otherwise }\n\\end{cases}\n\\end{equation}\nSimilarly, stochastic exponential quantization can be written as follows.\n\\begin{equation}\n    \\log_2 W_q= \\begin{cases}\n\\lceil \\log_2|W| \\rceil &\\text{with prob } p\\\\\n\\lfloor \\log_2|W| \\rfloor  &\\text{with prob } 1-p\n\\end{cases}\n\\end{equation}\nExponential quantization enables storing weights in low precision and eliminating multiplications. However, it still does not perform quantization in a way which is sensitive to the distribution of the weights. Distributions of parameters in neural networks are often imbalanced, such that the uniform quantization determined from extremal values may under utilize available bitwidth. When we quantize values, it may be desirable to make the quantized values have balanced distributions, to take full advantage of the available parameter space. Balanced quantization method~ starts by partitioning numbers into $2^{k}$ bins containing roughly the same number of entries (percentiles).  Refer Fig.~\\ref{fig:quantization}(D). Each partition is then mapped to an evenly-divided interval in the closed interval [0, 1]. Finally, the quantization step maps intervals into discrete values using Eq.~\\ref{eq:uq} and transforms the value range to be approximately the same as input.\nA na\\\"ive implementation using percentiles as thresholds would require sorting of weight values during each forward operation in back-propagation, which may slow down the training process. The $2^{k}$ evenly spaced percentiles required in histogram equalization can be computed from the recursive application of partitioning of numbers by medians. Further, the mean $\\mu$ can be used to approximate the median $m$. Thus, we can perform approximate histogram equalization without doing sorting.", "cites": [4494, 4491], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from the cited papers by explaining the motivation for balanced quantization and contrasting it with uniform and exponential quantization. It provides a critical evaluation of uniform quantization's limitations, particularly in handling imbalanced weight distributions. While it abstracts some general principles (e.g., the need for distribution-aware quantization), the analysis remains somewhat focused on methodological details rather than offering broader conceptual insights."}}
{"id": "4b1349e1-295c-4789-9acd-ec92a1cf9f02", "title": "KMeans based Quantization Schemes", "level": "subsubsection", "subsections": [], "parent_id": "be929a80-0b3b-4005-bf19-e84fee8ab111", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "General Quantized Networks"], ["subsubsection", "KMeans based Quantization Schemes"]], "content": "Yet another way of performing non-uniform quantization is to decide bin boundaries using clustering in a static manner. In this static-KMeans method~, We first train the neural network with full-precision parameters. Then apply KMeans to the weights. After clustering, the value of each pixel is set to the value of the center of the cluster it belongs to. We also need to store mapping from integers to cluster centers. Given $k$ clusters, we only need $\\log (k)$ bits to code the clusters.\nA better approach is to perform KMeans clustering during training. In this method~, multiple connections (belonging to the same cluster) share the same weight, and we fine-tune those shared weights. For the forward pass, the cluster index stored for each connection is mapped to a centroid which is then used as the weight. For back-propagation, during update, all the gradients are grouped by the cluster index and summed together, multiplied by the learning rate and subtracted from the shared centroids from last iteration. We use KMeans clustering to identify the shared weights for each layer of a trained network, so that all the weights that fall into the same cluster will share the same weight. Weights are not shared across layers. To calculate the compression rate, given $k$ clusters, we only need $\\log_2 k$ bits to encode the index. In general, for a network with $n$ connections and each connection is represented with $b$ bits, constraining the connections to have only $k$ shared weights will result in a compression rate of\n\\begin{eqnarray}\n r=\\frac{nb}{n\\log_2 k+kb}   \n\\end{eqnarray}\nThere are two other ways of using KMeans for non-uniform quantization: Product Quantization (PQ) and Residual Quantization (RQ)~. In product quantization (PQ), we partition the vector space into many disjoint subspaces, and perform quantization (KMeans) in each subspace. Weight matrix $W$ is partitioned columnwise: $W=[W^1, W^2, ..., W^s]$ where $W^i\\in R^{m\\times n/s}$ assuming $n$ is divisible by $s$. Then we perform KMeans on each submatrix $W^i$ to obtain clusters $c^i_1, ..., c^i_k$. Thus, we get $s$ codebooks. The reconstructed matrix is $\\hat{W}=[\\hat{W}^1, \\hat{W}^2, ..., \\hat{W}^s]$ where $\\hat{W}^i_j$ is the closest centroid $c^i_j$. PQ can be applied to either the x-axis or the y-axis of the matrix. We need to store the cluster indexes and codebooks for each subvector. The compression rate for this method can be written as follows.\n\\begin{eqnarray}\n r=\\frac{32mn}{32kn + log_2(k)ms} \n\\end{eqnarray}\nResidual quantization (RQ) is similar. In RQ, we first quantize the vectors into k-centers. Next we find out the residuals for each data point ($w-c$) and perform KMeans on the residuals. Do it recursively $t$ times.\nThen the resultant weight vectors are calculated as $\\hat{W}_z=c^1_j+c^2_j+...+c^t_j$ given we have recursively performed $t$ iterations. We need to store all the codebooks for each iteration, which potentially needs large amount of memory. The compression rate for this method can be written as follows.\n\\begin{eqnarray}\n r=\\frac{m}{tk + log_2(k)tn}   \n\\end{eqnarray}", "cites": [4496, 4495], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of KMeans-based quantization schemes, including static and training-time approaches, as well as variants like Product and Residual Quantization. While it mentions the cited papers, it integrates their ideas minimally and does not clearly attribute specific contributions or connect them to broader trends. There is little critical evaluation or comparison of strengths and weaknesses across methods."}}
{"id": "96bae440-819d-4a68-8df5-216ce70bd5a8", "title": "Loss Aware Quantization (LAQ)", "level": "subsubsection", "subsections": [], "parent_id": "be929a80-0b3b-4005-bf19-e84fee8ab111", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "General Quantized Networks"], ["subsubsection", "Loss Aware Quantization (LAQ)"]], "content": "Generalizing the loss aware binarization approach (Sec.~\\ref{subsubsec:lossAwareBin})~, we can perform $k$-bit quantization~ by attempting to solve the following problem. \n\\begin{eqnarray}\n \\min_{\\{\\alpha_i,b_i\\}_{i=1}^k} \\left|\\left|w-\\sum_{i=1}^k \\alpha_i b_i\\right|\\right|^2   \n\\end{eqnarray}\nwhere $w\\in R^n$ is the original weight vector, $\\alpha_i\\in R$ and $b_i\\in\\{-1,+1\\}^n$ are variables to be learned. This NP-hard problem can be solved using an iterative greedy approximation which sequentially minimizes the residue. In each iteration, first the residue is computed as \n\\begin{eqnarray}\nr_{i-1}=w-\\sum_{j=1}^{i-1}\\alpha_j b_j,\n\\end{eqnarray}\nand then $\\alpha_i$ and $b_i$ are computed as $\\alpha_i=\\frac{1}{n}||r_{i-1}||_1$ and $b_i=\\text{sign}(r_{i-1})$. Further, refined greedy approximation~ extends this to further decrease the quantization error. In the $j^{th}$ iteration after $\\alpha_j$ and $b_j$ have been updated, the method adds one extra step to refine all computed $\\{\\alpha_i\\}_{i=1}^j$ with the least squares solution as follows.\n\\begin{eqnarray}\n[\\alpha_1, ..., \\alpha_j]=((B_j^TB_j)^{-1}B_j^Tw)^T\n\\end{eqnarray}\nwhere $B_j=[b_1,...,b_j]$. Typically refined greedy is more accurate than the greedy approach. In refined greedy approximation, after modification on the computed $\\alpha$'s, $b$'s are no longer optimal while the method keeps all of them fixed. To improve the refined greedy approximation, alternating minimizing $\\alpha$'s and $b$'s becomes a natural choice. Xu et al.~ find that only two alternating cycles is good enough to find high precision quantization. Further, similar to~, for an LSTM, we can combine overall network loss minimization with the multi-bit quantization loss minimization using this bi-level optimization.\n\\begin{eqnarray}\n\\min_{w,\\{\\alpha_i, b_i\\}_{i=1}^k} \\text{LSTM}\\left(\\sum_{i=1}^k\\alpha_i b_i\\right)\\\\\n\\text{such that }\\{\\alpha_i, b_i\\}_{i=1}^k=\\argmin_{\\{\\alpha_i',b_i'\\}_{i=1}^k}\\left|\\left|w-\\sum_{i=1}^k \\alpha_i' b_i'\\right|\\right|^2\n\\end{eqnarray}", "cites": [4498, 4497, 851], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers on quantization methods, particularly linking loss-aware binarization to multi-bit quantization. It abstracts the quantization problem into a mathematical formulation and introduces refined greedy approximation as an improvement. However, critical analysis is limited, as it does not deeply evaluate the trade-offs, limitations, or comparative performance of these methods."}}
{"id": "c55e72b6-a137-4167-a584-9f7c420c0df0", "title": "Quantization for Word Embeddings and Transformers", "level": "subsubsection", "subsections": [], "parent_id": "be929a80-0b3b-4005-bf19-e84fee8ab111", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "General Quantized Networks"], ["subsubsection", "Quantization for Word Embeddings and Transformers"]], "content": "Each word vector is typically represented as a 300--500 dimensional vector, with each parameter being 32 bits. As there are millions of words, word vectors may take up to 3--6 GB of memory/storage. Can we quantize word vectors? We can clearly quantize them after training. But, we could also quantize when learning word embeddings. For example, Lam et al.~ perform 1-bit and 2-bit quantization while performing word2vec~ training using the Continuous Bag of Words (CBOW) method. They observe that quantization while training leads to better results compared to quantization after training. \nCheong et al.~ applied BS-Fixed and BS-Flexible binary quantization to Transformer models. They observed that the Transformer architecture is highly resistant to quantization, and is able to match the original model up to a 4-bit representation. Simple iterative pruning is much worse compared to quantization. Lastly, Shen et al.~ propose mixed-precision quantization for BERT based on the observation that  different encoder layers should use different number of bits for quantization. Layers that exhibit flatter curvature of the loss gradient surface can be quantized to lower bit precision. Thus, they use different number of bits at different levels of granularity: layers, attention heads and groups of neurons. They observe that quantizing embedding layers with 8 bits and other weight matrices with 2--4 bits leads to results comparable with full-precision BERT.", "cites": [2488], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates three distinct studies on quantization, connecting the idea of applying quantization during training (Lam et al.) and to different model components (Cheong et al. and Shen et al.), forming a basic narrative about quantization effectiveness in different contexts. It includes some critical observations, such as the resistance of Transformer models to quantization and the relative performance of quantization versus pruning. The abstraction is moderate, with generalizations about layer-wise quantization and bit allocation, but lacks a more comprehensive theoretical framework or meta-level insight."}}
{"id": "97e94011-bad6-4d07-98cb-26de46e32fe4", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "c0c0e7e6-55dc-426b-8669-a8adfb8fe243", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Quantization"], ["subsection", "Summary"]], "content": "\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|l|l|l|l|l|l|l|}\n    \\hline\nTask&Dataset&Model&Method&Eval&Bits (weights)&Bits (activation)&Metric\\\\\n\\hline\n\\hline\nLanguage modeling&IMDB&GRU&Uniform Q.~&0.882; 0.905&4&4&Acc (H)\\\\\n\\hline\nLanguage modeling&Linux Kernel&LSTM&BinaryConnect~&3.532; 1.329&1&FP&CE (L)\\\\\n\\hline\nLanguage modeling&Linux Kernel&LSTM&Loss Aware B.~&1.305/1.409; 1.329&1&FP/1&CE (L)\\\\\n\\hline\nLanguage modeling&Linux Kernel&LSTM&BNN~&3.624; 1.329&1&1&CE (L)\\\\\n\\hline\nLanguage modeling&Linux Kernel&LSTM&BWN~&1.307; 1.329&1&FP&CE (L)\\\\\n\\hline\nLanguage modeling&PTB&GRU&Uniform Q.~&102; 100&4&4&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&GRU&Balanced Q.~&116; 100&4&4&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Greedy~&118.9; 89.8&2&FP&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Refined Loss Aware~&95.6; 89.8&2&3&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Uniform Q.~&152/114; 109&2/4&2/4&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&BNN~&100; 97&4&4&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&HitNet~&110.3; 97.2&2&2&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Alternating LAQ~&103.1/91.4; 89.8&2/4&FP&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Alternating LAQ~&91.9; 89.8&2&3&PPW (L)\\\\\n\\hline\nLanguage modeling&PTB&LSTM&Balanced Q.~&126/123; 106&2&2/3&PPW (L)\\\\\n\\hline\nLanguage modeling&Text-8&LSTM&Refined Loss Aware~&122.3; 101.1&2&3&PPW (L)\\\\\n\\hline\nLanguage modeling&Text-8&LSTM&HitNet~&169.1; 151.4&2&2&PPW (L)\\\\\n\\hline\nLanguage modeling&Text-8&LSTM&Alternating LAQ~&105.1; 101.1&2&3&PPW (L)\\\\\n\\hline\nLanguage modeling&Text-8&RNN&Exponential Q.~&1.639; 1.588&2&FP&BPC (L)\\\\\n\\hline\nLanguage modeling&War and Peace&LSTM&BinaryConnect~&2.942; 1.268&1&FP&CE (L)\\\\\n\\hline\nLanguage modeling&War and Peace&LSTM&Loss Aware B.~&1.291/1.376; 1.268&1&FP/1&CE (L)\\\\\n\\hline\nLanguage modeling&War and Peace&LSTM&BNN~&3.05; 1.268&1&1&CE (L)\\\\\n\\hline\nLanguage modeling&War and Peace&LSTM&BWN~&1.313; 1.268&1&FP&CE (L)\\\\\n\\hline\nLanguage modeling&Wikidata-2&LSTM&HitNet~&126.72; 114.37&2&2&PPW (L)\\\\\n\\hline\nLanguage modeling&WikiText-2&LSTM&Refined Loss Aware~&105.8; 114.37&2&3&PPW (L)\\\\\n\\hline\nLanguage modeling&WikiText-2&LSTM&Alternating LAQ~&102.7; 100.1&2&3&PPW (L)\\\\\n\\hline\nNamed Entity Recognition&CoNLL-03&BERT-base&QBERT~&91.06; 95&2w8e&8&F1 (H)\\\\\n\\hline\nNamed Entity Recognition&CoNLL-03&BERT-base&Mixed-precision Q.~&94.37; 95&2-3w8e&8&F1 (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT17&Transformer&BS-Fixed/BS-Flexible~&11.61/12.11; 28.09&1&FP&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT17&Transformer&K-Means 1/4-bit Q.~&12.07/27.65; 28.09&1&FP&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT17&Transformer&K-Means 1-bit att-Q.~&24.96; 28.09&1&FP&BLEU (H)\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT17&Transformer&BS-Flexible 1-bit att-Q.~&25.54; 28.09&1&FP&BLEU (H)\\\\\n\\hline\nQuestion answering&SQuAD&BERT-base&QBERT~&79.6; 88.69&2w8e&8&F1 (H)\\\\\n\\hline\nQuestion answering&SQuAD&BERT-base&Mixed-precision Q.~&86.95; 88.69&2-3w8e&8&F1 (H)\\\\\n\\hline\nQuestion answering&SQuAD&Facebook’s DrQA&BS-Fixed~&77.04; 75.28&2&FP&F1 (H)\\\\\n\\hline\nSentiment analysis&IMDB&LSTM&Gaussian Q.~&79.64; 82.87&1&FP&Acc (H)\\\\\n\\hline\nSentiment analysis&IMDB&LSTM&Gaussian T./B.~&76.86/76.25; 82.87&2&FP&Acc (H)\\\\\n\\hline\nSentiment analysis&SST-2&BERT-base&QBERT~&84.63; 93&2w8e&8&Acc (H)\\\\\n\\hline\nSentiment analysis&SST-2&BERT-base&Mixed-precision Q.~&92.08; 93&2-3w8e&8&Acc (H)\\\\\n\\hline\nSpeech recognition&TIDIGITS&GRU&Pow2 T.~&99.18; 99.1&2&FP&Acc (H)\\\\\n\\hline\nSpeech recognition&TIMIT&4-layer MLP&Loss Aware T.~&29.97/28.35; 26.24&1/2&1&FER (L)\\\\\n\\hline\nSpeech recognition&WSJ&4-layer BiLSTM&Pow2 T.~&10.49; 11.16&2&FP&WER (L)\\\\\n\\hline\nTextual entailment&MNLI&BERT-base&QBERT~&77.02; 84.4&2w8e&8&Acc (H)\\\\\n\\hline\nTextual entailment&MNLI&BERT-base&Mixed-precision Q.~&82.29; 84.4&2-3w8e&8&Acc (H)\\\\\n\\hline\nTextual entailment&MNLI-m&BERT-base&QBERT~&76.56; 84&2w8e&8&Acc (H)\\\\\n\\hline\nTextual entailment&MNLI-m&BERT-base&Mixed-precision Q.~&81.75; 84&2-3w8e&8&Acc (H)\\\\\n\\hline\nWord similarity&M. Turk &Word embeddings&BS-Fixed~&0.602; 0.617&2&FP&CHR (H)\\\\\n\\hline\nWord similarity&MEN &Word embeddings&BS-Fixed~&0.764; 0.745&2&FP&CHR (H)\\\\\n\\hline\nWord similarity&Rare Words &Word embeddings&BS-Fixed~&0.362; 0.4&2&FP&CHR (H)\\\\\n\\hline\nWord similarity&SimLex&Word embeddings&BS-Fixed~&0.387; 0.358&2&FP&CHR (H)\\\\\n\\hline\nWord similarity&WordSim Relatedness &Word embeddings&BS-Fixed~&0.594; 0.529&2&FP&CHR (H)\\\\\n\\hline\nWord similarity&WordSim Similarity&Word embeddings&BS-Fixed~&0.752; 0.741&2&FP&CHR (H)\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various quantization methods (sorted by Task and then Dataset). Q.=Quantization, B.=Binarization, T.=Ternarization, PPW=Perplexity per word, BPC=Bits per character, CE=cross-entropy, FER=frame error rate, CHR=correlation with human rankings. FP=full precision (32 bits). For~, we report results with word embedding dimensions set to 1000. In the metric column, H means high is better while L means low is better. For  quantization of the BERT-base model~, we report number of bits used for encoders as well as for embeddings. `2-3w8e' means 2 or 3 bits were used for encoder weights while 8 bits were used for embeddings. For NMT results by Cheong et al.~, ``att-Q'' means only attention layers were quantized.}\n    \\label{tab:quantizationSummary}\n\\end{table}\nOtt et al.~ observed that the weight binarization methods do not work with RNNs. Hubara et al.~ were the first to attempt to quantize both weights and activations by trying to evaluate the accuracy of quantized recurrent models trained on the Penn Treebank dataset. Similar to~, Hubara et al.~ found that binarization of weight matrices lead to large accuracy degradation. Later techniques like the one by Xu et al.~ with 2 bits for weights and 3 bits for activations showed better results.\nTable~\\ref{tab:quantizationSummary} compares various quantization methods across different tasks and datasets. Accuracy of both the original and the quantized model are shown. Also, we report number of bits used for weights (which indicate the model size) as well as activations. For the same task, dataset and model combination, different papers report different accuracy of the full precision model because of slight changes in training hyper-parameters; hence we report accuracy of full precision model for each row. \nFor language modeling, PTB, Text-8, WikiText-2, Linux Kernel, IMDB and ``War and Peace'' are the popular datasets. Across all the datasets, loss aware binarization outperforms other weight binarization schemes. On the Linux Kernel dataset, it is even better than the full-precision network. BinaryConnect does not work well here because of the problem of exploding gradients. On PTB, Xu et al.'s Alternating LAQ~ with 2 bits for weights and 3 bits for activations leads to an LSTM which is just 2.1 points worse in terms of perplexity per word. By 3-bit quantization, Alternating LAQ can achieve $\\sim$10.5x memory saving and $\\sim$3× real inference acceleration. Uniform and Balanced quantization are rule-based and not aim at minimizing the error. Balanced quantization proposed by Zhou et al.~ performs better than HitNet~ and uniform quantization~. Balanced quantization leads to better results compared to unbalanced counterparts, especially when quantizing to 2-bit weights. However, for 4-bit weights, there is no clear gap between scaling by mean and scaling by max (i.e. balanced and unbalanced quantization).\nAcross multiple tasks like named entity recognition with CoNLL-03, question answering with SQuAD,  sentiment analysis with SST-2, textual entailment using MNLI, Shen et al.~ show that mixed precision quantization (where different number of bits are used for different groups of neurons -- 128 groups in each layer) of BERT is better than QBERT. The reason behind this is the discrepancy (in QBERT) that not all the layers have the same sensitivity to quantization. For more sensitive layers, higher bit precision needs to be set, while for layers that are less sensitive, 2-bits setting is already sufficient. With only additional 5MB memory storage, 2/3-bits mixed-precision Q-BERT is able to retain the performance drop within 2.3\\% for MNLI, SQuAD and 1.1\\% for SST-2, CoNLL-03, with up to 13x compression ratio in weights. For machine translation, Cheong et al.~ observe that Transformer architecture is highly resistant to quantization (unlike to pruning), and are, in essence, able to match the original model up to a 4-bit representation. Binary Scheme Flexible outperforms 1-bit k-means in both pure 1-bit compression and quantizing only the attention layers, suggesting not tying the weights to particular centroids improves performance, and outperform Binary Scheme Fixed, indicating learning the values to be a superior method. After binarizing only the attention layers, we are still able to recover 90\\% of the model's performance. Lam et al.~ experimented with quantization of word embeddings and showed that 2-bit quantized word vectors outperform full precision vectors on word similarity tasks, but do worse on word analogy tasks. Intuitively, they reason that full precision Word2Vec is prone to overfitting with increased epochs of training; quantized training does not seem to suffer as much from this. For sentiment analysis on IMDB, Alom et al.~ show that quantization to 4 bits is better than to 3 or 2 bits, which is expected. They also show that the normal distribution shows better performance against uniform distribution with quantized weights. For speech recognition, Ott et al.~ show that pow-2 ternarization is the best. \nCheong et al~ were the first to quantize Transformers. They observed that in the last attention layer of the decoder over the encoder hidden states, the attention distribution of the original and 4-bit model are highly similar, indicating 4 bit weights, i.e weights that take on one of 16 values, is enough to get the full effects of attention. Attention distributions in the encoder layers of the Transformer for the original and 4-bit models are almost indistinguishable from one another. This again highlights the idea that self-attention is highly resistant to quantization and could be heavily compressed. Later Shen et al.~ showed that comparable performance to full precision BERT can be achieved with at most 2.3\\% performance degradation across many tasks, even with ultra-low precision quantization down to 2 bits, corresponding up to 13x compression of the model parameters, and up to 4x compression of the embedding table as well as activations. \nOverall, quantization performs model compression by reducing the number of bits per weight value. Binary quantization does not work well by itself for text based neural models. But ternary and higher-bit quantization lead to significant model size reduction without loss in accuracy across tasks. One consideration for quantization is that 3-bit quantized execution is typically not supported in hardware. It is however possible to load 3-bit quantized values and cast them to higher bit precision such as 4 or 8 bits in the execution units. This would still have the benefit of reduced memory volume to/from DRAM. Non-uniform quantization methods like balanced quantization or KMeans based quantization methods are better than uniform quantization methods. Loss aware quantization done while training is better than static loss-unaware quantization. Mixed-precision quantization combined with pruning is highly effective for Transformer based models.", "cites": [4498, 8797, 4494, 4497, 2670, 4490, 4351, 4491, 4492, 2488, 851], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a comparative overview of quantization methods across various NLP tasks and models using a detailed table and some analysis of trends, such as the effectiveness of loss-aware binarization over uniform approaches. It connects results from different papers to highlight relative performance, which shows moderate synthesis. However, the analysis remains largely descriptive, with limited abstraction or deep critical evaluation of methodological limitations or broader implications."}}
{"id": "5addca6a-ae25-4e99-8ffc-a608a63231c7", "title": "Various Distillation Architectures", "level": "subsection", "subsections": [], "parent_id": "2c5b1b2b-3f74-442d-8ee6-503425cf4207", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Knowledge Distillation (KD)"], ["subsection", "Various Distillation Architectures"]], "content": "Ba and Caruna~ proposed Student Teacher networks (or mimic models) where the student uses the logits before softmax from the teacher network for training (see Fig.~\\ref{fig:kd}(A)). The student model is not trained on the original labels; it is trained to learn the function that was learned by the teacher model. Thus, the student model is optimized to minimize the L2 loss between the teacher logits and the student logits across all training instances. Such distilled student models are more accurate than the same shallow student trained directly on the original labeled training data mainly because: (1) Teacher removes noisy labels, if any. (2) The uncertainty from the teacher is more informative to the student than the original 0/1 labels. (3) The original targets may depend in part on features not available as inputs for learning, but the student sees targets that depend only on the input features. The dependence on unavailable features has been eliminated by filtering targets through the teacher.\nYet another way of utilizing logits is to have the student learn from noisy teacher logits~. After obtaining logits from the teacher, Gaussian noise with mean 0 and standard deviation $\\sigma$ is added to teacher’s logits. This perturbation can be applied to samples selected with probability $\\alpha$. The perturbed outputs produce the effect of a regularizer. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{kd}\n        \\caption{Different Types of Knowledge Distillation methods.}\n    \\label{fig:kd}\n\\end{figure}\nWhile Ba and Caruna~ suggested using only the logits, Hinton et al.~ suggested training the student by minimizing the cross entropy loss between the teacher softmax output and the student softmax output, besides minimizing the cross entropy between student prediction and actual label. The first part is called the soft loss and the second one is called the hard loss. Typically hard loss is given much lower weight compared to the soft loss term. To make the softmax output non-peaked and thereby transfer more useful information from teacher to student, softmax with temperature $>$1 should be used. The same temperature should be used for training both the teacher and the student, but after the student has been trained the temperature can be set to 1 at test time. Besides logits and softmax output, Sobolev training for neural networks is a method for incorporating target derivatives in addition to the target values while training student network  (see Fig.~\\ref{fig:kd}(C)). Czarnecki et al.~ experiment with first two derivatives of the targets.\nKD has also been used along with quantization for better model compression~. We start with a trained full-precision large teacher network and an apprentice (student) network that has been initialised with full-precision weights. The apprentice network's precision is lowered and is fine-tuned using KD. \nWhy just use the output from the last layer of the teacher for training the student? In FitNets~, the student performs hint-based training, i.e., the student is trained using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student  (see Fig.~\\ref{fig:kd}(B)). we choose a hidden layer of the FitNet, the guided layer, to learn from the teacher's hint layer. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. \nWhile the methods discussed so far use logits, softmax output or their derivatives to transfer knowledge, Yim et al.~ proposed a ``flow of solution procedure (FSP)'' method where the distilled knowledge is transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers  (see Fig.~\\ref{fig:kd}(D)). What does this ``flow'' capture intuitively? If we view the input of a deep network as the question and the output as the answer, we can think of the generated features at the middle of the network as the intermediate result in the solution process. There are many ways to solve the problem of generating the output from the input. Hence, mimicking the generated features of the teacher can be a hard constraint for the student. Learning the solution process from teacher is important. More concretely, the student is trained to minimize the L2 difference between the teacher and student FSP matrices computed across various pairs of layers and across multiple training instances. A similar method called Representational distance learning (RDL) has also been proposed in~.\nLastly, multiple KD variants have been proposed for sequence-level predictions~, e.g., for neural machine translation (NMT). In word-level KD, cross-entropy is minimized between the student/teacher distributions for each word in the actual target sequence, as well as between the student distribution and the degenerate data distribution, which has all of its probability mass on one word. In sequence-level KD (Seq-KD) the student network is trained on the output from beam search of the teacher network that had the highest score. In sequence-level interpolation (Seq-Inter) the student is trained on the output from beam search of the teacher network that had the highest similarity (say using BLEU score) with the target sequence.", "cites": [8798, 4499, 4500, 4503, 4504, 681, 8375, 8799, 8390, 4501, 4502], "cite_extract_rate": 0.9166666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing and connecting various KD approaches from multiple papers into a coherent narrative, highlighting distinctions such as logit-based, softmax-based, and sequence-level methods. It provides some critical analysis by noting limitations (e.g., the need to filter out noisy labels via teachers and the regularization effect of perturbed logits), though a more in-depth evaluation of trade-offs would enhance this. The abstraction level is good, as it generalizes across different KD variants and identifies higher-level ideas like the importance of mimicking internal representations or the solution process."}}
{"id": "d2899d20-743a-43e0-84c7-0fc90c1a975b", "title": "Collaborative Learning", "level": "subsection", "subsections": [], "parent_id": "2c5b1b2b-3f74-442d-8ee6-503425cf4207", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Knowledge Distillation (KD)"], ["subsection", "Collaborative Learning"]], "content": "Can multiple students learn from each other? Is a powerful teacher really required? In the deep mutual learning (DML) method~, different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Surprisingly, no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher. Specifically, each student is trained with two losses: a conventional supervised learning loss, and a mimicry loss that aligns each student's class posterior with the class probabilities of other students. \nAnil et al.~ propose a similar method but suggest letting the students learn independently just using the conventional supervised learning (hard) loss at least for a few burn in iterations  (see Fig.~\\ref{fig:kd}(E)). After this, the mutual learning can be done as in DML. They also propose a variant of their Co-Distillation method to perform this training in a distributed scenario where communication efficiency is also important. To update the parameters of one network using co-distillation one only needs the predictions of the other networks, which can be computed locally from copies of the other networks' weights. Empirically, using stale predictions instead of up-to-date predictions for the other neural networks has little to no adverse effect on the quality of the final trained model produced by co-distillation.", "cites": [4506, 4505], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core idea of Deep Mutual Learning from Paper 1 and integrates it with the Co-Distillation approach from Paper 2, showing how collaborative learning can occur without a traditional teacher. It critically notes that mutual learning can outperform static teacher-based distillation and discusses practical considerations like communication efficiency and the use of stale predictions. While it identifies a broader shift from one-way teacher-student learning to peer-based methods, it stops short of a deeper theoretical analysis or meta-level generalization."}}
{"id": "e1a31918-0804-4d5f-9f78-0ea43f83fc23", "title": "Multiple Teachers", "level": "subsection", "subsections": [], "parent_id": "2c5b1b2b-3f74-442d-8ee6-503425cf4207", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Knowledge Distillation (KD)"], ["subsection", "Multiple Teachers"]], "content": "So far we have talked about a student mimicing a single teacher. However, it is interesting to explore if the student can learn better in presence of multiple teachers or from a teacher assistant.\nIntuitively and also observed empirically, student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, Mirzadeh et al.~ introduced multi-step KD, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher  (see Fig.~\\ref{fig:kd}(F)).\nThe teacher assistant (TA) models are distilled from the teacher, and the student is then only distilled from the TAs. One could also perform multi-step TA distillation, for example, distillation path could be $10 \\rightarrow 6  \\rightarrow 4  \\rightarrow 2$.\nA simple way to do KD with multiple teachers is to train student with cross entropy loss between student  predictions and average prediction from multiple teachers  (see Fig.~\\ref{fig:kd}(G)). A more effective method is to augment this with a relative dissimilarity (RD) loss~ defined over intermediate layer outputs generated for a triplet of instances between the student and an ensemble of teachers. For the student, the middle layer is selected. For each teacher, we select the layer such that most teachers are consistent with the resulting order relationships under the voting strategy. We discuss the RD loss given a student and a teacher. Consider a triplet of instances ($x_i$, $x_i^+$, $x_i^-$) such that at an intermediate layer of the teacher network, distance between activations for $x_i^+$ and $x_i$ is smaller than the distance between activations for $x_i^-$ and $x_i$. Let $p_i$ be the intermediate output from student for example $x_i$. Then the RD loss for the triplet ($x_i$, $x_i^+$, $x_i^-$) can be written as follows. \n\\begin{eqnarray}\n\\text{Loss}=\\max(0,d(p_i, p_i^+)-d(p_i, p_i^-)+\\delta)   \n\\end{eqnarray}\nwhere $d$ is the distance function, and $\\delta$ is a small constant to prevent the trivial solution. To extend this loss function definition to multiple teachers, the order between the instances $x_i^+$ and $x_i^-$ given $x_i$ is decided based on majority voting between the teachers. \nThere are also specific settings when distilling from multiple teachers becomes natural, e.g., when the number of classes is large~ or in multi-lingual settings~. When the number of classes is very large, the teacher model could be an ensemble that contains one generalist model trained on all the data and many ``specialist'' models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom). Softmax distribution vector of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class. Each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. To derive groupings of object categories for the specialists, we focus on categories that the full generalist network often confuses. When training the student, for each instance, we first find the $set $k$ of n$ most probable classes according to the generalist model. Then, we take all the specialist models, $m$, whose special subset of confusable classes has a non-empty intersection with $k$ and call this the active set of specialists $A_k$. Given student's full probability distribution $q$ over all the classes, we minimize the following.\n\\begin{eqnarray}\n\\text{Loss}=KL(p^g,q)+\\sum_{m\\in A_k} KL(p^m, q)   \n\\end{eqnarray}\nwhere $p^g$ is output distribution from the generalist model, and $p^m$ is the output distribution from the $m^{th}$ specialist model.\nAn ensemble of teachers is also very useful in a multi-lingual NMT setting~. Individual models for each language pair are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through KD. When the accuracy of multilingual model surpasses the individual model for the accuracy threshold $\\tau$ on a certain language pair, we remove the distillation loss and just train the model with original negative log-likelihood loss for this pair. Lastly, when learning from a teacher ensemble, it is burdensome to load all the teacher models in the GPU memory for distillation. Alternatively, we first generate the output probability distribution of each teacher model for each instance offline, and then just load the top-$K$ probabilities of the distribution into memory and normalize them so that they sum to 1 for distillation. This reduces the memory cost from the scale of $|V|$ (the vocabulary size) to $K$.", "cites": [4508, 4507, 681], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to present a coherent discussion on the use of multiple teachers in knowledge distillation, including concepts from teacher assistants and ensembles. It offers critical analysis by explaining limitations (e.g., performance degradation with large teacher-student gaps) and proposing solutions (e.g., multi-step distillation and RD loss). It abstracts the ideas by generalizing the principles of multi-teacher distillation to different settings like multi-lingual and large-class problems."}}
{"id": "8fabba12-aca9-4230-b6a7-cc461dfdceaa", "title": "Distilling Transformers", "level": "subsection", "subsections": [], "parent_id": "2c5b1b2b-3f74-442d-8ee6-503425cf4207", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Knowledge Distillation (KD)"], ["subsection", "Distilling Transformers"]], "content": "Recently, there has been a lot of work around distilling Transformers to smaller Transformers with less number of layers or to Bidirectional LSTMs. Some of these methods aim at improving the accuracy versus model size tradeoff, while others focus on complex settings like mismatching student-teacher vocabulary~ or mismatch number of attention heads. \nZhao et al.~ learn a student with small vocabulary compared to the teacher using a dual training method. During distillation, for a given training sequence input to the teacher model, they mix the teacher and student vocabularies by randomly selecting tokens from the sequence to segment using the student vocabulary, with the other tokens segmented using the teacher vocabulary. As part of the masked language model (MLM) task, the model now needs to learn to predict words from the student vocabulary using context words segmented using the teacher vocabulary, and vice versa. The expectation is that the student embeddings can be learned effectively this way from the teacher embeddings as well as teacher model parameters. We perform dual training only for the teacher model inputs. The student model receives words segmented exclusively using the student vocabulary. Also, during MLM, the model uses different softmax layers for the teacher and the student vocabularies depending on which one was used to segment the word in question. Instead of distilling solely on the teacher model's final-layer outputs, layer-wise teacher model parameters can also be leveraged to directly optimize parameters of corresponding layers in the student model.\nIn Patient KD (PKD)~, the student learns from the teacher's output after every $k$ layers (see Fig.~\\ref{fig:kd}(H)) or the output from the last few layers of the teacher (see Fig.~\\ref{fig:kd}(I)). The student BERT is initialized using some layers of the pre-trained teacher BERT. TinyBERT~ further extends this idea by using extensive knowledge from embedding layer, and attention and hidden sub-layers of multiple teacher layers, and also the overall teacher output (see Fig.~\\ref{fig:kd}(J)). Each student layer is first mapped to a teacher layer before the student training. Liu et al.~ distill a multi-task student from a multi-task teacher, given the soft targets of the training data across multiple tasks. If task $t$ has a teacher, the task-specific loss is the average of two objective functions, one for the correct targets and the other for the soft targets assigned by the teacher. In MiniLM~, the student is trained by deeply mimicking the self-attention behavior of the last Transformer layer of the teacher (see Fig.~\\ref{fig:kd}(K)). Besides self-attention distributions, MiniLM introduces the self-attention value-relation transfer to help the student achieve a deeper mimicry. The value-relation is computed as pairwise correlation between different components of the value matrix across various attention heads of the final layer.  Pretrained Distillation~ pretrains the student model with a self-supervised masked LM objective on a\nlarge corpus first, then performs a standard KD on supervised tasks.  \nMost of these models learn one-to-one layer mapping, where each student layer is guided by only one specific teacher layer. Li et al.~ propose a method where each student intermediate layer learns from every teacher intermediate layer with learnable attention weights. Both the embedding-layer distillation and the prediction-layer distillation employ the one-to-one layer mapping as in TinyBERT and BERT-PKD.\nTang et al.~ propose distillation of a BERT model to a single layer BiLSTM using KL divergence between student and teacher logits (see Fig.~\\ref{fig:kd}(L)). Mukherjee et al.~ also distill a multi-lingual BERT (mBERT) model to a BiLSTM. Representation transfer is done from Transformer-based teacher model to BiLSTM-based student model with different embedding dimensions and disparate output spaces. Distillation features include teacher logits and internal teacher representations for one teacher layer. To make all output spaces compatible, a non-linear projection of the parameters in student representation is done to have same shape as teacher representation for each token. The projection parameters are learned by minimizing the KL-divergence (KLD) between the representations of the student and the chosen layer from the teacher. Overall there are multiple loss functions for the student training: supervised hard loss, soft loss wrt output logits, and soft loss wrt internal teacher layer. Rather than optimizing for all loss functions jointly, stage-wise training is performed where each loss function is sequentially used for optimization. \nLastly, there have been recent efforts~ to distill Transformer models to slightly modified Transformer architectures. MobileBERT~ is a thin version of BERT-large, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks (FFN). To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT-large model (IB-BERT). The IB-BERT uses the inverted-bottleneck structures to adjust its feature map size to 512. Thus, in the bottleneck structure, the inputs to the multi-head attention (MHA) module are from wider feature maps (of inter-block size), while the inputs to the FFN are from narrower bottlenecks (of intra-block size). To fix this issue, MobileBERT uses stacked feed-forward networks to re-balance the relative size between MHA and FFN. Each MobileBERT layer contains one MHA but 4 stacked FFN after each MHA. Then, we conduct knowledge transfer from this teacher to MobileBERT using feature map transfer and attention transfer across all layers. Also, while distilling they perform progressive knowledge transfer, i.e., they progressively train each layer in $L$ stages, where $L$ is the number of layers. When the $l$-th layer is trained, all the trainable parameters in the layers below are frozen. Another difference in the MobileBERT student architecture is usage of high information flow residual connections between the high-channel-count layers. MobileBERT is 4.3x smaller and 5.5x faster than BERT-base while achieving competitive results on well-known benchmarks. Iandola et al.~ propose a new Transformer model architecture called SqueezeBERT which is much like BERT-base, but with the position-wise fully connected layers implemented as convolutions, and grouped convolutions for many of the layers. Distillation for SqueezeBERT is rather straightforward. Distillation is applied only to the final layer, and only during finetuning using soft cross entropy loss with respect to a weighted sum of the teacher's logits and a one-hot encoding of the ground-truth. Teacher model is a BERT-base model finetuned independently on each GLUE task, and these task-specific teacher weights are used for distillation. Xu et al.~ propose a method for progressive module replacement for compressing BERT. Their approach first divides the original BERT into several modules and builds their compact substitutes. Then, the original modules are randomly replaced with their substitutes to train the compact modules to mimic the behavior of\nthe original modules. We progressively increase the probability of replacement through\nthe training. In this way, their approach brings\na deeper level of interaction between the original and compact models.", "cites": [4513, 4509, 4511, 2481, 4512, 4514, 4510, 854], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple distillation methods for Transformers with clear organization, connecting techniques like dual training, layer mapping, and progressive distillation. It provides some critical insights, such as the limitations of one-to-one layer mapping and the benefits of stage-wise training. While it identifies patterns (e.g., use of KL divergence, residual connections), it stops short of offering a fully abstracted framework or deep critique of the approaches."}}
{"id": "4cd2b9eb-147c-41ba-996b-a30e824f168f", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "2c5b1b2b-3f74-442d-8ee6-503425cf4207", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Knowledge Distillation (KD)"], ["subsection", "Summary"]], "content": "\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|l|l|l|l|l|p{0.5in}|}\n    \\hline\nTask&Dataset&Teacher/Student models&Method&Metric&Size (Distilled; Orig)&Eval. (Distilled; Orig)\\\\\n\\hline\n\\hline\nAbs. summarization&CNN/DailyMail&UniLM-large/12-layer BERT&MiniLM~&ROUGE-L (H)&33M; 340M&39.73; 40.34\\\\\n\\hline\nAd CTR prediction&Criteo&No teacher/DNN&CoDistillation~&MAE (L)&-&0.019; 0.022\\\\\n\\hline\nCross-lingual NLI&XNLI (15 langs.)&XLM-R-base/12-layer BERT&MiniLM~&Acc (H)&21M; 85M&71.1; 74.5\\\\\n\\hline\nCross-lingual QA&MLQA (7 langs.)&XLM-R-base/12-layer BERT&MiniLM~&F1 (H)&21M; 85M&63.2; 64.9\\\\\n\\hline\nIntent detection&SNIPS (7-class)&BERT-base/6-layer BERT&Mixed-vocabulary training~&Acc (H)&6.2M; 109M&98.7; 98.6\\\\\n\\hline\nLanguage modeling&Common Crawl&No teacher/2-layer LSTM&CoDistillation~&CE (L)&-&3.91; 3.92\\\\\n\\hline\nMachine reading comp.&RACE&BERT-base/6-layer BERT&Patient KD~&Acc (H)&67M; 109M&60.34; 65.30\\\\\n\\hline\nMachine reading comp.&RACE&BERT-base/6-layer BERT&Vanilla KD~&Acc (H)&67M; 109M&58.74; 65.30\\\\\n\\hline\nNER (41 langs.)&Wikiann-41&Multiple mBERT/BiLSTM&XtremeDistill~&F1 (H)&31.8M; 179M*41&88.64; 90.76\\\\\n\\hline\nNMT (de$\\rightarrow$en)&OpenNMT&4-layer LSTM/2-layer LSTM&Vanilla KD~&BLEU (H)&64.8M; 84.8M&15.48; 15.88\\\\\n\\hline\nNMT (de$\\rightarrow$en)&OpenNMT&4-layer LSTM/2-layer LSTM&Quantized Distillation (4 bits)~&BLEU (H)&64.8M; 84.8M&15.26; 15.88\\\\\n\\hline\nNMT (de$\\rightarrow$en)&WMT13&4-layer LSTM/2-layer LSTM&Quantized Distillation (4 bits)~&BLEU (H)&81.6M; 84.8M&35.32; 34.7\\\\\n\\hline\nNMT (de$\\rightarrow$en)&WMT13&4-layer LSTM/2-layer LSTM&Vanilla KD~&BLEU (H)&81.6M; 84.8M&30.21; 34.7\\\\\n\\hline\nNMT (en$\\rightarrow$Others)&IWSLT (13 langs.)&Multiple Transformers/Transformer&Multiple teachers~&BLEU (H)&44M; 44M*12&22.96; 22.72\\\\\n\\hline\nNMT (en$\\rightarrow$Others)&IWSLT (13 langs.)&Multiple Transformers/Transformer&Seq-KD with Multiple teachers~&BLEU (H)&44M; 44M*12&21.98; 22.72\\\\\n\\hline\nNMT (en$\\rightarrow$Others)&WMT (7 langs.)&Multiple Transformers/Transformer&Multiple teachers~&BLEU (H)&44M; 44M*6&24.47; 24.50\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&4-layer LSTM/2-layer LSTM&Word-KD~&BLEU (H)&49M; 221M&14.9; 17.7\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&4-layer LSTM/2-layer LSTM&Seq-KD~&BLEU (H)&49M; 221M&18.1; 17.7\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&4-layer LSTM/2-layer LSTM&Seq-KD + Seq-Inter + Word-KD~&BLEU (H)&49M; 221M&18.5; 17.7\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&4-layer LSTM/2-layer LSTM&Pruned Seq-KD + Seq-Inter~&BLEU@5 (H)&8M/17M; 221M&18.5/19.1; 19.5\\\\\n\\hline\nNMT (Others$\\rightarrow$en)&IWSLT (13 langs.)&Multiple Transformers/Transformer&Multiple teachers~&BLEU (H)&44M; 44M*12&30.34; 29.7\\\\\n\\hline\nNMT (Others$\\rightarrow$en)&Ted Talk (45 langs.)&Multiple Transformers/Transformer&Multiple teachers~&BLEU (H)&44M; 44M*43&28.95; 25.17\\\\\n\\hline\nNMT (Others$\\rightarrow$en)&WMT (7 langs.)&Multiple Transformers/Transformer&Multiple teachers~&BLEU (H)&44M; 44M*6&28.61; 27.07\\\\\n\\hline\nNMT (th$\\rightarrow$en)&IWSLT15&4-layer LSTM/2-layer LSTM&Word-KD~&BLEU (H)&8M; 47M&11.8; 14.3\\\\\n\\hline\nNMT (th$\\rightarrow$en)&IWSLT15&4-layer LSTM/2-layer LSTM&Seq-KD~&BLEU (H)&8M; 47M&12.8; 14.3\\\\\n\\hline\nNMT (th$\\rightarrow$en)&IWSLT15&4-layer LSTM/2-layer LSTM&Seq-KD + Seq-Inter + Word-KD~&BLEU (H)&8M; 47M&14.2; 14.3\\\\\n\\hline\nQuestion generation&SQuAD 1.1&UniLM-large/12-layer BERT&MiniLM~&BLEU@4 (H)&33M; 340M&23.27; 24.32\\\\\n\\hline\nSlot filling&SNIPS (39 slots)&BERT-base/6-layer BERT&Mixed-vocabulary training~&F1 (H)&6.2M; 109M&95.0; 97.0\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various knowledge distillation methods (sorted by Task and then Dataset). CE=cross entropy, MAE=mean absolute error. en=English, th=Thai. MRC=Machine Reading Comprehension. NLI=Natural Language Inference. QA=Question Answering. NER=Named Entity Recognition. In the metric column, H means high is better while L means low is better.}\n    \\label{tab:kdSummary1}\n\\end{table}\nTable~\\ref{tab:kdSummary1} compares various knowledge distillation methods across different tasks and datasets. Accuracy of both the original and the distilled model are shown in the Eval column. Also, we report model size for both the student as well as the teacher models. Note that sometimes smaller student models perform better than the teacher models. This could be because (1) for some (task, dataset) pairs, the smaller models are a better regularized fit compared to potentially overfitted teacher models, and (2) student models can be rigorously trained using additional semi-supervision while teacher models depend on limited labeled training data.\nWhile knowledge distillation has been used for distilling NLP models across many applications, NMT is the most popular one. For abstractive summarization, MiniLM~ leads to a student models which is less than one tenth of the teacher without much loss in ROUGE-L. Similarly, MiniLM shows good results for cross-lingual NLI and multi-lingual QA as well. For Ad click through rate (CTR) prediction and language modeling, Anil et al.~ show that co-distillation leads to lower MAE and cross entropy respectively compared to the individually trained models. Zhao et al.~'s mixed-vocab training leads to 6-layer model that retains over 95\\% of the BERT-base model’s slot filling F1 score while being 30x smaller ($<$10 MB without quantization) and 57x faster on a mobile device, yet task-agnostic. For Named Entity Recognition (NER), Mukherjee et al.~ show that XtremeDistil leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95\\% of its F1-score for NER over 41 languages. \nFor NMT, experiments have been done on OpenNMT, WMT, IWSLT and TedTalk datasets. Kim et al.~ make the following observations: (1) Sequence-level knowledge distillation (Seq-KD) does better than word-level knowledge distillation (Word-KD) on English $\\rightarrow$ German and performs similarly on Thai $\\rightarrow$ English. (2) Combining them (Seq-KD + Word-KD) results in further gains, indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level. (3) Applying weight pruning on top of knowledge distillation results in a student model that has 13x fewer parameters than the original teacher model, with a decrease of 0.4 BLEU. Tan et al.~ show that one model is enough to handle multiple languages (up to 44 languages), with comparable or even better accuracy than individual models. Their method achieves larger improvements on some languages, such as Da, Et, Fi, Hi and Hy, than others. This is correlated with the data size of the languages. When a language is of smaller data size, it may get more improvement due to the benefit of multilingual training.\n\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|p{0.3in}|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}\n\\hline\nMethod&Method&Teacher/Student&Size&MRPC&MNLI&MNLI-m&SST-2&QQP&QNLI&RTE&CoLA&STS-B&SQuAD&SQuAD\\\\ \nCategory&& models&& F1& Acc& Acc& Acc&F1& Acc& Acc& MCC&Spearman& 1.1 F1& 2.0 F1\\\\\n\\hline\n\\hline\n\\multirow{3}{0.3in}{Original Models}&BERT-B~&-& 109M&88.9&83.4&84.6&93.5&71.2&90.5&66.4&52.1&85.8&88.4&77.7\\\\\n\\cline{2-15}\n&BERT-L~&-& 340M&89.3&85.9&86.7&94.9&72.1&92.7&70.1&60.5&86.5&90.9&81.9\\\\\n\\cline{2-15}\n&MTDNN (ensemble)~&-& 340*4M&90.0&87.2&86.7&95.6&72.4&93.9&80.9&61.5&88.3&-&-\\\\\n\\hline\n\\hline\n\\multirow{19}{*}{\\rotatebox{90}{Knowledge Distillation Methods}}&Distilled-BiLSTM~&BERT-L/BiLSTM&0.96M&-&72.6&73.0&90.7&68.2&-&-&-&-&-&-\\\\\n\\cline{2-15}\n&Mixed-vocab. training~&BERT-L/BERT-12&10.9M&87.2&80.5&80.7&90.6&-&-&-&-&-&-&-\\\\\n\\cline{2-15}\n&TinyBERT~&BERT-B/BERT-4&14.5M&86.4&81.8&82.5&92.6&71.3&87.7&66.6&44.1&80.4&82.1&71.8\\\\\n\\cline{2-15}\n&BERT-EMD~&BERT-B/BERT-4&14.5M&87.6&80.6&82.1&91.0&69.3&87.2&66.2&25.6&82.3&-&-\\\\\n\\cline{2-15}\n&MobileBERT~&BERT-L/BERT-6&25.3M&88.8&82.6&83.3&92.8&70.2&90.6&66.2&50.5&84.4&90.0&79.2\\\\\n\\cline{2-15}\n&MobileBERT+Quantization~&BERT-L/BERT-6&25.3M&87.0&-&83.9&91.9&-&90.8&-&-&-&90.0&-\\\\\n\\cline{2-15}\n&MiniLM~&BERT-B/BERT-12&33M&89.5&-&85.7&93.0&91.3&91.5&73.3&58.5&-&-&81.7\\\\\n\\cline{2-15}\n&SqueezeBERT~&BERT-B/SqueezeBERT&51.1M&87.8&81.1&82.0&91.4&80.3&90.1&73.2&46.5&86.7&-&-\\\\\n\\cline{2-15}\n&DistilBERT~&BERT-B/BERT-4&52.2M&82.4&78.0&78.9&91.4&68.5&85.2&54.1&32.8&76.1&81.2&64.1\\\\\n\\cline{2-15}\n&Patient KD~&BERT-B/BERT-4&52.2M&82.6&79.3&79.9&89.4&70.2&85.1&62.3&24.8&79.8&79.5&64.6\\\\\n\\cline{2-15}\n&BERT-of-Theseus~&BERT-B/BERT-6&66M&87.6&82.1&82.4&92.2&71.6&89.6&66.2&47.8&84.1&-&-\\\\\n\\cline{2-15}\n&MiniLM~&BERT-B/BERT-6&66M&88.4&-&84.0&92.0&91.0&91.0&71.5&49.2&-&-&76.4\\\\\n\\cline{2-15}\n&BERT-EMD~&BERT-B/BERT-6&66M&89.8&83.5&84.7&93.3&72.0&90.7&71.7&47.5&86.8&-&-\\\\\n\\cline{2-15}\n&Patient KD~&BERT-B/BERT-6&67M&85.0&81.0&81.5&92.0&70.7&89.0&65.5&43.5&81.6&85.3&69.8\\\\\n\\cline{2-15}\n&Vanilla KD~&BERT-B/BERT-6&67M&86.2&79.8&80.2&91.5&70.1&88.3&64.7&-&-&-&-\\\\\n\\cline{2-15}\n&DistilBERT~&BERT-B/BERT-6&67M&86.9&81.3&82.6&92.5&70.1&88.9&58.4&49.0&81.3&86.2&69.5\\\\\n\\cline{2-15}\n&TinyBERT~&BERT-B/BERT-6&67M&87.3&83.2&84.6&93.1&71.6&90.4&70.0&51.1&83.7&87.5&77.7\\\\\n\\cline{2-15}\n&Pretrained Distillation~&BERT-B/BERT-6&67M&86.8&82.2&82.8&91.8&70.4&88.9&65.3&-&-&-&-\\\\\n\\cline{2-15}\n&MTDNN-KD~&MTDNN/MTDNN-KD&340M&91.1&86.7&87.5&95.6&72.7&96.0&85.1&65.4&89.6&-&-\\\\\n\\hline\n\\hline\nParam.&ALBERT-B~ (dev)&-&12M&-&-&81.6&90.3&-&-&-&-&-&89.3&80\\\\\n\\cline{2-15}\nSharing&ALBERT-L~ (dev)&-&18M&-&-&83.5&91.7&-&-&-&-&-&90.6&82.3\\\\\n\\hline\nTensor Decomp.&FLOP~&-&80M&88.61&-&-&92.09&-&89.05&-&-&88.18&-&-\\\\\n\\hline\n\\multirow{3}{*}{Pruning}&RPP Iterative Magnitude Pruning~&-&138M&88.1&86.1&85.7&92.4&91.2&92.3&70.1&82.8&-&90.23&75.3\\\\\n\\cline{2-15}\n&Iterative Magnitude Pruning~&-&170M&83.5&77&82.5&91.3&85.1&90.2&68.6&76.3&-&85.3&-\\\\\n\\cline{2-15}\n&LayerDrop~&-&66M&85.3&-&82.9&92.5&-&89.4&-&-&-&-&-\\\\\n\\hline\nQuant.&Mixed-precision quant. (QBERTMP)~&-&-&-&82.29&81.75&92.08&-&-&-&-&-&86.95&-\\\\\n\\hline\nSubQuad Trans.&Linformer~&-&-&-&-&-&93.1&90.8&91.2&-&-&-&-&-\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various methods  across various GLUE~ and SQuAD tasks. Please refer~ for detailed description of tasks. Top part (first three rows) shows results for basic Transformer methods (teacher models). Middle part shows results for knowledge distillation methods. Bottom part shows results for a mix of other methods across categories. BERT-L=BERT-large, BERT-B=BERT-base, BERT-$i$=$i$-layer BERT. MCC refers to Matthews correlation. Results for SQuAD are on dev set. Empty entries indicate that the papers do not report those results, or NA entries.}\n    \\label{tab:kdSummary2}\n\\end{table}\nTable~\\ref{tab:kdSummary2} compares various knowledge distillation methods (besides other model compression methods) across various GLUE~ and SQuAD tasks. Also, we report model size for both the student as well as the teacher models. Different distillation methods use one of these as the teacher: BERT-large, BERT-base or MTDNN. PKD~ outperforms the Vanilla KD~ on almost all the datasets except for MRPC. TinyBERT-4 significantly outperforms the 4-layer BERT-PKD and DistilBERT by a margin of at least 4.4\\%, with ~28\\% parameters and 3.1x inference speedup. Compared with the teacher BERT-base, 4-layer TinyBERT is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive performances. The 6-layer TinyBERT achieves comparable results with the teacher. Overall, TinyBERT consistently outperforms both the 4-layer and 6-layer baselines like PKD, DistilBERT and MiniLM.\nTurc et al.~ show how appropriate pretraining can improve the quality of distillation. MiniLM outperforms DistilBERT and TinyBERT across most tasks. The 6-layer MiniLM is 2.0x faster than original BERTBASE, while retaining more than 99\\% performance on a variety of tasks, such as SQuAD 2.0 and MNLI. Distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks. 4-layer BERT-EMD outperforms 4-layer DistilBERT and BERT-PKD by a substantial margin, even with only 30\\% parameters and inference time. Furthermore, it exceeds the TinyBERT model by 2.3\\% accuracy on RTE, 2.2\\% F1 on MRPC, and 1.9\\% Spearman correlation on STS-B. 6-layer BERT-EMD performs better than the 12-layer BERT-base model on 7 out of 9 tasks, with only about 50\\% parameters and inference time of the original BERT-base model. Tang et al.~ distill BERT to BiLSTMs. They observe that the distilled BiLSTM model uses 349 times fewer parameters than BERT-large and is 434 times faster. Also, mixed vocab training by Zhao et al.~ produces a small 12-layer model that performs competitively with 6-layer PKD and 4-layer DistilBERT while being $\\sim$5-6x smaller.\nMobileBERT is 4.3x smaller and 5.5x faster than BERT-base.  On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT-base). On the natural language inference tasks of GLUE, MobileBERT can achieve a GLUE score of 77.7, which is only 0.6 lower than BERT-base, with a latency of 62 ms on a Pixel 4 phone. While quantization can further compress MobileBERT by 4x, there is nearly no performance degradation from it.  SqueezeBERT is approximately half the size of BERT-base. MobileBERT is half the size of SqueezeBERT. SqueezeBERT is 4.3x faster than BERT-base, while MobileBERT is 3.0x faster than BERT-base. On 4 of GLUE tasks SqueezeBERT outperforms the accuracy of MobileBERT, while on the other 4 of the GLUE tasks MobileBERT outperforms SqueezeBERT. MobileBERT and SqueezeBERT outperform BERT-base significantly across all tasks.\nTo summarize, KD is a popular method for text based model compression. Various methods have proposed information copying using logits, softmax output, attention sub-layer output, value relation, relative dissimilarity information from both the last layer as well as intermediate layers of the teacher. Many methods have been proposed to handle complex teacher-student configuration mismatches in terms of vocabulary, number of attention heads, and hidden layer sizes. Also, KD has been found to be very effective in complex problem settings like multi-lingual tasks and tasks with large number of classes. Learning from noisy teachers, teacher assistants, an ensemble of teachers has been found to be effective as well. KD is the best model compression method especially in settings where a large amount of unlabeled data exists; distillation with data pseudo-labeled by teacher leads to very effective students.", "cites": [4479, 4515, 1150, 4505, 4514, 9120, 4513, 856, 681, 854, 7333, 4486, 4500, 1568, 7, 8390, 4509, 4511, 2481, 4512, 4507, 2488, 4510], "cite_extract_rate": 0.8518518518518519, "origin_cites_number": 27, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a detailed comparative overview of knowledge distillation methods through a well-structured table and highlights varying results across tasks and datasets. It synthesizes key findings from multiple papers, such as the effectiveness of Seq-KD over Word-KD and the impact of multilingual training. However, while it describes outcomes and some trends, it lacks deeper critical analysis of the limitations or trade-offs of the methods and offers only limited abstraction beyond the presented data."}}
{"id": "1d632f6e-824d-41b6-8395-ec40e6c541ab", "title": "Character-aware Language Models", "level": "subsection", "subsections": [], "parent_id": "57903397-1a0f-4365-9fd9-a6bf95e76421", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Parameter sharing"], ["subsection", "Character-aware Language Models"]], "content": "Fig.~\\ref{fig:charCNN} illustrates various character-aware language model architectures. Ling et al.~ proposed their character to word (C2W) model which constructs vector representations of words by composing characters using BiLSTMs. Relative to traditional word representation models that have independent vectors for each word type, C2W requires only a single vector per character type and a fixed set of parameters for the compositional model. As input, we define an alphabet of characters $C$. For English, this vocabulary would contain an entry for each uppercase and lowercase letter as well as numbers and punctuation. Thus compared to the word embedding matrix, this model is much smaller. Despite the compactness of this model, this ``composed'' word representations yield comparable results across multiple text classification tasks. \nJozefowicz et al.~ propose two variants for composing word embeddings using character embeddings. In the first CNN-Softmax variant, they use character CNNs (Convolutional Neural Networks) to compose word embeddings from character embeddings both at the input side as well as at the output softmax layer. The character-CNN sub-networks at the input or the output do not share weights. The composed word embeddings are fed to an LSTM to generate the output. In the second Char-LSTM variant, character CNN is used to compose word embeddings on the input side. The composed word embeddings are fed to an LSTM to generate an output which is further fed to a small LSTM that predicts the target word one character at a time. Thus, the word and character-level models are combined, and predictions are made one character at a time, thus allowing to compute probabilities over a much smaller vocabulary. Kim et al.~ propose another variant where at the output side they continue to use word embeddings, but at the input side they compose word embeddings using a highway network on top of a character CNN. The highway network's output is used as the input to a multi-layer LSTM, whose last hidden state output is fed to the output softmax layer.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.6\\columnwidth]{charCNN}\n    \\caption{Character-aware Language Models}\n    \\label{fig:charCNN}\n\\end{figure}", "cites": [4516, 4517, 4036], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of character-aware language model architectures and their variants, referencing three key papers. It synthesizes basic structural elements from each paper, such as the use of BiLSTM, CNN, and highway networks, but lacks deeper comparative or critical analysis. There is minimal abstraction or generalization to broader principles or frameworks in model compression."}}
{"id": "e8e447ef-9838-46fc-a0ca-4f8617f32377", "title": "Parameter Sharing in the Embedding Matrix", "level": "subsection", "subsections": [], "parent_id": "57903397-1a0f-4365-9fd9-a6bf95e76421", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Parameter sharing"], ["subsection", "Parameter Sharing in the Embedding Matrix"]], "content": "Given a weight matrix $W$ and a budget $K$, we want to share weights within $W$ to have a max of $K$ unique values. A na\\\"ive implementation of random weight sharing can be trivially achieved by maintaining a secondary matrix consisting of each connection's group assignment. But this needs memory space itself. Hence, Chen et al.~ propose to use hashing. HashedNets use a low-cost hash function (like xxhash\\footnote{\\url{https://code.google.com/p/xxhash/\n}}) to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value.\nUnlike HashedNets where weights are randomly grouped, parameter sharing mechanisms in Toeplitz-like structured matrices~ are highly specific and deterministic. Toeplitz matrices have parameters tied along diagonals. The displacement rank of all Toeplitz matrices is up to 2. Toeplitz-like matrices allow the displacement rank $r$ to be higher. They include products and inverses of Toeplitz matrices, and their linear combinations. The displacement rank $r$ serves as a knob on modeling capacity. High displacement rank matrices are increasingly unstructured. With displacement rank $r$, there are $2nr$ free parameters in the Toeplitz-like structured matrix. Toeplitz transforms can be applied not just to embedding matrix but to all weight matrices in an RNN model. Tay et al.~ use a similar Toeplitz-like structured matrix method with Hamilton Products in Quaternion Algebra to propose Quaternion Transformers which lead to 75\\% parameter reduction in the Transformer architecture.\nAnother method for parameter sharing is to share low-rank factors across layers in a recurrent model. In this method, we first represent a weight matrix $W$ using matrix factorization as $W=W_a W_b$. Thus, hidden layer output for layer $l$ at time $t$ can be written as follows.\n\\begin{eqnarray}\nh_t^l=\\sigma\\left[W_a^l W_b^l h_t^{l-1}+U_a^l U_b^l h_{t-1}^l + b^l\\right]    \n\\end{eqnarray}\nBut we can share some low-rank factors by setting $W_b^l=U_b^{l-1}$. The combination of matrix factorization and parameter sharing leads to large model compression.\nAnother way of compressing the embedding matrix is to divide the vocabulary $V$ into frequent and infrequent word sets $B$ and $C$ respectively. Infrequent words' embeddings are represented with frequent words' by sparse linear combinations~. This is inspired by the observation that, in a dictionary, an unfamiliar word is typically defined by common words. A dense embedding is assigned to each common word; an infrequent word, on the other hand, computes its vector representation by a sparse combination of common words' embeddings. This compression is useful for both word embedding matrix as well as output layer of RNNs/LSTMs. Let $U\\in R^{E\\times |B|}$ be the learned embedding matrix of common words where $E$ is the embedding dimension. For a word $w\\in C$, we shall learn a sparse vector $x\\in R^{|B|}$ as the sparse code of the word. Once we know $x$, embedding for a word $w\\in C$ can be written as follows.\n\\begin{eqnarray}\n    \\text{embedding}(w)=\\sum_{j=1}^B x_jU_j\n\\end{eqnarray}\nwhere $U_j$ is the $j^{th}$ column of $U$. To learn the sparse representation of word $w\\in C$, the following problem needs to be solved.\n\\begin{eqnarray}\n\\min_x ||Ux-A||_2^2+\\alpha ||x||_1+\\beta |1^Tx-1|+\\gamma 1^T \\max(0,-x)\n\\end{eqnarray}\nwhere $A$ is embedding for the rare word $w$. The last two regularization terms favor a solution that sums to 1 and that is non-negative (for psychological interpretation concerns), respectively. \nLightRNN~ compresses word embedding matrix from $O(|V|)$ to $O(\\sqrt{|V|})$. It uses a 2-Component shared embedding for word representations. We allocate every word in the vocabulary into a word-allocation table, each row of which is associated with a learned vector, and each column associated with another learned vector. Table~\\ref{tab:wordAllocation} shows an example of a word allocation table. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Thus, we only need $2\\sqrt{|V|}$ vectors to represent a vocabulary of $|V|$ unique words, which are far less than the $|V|$ vectors. \nThe input and output use different embedding row/column vectors but they share the same word-allocation table. Word Allocation table creation uses a bootstrap procedure to iteratively refine word allocation based on the learned word embedding. Embeddings (i.e. row and column vectors) are learned using language modeling loss using an RNN on top of the embedding layer.\n\\begin{table}\n    \\centering\n    \\begin{tabular}{|c|c|c|c|c|}\n    \\hline\nEmbedding&$x_1^c$&$x_2^c$&$x_3^c$&$x_4^c$\\\\\n\\hline\n$x_1^r$&january&february&...&...\\\\\n\\hline\n$x_2^r$&one&two&...&...\\\\\n\\hline\n$x_3^r$&...&...&...&...\\\\\n\\hline\n$x_4^r$&...&...&...&...\\\\\n\\hline\n\\end{tabular}\n    \\caption{An Example of a Word Allocation Table}\n    \\label{tab:wordAllocation}\n\\end{table}\nFinally, Suzuki et al.~ propose a Skipgram~ training method with parameter sharing as follows. Split every embedding vector of size $D$ into $B$ equal sub-vectors of size $C$. Thus $D=B\\times C$. We assign a limited number of reference vectors to each block of block-splitting vectors. E.g., the number of reference vectors becomes $K\\times B$ if we assign $K$ reference vectors to each block. Each reference vector is of size $C$. Skipgram training optimization remains the same except for these extra parameter sharing constraints (applied to both the input and output embedding vectors). Liu et al.~ propose a very similar method, Slim Embeddings, where the embeddings are learned using a RNN language model rather than the Skipgram method. Slim Embeddings is very related to HashedNets~, LightRNN~ and\nCharacter Aware Language model~. In HashedNets, all elements in a parameter matrix are mapped into a vector through a hash function. However in Slim Embeddings approach, we randomly share subvectors instead of single elements. Slim Embeddings differs from LightRNN  in\nthat it is able to control the compression ratio\nto any arbitrary value, while LightRNN can only compress\nat the rate of square or cube root of vocabulary size, which\ncould be too harsh in practical applications. In character aware language model, if we treat the sequence of subvector ids (virtual characters) as each word's representation,\nthe word embedding then can be treated as concatenated unigram character feature vectors. The drawback of such an approach is that the model is\nmore complicated and to speed up inference, it needs to pre-compute the word embeddings for the words, so it couldn't\nstay in its compact form during inference. The Slim embeddings model is much simpler, and easier to tune. And during inference, it uses much less space and can even decrease the complexity of inference.", "cites": [4522, 4520, 4036, 4518, 4521, 4519], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes several parameter-sharing methods in embedding matrices, connecting techniques like HashedNets, Toeplitz-like matrices, LightRNN, and Slim Embeddings into a coherent narrative. It provides critical comparisons, such as the differences in compression ratios and inference efficiency between LightRNN and Slim Embeddings, and highlights limitations in certain approaches (e.g., pre-computation in character-aware models). The section abstracts these methods into broader principles like structured parameter sharing, sparsity, and trade-offs between model size and performance."}}
{"id": "82aba295-2827-4559-b383-3667b9bd0577", "title": "Parameter Sharing in Transformers", "level": "subsection", "subsections": [], "parent_id": "57903397-1a0f-4365-9fd9-a6bf95e76421", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Parameter sharing"], ["subsection", "Parameter Sharing in Transformers"]], "content": "A standard Transformer does not share parameters across layers and also has a fixed number of encoder layers. ALBERT~ incorporates two parameter reduction techniques: \n\\begin{itemize}\n    \\item Factorized embedding parameterization. That is, it decomposes large vocabulary embedding matrix into two small matrices. Thus, it reduces the embedding parameters from $O(V \\times H)$ to $O(V \\times E + E \\times H)$ where $H >> E$.\n    \\item Cross-layer parameter sharing: There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers.\n\\end{itemize}\nAn ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. Dehghani et al.~ propose Universal Transformers where the number of encoder layers are not pre-decided, and all the encoder layers share the parameters. Certain symbols (e.g. some words or phonemes) are usually more ambiguous than others. It is therefore reasonable to allocate more processing resources to these more ambiguous symbols. Thus, ambiguous symbols undergo more self-attention transformations compared to non-ambiguous ones. Thus, they provide a dynamic per-position halting mechanism for dynamically modulating the number of computational steps needed to process each input symbol (called the ``ponder time'') before the representation is passed on as input to the decoder. The idea of sharing weights across layers in Transformers has also been explored in~.", "cites": [1150, 4523], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of parameter sharing in Transformers, particularly in ALBERT and Universal Transformers. It integrates some concepts (e.g., comparing ALBERT to BERT-large) but lacks deeper synthesis across the cited works. There is minimal critical analysis or abstraction to broader principles or frameworks."}}
{"id": "736e641f-1998-44af-a324-58eb82e0693d", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "57903397-1a0f-4365-9fd9-a6bf95e76421", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Parameter sharing"], ["subsection", "Summary"]], "content": "\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|p{0.8in}|l|l|l|l|l|l|}\n    \\hline\nTask&Dataset&Method&Base Model&Metric&Size (Comp; Orig)&Eval. (Comp; Orig)\n\\\\\n\\hline\n\\hline\nLanguage modeling&1B Word Benchmark&Char-CNN (input embeddings)~&2-layer word-BiLSTM&Perplexity (L)&1.04B; 1.8B&30.0; 30.6\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&Char-CNN (in/output embeddings)~&2-layer word-BiLSTM&Perplexity (L)&0.39B; 1.8B&35.8; 30.6\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&LightRNN~&word-LSTM&Perplexity (L)&41M; 1.6G&66; 85\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&Slim Embeddings~&2-layer word-BiLSTM&Perplexity (L)&0.25B; 1.8B&38.3; 30.6\\\\\n\\hline\nLanguage modeling&ACLW-Czech&CNN+Highway network~&word-LSTM&Perplexity (L)&64M; 83M&578; 701\\\\\n\\hline\nLanguage modeling&ACLW-Czech&LightRNN~&word-LSTM&Perplexity (L)&18M; 83M&558; 701\\\\\n\\hline\nLanguage modeling&ACLW-Czech&Slim Embeddings~&word-LSTM&Perplexity (L)&17M; 83M&528; 701\\\\\n\\hline\nLanguage modeling&ACLW-English&CNN+Highway network~&word-LSTM&Perplexity (L)&20M; 25M&216; 236\\\\\n\\hline\nLanguage modeling&ACLW-English&LightRNN~&word-LSTM&Perplexity (L)&17M; 25M&191; 236\\\\\n\\hline\nLanguage modeling&ACLW-English&Slim Embeddings~&word-LSTM&Perplexity (L)&7M; 25M&187; 236\\\\\n\\hline\nLanguage modeling&ACLW-French &CNN+Highway network~&word-LSTM&Perplexity (L)&44M; 56M&190; 202\\\\\n\\hline\nLanguage modeling&ACLW-French &LightRNN~&word-LSTM&Perplexity (L)&17M; 56M&176; 202\\\\\n\\hline\nLanguage modeling&ACLW-French &Slim Embeddings~&word-LSTM&Perplexity (L)&12M; 56M&162; 202\\\\\n\\hline\nLanguage modeling&ACLW-German &CNN+Highway network~&word-LSTM&Perplexity (L)&104M; 137M&305; 347\\\\\n\\hline\nLanguage modeling&ACLW-German &LightRNN~&word-LSTM&Perplexity (L)&18M; 137M&281; 347\\\\\n\\hline\nLanguage modeling&ACLW-German &Slim Embeddings~&word-LSTM&Perplexity (L)&17M; 137M&261; 347\\\\\n\\hline\nLanguage modeling&ACLW-Russian&CNN+Highway network~&word-LSTM&Perplexity (L)&152M; 200M&313; 353\\\\\n\\hline\nLanguage modeling&ACLW-Russian&LightRNN~&word-LSTM&Perplexity (L)&19M; 200M&288; 353\\\\\n\\hline\nLanguage modeling&ACLW-Russian&Slim Embeddings~&word-LSTM&Perplexity (L)&19M; 200M&274; 353\\\\\n\\hline\nLanguage modeling&ACLW-Spanish&CNN+Highway network~&word-LSTM&Perplexity (L)&48M; 61M&169; 186\\\\\n\\hline\nLanguage modeling&ACLW-Spanish&LightRNN~&word-LSTM&Perplexity (L)&18M; 61M&157; 186\\\\\n\\hline\nLanguage modeling&ACLW-Spanish&Slim Embeddings~&word-LSTM&Perplexity (L)&8M; 61M&149; 186\\\\\n\\hline\nLanguage modeling&PTB&CNN+Highway network~&word-LSTM&Perplexity (L)&5M; 20M&92.3; 85.4\\\\\n\\hline\nLanguage modeling&Wikipedia articles (ca)&C2W~&word emb.&Perplexity (L)&182K; 4.3M&34.92; 35.34\\\\\n\\hline\nLanguage modeling&Wikipedia articles (de)&C2W~&word emb.&Perplexity (L)&183K; 6.3M&41.94; 43.02\\\\\n\\hline\nLanguage modeling&Wikipedia articles (en)&C2W~&word emb.&Perplexity (L)&180K; 4.3M&57.39; 59.38\\\\\n\\hline\nLanguage modeling&Wikipedia articles (pt)&C2W~&word emb.&Perplexity (L)&178K; 4.2M&40.92; 46.17\\\\\n\\hline\nLanguage modeling&Wikipedia articles (tr)&C2W~&word emb.&Perplexity (L)&174K; 5.7M&32.88; 44.01\\\\\n\\hline\nMachine reading comp.&ReAding Comprehension from Examinations&ALBERT~&BERT-B&Acc (H)&18M; 108M&68.5; 68.2\\\\\n\\hline\nNLI&MNLI&Quaternion Attention~&2-layer att-GloVe&Acc (H)&200K; 700K&72.3; 73.6\\\\\n\\hline\nNLI&SciTail&Quaternion Attention~&2-layer att-GloVe&Acc (H)&200K; 700K&79.6; 79.0\\\\\n\\hline\nNLI&SNLI&Quaternion Attention~&2-layer att-GloVe&Acc (H)&200K; 700K&85.4; 86.2\\\\\n\\hline\nNMT  (en$\\rightarrow$et)&IWSLT15&Quaternion Transformer~&Transformer&BLEU (H)&11M; 44M&13.1; 14.1\\\\\n\\hline\nNMT (en$\\rightarrow$ro)&IWSLT15&Quaternion Transformer~&Transformer&BLEU (H)&11M; 44M&18.5; 22.8\\\\\n\\hline\nNMT (en$\\rightarrow$vi)&IWSLT15&Quaternion Transformer~&Transformer&BLEU (H)&11M; 44M&28.0; 28.4\\\\\n\\hline\nPOS Tagging&PTB (ca)&C2W~&word-BiLSTM&Acc (H)&150K; 2M&98.92; 98.09\\\\\n\\hline\nPOS Tagging&PTB (de)&C2W~&word-BiLSTM&Acc (H)&150K; 2M&98.08; 97.51\\\\\n\\hline\nPOS Tagging&PTB (en)&C2W~&word-BiLSTM&Acc (H)&150K; 2M&97.36; 96.97\\\\\n\\hline\nPOS Tagging&PTB (pt)&C2W~&word-BiLSTM&Acc (H)&150K; 2M&97.47; 95.67\\\\\n\\hline\nPOS Tagging&PTB (tr)&C2W~&word-BiLSTM&Acc (H)&150K; 2M&91.59; 83.43\\\\\n\\hline\nQuestion answering&BABI&Universal Transformer~&Transformer&Avg error (L)&7.3M; 44M&0.21; 15.2\\\\\n\\hline\nQuestion answering&WikiQA&Quaternion Attention~&2-layer att-GloVe&MAP (H)&200K; 700K&66.2; 67.2\\\\\n\\hline\nSentiment analysis&IMDB&Quaternion Transformer~&2-layer Transformer&Acc (H)&100K; 400K&83.9; 82.6\\\\\n\\hline\nSentiment analysis&SST&Quaternion Transformer~&2-layer Transformer&Acc (H)&100K; 400K&80.5; 78.9\\\\\n\\hline\nSpeech recognition&2000 hour En. Speech&Toeplitz-like~&3-layer RNN&WER (L)&790K; 1.85M&48.4; 43.5\\\\\n\\hline\nSpeech recognition&2000 hour En. Speech&Toeplitz-like~&5-layer LSTM&WER (L)&2.00M; 9.12M&33.5; 33.1\\\\\n\\hline\nSubject verb agreement&SVA dataset&Quaternion Transformer~&2-layer Transformer&Acc (H)&100K; 400K&94.7; 94.8\\\\\n\\hline\nSubject verb agreement&SVA dataset&Universal Transformer~&Transformer&Acc (H)&7.3M; 44M&0.992; 0.962\\\\\n\\hline\nWord analogy &GSEm, GSYN, MSYN&Shared ref. vec. ($K$=16/256, $B$=256)~&SGNS&Acc (H)&98/196MB; 1565MB&64.6/64.5; 64.7\\\\\n\\hline\nWord analogy &GSEm, GSYN, MSYN&k-means quant. ($K$=16/256, $B$=256)~&SGNS&Acc (H)&98/196MB; 1565MB&64.0/64.5; 64.7\\\\\n\\hline\nWord similarity&7 datasets&Shared ref. vec. ($K$=16/256, $B$=256)~&SGNS&Acc (H)&98/196MB; 1565MB&65.5/67.1; 67.2\\\\\n\\hline\nWord similarity&7 datasets&k-means quant. ($K$=16/256, $B$=256)~&SGNS&Acc (H)&98/196MB; 1565MB&64.4/67.0; 67.2\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various parameter sharing methods (sorted by Task and then Dataset). 7 datasets for word similarity are MEN, MTurk, RARE, SLex, SCWS, WSR, WSS. SGNS=SkipGram with negative sampling. In the metric column, H means high is better while L means low is better.}\n    \\label{tab:paramSharingSummary}\n\\end{table}\nTable~\\ref{tab:paramSharingSummary} compares various parameter sharing methods across different tasks and datasets. Accuracy of both the original and the compressed (comp.) model are shown. Also, we report model size (in terms of number of parameters or memory size) for both the original as well as the compressed models. For the same task, dataset and model combination, different papers report different accuracy of the original model because of slight changes in training hyper-parameters; hence we report accuracy of the original model for each row. Since many parameter sharing methods have been used to compress word embeddings, the most common application is language modeling. \nFor language modeling, experiments have been done on the One Billion Word Benchmark, ACLW, PTB and Wikipedia articles datasets across multiple languages using parameter sharing methods like Char-CNN~, LightRNN~, Slim embeddings~, CNN+Highway networks~ and C2W~. C2W~ always outperforms word lookup tables and improvements are especially pronounced in Turkish, which is a highly morphological language, where word meanings differ radically depending on the suffixes used. Jozefowicz et al.~ observe that Char-CNNs especially with character embeddings being used both at input as well as output can lead to 4.6x model size reduction with a slight increase in perplexity. On the One-Billion-Word benchmark, LightRNN~ achieves much lower perplexity compared to word-LSTMs, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. LightRNN~ significantly reduces the model size, while at the same time outperforms CNN+Highway network~ method. While the model sizes of the CNN+Highway network method increase linearly with respect to the vocabulary size, the model size of LightRNN almost keeps constant on the ACLW datasets. Slim embeddings~ is way better than LightRNN (low perplexity with smaller models). On PTB and 44M giga word corpus datasets, Slim embeddings applied at input layer can maintain same perplexity for a word-LSTM using just 1\\% (and 0.2\\%) of trainable parameters respectively. \nk-means quantization and shared reference vectors are also methods for compression of word embeddings using parameter sharing. Suzuki et al.~ showed significant gains over typical skipgram (SGNS) embeddings in terms of model size reduction while retaining similar accuracies for word analogy and similarity tasks. The model size of their method with shared reference vectors with `K = 16, B = 64' was just 24MB, approximately 65 times smaller than that of original SGNS method. They also showed that SGNS with shared reference vectors was better than SGNS with block-wise k-means post-processing method. Unfortunately, there exists no good comparison between the slim embeddings and the shared reference vectors methods.\nFor the MRC task, ALBERT~ pushed the accuracy from 68.5 to 68.2 using 6x fewer parameters compared to BERT-base. On the NLI task, a tiny Quaternion-Att model (50 dimensions) achieves comparable (or occasionally marginally better or worse) performance compared to typical attention over GloVe (200 dimensions), gaining a 68\\% parameter savings across three datasets. For sentiment analysis, Quaternion Transformers~ leads by +1.3\\%/1.6\\% gains on IMDb and SST datasets respectively while maintaining a 75\\% reduction in parameter cost. Quaternion Transformers~ have been shown to outperform the vanilla Transformer for the mathematical language understanding task as well, with 25\\% parameters. At  the same compression rate, Quaternion Transformers lose only very minor BLEU on IWSLT 2015 NMT datasets.\nFor POS tagging, we can observe that the model using word lookup tables performs consistently worse than the C2W model~. Universal Transformers~ (while being 1/6th the size) outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task. On speech recognition, Lu et al.~ study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75\\%, at a small cost of 0.3\\% increase in WER, on a 2000-hr English Voice Search task. For LSTM parameter reduction, architecting upper layers with projection nodes to moderate rank, and bottom layers with Toeplitz-like transforms was found to be a particularly effective strategy. \nOverall, besides model compression, parameter sharing methods also act as a good regularizer. Parameter sharing in Transformers has been very successful. ALBERT was at the top of the GLUE leaderboard when it was proposed. Parameter sharing methods have also been widely used for compressing embedding matrix. Slim embeddings has the best method for compressing embedding matrices.", "cites": [4522, 4516, 4036, 1150, 4517, 4521, 4519], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a detailed comparison of parameter sharing methods across various NLP tasks and datasets, integrating results from multiple papers into a structured table. However, it lacks deeper synthesis of underlying principles or a novel framework. The critical analysis is limited, mostly highlighting relative performance improvements without evaluating limitations or trade-offs in detail. Some general patterns (e.g., effectiveness on morphologically rich languages) are mentioned, but abstraction remains at a basic level."}}
{"id": "da1505f4-583c-4b3b-a84f-338a8c53f73d", "title": "Two Low-Rank Factors", "level": "subsection", "subsections": [], "parent_id": "a059247f-141a-43b7-a6a9-f3440e7622cd", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Tensor decomposition"], ["subsection", "Two Low-Rank Factors"]], "content": "In this part, we will discuss methods where a matrix is factorized into two low-rank factors. Specifically, we replace a weight matrix $W$ with $W_1\\times W_2$ such that the total number of parameters are significantly lesser.\nA multi-layer RNN can be represented as follows.\n\\begin{eqnarray}\n    h_t^l=\\sigma(W_x^{l-1}h_t^{l-1}+W_h^l h_{t-1}^l +b^l)\\\\\n    h_t^{l+1}=\\sigma(W_x^l h_t^l + W_h^{l+1}h_{t-1}^{l+1}+b^{l+1})\n\\end{eqnarray}\nThus, there are two important weight matrices: the recurrent $W_h^l$ and inter-layer matrices $W_l^x$. Prabhavalkar et al.~ propose a method to jointly compress the recurrent and inter-layer matrices corresponding to a specific layer $l$ by determining a suitable recurrent projection matrix, denoted by $P^l\\in R^{r_l\\times N_l}$ of rank $r^l<N^l$ such that $W_h^l=Z_h^l P^l$ and $W_l^x=Z_x^l P^l$. First, $P^l$ is determined by  computing a truncated SVD of the recurrent weight matrix, which we then truncate, retaining only the top $r^l$ singular values. Thus, $W_h^l$ can be written as follows.\n\\begin{eqnarray}\n    W_h^l=(U_h^l \\Sigma_h^l)(V_h^l)^T=Z_h^lP^l\n\\end{eqnarray}\nThus, $P^l$ is set to $(V_h^l)^T$. Further, we determine $Z_x^l$ as the solution to the following least-squares problem.\n\\begin{eqnarray}\n   Z_x^l=\\argmin_Y ||YP^l-W_x^l||_2^2\n\\end{eqnarray}\nThis solution can also be easily extended to LSTMs. Sak et al.~ also proposed a similar solution based on a combination of parameter sharing and matrix decomposition but without SVD initialization. However, typically SVD initialization has been found to perform better.\nBesides SVD, another way of matrix decomposition is sparse coding. Faruqui et al.~ propose using sparse coding to decompose word embedding matrices. Thus, given vocabulary of size $V$, word embedding matrix  $X\\in R^{L\\times V}$, sparse coding aims at representing each input vector $x_i$ as a sparse linear combination of basis vectors $a_i$ by solving the following problem. \n\\begin{eqnarray}\n  \\argmin_{D,A} \\sum_{i=1}^V ||x_i-Da_i||_2^2+\\lambda||a_i||_1+\\tau ||D||_2^2\n\\end{eqnarray}\nwhere $D\\in R^{L\\times K}$ and $A\\in R^{K\\times V}$. Further, for interpretability, one can enforce all elements of $A$ and $D$ to be non-negative. For further compression, one can also enforce $A$ to be binary or ensure that each column of $A$ is a $K$ sized one hot vector~. \nLastly, Wang et al.~ combine pruning with matrix factorization for model compression and propose the FLOP (Factorized Low-rank Pruning) method. Let $W$ be a weight matrix. Structured pruning (removing a neuron, i.e., removing a column from weight matrix) can be achieved by replacing the computation $Wx$ by $WGx$ where diagonal sparsity-inducing matrix $G$ is learned using $L_0$ regularization over $WG$ along with the supervised loss. This effectively removes a subset of columns of $W$ for column indices $k$ with $z_k=0$. One limitation is that this structured pruning method tends to produce lower performance than its unstructured counterpart. Hence, in the FL0P (Factorized L0 Pruning) model, we first factorize $W=PQ$. Let $r$ be \\#columns of $P$ (or equivalently \\#rows of $Q$), $p_k$ and $q_k$ be the $k$-th column of $P$ and $k$-th row of $Q$ respectively. We achieve structured pruning by introducing a pruning variable $z_k$ for each component. Thus, now, we can write $W$ as follows.\n\\begin{eqnarray}\n  W=PGQ=\\sum_{k=1}^r z_k\\times (p_kq_k)\n\\end{eqnarray}\nwhere $G$ is again the diagonal matrix of pruning variables. After training, only columns and rows corresponding to non-zero diagonal values need to be stored, resulting in much smaller (but still dense) matrices $P$ and $Q$. The nonzero values of $G$ can be absorbed into either $P$ or $Q$. This structured pruning with factorization is much more effective compared to the vanilla structured pruning.", "cites": [4515, 4524, 4525], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates methods involving two low-rank factors, connecting SVD-based approaches from Prabhavalkar et al. with sparse coding from Faruqui et al. and structured pruning via factorization from Wang et al. It provides a coherent narrative by explaining how each method uses matrix decomposition for compression but lacks a deeper synthesis into a unified framework. The section includes some critical evaluation, such as noting the performance trade-offs in structured pruning and the optional constraints in sparse coding for interpretability. It identifies general principles like low-rank factorization for weight matrix compression but could offer broader meta-level insights to elevate the abstraction level."}}
{"id": "1b3ed6c2-9088-4bfb-8866-160b3884224a", "title": "Factorizing into Block Diagonal Matrices", "level": "subsection", "subsections": [], "parent_id": "a059247f-141a-43b7-a6a9-f3440e7622cd", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Tensor decomposition"], ["subsection", "Factorizing into Block Diagonal Matrices"]], "content": "The last layer of a language model is very large of the size $HV$ where $H$ is the size of the hidden layer and $V$ is vocabulary size. Each word by an output embedding of the same size $H$. Chen et al.~ propose a differentiated softmax method which varies the dimension of the output embeddings across words depending on how much model capacity is deemed suitable for a given word. In particular, it is meaningful to assign more parameters to frequent words than to rare words. By definition, frequent words occur more often in the training data than rare words and therefore allow to fit more parameters. They define partitions of the output vocabulary based on word frequency and the words in each partition share the same embedding size. Partitioning results in a sparse final weight matrix which arranges the embeddings of the output words in blocks, each one corresponding to a separate partition. The size of the final hidden layer $H$ is the sum of the embedding sizes of the partitions. While this method does not involve creation of multiple factors, it factorizes the original matrix into multiple blocks while setting the remaining part of the matrix to 0.\nVariani et al.~ propose a method called Word Encoded Sequence Transducers (WEST) which factorizes a matrix $E=C\\times D$ where $D$ is constrained to be a block diagonal matrix. The block diagonal nature of the second factor leads to large compression rates.", "cites": [4526, 8800], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes two methods for matrix factorization in the context of model compression but does so primarily through factual summary rather than deeper analysis or synthesis. It makes a basic connection between the two methods in terms of their use of block structures, but lacks critical evaluation or broader abstraction to highlight underlying principles or trends in the field."}}
{"id": "4c1e5567-aabc-47e9-8344-624bb81f275f", "title": "Tensor Train and Block Term Decomposition", "level": "subsection", "subsections": [], "parent_id": "a059247f-141a-43b7-a6a9-f3440e7622cd", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Tensor decomposition"], ["subsection", "Tensor Train and Block Term Decomposition"]], "content": "Tensor train decomposition (TTD)~ is a standard tensor decomposition technique which decomposes a high dimensional tensor into multiple 2D and 3D tensors which can be multiplied together to reconstruct the original tensor. These factors are called TT-cores and the other dimensions are referred to as TT-ranks. TTD can be leveraged to compress various weight matrices in RNNs and LSTMs~. The first step is to represent a matrix as a multi-dimensional tensor by simple reshaping transformation and then use TTD on it. The values of TT–ranks directly define the compression ratio, so choosing them to be too small or too large will result into either significant performance drop or little reduction of the number of parameters. Typically TT-ranks around 16 for small matrices and 64-192 for larger matrices result in a good trade-off between compression ratio and the accuracy metric of interest. Also, when we use TTD for weight matrices, we also need change the inputs appropriately to be compatible in terms of dimensions. \nCompared with TT-RNN, Block-Term RNN (BTRNN)~ is not only more concise (when using the same rank), but also able to attain a better approximation to the original RNNs with much fewer parameters. BTD decomposes a high order tensor into a sum of multiple Tucker decomposition models. The redundant dense connections between input and hidden state is first tensorized to a $d$-dimensional tensor and then decomposed using low-rank BTD into a sum of $N$ different Tucker decompositions where $N$ is the CP-rank. Each Tucker decomposition in turn consists of a core $d$-dimensional tensor and $d$ 3-dimensional factor tensors. While Ye et al.~ used BTD to compress RNNs, Ma et al.~ used BTD to compress the self-attention matrix in Transformers. They first build a single-block attention based on the Tucker decomposition where the query, key and value are mapped into three factor matrices and the core tensor is trainable and randomly initialized. It is then straightforward to represent the multi-head attention using BTD.", "cites": [4529, 4527, 4530, 4528], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information across four cited papers, connecting Tensor Train and Block-Term decomposition methods within the context of model compression for RNNs and Transformers. It provides a critical comparison between TT-RNN and BTRNN, noting the latter’s conciseness and better approximation. The abstraction is moderate, as it generalizes these methods to the broader goal of parameter reduction but does not reach meta-level insights or novel conceptual frameworks."}}
{"id": "2c213350-b4bf-49d2-a046-a4fe16413654", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "a059247f-141a-43b7-a6a9-f3440e7622cd", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Tensor decomposition"], ["subsection", "Summary"]], "content": "\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|l|l|l|l|l|l|l|}\n    \\hline\nTask&Dataset&Method&Base Model&Metric&Size (Comp; Orig)&Eval. (Comp; Orig)\n\\\\\n\\hline\n\\hline\nCTR prediction&Criteo CTR Challenge&TT-embedding~&MLP&LogLoss (L)&4.7M; 41.2M&0.4433; 0.4440\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&BTD~&Transformer-XL Large&Perplexity (L)&0.16B; 0.8B&19.5; 21.8\\\\\n\\hline\nLanguage modeling&Enwiki-8&FLOP~&Transformer&BPC (L)&8M; 41M&1.13; 1.08\\\\\n\\hline\nLanguage modeling&PTB&WEST~&LSTM&Perplexity (L)&3.5M; 4.51M&116.84; 115.91\\\\\n\\hline\nLanguage modeling&PTB&BTD~&Transformer-XL-Base&Perplexity (L)&12M; 24M&49.8; 54.52\\\\\n\\hline\nLanguage modeling&PTB&TT-embedding~&Transformer-XL-Base&Perplexity (L)&18M; 24M&55.4; 54.52\\\\\n\\hline\nLanguage modeling&WikiText-103&FLOP~&Transformer&Perplexity (L)&50M; 151M&25.3; 24.1\\\\\n\\hline\nLanguage modeling&WikiText-103&TT-embedding~&Transformer-XL&Perplexity (L)&91M; 192M&25.67; 24.37\\\\\n\\hline\nLanguage modeling&WikiText-103&BTD~&Transformer-XL-Base&Perplexity (L)&85.3M; 151M&20.9; 24.0\\\\\n\\hline\nLanguage modeling&WikiText-103&TT-embedding~&Transformer-XL-Base&Perplexity (L)&130M; 151M&25.7; 24.0\\\\\n\\hline\nNMT (en$\\rightarrow$ja)&ASPEC&Compositional codes~&LSTM&BLEU (H)&2.97MB; 274MB&38.89; 37.93\\\\\n\\hline\nNMT (de$\\rightarrow$en)&IWSLT14&Compositional codes~&LSTM&BLEU (H)&2.11MB; 35MB&29.56; 29.45\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&TT-embedding~&Transformer-Big&BLEU (H)&179M; 210M&28.53; 28.84\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT16&BTD~&Transformer&BLEU (H)&21.2M; 52M&34.91; 34.5\\\\\n\\hline\nNP bracketing&Subset of PTB&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&82.3; 77.9\\\\\n\\hline\nQuestion classification&TREC Questions&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&81.5; 76.2\\\\\n\\hline\nSentiment analysis&IMDB&Compositional codes~&LSTM&Acc (H)&1.23MB; 78MB&87.37; 87.18\\\\\n\\hline\nSentiment analysis&IMDB&TT-embedding~&LSTM&Acc (H)&0.81M; 7.19M&89.7; 88.6\\\\\n\\hline\nSentiment analysis&SST&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&81.4; 77.7\\\\\n\\hline\nSpeech recognition&3M Google voice utterances&Joint-SVD~&5-layer RNN&WER (L)&3.1M; 9.7M&12.9; 12.4\\\\\n\\hline\nSpeech recognition&3M Google voice utterances&Projections~&LSTM&WER (L)&2M; 2.2M&14.8; 17.5\\\\\n\\hline\nSpeech recognition&Live traffic utterances&WEST~&3-layer LSTM&WER (L)&4.75MB; 15MB&13.6; 13.7\\\\\n\\hline\nSpeech recognition&Live traffic utterances&WEST+Quantization~&3-layer LSTM&WER (L)&1.35MB; 15MB&13.7; 13.7\\\\\n\\hline\nText classification&20 Newsgroup (Computers)&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&87.0; 79.7\\\\\n\\hline\nText classification&20 Newsgroup (Religion)&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&88.8; 86.7\\\\\n\\hline\nText classification&20 Newsgroup (Sports)&Sparse coding~&Logistic regression&Acc (H)&120M; 120M&96.3; 95.9\\\\\n\\hline\nWord similarity&Simlex-999&Sparse coding~&Logistic regression&Correlation (H)&120M; 120M&38.9; 36.9\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various tensor decomposition methods (sorted by Task and then Dataset). In the metric column, H means high is better while L means low is better. For compositional codes, 16x32 coding was used. For BTD, two block term tensors were used. In~, logistic regression uses GloVe embeddings.}\n    \\label{tab:tensorDecompSummary}\n\\end{table}\nTable~\\ref{tab:tensorDecompSummary} compares various tensor decomposition methods across different tasks and datasets. Accuracy of both the original and the compressed (comp.) model are shown. Also, we report model size (in terms of number of parameters or memory size) for both the student as well as the teacher models. For the same task, dataset and model combination, different papers report different accuracy of the original model because of slight changes in training hyper-parameters; hence we report accuracy of the original model for each row. \nFor CTR prediction, the TT-embedding method~ in Table~\\ref{tab:tensorDecompSummary} uses 3 factors with TT-rank of 16. It actually leads to an embedding compression of 16. With 4 factors and TT-rank=2, test loss increases to 0.4530 with a massive embedding compression of 4193 and overall model size of 0.53M. Thus, substitution of large embedding layers with TT–embeddings leads to significant compression ratios (up to 2011 times) with a slight improvement in the test loss, and up to 4200 with a small drop in the test loss. \nFor language modeling, BTD~ leads to an improved model with 20\\% of the Transformer-XL large model. For character level language modeling using FLOP~, an 8M sized FLOP model achieves 1.13 on Enwiki8 while gradual pruning achieves 1.14. Thus, FLOP based pruning which combines pruning with matrix factorization is better than both structured neuron pruning as well as gradual unstructured pruning. On PTB, BTD is clearly much better than TT-embedding. With half the model size compared to Transformer-XL-Base, BTD leads to a model with $\\sim$10\\% lower perplexity. On WikiText-103, while FLOP pruned model achieves 25.3 perplexity with model size of 50M, gradual unstructured pruning and neuron pruning achieve 25.7 and 26.7 respectively with the same model size for language modeling on Wiki-103 dataset. Thus, FLOP is better than other pruning methods. Again on Wiki-103, BTD is superior to TT-embedding.    \nFor NMT, the loss-free compression rate reaches 92\\% on ASPEC dataset by pruning 90\\% of the connections. However, with the same pruning ratio, a modest performance loss is observed in IWSLT14 dataset. For the models using compositional coding~, the loss-free compression rate is 94\\% for the IWSLT14 dataset and 99\\% for the ASPEC dataset. Thus, compositional codes are much better compared to pruning. TT-embedding with TT-rank=64 leads to embedding compression of 15.3 on WMT14 dataset with marginal loss in the BLEU score. Lastly, BTD has been used to reduce Transformer size by more than half with improved BLEU on WMT16 (en$\\rightarrow$de) data.\nSparse coding for GloVe vectors~ has led to improved accuracies for multiple tasks like Noun Phrase (NP) bracketing, question classification, text classification and word similarity, while retaining the same model size. For sentiment analysis on the IMDB dataset, compositional codes method achieves a compression rate of 98\\% without performance loss. Further, TT-embedding method leads to a much smaller model compared to compositional codes with better accuracy. In the TT-embedding case, embedding compression rate is 441  with TT-rank=16.\nFor Speech recognition, Prabhavalkar et al.~ experimented with a 3M Google voice utterances dataset and found that a joint SVD with explained variance retained after the SVD as 0.6 leads to a 3x smaller RNN model without much performance loss. They improved upon Sak et al.'s projections method which led to a much higher WER on the same dataset. On live traffic utterances dataset, WEST~ leads to a 3x smaller model with a slightly reduced word error rate when using 3-layer LSTMs. Variani et al.~ further compress this model using quantization to obtain a 11x compressed 3-layer LSTM model without any performance loss. Thus, using quantization along with matrix decomposition seems to work well.\nOverall, matrix decomposition techniques are usually used in combination with parameter sharing and sometimes with quantization. They have been very effective in dealing with large input/output embedding matrices in RNNs and LSTMs. SVD, Tensor Train, CP, Tucker, BTD have been the most popular decomposition techniques found to be useful for model compression.", "cites": [4527, 4530, 1568, 4515, 4524, 4525, 4526], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple tensor decomposition papers effectively by comparing them across different tasks, datasets, and metrics. It shows critical analysis by evaluating the relative performance of methods like TT-embedding, BTD, and FLOP. However, it remains largely task-specific and does not fully abstract to broader principles or theoretical insights."}}
{"id": "65404f84-cbf0-4721-8f05-178f8230aa55", "title": "Transformers with Sub-Quadratic Complexity", "level": "section", "subsections": ["4db79252-def3-4c73-83ab-c7863b1703fc", "f00ad6f8-f45a-4b14-ab1c-135e291338eb", "2202feab-4da3-4fb5-8d59-2d1d4fbac3f9"], "parent_id": "3efda6e0-cf0a-4f26-8cb1-3dde0497c32c", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Transformers with Sub-Quadratic Complexity"]], "content": "\\label{sec:linearTransformers}\nMemory usage throughout neural network training can be categorized into three main types: (1) Model memory is used to store model parameters; (2) Optimizer memory is the additional memory used by the specific learning algorithm during the process; (3) Activation memory consists of the outputs of each layer, which are cached for reuse in backpropagation to compute gradients. For a BERT-base model, model memory is around 0.2 GB, optimizer memory is around 1GB, while activation memory is around 8.5GB~. Time and activation memory in Transformers grows quadratically with the sequence length. This is because in every layer, every attention head attempts to come up with a transformed representation for every position by ``paying attention'' to tokens at every other position. Quadratic complexity implies that practically the maximum input size is rather limited. Thus, we cannot extract semantic representation for long documents by passing them as input to Transformers.", "cites": [4531], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the memory components in Transformer models and mentions the quadratic complexity issue without deeply synthesizing or connecting the ideas from the cited paper. It lacks critical evaluation or comparison of different methods and only briefly hints at the general problem of handling long documents, without abstracting broader principles or trends in sub-quadratic complexity approaches."}}
{"id": "4db79252-def3-4c73-83ab-c7863b1703fc", "title": "Transformers with Super-Linear Complexity", "level": "subsection", "subsections": [], "parent_id": "65404f84-cbf0-4721-8f05-178f8230aa55", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Transformers with Sub-Quadratic Complexity"], ["subsection", "Transformers with Super-Linear Complexity"]], "content": "A few efforts like BlockBERT~ try to reduce this quadratic complexity by a constant factor by introducing sparse block structures into the attention matrix. If we split the length-$N$ input sequence into $n$ blocks, $N\\times N$ attention matrix gets partitioned into $n\\times n$ blocks, where each block matrix is of the size $\\frac{N}{n}\\times\\frac{N}{n}$. Thus, BlockBERT reduces $O(N^2)$ memory consumption by a factor of $n$. \nChild et al.~ propose sparse transformers where sparse factorizations of the attention matrix reduce the quadratic complexity  to $O(n\\sqrt{n})$.  The key idea is to reduce the dense attention matrix to a sparse version by only computing attention on a sparse number of (query, key) pairs. They propose two kinds of sparse factorizations: strided and fixed. Strided attention implies having one head attend to the previous $l$ locations, and the other head attend to every $l^{th}$ location, where $l$ is the stride and chosen to be close to $\\sqrt{n}$. More heads could be used with a different stride value. Fixed attention assumes that specific positions summarize previous locations and propagate that information to all future positions. \nThe Reformer architecture~ replaces the dot-product attention in a typical Transformer by one that uses locality-sensitive hashing (LSH), changing its complexity from $O(n^2)$ to $O(n\\log n)$, where $n$ is the length of the sequence. In a standard Transformer, we compute scaled dot-product attention as follows.\n\\begin{eqnarray}\n  \\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n\\end{eqnarray}\nwhere $Q$, $K$ and $V$ are the standard query, key and value components and $d$ is a scaling factor. Reformer uses a Shared QK Transformer, i.e., $Q=K$ enabled by sharing the matrix that projects words/hidden layer to $Q$ or $K$. Further, note that we are actually only interested in $\\text{softmax}(QK^T)$. Since softmax is dominated by the largest elements, for each query $q_i$ we only need to focus on the keys in $K$ that are closest to $q_i$. How can we find the nearest neighbors among the keys? Reformer uses  LSH. LSH is used to cluster (hash-bucket) the positions into various groups, and then every position needs to focus only on others within the same bucket.", "cites": [4531, 793, 794], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key concepts from BlockBERT, Sparse Transformers, and Reformer, integrating them into a cohesive discussion on reducing Transformer complexity. It abstracts the idea of attention matrix sparsification and offers a general explanation of how each method achieves efficiency. However, it lacks deeper critical analysis of trade-offs, limitations, or comparative strengths/weaknesses between these approaches."}}
{"id": "f00ad6f8-f45a-4b14-ab1c-135e291338eb", "title": "Transformers with Linear Complexity", "level": "subsection", "subsections": [], "parent_id": "65404f84-cbf0-4721-8f05-178f8230aa55", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Transformers with Sub-Quadratic Complexity"], ["subsection", "Transformers with Linear Complexity"]], "content": "Even better, there have been several efforts recently to reduce this quadratic complexity to linear. Most of these efforts choose a constant number of other positions to ``pay attention'' to so as to compute a transformed representation for any given position. They can model sequences tens of thousands of timesteps long using hundreds of layers. The methods differ in their approach towards selecting this constant number of other positions. We discuss a few of such recently proposed methods in this section.\nIn Star-Transformers~, to reduce model complexity from $O(n^2)$ to linear, we replace the fully-connected attention matrix structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. While ring connections connect a satellite node with two other satellite nodes, a radical connection connects a satellite node with the relay node. The idea is to update the star-center relay node based on satellite nodes and then update satellite nodes using information from the star node, and adjacent satellite nodes. \nLinformer architecture~ exploits low-rank factorization of the self-attention matrix to reduce overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The main idea is to add two linear projection matrices $E_i, F_i\\in R^{n\\times k}$ when computing key and value. We first project the original $(n\\times d)$-dimensional key and value layers into $(k\\times d)$-dimensional projected key and value layers. We then compute an $(n \\times k)$-dimensional context mapping matrix using scaled dot-product attention. If we can choose a very small projected dimension $k$, such that $k<<n$, then we can significantly reduce the memory and space consumption. Overall, it is $O(nk)$. Further, we can do three other forms of parameter sharing: (1) Headwise sharing: $E_i=E$ and $F_i=F$ across all heads $i$ in a  layer. (2) Key-value sharing: $E_i=F_i=E$ across all heads $i$ in a layer. (3) Layerwise sharing: Single projection matrix $E$ is used across all layers, all heads for both key and value.\nSparse Sinkhorn Attention based Transformer~ is based on differentiable sorting of internal representations. First, they divide the input sequence into $B$ equal sized blocks each of size $n/B$. A meta sorting network learns to generate latent permutations over these block sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. They also propose Causal Sinkhorn Balancing and SortCut algorithms for causal scenarios for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Their method reduces the memory complexity from $O(n^2)$ to $O(B^2+(n/B)^2)$. The SortCut variant further reduces complexity to linear-time, i.e., $O(nk)$ where $k$ is a user defined budget hyper-parameter much smaller than $n$.\nShen et al.~ propose a very simple mathematical trick to reduce quadratic complexity to linear. A typical dot-product attention can be written as $\\text{softmax}(QK^T)V$ ignoring the scale factor. This is quadratic because $QK^T$ is $n^2$ in size. This can be rewritten as follows.\n\\begin{eqnarray}\n   \\text{Attention}(Q,K,V)=\\text{softmax}_r(Q)(\\text{softmax}_c(K^TV))\n\\end{eqnarray}\nwhere $\\text{softmax}_r$ and $\\text{softmax}_c$ are softmax applied to rows and columns respectively. This revised formulation has terms which are only linear in $n$. Finally, Katharopoulos et al.~ express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $O(n^2)$ to $O(n)$, where $n$ is the sequence length.\nFinally, Longformer~ propose to reduce Transformer complexity to linear by sparsifying the full self-attention matrix using multiple kinds of attention patterns. The simplest attention pattern is the sliding window pattern which employs a fixed-size window attention surrounding each token. Given a fixed window size $w$, each token attends to $0.5w$ tokens on each side. The computation complexity of this pattern is $O(nw)$ which scales linearly with input sequence length $n$. To further increase the receptive field without increasing computation, the sliding window can be ``dilated''. This is analogous to dilated CNNs where the window has gaps of size dilation $d$. The windowed and dilated attention are not flexible enough to learn task-specific representations. Accordingly, Longformer also has ``global attention'' on few pre-selected input locations. Moreover, this attention operation is symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. Further, two different sets of projections are used: $Q_s$, $K_s$, $V_s$ to compute attention scores of sliding window attention, and $Q_g$, $K_g$, $V_g$ to compute attention scores for the global attention. The pretrained Longformer consistently outperforms RoBERTa on multiple downstream long document tasks.", "cites": [7333, 7298, 798, 7371, 1473, 7827], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers effectively, presenting a coherent narrative about methods that reduce Transformer complexity to linear. It identifies common themes like attention sparsification and low-rank approximations. However, while it touches on limitations (e.g., fixed window sizes in Longformer), the critical analysis is limited. Some abstraction is present, but it mainly operates at the methodological level rather than offering higher-level principles."}}
{"id": "2202feab-4da3-4fb5-8d59-2d1d4fbac3f9", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "65404f84-cbf0-4721-8f05-178f8230aa55", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Transformers with Sub-Quadratic Complexity"], ["subsection", "Summary"]], "content": "\\begin{table}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|l|l|l|l|l|l|}\n    \\hline\nTask&Dataset&Model&Base Model&Metric&Eval. (Opt.; Orig)\\\\\n\\hline\n\\hline\nChar-level language modeling&1B Word Benchmark&Sinkhorn Mixture Big~&Transformer Big&BPC (L)&1.134; 1.825\\\\\n\\hline\nChar-level language modeling&1B Word Benchmark&Sparse Transformer~&Transformer Big&BPC (L)&1.119; 1.825\\\\\n\\hline\nChar-level language modeling&Enwik8&Longformer~&Transformer&BPC (L)&1.00; 1.11\\\\\n\\hline\nChar-level language modeling&Enwik8&Reformer~&Transformer&BPC (L)&1.05; 1.11\\\\\n\\hline\nChar-level language modeling&text8&Longformer~&Transformer&BPC (L)&1.10; 1.18\\\\\n\\hline\nCoreference resolution&OntoNotes&Longformer-base~&RoBERTa-base&F1 (H)&78.6; 78.4\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&Sinkhorn Mixture Big~&Transformer Big&Perplexity (L)&27.34; 27.59\\\\\n\\hline\nLanguage modeling&1B Word Benchmark&Sparse Transformer~&Transformer Big&Perplexity (L)&28.77; 27.59\\\\\n\\hline\nNamed Entity Recognition&CoNLL2003&Star Transformer~&Transformer&Acc (H)&90.93; 86.48\\\\\n\\hline\nNamed Entity Recognition&CoNLL2012&Star Transformer~&Transformer&Acc (H)&86.30; 83.57\\\\\n\\hline\nNLI&MNLI&SortCut Sinkhorn~&Transformer&Acc (H)&55.80; 53.69\\\\\n\\hline\nNLI&QNLI&Linformer~&BERT-base&Acc (H)&91.2; 91.8\\\\\n\\hline\nNLI&SNLI&Star Transformer~&Transformer&Acc (H)&86.0; 82.2\\\\\n\\hline\nNLI&SNLI&SortCut Sinkhorn~&Transformer&Acc (H)&80.30; 78.87\\\\\n\\hline\nNMT (en$\\rightarrow$de)&WMT14&Reformer big~&Transformer Big&BLEU (H)&29.1; 27.3\\\\\n\\hline\nPOS Tagging&PTB&Star Transformer~&Transformer&Acc (H)&97.14; 96.31\\\\\n\\hline\nQuestion answering&HotpotQA&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&78.94; 77.08\\\\\n\\hline\nQuestion answering&HotpotQA&SparseBERT~&BERT&F1 (H)&76.02; 77.08\\\\\n\\hline\nQuestion answering&NaturalQA&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&79.39; 78.29\\\\\n\\hline\nQuestion answering&NaturalQA&SparseBERT~&BERT&F1 (H)&77.31; 78.29\\\\\n\\hline\nQuestion answering&NewsQA&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&70.08; 66.25\\\\\n\\hline\nQuestion answering&NewsQA&SparseBERT~&BERT&F1 (H)&67.16; 66.25\\\\\n\\hline\nQuestion answering&SearchQA&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&83.51; 80.37\\\\\n\\hline\nQuestion answering&SearchQA&SparseBERT~&BERT&F1 (H)&80.54; 80.37\\\\\n\\hline\nQuestion answering&SQuAD 1.1&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&90.74; 88.45\\\\\n\\hline\nQuestion answering&SQuAD 1.1&SparseBERT~&BERT&F1 (H)&88.37; 88.45\\\\\n\\hline\nQuestion answering&SQuAD 2.0&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&81.45; 77.16\\\\\n\\hline\nQuestion answering&SQuAD 2.0&SparseBERT~&BERT&F1 (H)&77.57; 77.16\\\\\n\\hline\nQuestion answering&TriviaQA&BlockBERT (n=2, N=1024)~&BERT&F1 (H)&79.41; 75.35\\\\\n\\hline\nQuestion answering&TriviaQA&SparseBERT~&BERT&F1 (H)&75.34; 75.35\\\\\n\\hline\nQuestion answering&TriviaQA&Longformer-base~&RoBERTa-base&F1 (H)&75.2; 74.3\\\\\n\\hline\nQuestion answering&WikiHop&Longformer-base~&RoBERTa-base&Acc (H)&75.0; 72.4\\\\\n\\hline\nSentiment analysis&IMDB&Linformer~&BERT-base&Acc (H)&94.1; 93.5\\\\\n\\hline\nSentiment analysis&IMDB&Longformer-base~&RoBERTa-base&Acc (H)&95.7; 95.3\\\\\n\\hline\nSentiment analysis&SST&Star Transformer~&Transformer&Acc (H)&52.9; 50.4\\\\\n\\hline\nSentiment analysis&SST&Sinkhorn ~&Transformer&Acc (H)&77.52; 76.83\\\\\n\\hline\nSentiment analysis&SST-2&Linformer~&BERT-base&Acc (H)&93.1; 92.7\\\\\n\\hline\nSpeech recognition&WSJ&Linear Transformer~&Reformer&Phoneme Error Rate (L)&8.08; 9.33\\\\\n\\hline\nText classification&Hyperpartisan&Longformer-base~&RoBERTa-base&F1 (H)&94.8; 87.4\\\\\n\\hline\nText classification&MTL-16&Star Transformer~&Transformer&Acc (H)&86.98; 82.78\\\\\n\\hline\nTextual similarity&QQP&Linformer~&BERT-base&Acc (H)&90.8; 89.6\\\\\n\\hline\n    \\end{tabular}\n\\caption{Comparison of various sub-quadratic complexity Transformer methods (sorted by Task and then Dataset). In the metric column, H means high is better while L means low is better. Note that in this case, model sizes do not reduce much; activation memory reduces as described in Section~\\ref{sec:linearTransformers} (with comparable or better accuracy) compared to the standard Transformer.}\n    \\label{tab:linearTransSummary}\n\\end{table}\nTable~\\ref{tab:linearTransSummary} compares various sub-quadratic Transformer methods across different tasks and datasets. Accuracy of both the original Transformer and the optimized (opt.) model are shown. These models have been applied for various applications like language modeling (both word level as well as character level), coreference resolution, NER, NLI, NMT, POS, question answering, sentiment analysis, speech recognition, text classification and text similarity analysis. For the same task, dataset and model combination, different papers report different accuracy of the original model because of slight changes in training hyper-parameters; hence we report accuracy of the original model for each row. \nStar Transformers were shown to outperform the vanilla Transformer model across various tasks like Sentiment analysis, Text classification, NLI, POS and NER. On the SST, the Star Transformer achieves 2.5 points improvement against the standard Transformer. On MTL-16, the Star-Transformer outperform the standard Transformer in all 16 datasets, the improvement of the average accuracy is 4.2. Average test times are 10.94 and 49.31ms per batch with batch-size=128 for Star Transformer and standard Transformer respectively.\nLongformer achieves a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively. Longformer-large model of the same size as Sparse Transformer achieves a BPC of 0.99 on Enwik8, which is the same as that obtained using Sparse Transformer. Also, for both character-level as well as word-level language modeling, on 1B word benchmark, we observe that Sinkhorn mixture (which is a combination of the Sinkhorn attention by mixing it with the vanilla standard dot product attention) performs better than Sparse Transformer. \nOn question answering, results have been reported across multiple datasets like HotpotQA, NaturalQA, NewsQA, SearchQA, TriviaQA, WikiHop, SQuAD 1.1 and SQuAD 2.0. We observe that BlockBERT performs better than SparseBERT. Also, it is not surprising that BlockBERT with 2 blocks (n = 2) performs better than that with 3 blocks (n = 3), because it keeps more attention matrix entries. Also, not shown in the table, Longformer-large achieves scores of 81.9 and 73.2 for WikiHop and HotpotQA beating state-of-the-art results by 3.6 and 4 points respectively. \nLongformer consistently outperforms the RoBERTa baseline across many tasks like coreference resolution, question answering, sentiment analysis and text classification. Its performance gain is especially obvious for tasks that require long context such as WikiHop (question answering) and Hyperpartisan (text classification). For TriviaQA (question answering), the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA (question answering), the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. On the IMDB (sentiment analysis) and OntoNotes (coreference resolution) datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions.\nOn speech recognition task, Linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences. The linear Transformer model outperforms the LSTM and Reformer while being faster to train and evaluate. Reformer takes 2250 seconds per epoch, while Linear Transformers take just 824s/epoch.\nTo summarize, multiple methods have been proposed to reduce the quadratic complexity of the standard Transformer model. While Sparse Transformers reduce it to $O(n\\sqrt{n})$, Reformers reduce it to $O(n\\log n)$. Other methods like Star Transformer, Linformer, Sparse Sinkhorn Transformer, Efficient Attention, Linear Transformers and Longformer promise linear complexity. In particular Sparse Sinkhorn Transformer and Longformer have been shown to result into very good accuracy, latency and RAM tradeoff across many tasks.", "cites": [7333, 4531, 7298, 793, 798, 7371, 794, 1473], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured comparison of sub-quadratic complexity Transformer methods across multiple NLP tasks and datasets, drawing on several cited papers. It synthesizes results and integrates some insights, such as the trade-off between the number of blocks and performance in BlockBERT. However, it lacks deeper critical analysis or discussion of broader principles that could elevate the insight level."}}
{"id": "ab02f8bb-9d54-4f84-82fc-3f831c1bc39a", "title": "Comparison across model compression method types.", "level": "subsection", "subsections": [], "parent_id": "d06ca9cc-bcd7-4f53-9be8-ee86f8754664", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Summary and Future Directions"], ["subsection", "Comparison across model compression method types."]], "content": "In the previous six sections, we have compared across multiple methods within each of the six broad types of model compression. In this subsection, we attempt to compare across multiple model compression method types.\nTable~\\ref{tab:kdSummary2} provides a comparison of model size versus accuracy for various methods across various GLUE~ and SQuAD tasks. We observe that most of the methods tried on GLUE datasets have been based on knowledge distillation. However, some pruning methods (iterative magnitude pruning, RPP and LayerDrop), quantization (mixed precision QBERT), parameter sharing (ALBERT), tensor decomposition (FLOP) and linear Transformer (Linformer) have also been tried. Quantization methods do not reduce the number of parameters but reduce the number of bits per parameter. Particularly, mixed precision QBERT uses 8 bits for embeddings and 2/3/4 bits for encoder layer weights. Similarly, Linformer does not reduce number of weights but reduces activation memory as well as latency of the overall Transformer model. \nFor the remaining models, for each model, we first computed an average GLUE score based on any of the 9 tasks for which scores have been reported. Next, we computed the ratio GLUE score/model size (in Millions). We find the following as the top three \n\\begin{itemize}\n    \\item Distilled-BiLSTM~ (ratio=79.3)\n    \\item Mixed-vocab. training~ (ratio=7.8)\n    \\item ALBERT-B~ (ratio=7.2)\n\\end{itemize}\nThis clearly tells us that Distilled BiLSTMs provide us the best accuracy versus size tradeoff on GLUE. However, each of these three models actually report results on only 4 out of 9 GLUE tasks. Hence, further, we considered only those methods for which results on at least 5 tasks have been reported and computed the GLUE score/model size ratio. We find the following as the top three \n\\begin{itemize}\n\\item TinyBERT~ (ratio=5.31)\n\\item BERT-EMD~ (ratio=5.15)\n\\item MobileBERT+Quantization~ (ratio=3.49)\n\\end{itemize}\nThus, on GLUE tasks, it is clear that distillation based methods (combined with quantization) are better than other types of methods. \nTables~\\ref{tab:pruningSummary},~\\ref{tab:quantizationSummary},~\\ref{tab:kdSummary1},~\\ref{tab:paramSharingSummary},~\\ref{tab:tensorDecompSummary} and~\\ref{tab:linearTransSummary} compare various pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition and sub-quadratic Transformer methods across different tasks and datasets. Overall, there are 123 (task, dataset) combinations across these six tables which implies that unlike GLUE, not many methods have been applied on the same set of (task, dataset) combinations\\footnote{We make the entire statistics available as an excel file at \\url{https://bit.ly/3vmaxZ9}.}. The most popular tasks are language modeling (on PTB and 1B word benchmark), sentiment analysis (on SST) and NMT (WMT14 en$\\rightarrow$de). \nFor language modeling on PTB, on LSTM models, bank balanced sparsity~ based pruning method worked best. With Transformer models, Block Term Decomposition (BTD)~ method seems to work best. Among various methods like parameter sharing, tensor decomposition and sub-quadratic complexity Transformer which have been tried for language modeling on 1B Word Benchmark, again, BTD~ method seems to work best leading to a model with 0.16B parameters and a perplexity as low as 19.5. Multiple datasets have been used for Neural machine translation (NMT). Datasets from WMT and IWSLT are the most popular. Among these, the en$\\rightarrow$de from WMT14 is the most popular dataset used for testing various NMT models. For en$\\rightarrow$de NMT with WMT14, using 2-layer LSTMs, the best accuracy versus size tradeoff is using Pruned Seq-KD + Seq-Inter~ which gives a 8M size model leading to 18.5 BLEU. Among Transformer based models, BTD~ leads to a Transformer model which provides 34.91 BLEU with 21.2M model size.\n\\begin{table*}\n    \\centering\n    \\scriptsize\n    \\begin{tabular}{|p{1in}|p{2in}|p{2.7in}|}\n    \\hline\nTask&Popular Datasets&References\\\\\n\\hline\n\\hline\nLanguage modeling&Penn TreeBank Corpus, One billion word benchmark, Europarl, WikiText-103, text8, source code of Linux kernel, 2013 ACL Workshop Morphological Language Datasets (ACLW), Arabic news commentary corpus, 2013 ACL workshop on MT, enwik8 (from Wikipedia), Lambada&Neuron Pruning~, Iterative magnitude pruning~, Block sparsity~,Loss -Aware Quantization~, Uniform Quantization~, Binary Quantization~, HitNet~, Sparse Word Representations~, LightRNN~, Slim Embeddings~, C2W~, LayerDrop~, Reformer~, Linformer~, Char-CNN~, CNN+Highway Network~, SparseBERT~, FLOP~, Deep Equilibrium Models~, WEST~, Sparse Sinkhorn Attention~, BTD~, Universal Transformers~, TT-embedding~, multiple methods~\\\\\n\\hline\nNeural Machine translation (NMT)&IWSLT German-English, IWSLT Thai-English, ASPEC English-Japanese, WMT English-German, WMT German-English, WMT English-Russian, IWSLT English Vietnamese, WMT English-Romanian, WMT  English-Estonian, Ted Talk&Compositional codes~, LayerDrop~, Pruning attention heads~, Neuron Pruning~, Magnitude Pruning~, Iterative magnitude pruning~, Pruned Seq-KD + Seq-Inter~, Quantized Distillation~, Teacher ensembles KD~, Multiple teachers KD~, BTD~, Quaternion Attention~, Universal Transformers~, TT-embedding~\\\\\n\\hline\nSentiment Analysis&IMDB movie review, SST, SST-2, Elec (electronic product reviews)&Compositional codes~, Star Transformer~, TinyBERT~, MiniLM~, Linformer~, XtremeDistil~, Gaussian Quantization~, Uniform Quantization~, Sparse coding~, Quaternion Attention~, Sparse Sinkhorn Attention~, RPP~, ALBERT~, Patient KD~, Mixed-vocabulary KD training~, Distilled-BiLSTM~, MTDNN~, TT-embedding~\\\\\n\\hline\nQuestion Answering&SQuAD1.1, SQuAD2.0, ELI5, SemEval, BABI&LayerDrop~, MiniLM~, RPP~, BS-Fixed Quantization~, ALBERT~, KD~, Universal Transformers~\\\\\n\\hline\nNatural Language Inference&SNLI, MNLI-m, MNLI-mm, QNLI, RTE, WNLI, XNLI&Star Transformer~, LayerDrop~, TinyBERT~, MiniLM~, Linformer~, Sparse Sinkhorn Attention~, RPP~, ALBERT~, Patient KD~, Mixed-vocabulary KD training~, Distilled-BiLSTM~, MTDNN~\\\\\n\\hline\nParaphrasing&QQP, STS-B&TinyBERT~, MiniLM~, Linformer~, RPP~, ALBERT~, Patient KD~, Distilled-BiLSTM~, MTDNN~\\\\\n\\hline\nImage captioning&MSCOCO&Grow and Prune~, Magnitude Pruning~, Iterative Magnitude Pruning and Densification~\\\\\n\\hline\nHandwritten character recognition&ICDAR&SVD and Pruning\\\\\n\\hline\nPart-of-speech (POS) tagging&Wall Street Journal of the Penn Treebank dataset, WikiAnn NER corpus&C2W~,  XtremeDistil~\\\\\n\\hline\nSummarization&CNN-DailyMail, XSum&LayerDrop~, MiniLM~\\\\\n\\hline\nMachine Reading Comprehension& Microsoft Research Paraphrase Corpus (MRPC), ReAding Comprehension from Examinations (RACE)&LayerDrop~, TinyBERT~, MiniLM~, RPP~, ALBERT~, Patient KD~, Mixed-vocabulary KD training~, MTDNN~\\\\\n\\hline\nLinguistic Acceptability&CoLA&TinyBERT~, MiniLM~, RPP~, ALBERT~, MTDNN~\\\\\n\\hline\nTopic Classification&DbPedia, Ag News,20 Newsgroup&XtremeDistil~, Sparse coding~\\\\\n\\hline\nQuestion Type Classification&TREC&Sparse coding~\\\\\n\\hline\nNoun Phrase Bracketing&Lazaridou~&Sparse coding~\\\\\n\\hline\nWord Similarity&SimLex-999, MEN, MTurk, RARE, SCWS, WSR, WSS&Sparse coding~, Shared reference vectors~\\\\\n\\hline\nMathematical Language Understanding&Wangperawong's MLU~&Quaternion Attention~\\\\\n\\hline\nSubject Verb Agreement&Linzen~&Quaternion Attention~, Universal Transformers~\\\\\n\\hline\nWord Analogy&GSEM, GSYN, MSYN&Shared reference vectors~\\\\\n\\hline\nSentence Completion&MSC&Shared reference vectors~\\\\\n\\hline\nLearning to execute&Zaremba and Sutskever~&Universal Transformers~\\\\\n\\hline\nAd Click Through Rate Prediction&Criteo Kaggle&TT-embedding~\\\\\n\\hline\nSpeech Recognition&2100 hours English Speech, AN4, Switchboard, TIMIT, WSJ 92, WSJ 93, TIDIGITS, 3M Google voice utterances, Live traffic utterances&Iterative Magnitude Pruning~, Grow and Prune~, Neuron Pruning~, Block Sparsity~, BBS~, DSD~, Pow2 Ternarization~, Loss Aware Quantization~, Toeplitz-like~, Joint-SVD~, Projections~, WEST~\\\\\n\\hline\nNamed entity recognition (NER)&CoNLL2003, Wikiann-41, CoNLL2012&QBERT~, XtremeDistill~, Star Transformer~\\\\\n\\hline\nIntent Detection&SNIPS&Mixed-vocabulary KD training~\\\\\n\\hline\nQuestion Generation&SQuAD 1.1&MiniLM~\\\\\n\\hline\nSlot Filling&SNIPS&Mixed-vocabulary KD training~\\\\\n\\hline\nText classification&20 Newsgroup, Hyperpartisan, MTL-16&Sparse coding~, Longformer~, Star Transformer~\\\\\n\\hline\nCoreference Resolution&OntoNotes&Longformer~\\\\\n\\hline\n    \\end{tabular}\n    \\caption{Applications of Model Compression Methods for Text}\n    \\label{tab:applications}\n\\end{table*}\nCombinations of multiple model compression method types has also been experimented with and found to be effective. Some examples of such combinations include the following:\n\\begin{itemize}\n    \\item Pruning + Tensor Decomposition ~\n    \\item Pruning + Quantization~\n    \\item Knowledge distillation + Quantization~\n    \\item Knowledge distillation + Pruning~\n    \\item Tensor decomposition + Parameter sharing~\n    \\item Tensor decomposition + Quantization~\n\\end{itemize}\nRecently, Kim et al.~ combined  knowledge distillation, structured pruning and quantization leading to drastic improvements on inference efficiency. First, they  investigate the efficacy of various Knowledge Distillation techniques to significantly reduce the size of the models with respect to the depth and hidden state sizes while preserving the\naccuracy. Second, they explore Structured Pruning that further reduces the size of the models by reducing the number of self-attention heads and the number of intermediate hidden states in the feedforward layers to achieve more efficiency while\ntrying to preserve the accuracy as well. Finally, they \nexplore model quantization which enables faster model executions by optimally utilizing hardware acceleration capabilities. Such a combined method leads to heavily reduced model size, 12.4x GPU speed-up and 6.9x-125.8x reduction in energy consumption.", "cites": [4479, 4478, 7298, 4515, 1150, 4517, 4480, 4521, 4514, 4485, 4492, 1473, 4491, 8375, 9120, 4522, 4536, 4497, 4533, 8799, 4525, 4519, 4526, 854, 4484, 7829, 7333, 8797, 4486, 4516, 4527, 4500, 793, 4530, 1568, 4036, 4523, 7371, 4532, 4524, 8390, 4499, 4331, 4509, 4534, 4511, 2481, 844, 4535, 4482, 7828, 4518, 794, 4351, 4507, 2488, 4510], "cite_extract_rate": 0.76, "origin_cites_number": 75, "insight_result": {"type": "comparative", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and integrates a large number of cited papers, creating a structured comparison of model compression methods across different NLP tasks. It critically evaluates the performance of various compression types, highlights their strengths and limitations, and provides a quantitative analysis to support its claims. The abstraction level is strong as it generalizes findings and identifies trends in the effectiveness of methods like distillation and quantization."}}
{"id": "7ea3f834-f595-4c1b-afed-044d53e835ea", "title": "Future Trends", "level": "subsection", "subsections": [], "parent_id": "d06ca9cc-bcd7-4f53-9be8-ee86f8754664", "prefix_titles": [["title", "Compression of Deep Learning Models for Text: A Survey"], ["section", "Summary and Future Directions"], ["subsection", "Future Trends"]], "content": "Although there has been so much of work already in this field, there is a lot more work to be done. \n\\begin{itemize}\n    \\item With linear Transformer models, one can afford to have input with tens of thousands of tokens. Hence, many tasks need to be redesigned where large context can now be included as input to improve accuracy. \n    \\item Combinations of several methods have not been tested well. Recently, Fastformers method~ showed that combining multiple methods like knowledge distillation, 16-bit quantization, structured pruning and numerical optimizations can lead to drastic improvements. However, lot of experiments are needed to further check how models respond to combination of model compression methods.\n    \\item Latency results vary based on GPU architectures. With new GPU architectures (Nvidia RTX 3080, Nvidia T4), some methods like quantization may become more impactful.\n    \\item Real world settings are often complex: multi-modal, multi-task, multi-label, small-data, noisy labels, multi-teachers, mismatching teacher-student architectures. Efficient ways of recommending the most promising method is necessary.\n    \\item Different components/structures of a model may respond to different kinds of compression methods with specific hyper-parameters. A generic method to choose the right method for various structures is needed.\n    \\item How does compression of models impact their interpretability? Can we design model compression mechanisms aimed at looking at a tradeoff between model accuracy, size, latency and interpretability.\n    \\item None of the model compression methods performs any application specific compression. Can we obtain further compression by exploiting some task-specific patterns?\n    \\item Recently, deep reinforcement learning based methods have been proposed in the computer vision community~. It will be nice to check the effectiveness of such methods for NLP tasks.\n\\end{itemize}\nWe hope that this survey acts as a good guide to folks across academia and industry. Also, we hope that a significantly large chunk of research gets done in the area of model compression to enable good accuracy across many NLP tasks while keeping model sizes and latencies in check. \n\\begin{thebibliography}{100}\n\\scriptsize\n\\providecommand{\\url}[1]{#1}\n\\csname url@samestyle\\endcsname\n\\providecommand{\\newblock}{\\relax}\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\BIBentrySTDinterwordspacing}{\\spaceskip=0pt\\relax}\n\\providecommand{\\BIBentryALTinterwordstretchfactor}{4}\n\\providecommand{\\BIBentryALTinterwordspacing}{\\spaceskip=\\fontdimen2\\font plus\n\\BIBentryALTinterwordstretchfactor\\fontdimen3\\font minus\n  \\fontdimen4\\font\\relax}\n\\providecommand{\\BIBforeignlanguage}[2]{{\n\\expandafter\\ifx\\csname l@#1\\endcsname\\relax\n\\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\n\\typeout{** loaded for the language `#1'. Using the pattern for}\n\\typeout{** the default language instead.}\n\\else\n\\language=\\csname l@#1\\endcsname\n\\fi\n#2}}\n\\providecommand{\\BIBdecl}{\\relax}\n\\BIBdecl\n\\bibitem{vaswani2017attention}\nA.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,\n  {\\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in\n  \\emph{NIPS}, 2017, pp. 5998--6008.\n\\bibitem{devlin2018bert}\nJ.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep\n  bidirectional transformers for language understanding,''\n  \\emph{arXiv:1810.04805}, 2018.\n\\bibitem{wang2019glue}\nA.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman, ``{GLUE}: A\n  multi-task benchmark and analysis platform for natural language\n  understanding,'' in \\emph{ICLR}, 2019.\n\\bibitem{wang2019superglue}\nA.~Wang, Y.~Pruksachatkun, N.~Nangia, A.~Singh, J.~Michael, F.~Hill, O.~Levy,\n  and S.~R. Bowman, ``Super{GLUE}: A stickier benchmark for general-purpose\n  language understanding systems,'' \\emph{1905.00537}, 2019.\n\\bibitem{radford2019language}\nA.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language\n  models are unsupervised multitask learners,'' \\emph{OpenAI Blog}, vol.~1,\n  no.~8, 2019.\n\\bibitem{liu2019multi}\nX.~Liu, P.~He, W.~Chen, and J.~Gao, ``Multi-task deep neural networks for\n  natural language understanding,'' \\emph{arXiv:1901.11504}, 2019.\n\\bibitem{yang2019xlnet}\nZ.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le,\n  ``Xlnet: Generalized autoregressive pretraining for language understanding,''\n  \\emph{arXiv:1906.08237}, 2019.\n\\bibitem{shoeybi2019megatron}\nM.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro,\n  ``Megatron-lm: Training multi-billion parameter language models using gpu\n  model parallelism,'' \\emph{arXiv:1909.08053}, 2019.\n\\bibitem{raffel2019exploring}\nC.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,\n  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a\n  unified text-to-text transformer,'' \\emph{arXiv:1910.10683}, 2019.\n\\bibitem{rosset2019turing}\nC.~Rosset, ``Turing-nlg: A 17-billion-parameter language model by microsoft,''\n  \\emph{Microsoft Blog}, 2019.\n\\bibitem{lepikhin2020gshard}\nD.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer,\n  and Z.~Chen, ``Gshard: Scaling giant models with conditional computation and\n  automatic sharding,'' \\emph{arXiv:2006.16668}, 2020.\n\\bibitem{bianco2018benchmark}\nS.~Bianco, R.~Cadene, L.~Celona, and P.~Napoletano, ``Benchmark analysis of\n  representative deep neural network architectures,'' \\emph{IEEE Access},\n  vol.~6, pp. 64\\,270--64\\,277, 2018.\n\\bibitem{sanh2019distilbert}\nV.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf, ``Distilbert, a distilled version\n  of bert: smaller, faster, cheaper and lighter,'' \\emph{arXiv:1910.01108},\n  2019.\n\\bibitem{han2016eie}\nS.~Han, X.~Liu, H.~Mao, J.~Pu, A.~Pedram, M.~A. Horowitz, and W.~J. Dally,\n  ``Eie: efficient inference engine on compressed deep neural network,''\n  \\emph{ACM SIGARCH Computer Architecture News}, vol.~44, no.~3, pp. 243--254,\n  2016.\n\\bibitem{diamos2016persistent}\nG.~Diamos, S.~Sengupta, B.~Catanzaro, M.~Chrzanowski, A.~Coates, E.~Elsen,\n  J.~Engel, A.~Hannun, and S.~Satheesh, ``Persistent rnns: Stashing recurrent\n  weights on-chip,'' in \\emph{ICML}, 2016, pp. 2024--2033.\n\\bibitem{denil2013predicting}\nM.~Denil, B.~Shakibi, L.~Dinh, M.~Ranzato, and N.~De~Freitas, ``Predicting\n  parameters in deep learning,'' in \\emph{NIPS}, 2013, pp. 2148--2156.\n\\bibitem{cheng2017survey}\nY.~Cheng, D.~Wang, P.~Zhou, and T.~Zhang, ``A survey of model compression and\n  acceleration for deep neural networks,'' \\emph{arXiv:1710.09282}, 2017.\n\\bibitem{deng2020model}\nL.~Deng, G.~Li, S.~Han, L.~Shi, and Y.~Xie, ``Model compression and hardware\n  acceleration for neural networks: A comprehensive survey,'' \\emph{IEEE}, vol.\n  108, no.~4, pp. 485--532, 2020.\n\\bibitem{walsh2013peter}\nC.~A. Walsh, ``Peter huttenlocher (1931--2013),'' \\emph{Nature}, vol. 502, no.\n  7470, pp. 172--172, 2013.\n\\bibitem{zhu2017prune}\nM.~Zhu and S.~Gupta, ``To prune, or not to prune: exploring the efficacy of\n  pruning for model compression,'' \\emph{arXiv:1710.01878}, 2017.\n\\bibitem{lecun1990optimal}\nY.~LeCun, J.~S. Denker, and S.~A. Solla, ``Optimal brain damage,'' in\n  \\emph{NIPS}, 1990, pp. 598--605.\n\\bibitem{hassibi1993second}\nB.~Hassibi and D.~G. Stork, ``Second order derivatives for network pruning:\n  Optimal brain surgeon,'' in \\emph{NIPS}, 1993, pp. 164--171.\n\\bibitem{han2015deep}\nS.~Han, H.~Mao, and W.~J. Dally, ``Deep compression: Compressing deep neural\n  networks with pruning, trained quantization and huffman coding,''\n  \\emph{arXiv:1510.00149}, 2015.\n\\bibitem{see2016compression}\nA.~See, M.-T. Luong, and C.~D. Manning, ``Compression of neural machine\n  translation models via pruning,'' \\emph{arXiv:1606.09274}, 2016.\n\\bibitem{han2015learning}\nS.~Han, J.~Pool, J.~Tran, and W.~Dally, ``Learning both weights and connections\n  for efficient neural network,'' in \\emph{NIPS}, 2015, pp. 1135--1143.\n\\bibitem{narang2017exploring}\nS.~Narang, E.~Elsen, G.~Diamos, and S.~Sengupta, ``Exploring sparsity in\n  recurrent neural networks,'' \\emph{arXiv:1704.05119}, 2017.\n\\bibitem{cheong2019transformers}\nR.~Cheong and R.~Daniel, ``transformers. zip: Compressing transformers with\n  pruning and quantization,'' Technical report, Stanford University, Stanford,\n  California, 2019., Tech. Rep., 2019.\n\\bibitem{guo2019reweighted}\nF.-M. Guo, S.~Liu, F.~S. Mungall, X.~Lin, and Y.~Wang, ``Reweighted proximal\n  pruning for large-scale language representation,'' \\emph{arXiv:1909.12486},\n  2019.\n\\bibitem{han2016dsd}\nS.~Han, J.~Pool, S.~Narang, H.~Mao, E.~Gong, S.~Tang, E.~Elsen, P.~Vajda,\n  M.~Paluri, J.~Tran \\emph{et~al.}, ``Dsd: Dense-sparse-dense training for deep\n  neural networks,'' \\emph{arXiv:1607.04381}, 2016.\n\\bibitem{dai2018grow}\nX.~Dai, H.~Yin, and N.~K. Jha, ``Grow and prune compact, fast, and accurate\n  lstms,'' \\emph{arXiv:1805.11797}, 2018.\n\\bibitem{he2014reshaping}\nT.~He, Y.~Fan, Y.~Qian, T.~Tan, and K.~Yu, ``Reshaping deep neural network for\n  fast decoding by node-pruning,'' in \\emph{ICASSP}.\\hskip 1em plus 0.5em minus\n  0.4em\\relax IEEE, 2014, pp. 245--249.\n\\bibitem{murray2015auto}\nK.~Murray and D.~Chiang, ``Auto-sizing neural networks: With applications to\n  n-gram language models,'' \\emph{arXiv:1508.05051}, 2015.\n\\bibitem{pan2016dropneuron}\nW.~Pan, H.~Dong, and Y.~Guo, ``Dropneuron: Simplifying the structure of deep\n  neural networks,'' \\emph{arXiv:1606.07326}, 2016.\n\\bibitem{srinivas2015data}\nS.~Srinivas and R.~V. Babu, ``Data-free parameter pruning for deep neural\n  networks,'' \\emph{arXiv:1507.06149}, 2015.\n\\bibitem{narang2017block}\nS.~Narang, E.~Undersander, and G.~Diamos, ``Block-sparse recurrent neural\n  networks,'' \\emph{arXiv:1711.02782}, 2017.\n\\bibitem{cao2019efficient}\nS.~Cao, C.~Zhang, Z.~Yao, W.~Xiao, L.~Nie, D.~Zhan, Y.~Liu, M.~Wu, and\n  L.~Zhang, ``Efficient and effective sparse lstm on fpga with bank-balanced\n  sparsity,'' in \\emph{SIGDA Intl. Symp. on FPGA}.\\hskip 1em plus 0.5em minus\n  0.4em\\relax ACM, 2019, pp. 63--72.\n\\bibitem{michel2019sixteen}\nP.~Michel, O.~Levy, and G.~Neubig, ``Are sixteen heads really better than\n  one?'' \\emph{arXiv:1905.10650}, 2019.\n\\bibitem{voita2019analyzing}\nE.~Voita, D.~Talbot, F.~Moiseev, R.~Sennrich, and I.~Titov, ``Analyzing\n  multi-head self-attention: Specialized heads do the heavy lifting, the rest\n  can be pruned,'' \\emph{arXiv:1905.09418}, 2019.\n\\bibitem{ding2017visualizing}\nY.~Ding, Y.~Liu, H.~Luan, and M.~Sun, ``Visualizing and understanding neural\n  machine translation,'' in \\emph{ACL}, 2017, pp. 1150--1159.\n\\bibitem{fan2019reducing}\nA.~Fan, E.~Grave, and A.~Joulin, ``Reducing transformer depth on demand with\n  structured dropout,'' \\emph{arXiv:1909.11556}, 2019.\n\\bibitem{prasanna2020bert}\nS.~Prasanna, A.~Rogers, and A.~Rumshisky, ``When bert plays the lottery, all\n  tickets are winning,'' \\emph{arXiv:2005.00561}, 2020.\n\\bibitem{bartol2015hippocampal}\nT.~M. Bartol, C.~Bromer, J.~Kinney, M.~A. Chirillo, J.~N. Bourne, K.~M. Harris,\n  and T.~J. Sejnowski, ``Hippocampal spine head sizes are highly precise,''\n  \\emph{bioRxiv}, p. 016329, 2015.\n\\bibitem{linden2018think}\nD.~J. Linden, \\emph{Think Tank: Forty Neuroscientists Explore the Biological\n  Roots of Human Experience}.\\hskip 1em plus 0.5em minus 0.4em\\relax Yale\n  University Press, 2018.\n\\bibitem{hubara2017quantized}\nI.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio, ``Quantized\n  neural networks: Training neural networks with low precision weights and\n  activations,'' \\emph{JMLR}, vol.~18, no.~1, pp. 6869--6898, 2017.\n\\bibitem{lam2018word2bits}\nM.~Lam, ``Word2bits-quantized word vectors,'' \\emph{arXiv:1803.05651}, 2018.\n\\bibitem{courbariaux2015binaryconnect}\nM.~Courbariaux, Y.~Bengio, and J.-P. David, ``Binaryconnect: Training deep\n  neural networks with binary weights during propagations,'' in \\emph{NIPS},\n  2015, pp. 3123--3131.\n\\bibitem{bengio2013estimating}\nY.~Bengio, N.~L{\\'e}onard, and A.~Courville, ``Estimating or propagating\n  gradients through stochastic neurons for conditional computation,''\n  \\emph{arXiv:1308.3432}, 2013.\n\\bibitem{lin2015neural}\nZ.~Lin, M.~Courbariaux, R.~Memisevic, and Y.~Bengio, ``Neural networks with few\n  multiplications,'' \\emph{arXiv:1510.03009}, 2015.\n\\bibitem{hubara2016binarized}\nI.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio, ``Binarized\n  neural networks,'' in \\emph{NIPS}, 2016, pp. 4107--4115.\n\\bibitem{rastegari2016xnor}\nM.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi, ``Xnor-net: Imagenet\n  classification using binary convolutional neural networks,'' in\n  \\emph{ECCV}.\\hskip 1em plus 0.5em minus 0.4em\\relax Springer, 2016, pp.\n  525--542.\n\\bibitem{hou2016loss}\nL.~Hou, Q.~Yao, and J.~T. Kwok, ``Loss-aware binarization of deep networks,''\n  \\emph{arXiv:1611.01600}, 2016.\n\\bibitem{lee2014proximal}\nJ.~D. Lee, Y.~Sun, and M.~A. Saunders, ``Proximal newton-type methods for\n  minimizing composite functions,'' \\emph{J. Optimization}, vol.~24, no.~3, pp.\n  1420--1443, 2014.\n\\bibitem{ott2016recurrent}\nJ.~Ott, Z.~Lin, Y.~Zhang, S.-C. Liu, and Y.~Bengio, ``Recurrent neural networks\n  with limited numerical precision,'' \\emph{arXiv:1608.06902}, 2016.\n\\bibitem{alom2018effective}\nM.~Z. Alom, A.~T. Moody, N.~Maruyama, B.~C. Van~Essen, and T.~M. Taha,\n  ``Effective quantization approaches for recurrent neural networks,'' in\n  \\emph{IJCNN}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE, 2018, pp. 1--8.\n\\bibitem{li2016ternary}\nF.~Li, B.~Zhang, and B.~Liu, ``Ternary weight networks,''\n  \\emph{arXiv:1605.04711}, 2016.\n\\bibitem{hwang2014fixed}\nK.~Hwang and W.~Sung, ``Fixed-point feedforward deep neural network design\n  using weights+ 1, 0, and- 1,'' in \\emph{SiPS}.\\hskip 1em plus 0.5em minus\n  0.4em\\relax IEEE, 2014, pp. 1--6.\n\\bibitem{lloyd1982least}\nS.~Lloyd, ``Least squares quantization in pcm,'' \\emph{Tran. on information\n  theory}, vol.~28, no.~2, pp. 129--137, 1982.\n\\bibitem{zhu2016trained}\nC.~Zhu, S.~Han, H.~Mao, and W.~J. Dally, ``Trained ternary quantization,''\n  \\emph{arXiv:1612.01064}, 2016.\n\\bibitem{wang2018hitnet}\nP.~Wang, X.~Xie, L.~Deng, G.~Li, D.~Wang, and Y.~Xie, ``Hitnet: hybrid ternary\n  recurrent neural network,'' in \\emph{NIPS}, 2018, pp. 604--614.\n\\bibitem{he2016effective}\nQ.~He, H.~Wen, S.~Zhou, Y.~Wu, C.~Yao, X.~Zhou, and Y.~Zou, ``Effective\n  quantization methods for recurrent neural networks,''\n  \\emph{arXiv:1611.10176}, 2016.\n\\bibitem{zhou2017balanced}\nS.-C. Zhou, Y.-Z. Wang, H.~Wen, Q.-Y. He, and Y.-H. Zou, ``Balanced\n  quantization: An effective and efficient approach to quantized neural\n  networks,'' \\emph{J. of Computer Science and Technology}, vol.~32, no.~4, pp.\n  667--682, 2017.\n\\bibitem{muller2015rounding}\nL.~K. Muller and G.~Indiveri, ``Rounding methods for neural networks with low\n  resolution synaptic weights,'' \\emph{arXiv:1504.05767}, 2015.\n\\bibitem{gong2014compressing}\nY.~Gong, L.~Liu, M.~Yang, and L.~Bourdev, ``Compressing deep convolutional\n  networks using vector quantization,'' \\emph{arXiv:1412.6115}, 2014.\n\\bibitem{guo2017network}\nY.~Guo, A.~Yao, H.~Zhao, and Y.~Chen, ``Network sketching: Exploiting binary\n  structure in deep cnns,'' in \\emph{CVPR}, 2017, pp. 5955--5963.\n\\bibitem{xu2018alternating}\nC.~Xu, J.~Yao, Z.~Lin, W.~Ou, Y.~Cao, Z.~Wang, and H.~Zha, ``Alternating\n  multi-bit quantization for recurrent neural networks,''\n  \\emph{arXiv:1802.00150}, 2018.\n\\bibitem{mikolov2013word2vec}\nT.~Mikolov, K.~Chen, G.~Corrado, J.~Dean, L.~Sutskever, and G.~Zweig,\n  ``word2vec,'' \\emph{URL https://code. google. com/p/word2vec}, vol.~22, 2013.\n\\bibitem{shen2019q}\nS.~Shen, Z.~Dong, J.~Ye, L.~Ma, Z.~Yao, A.~Gholami, M.~W. Mahoney, and\n  K.~Keutzer, ``Q-bert: Hessian based ultra low precision quantization of\n  bert,'' \\emph{arXiv:1909.05840}, 2019.\n\\bibitem{ba2014deep}\nJ.~Ba and R.~Caruana, ``Do deep nets really need to be deep?'' in \\emph{NIPS},\n  2014, pp. 2654--2662.\n\\bibitem{sau2016deep}\nB.~B. Sau and V.~N. Balasubramanian, ``Deep model compression: Distilling\n  knowledge from noisy teachers,'' \\emph{arXiv:1610.09650}, 2016.\n\\bibitem{hinton2015distilling}\nG.~Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural\n  network,'' \\emph{arXiv:1503.02531}, 2015.\n\\bibitem{czarnecki2017sobolev}\nW.~M. Czarnecki, S.~Osindero, M.~Jaderberg, G.~Swirszcz, and R.~Pascanu,\n  ``Sobolev training for neural networks,'' in \\emph{NIPS}, 2017, pp.\n  4278--4287.\n\\bibitem{mishra2017apprentice}\nA.~Mishra and D.~Marr, ``Apprentice: Using knowledge distillation techniques to\n  improve low-precision network accuracy,'' \\emph{arXiv:1711.05852}, 2017.\n\\bibitem{polino2018model}\nA.~Polino, R.~Pascanu, and D.~Alistarh, ``Model compression via distillation\n  and quantization,'' \\emph{arXiv:1802.05668}, 2018.\n\\bibitem{romero2014fitnets}\nA.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio,\n  ``Fitnets: Hints for thin deep nets,'' \\emph{arXiv:1412.6550}, 2014.\n\\bibitem{yim2017gift}\nJ.~Yim, D.~Joo, J.~Bae, and J.~Kim, ``A gift from knowledge distillation: Fast\n  optimization, network minimization and transfer learning,'' in \\emph{CVPR},\n  2017, pp. 4133--4141.\n\\bibitem{mcclure2016representational}\nP.~McClure and N.~Kriegeskorte, ``Representational distance learning for deep\n  neural networks,'' \\emph{Frontiers in computational neuroscience}, vol.~10,\n  p. 131, 2016.\n\\bibitem{kim2016sequence}\nY.~Kim and A.~M. Rush, ``Sequence-level knowledge distillation,''\n  \\emph{arXiv:1606.07947}, 2016.\n\\bibitem{freitag2017ensemble}\nM.~Freitag, Y.~Al-Onaizan, and B.~Sankaran, ``Ensemble distillation for neural\n  machine translation,'' \\emph{arXiv:1702.01802}, 2017.\n\\bibitem{zhang2018deep}\nY.~Zhang, T.~Xiang, T.~M. Hospedales, and H.~Lu, ``Deep mutual learning,'' in\n  \\emph{CVPR}, 2018, pp. 4320--4328.\n\\bibitem{anil2018large}\nR.~Anil, G.~Pereyra, A.~Passos, R.~Ormandi, G.~E. Dahl, and G.~E. Hinton,\n  ``Large scale distributed neural network training through online\n  distillation,'' \\emph{arXiv:1804.03235}, 2018.\n\\bibitem{mirzadeh2019improved}\nS.-I. Mirzadeh, M.~Farajtabar, A.~Li, N.~Levine, A.~Matsukawa, and\n  H.~Ghasemzadeh, ``Improved knowledge distillation via teacher assistant,''\n  \\emph{arXiv:1902.03393}, 2019.\n\\bibitem{you2017learning}\nS.~You, C.~Xu, C.~Xu, and D.~Tao, ``Learning from multiple teacher networks,''\n  in \\emph{KDD}, 2017, pp. 1285--1294.\n\\bibitem{tan2019multilingual}\nX.~Tan, Y.~Ren, D.~He, T.~Qin, Z.~Zhao, and T.-Y. Liu, ``Multilingual neural\n  machine translation with knowledge distillation,'' \\emph{arXiv:1902.10461},\n  2019.\n\\bibitem{heo2019knowledge}\nB.~Heo, M.~Lee, S.~Yun, and J.~Y. Choi, ``Knowledge distillation with\n  adversarial samples supporting decision boundary,'' in \\emph{AAAI}, vol.~33,\n  2019, pp. 3771--3778.\n\\bibitem{xu2017training}\nZ.~Xu, Y.-C. Hsu, and J.~Huang, ``Training shallow and thin networks for\n  acceleration via knowledge distillation with conditional adversarial\n  networks,'' \\emph{arXiv:1709.00513}, 2017.\n\\bibitem{wang2018kdgan}\nX.~Wang, R.~Zhang, Y.~Sun, and J.~Qi, ``Kdgan: Knowledge distillation with\n  generative adversarial networks,'' in \\emph{NIPS}, 2018, pp. 775--786.\n\\bibitem{zhao2019extreme}\nS.~Zhao, R.~Gupta, Y.~Song, and D.~Zhou, ``Extreme language model compression\n  with optimal subwords and shared projections,'' \\emph{arXiv:1909.11687},\n  2019.\n\\bibitem{sun2019patient}\nS.~Sun, Y.~Cheng, Z.~Gan, and J.~Liu, ``Patient knowledge distillation for bert\n  model compression,'' \\emph{arXiv:1908.09355}, 2019.\n\\bibitem{jiao2019tinybert}\nX.~Jiao, Y.~Yin, L.~Shang, X.~Jiang, X.~Chen, L.~Li, F.~Wang, and Q.~Liu,\n  ``Tinybert: Distilling bert for natural language understanding,''\n  \\emph{arXiv:1909.10351}, 2019.\n\\bibitem{liu2019improving}\nX.~Liu, P.~He, W.~Chen, and J.~Gao, ``Improving multi-task deep neural networks\n  via knowledge distillation for natural language understanding,''\n  \\emph{arXiv:1904.09482}, 2019.\n\\bibitem{wang2020minilm}\nW.~Wang, F.~Wei, L.~Dong, H.~Bao, N.~Yang, and M.~Zhou, ``Minilm: Deep\n  self-attention distillation for task-agnostic compression of pre-trained\n  transformers,'' \\emph{arXiv:2002.10957}, 2020.\n\\bibitem{tang2019distilling}\nR.~Tang, Y.~Lu, L.~Liu, L.~Mou, O.~Vechtomova, and J.~Lin, ``Distilling\n  task-specific knowledge from bert into simple neural networks,''\n  \\emph{arXiv:1903.12136}, 2019.\n\\bibitem{mukherjee2020xtremedistil}\nS.~Mukherjee and A.~H. Awadallah, ``Xtremedistil: Multi-stage distillation for\n  massive multilingual models,'' in \\emph{ACL}, 2020, pp. 2221--2234.\n\\bibitem{ling2015finding}\nW.~Ling, T.~Lu{\\'\\i}s, L.~Marujo, R.~F. Astudillo, S.~Amir, C.~Dyer, A.~W.\n  Black, and I.~Trancoso, ``Finding function in form: Compositional character\n  models for open vocabulary word representation,'' \\emph{arXiv:1508.02096},\n  2015.\n\\bibitem{jozefowicz2016exploring}\nR.~Jozefowicz, O.~Vinyals, M.~Schuster, N.~Shazeer, and Y.~Wu, ``Exploring the\n  limits of language modeling,'' \\emph{arXiv:1602.02410}, 2016.\n\\bibitem{kim2016character}\nY.~Kim, Y.~Jernite, D.~Sontag, and A.~M. Rush, ``Character-aware neural\n  language models,'' in \\emph{AAAI}, 2016.\n\\bibitem{chen2015compressing}\nW.~Chen, J.~Wilson, S.~Tyree, K.~Weinberger, and Y.~Chen, ``Compressing neural\n  networks with the hashing trick,'' in \\emph{ICML}, 2015, pp. 2285--2294.\n\\bibitem{lu2016learning}\nZ.~Lu, V.~Sindhwani, and T.~N. Sainath, ``Learning compact recurrent neural\n  networks,'' in \\emph{ICASSP}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE,\n  2016, pp. 5960--5964.\n\\bibitem{tay2019lightweight}\nY.~Tay, A.~Zhang, L.~A. Tuan, J.~Rao, S.~Zhang, S.~Wang, J.~Fu, and S.~C. Hui,\n  ``Lightweight and efficient neural natural language processing with\n  quaternion networks,'' \\emph{arXiv:1906.04393}, 2019.\n\\bibitem{chen2016compressing}\nY.~Chen, L.~Mou, Y.~Xu, G.~Li, and Z.~Jin, ``Compressing neural language models\n  by sparse word representations,'' \\emph{arXiv:1610.03950}, 2016.\n\\bibitem{li2016lightrnn}\nX.~Li, T.~Qin, J.~Yang, and T.-Y. Liu, ``Lightrnn: Memory and\n  computation-efficient recurrent neural networks,'' in \\emph{NIPS}, 2016, pp.\n  4385--4393.\n\\bibitem{suzuki2016learning}\nJ.~Suzuki and M.~Nagata, ``Learning compact neural word embeddings by parameter\n  space sharing.'' in \\emph{IJCAI}, 2016, pp. 2046--2052.\n\\bibitem{li2018slim}\nZ.~Li, R.~Kulhanek, S.~Wang, Y.~Zhao, and S.~Wu, ``Slim embedding layers for\n  recurrent neural language models,'' in \\emph{AAAI}, 2018.\n\\bibitem{lan2019albert}\nZ.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut, ``Albert: A\n  lite bert for self-supervised learning of language representations,''\n  \\emph{arXiv:1909.11942}, 2019.\n\\bibitem{dehghani2018universal}\nM.~Dehghani, S.~Gouws, O.~Vinyals, J.~Uszkoreit, and {\\L}.~Kaiser, ``Universal\n  transformers,'' \\emph{arXiv:1807.03819}, 2018.\n\\bibitem{bai2019deep}\nS.~Bai, J.~Z. Kolter, and V.~Koltun, ``Deep equilibrium models,''\n  \\emph{arXiv:1909.01377}, 2019.\n\\bibitem{oseledets2011tensor}\nI.~V. Oseledets, ``Tensor-train decomposition,'' \\emph{SIAM J. on Scientific\n  Computing}, vol.~33, no.~5, pp. 2295--2317, 2011.\n\\bibitem{carroll1970analysis}\nJ.~D. Carroll and J.-J. Chang, ``Analysis of individual differences in\n  multidimensional scaling via an n-way generalization of “eckart-young”\n  decomposition,'' \\emph{Psychometrika}, vol.~35, no.~3, pp. 283--319, 1970.\n\\bibitem{tucker1966some}\nL.~R. Tucker, ``Some mathematical notes on three-mode factor analysis,''\n  \\emph{Psychometrika}, vol.~31, no.~3, pp. 279--311, 1966.\n\\bibitem{prabhavalkar2016compression}\nR.~Prabhavalkar, O.~Alsharif, A.~Bruguier, and L.~McGraw, ``On the compression\n  of recurrent neural networks with an application to lvcsr acoustic modeling\n  for embedded speech recognition,'' in \\emph{ICASSP}.\\hskip 1em plus 0.5em\n  minus 0.4em\\relax IEEE, 2016, pp. 5970--5974.\n\\bibitem{sak2014long}\nH.~Sak, A.~Senior, and F.~Beaufays, ``Long short-term memory based recurrent\n  neural network architectures for large vocabulary speech recognition,''\n  \\emph{arXiv:1402.1128}, 2014.\n\\bibitem{faruqui2015sparse}\nM.~Faruqui, Y.~Tsvetkov, D.~Yogatama, C.~Dyer, and N.~Smith, ``Sparse\n  overcomplete word vector representations,'' \\emph{arXiv:1506.02004}, 2015.\n\\bibitem{shu2017compressing}\nR.~Shu and H.~Nakayama, ``Compressing word embeddings via deep compositional\n  code learning,'' \\emph{arXiv:1711.01068}, 2017.\n\\bibitem{wang2019structured}\nZ.~Wang, J.~Wohlwend, and T.~Lei, ``Structured pruning of large language\n  models,'' \\emph{arXiv:1910.04732}, 2019.\n\\bibitem{chen2015strategies}\nW.~Chen, D.~Grangier, and M.~Auli, ``Strategies for training large vocabulary\n  neural language models,'' \\emph{arXiv:1512.04906}, 2015.\n\\bibitem{variani2019west}\nE.~Variani, A.~T. Suresh, and M.~Weintraub, ``West: Word encoded sequence\n  transducers,'' in \\emph{ICASSP}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE,\n  2019, pp. 7340--7344.\n\\bibitem{tjandra2017compressing}\nA.~Tjandra, S.~Sakti, and S.~Nakamura, ``Compressing recurrent neural network\n  with tensor train,'' in \\emph{IJCNN}.\\hskip 1em plus 0.5em minus 0.4em\\relax\n  IEEE, 2017, pp. 4451--4458.\n\\bibitem{khrulkov2019tensorized}\nV.~Khrulkov, O.~Hrinchuk, L.~Mirvakhabova, and I.~Oseledets, ``Tensorized\n  embedding layers for efficient model compression,'' \\emph{arXiv:1901.10787},\n  2019.\n\\bibitem{ye2018learning}\nJ.~Ye, L.~Wang, G.~Li, D.~Chen, S.~Zhe, X.~Chu, and Z.~Xu, ``Learning compact\n  recurrent neural networks with block-term tensor decomposition,'' in\n  \\emph{CVPR}, 2018, pp. 9378--9387.\n\\bibitem{ma2019tensorized}\nX.~Ma, P.~Zhang, S.~Zhang, N.~Duan, Y.~Hou, D.~Song, and M.~Zhou, ``A\n  tensorized transformer for language modeling,'' \\emph{arXiv:1906.09777},\n  2019.\n\\bibitem{child2019generating}\nR.~Child, S.~Gray, A.~Radford, and I.~Sutskever, ``Generating long sequences\n  with sparse transformers,'' \\emph{arXiv:1904.10509}, 2019.\n\\bibitem{guo2019star}\nQ.~Guo, X.~Qiu, P.~Liu, Y.~Shao, X.~Xue, and Z.~Zhang, ``Star-transformer,'' in\n  \\emph{NAACL-HLT}, 2019, pp. 1315--1325.\n\\bibitem{kitaev2020reformer}\nN.~Kitaev, {\\L}.~Kaiser, and A.~Levskaya, ``Reformer: The efficient\n  transformer,'' \\emph{arXiv:2001.04451}, 2020.\n\\bibitem{wang2020linformer}\nS.~Wang, B.~Li, M.~Khabsa, H.~Fang, and H.~Ma, ``Linformer: Self-attention with\n  linear complexity,'' \\emph{arXiv:2006.04768}, 2020.\n\\bibitem{tay2020sparse}\nY.~Tay, D.~Bahri, L.~Yang, D.~Metzler, and D.-C. Juan, ``Sparse sinkhorn\n  attention,'' \\emph{arXiv:2002.11296}, 2020.\n\\bibitem{shen2018efficient}\nZ.~Shen, M.~Zhang, H.~Zhao, S.~Yi, and H.~Li, ``Efficient attention: Attention\n  with linear complexities,'' \\emph{arXiv:1812.01243}, 2018.\n\\bibitem{katharopoulos2020transformers}\nA.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret, ``Transformers are rnns:\n  Fast autoregressive transformers with linear attention,''\n  \\emph{arXiv:2006.16236}, 2020.\n\\bibitem{kapur2017low}\nS.~Kapur, A.~Mishra, and D.~Marr, ``Low precision rnns: Quantizing rnns without\n  losing accuracy,'' \\emph{arXiv:1710.07706}, 2017.\n\\bibitem{hou2018loss}\nL.~Hou and J.~T. Kwok, ``Loss-aware weight quantization of deep networks,''\n  \\emph{arXiv:1802.08635}, 2018.\n\\bibitem{grachev2019compression}\nA.~M. Grachev, D.~I. Ignatov, and A.~V. Savchenko, ``Compression of recurrent\n  neural networks for efficient language modeling,'' \\emph{Applied Soft\n  Computing}, vol.~79, pp. 354--362, 2019.\n\\bibitem{damani20_pakdd}\nS.~Damani, K.~N. Narahari, A.~Chatterjee, M.~Gupta, and P.~Agrawal, ``Optimized\n  transformer models for faq answering,'' in \\emph{PAKDD}, 2020, p. To appear.\n\\bibitem{yang2018accelerating}\nY.~Yang, K.~Liang, X.~Xiao, Z.~Xie, L.~Jin, J.~Sun, and W.~Zhou, ``Accelerating\n  and compressing lstm based model for online handwritten chinese character\n  recognition,'' in \\emph{ICFHR}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE,\n  2018, pp. 110--115.\n\\bibitem{lazaridou2013fish}\nA.~Lazaridou, E.~M. Vecchi, and M.~Baroni, ``Fish transporters and miracle\n  homes: How compositional distributional semantics can help np parsing,'' in\n  \\emph{EMNLP}, 2013, pp. 1908--1913.\n\\bibitem{wangperawong2018attending}\nA.~Wangperawong, ``Attending to mathematical language with transformers,''\n  \\emph{arXiv:1812.02825}, 2018.\n\\bibitem{linzen2016assessing}\nT.~Linzen, E.~Dupoux, and Y.~Goldberg, ``Assessing the ability of lstms to\n  learn syntax-sensitive dependencies,'' \\emph{TACL}, vol.~4, pp. 521--535,\n  2016.\n\\bibitem{zaremba2014learning}\nW.~Zaremba and I.~Sutskever, ``Learning to execute,'' \\emph{arXiv:1410.4615},\n  2014.\n\\end{thebibliography}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{referencesShort}\n\\end{document}\n\\endinput", "cites": [849, 4536], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating key findings from recent works like FastFormers and AMC to highlight the potential of combining model compression methods. It also identifies critical gaps, such as the lack of application-specific compression and the need for methods that account for model structure variability. The abstraction level is high as it frames these issues in broader contexts like real-world complexity, interpretability, and future research opportunities."}}
