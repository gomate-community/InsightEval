{"id": "74b358c3-1042-4f31-a16f-380254516292", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4fe25043-06f8-4577-bd62-3fe42c7ab899", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nMalware (malicious software) is a big cyber threat and has received a due amount of attention. For instance, Kaspersky reports that 21,643,946 unique malicious files were detected in the year 2018, 24,610,126 in 2019, and 33,412,568 in 2020 .\nA popular defense against malware is to use  {\\em signature-based} detectors , where a signature is often extracted by malware analysts from known malware examples. This approach has two drawbacks: signatures are tedious to extract and can be evaded  by a range of techniques (e.g., encryption, repacking, polymorphism ). This incompetence has motivated the use of Machine Learning (ML) based malware detectors, which can be automated to some degree and can possibly detect \\emph{new} malware examples (via model generalization or knowledge adaptation  ). More recently, Deep Learning (DL) has been used for malware detection (see, e.g., ). \nWhile promising, ML-based malware detectors are vulnerable to attacks known as {\\em adversarial examples} . \nThere are two kinds of attacks. One is {\\em evasion attack}, where the attacker perturbs test examples to {\\em adversarial examples} to evade malware detectors . The other is {\\em poisoning attack}, where the attacker manipulates the training dataset for learning malware detectors . These attacks usher in the new field of Adversarial Malware Detection (AMD) . \nThe state-of-the-art in AMD is that there are some specific results scattered in the literature but there is no systematic understanding.\nThis is true despite that there have been attempts at systematizing the related field of Adversarial Machine Learning (AML) , which however cannot be automatically translated to AMD. This is so because malware detection \nhas three {\\em unique} characteristics which are not exhibited by the other application domains (e.g., image or audio processing). (i) There are no common, standard feature definitions because both attackers and defenders can define their own features to represent computer files. As a consequence, attackers can leverage this ``freedom'' in feature definition to craft adversarial examples. (ii) Malware features are often discrete rather than continuous and program files are often highly structured with multiple modalities. This means that arbitrarily perturbing malware files or their feature representations might make the perturbed files no more executable. \nThis also means that the discrete domain makes perturbation a {\\em non-differentiable} and {\\em non-convex} task. \n(iii) Any meaningful perturbation to a malware example or its feature representation must preserve its malicious functionality. For example, the Android Package Kit (APK) requires that the used permissions are publicized in the AndroidManifest.xml, meaning that removing permissions in this manifest file would incur a runtime error. The preceding (ii) and (iii) make both the attacker's and defender's tasks more challenging than their \ncounterparts where small perturbations are not noticeable (e.g., images).\n\\noindent{\\bf Our Contributions}. We propose a conceptual framework for systematizing the AMD field through the lens of {\\em assumptions}, {\\em attacks}, {\\em defenses}, and {\\em security properties}. In specifying these, we seek rigorous definitions whenever possible, while noting that these definitions have been scattered in the literature. Rigorous definitions are important because they can serve as a common reference for future studies. The framework allows us to map the known attacks and defenses into some partial order structures and systematize the AMD attack-defense arms race. \nWe make a number of observations, including: (i) the indiscriminate attack that treats malicious examples as equally important has been extensively investigated, but targeted and availability attacks are much less investigated; (ii) the evasion attack is much more extensively studied than the poisoning attack; (iii) there is no silver-bullet defense against evasion and poisoning attacks; (iv) sanitizing examples is effective against black-box and grey-box attacks, but not white-box attacks; (v) AMD security properties have been evaluated empirically rather than rigorously; (vi) there is no theoretical evidence to support that the effectiveness of defense techniques on the training set can generalize to other adversarial examples.\nWe draw a number of insights, including: (i) knowing defender's feature set is critical to the success of transfer attacks, highlighting the importance of keeping defender's feature set secret (e.g., by randomizing defender's feature set);\n(ii) the effectiveness of practical evasion attacks largely depends on the attacker's degree of freedom in conducting manipulations in the problem space (i.e., a small degree of freedom means harder to succeed); (iii) effective defenses often require the defender to know the attacker's manipulation set, explaining from one perspective why it is hard to design effective defenses;\n(iv) effectiveness of adversarial training depends on the defender's capability in identifying the most powerful attack.\nFinally, we discuss a number of future research directions, which hopefully will inspire and encourage many researchers to explore them.\n\\smallskip\n\\noindent{\\bf Related Work}.\nThe closely related prior work is Maiorca et al. , which surveys previous studies in adversarial malicious PDF document detection. In contrast, we consider the broader context of AMD and propose novel partial orders to accommodate AMD assumptions, attacks, defenses, and properties.\nThere are loosely-related prior studies, which survey prior AML studies (but not focusing on AMD), including .\nFor example, Yuan et al.  survey attack methods for generating adversarial examples, while briefly discussing evasion attacks in the AMD context; \nBarreno et al.  propose a taxonomy of AML attacks\n(causative vs. exploratory attacks, integrity vs. availability attacks, and targeted vs. indiscriminate attacks); Biggio et al.  propose a defense framework for protecting \nSupport Vector Machines (SVMs) from evasion attacks, poisoning attacks and privacy violations; Papernot et al.  systematize AML security and privacy with emphasis on demonstrating the trade-off between detection accuracy and robustness.\n\\smallskip\n\\noindent{\\bf Paper Outline}.\nSection \\ref{sec:fw} describes our survey and systematization methodology and framework.\nSection \\ref{sec:systematizstion-AMD-studies} applies  our framework to systematize the literature AMD studies.\nSection \\ref{sec:cd} discusses future research directions. \nSection \\ref{sec:conc} concludes the paper.", "cites": [314, 7755, 7337, 893, 3856, 3857, 937, 3853, 3854, 7754, 7153, 3855, 3861, 7154, 3860, 3859, 3858], "cite_extract_rate": 0.3269230769230769, "origin_cites_number": 52, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from multiple cited papers to build a conceptual framework for AMD. It offers critical analysis by identifying gaps (e.g., lack of systematic understanding, limited study of certain attack types) and limitations in existing works. The abstraction is strong, as it generalizes patterns like the importance of feature sets and the challenges of discrete malware features, which are key to understanding the AMD arms race."}}
{"id": "b7dbff6a-c1f8-49f8-8c03-efdb7ba23041", "title": "Brief Review on ML-based Malware Detection", "level": "subsection", "subsections": [], "parent_id": "a1283bee-37c1-4371-bed3-ceeb8af64d75", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Survey and Systematization Methodology"], ["subsection", "Brief Review on ML-based Malware Detection"]], "content": "\\label{sec:ml}\nLet $\\mathcal Z$ be the {\\em example space} of benign/malicious adversarial/non-adversarial files. Let $\\mathcal{Y}=\\{+,-\\}$ or $\\mathcal{Y}=\\{1,0\\}$ be the {\\em label space} of binary classification, where $+$/$1$ ($-$/$0$) means a file is malicious (benign). Let $\\mathcal D =\\mathcal Z \\times \\mathcal Y$ be the file-label (example-label) space. For training and evaluating a classifier in the absence of adversarial files, $\\mathcal I$ is given a set $D\\subset \\mathcal{D}$ of non-adversarial benign/malicious files as well as their {\\em ground-truth} labels. $\\mathcal I$ splits $D$ into three disjoint sets: a training set $D_{train}=\\{(z_i,y_i)\\}_{i=1}^n$, a validation set for model selection, and a test set for evaluation. \nEach file $z_i\\in \\mathcal{Z}$ is characterized by a set $S$ of features and represented by a numerical vector $\\mathbf{x}_i=(x_{i,1},\\ldots,x_{i,d})$ in the $d$-dimensional {\\em feature space} $\\mathcal X=\\mathbb R^d$, which accommodates both continuous and discrete feature representations .\nThe process for obtaining feature representation $\\mathbf{x}_i$ of $z_i\\in \\mathcal{Z}$ is called {\\it feature extraction}, denoted by a function $\\phi:\\mathcal{Z}\\to \\mathcal{X}$ with ${\\bf x}_i\\leftarrow \\phi(S,z_i)$. Because $\\phi$ can be hand-crafted (denoted by $\\phi_c$), automatically learned (denoted by $\\phi_a$), or a hybrid of both , we unify them into $\\phi$ such that $\\phi(S,z)=\\phi_a(\\phi_c(S,z))$; when only manual (automatic) feature extraction is involved, we can set $\\phi_a$ ($\\phi_c$) as the identity map.\nThere are two kinds of features: {\\em static features} are extracted via static analysis (e.g., strings, API calls\n); {\\em dynamic features} are extracted via dynamic analysis (e.g., instructions, registry activities\n).\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\scalebox{0.32}{\n\t\\includegraphics{figures/ml.pdf}\n\t}\n\\caption{Illustration of ML-based malware detector.\n}\n\t\\label{fig:ml}\n\\end{figure}\nAs highlighted in Figure \\ref{fig:ml},\n$\\mathcal{I}$ uses $\\{(z_i,y_i)\\}_{i=1}^n$ to learn a malware detector or classifier $f:\\mathcal{Z}\\to [0,1]$, where $f(z)=\\varphi(\\phi(S,z))$ is composed of feature extraction function $\\phi: \\mathcal{Z}\\to \\mathcal{X}$ and classification function $\\varphi:\\mathcal{X} \\to [0,1]$.\nNote that $f(z)\\in [0,1]$, namely $\\varphi({\\bf x})\\in [0,1]$ with ${\\bf x}\\leftarrow \\phi(S,z)$, can be interpreted as the probability that $z$ is malicious (while noting that calibration may be needed ). For a given threshold $\\tau\\in[0,1]$, we further say (by slightly abusing notations) $z$ is labeled by $f$ as $+$, or $+\\leftarrow f(z)$, if $f(z)\\geq \\tau$, and labeled as $-$ or $-\\leftarrow f(z)$ otherwise. In practice, $f$ is often specified by a learning algorithm $F$ with learnable parameter $\\theta$ (e.g., weights) and a hand-crafted feature extraction $\\phi_c$; then, $\\theta$ is tuned to minimize the empirical risk associated with a loss function $L:[0,1] \\times \\mathcal Y \\to \\mathbb{R}$ measuring the prediction error of $F_\\theta$  (e.g., cross-entropy ), namely\n\\begin{equation}\n\\min \\limits_{\\theta}~\\mathcal{L}(\\theta, D_{train}) = \\min \\limits_{\\theta}~\\frac{1}{n}\\sum_{(z_i, y_i)\\in D_{train}}\\left(L(F_\\theta(\\phi_c(S,z_i)), y_i)\\right). \\label{eq:rsk_emp}\n\\end{equation}\n\\noindent{\\bf Example 1: The Drebin malware detector}. Drebin is an Android malware detector trained from static features . Table \\ref{table:drebin} summarizes Drebin's feature set, which includes 4 subsets \n\\begin{table}[htbp!]\n\t\\centering\\caption{Drebin features \\label{table:drebin}}\n\t\\begin{tabular}{c|p{.32\\textwidth}}\n\t\t\\hline\n\t\t\\multicolumn{2}{c}{Feature set} \\\\\\hline\n\t\t\\multirow{4}{*}{Manifest} \n\t\t& $S_1$~~Hardware components \\\\\n\t\t& $S_2$~~Requested permissions\\\\\n\t\t& $S_3$~~App components \\\\\n\t\t& $S_4$~~Filtered intents \\\\\\hline\n\t\t\\multirow{4}{*}{Dexcode}\n\t\t& $S_5$~~Restricted API calls\\\\\n\t\t& $S_6$~~Used permissions\\\\\n\t\t& $S_7$~~Suspicious API calls\\\\\n\t\t& $S_8$~~Network addresses\\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\nof features $S_1, S_2, S_3, S_4$ extracted from the AndroidManifest.xml and another 4 subsets of features $S_5, S_6, S_7, S_8$ extracted from the disassembled DEX code files (recalling that DEX code is compiled from a program written in some language and can be understood by the Android Runtime). Specifically, $S_1$ contains features related to the access of an Android package (APK) to smartphone hardware (e.g., camera, touchscreen, or GPS module); $S_2$ contains features related to APK's requested permissions listed in the manifest prior to installation; $S_3$ contains features related to application components (e.g., {\\em activities}, {\\em service}, {\\em receivers}, and {\\em providers}.); $S_4$ contains features related to APK's communications with the other APKs; $S_5$ contains features related to critical system API calls, which cannot run without appropriate permissions or the {\\em root} privilege; $S_6$ contains features corresponding to the used permissions; $S_7$ contains features related to API calls that can access sensitive data or resources in a smartphone; and $S_8$ contains features related to IP addresses, hostnames and URLs found in the disassembled codes.\nThe feature representation is binary, meaning $\\phi=\\phi_c:\\mathcal{Z} \\mapsto \\{0,1\\}^d$ with $|S|=d$ and ${\\bf x}=(x_1,\\ldots,x_d)$, where $x_i=1$ if the corresponding feature is present in the APK  $z$ and $x_i=0$ otherwise. A file $z$ in the feature space looks like the following: \n\\[\n\\mathbf x = \\phi(z) \\to\n\\begin{pmatrix}\n\\cdots \\\\ 0\\\\ 1\\\\\n\\cdots \\\\ 1\\\\ 0\\\\\n\\cdots\\\\\n\\end{pmatrix}\n\\begin{array}{ll}\n\\cdots & \\multirow{4}{*}{\\hspace{-1mm}\\bigg \\} $S_2$ }\\\\\n\\texttt{\\small permission::SEND\\_SMS} \\\\\n\\texttt{\\small permission::READ\\_CONTACTS}\\\\\n\\cdots & \\multirow{4}{*}{\\hspace{-1mm}\\bigg \\} $S_5$ }\\\\\n\\texttt{\\small api\\_call::getDeviceID}\\\\\n\\texttt{\\small api\\_call::setWifiEnabled}\\\\\n\\cdots & \\\\\n\\end{array}\n\\]\nDrebin uses a linear Support Vector Machine (SVM) to learn classifiers.\n\\smallskip\n\\noindent{\\bf Example 2: The MalConv malware detector}.\nMalConv  is Convolutional Neural Network (CNN)-based Windows Portable Executable (PE) malware detector learned from raw binary programs (i.e., end-to-end detection)\n. \nFigure \\ref{fig:malconv} depicts its architecture.\nThe sequence of binary code is transformed into byte values (between 0 to 255) with the maximum length bounded by $N_{max}$ (e.g., $N_{max}=2^{21}$ bytes or 2MB). Each byte is further mapped into a real-valued vector using the embedding . The CNN layer and pooling layer learn abstract representations. The embedding, CNN and pooling layers belong to feature extraction $\\phi_a$, and the fully-connected and softmax layers belong to the classification operation $\\varphi$.\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\includegraphics[width=.7\\textwidth]{figures/malconv.pdf}\n\t\\caption{MalConv architecture  .}\n\t\\label{fig:malconv}\n\\end{figure}", "cites": [1159, 8565, 318, 166, 3861], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of ML-based malware detection, using formal definitions and concrete examples (Drebin and MalConv). It shows some synthesis by linking feature extraction methods to both hand-crafted and learned representations, as supported by the cited papers. However, it lacks deeper critical evaluation or abstraction into broader principles, focusing more on summarizing approaches rather than analyzing their strengths, limitations, or implications."}}
{"id": "18cf934c-a897-4b88-8a38-4cd32c20a2a0", "title": "Systematizing Assumptions", "level": "subsubsection", "subsections": [], "parent_id": "8e32af6c-587d-43e1-aabc-dc5d8345628d", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Survey and Systematization Methodology"], ["subsection", "Framework"], ["subsubsection", "Systematizing Assumptions"]], "content": "\\label{sec:assumption}\nFive assumptions have been made in the AMD literature. Assumption \\ref{assumption:iid} below says that the data samples in $D$ are Independent and Identically Distributed (IID), which is a strong assumption and researchers have started to weaken it . \n\\begin{assumption}[{\\sf IID} assumption; see, e.g., ]\n\\label{assumption:iid}\nComputer files in training data and testing data are independently drawn from the same distribution.\n\\end{assumption}\nAssumption \\ref{assumption:oracle} below is adapted from AML context, where humans can serve as an oracle $\\mathcal{O}$ for determining whether two images are the same . In the AMD context, $\\mathcal{O}$ can be instantiated as (or approximated by) malware analysts  or automated tools (e.g., Sandbox ), with the latter often using heuristic rules produced by malware analysts (e.g., YARA ).\n\\begin{assumption}[{\\sf Oracle} assumption; adapted from ]\n\\label{assumption:oracle}\nThere is an oracle $\\mathcal{O}:\\mathcal{Z}\\times\\mathcal{Z}\\to\\{{\\tt true},{\\tt false}\\}$ that tells if two files $z,z'\\in\\mathcal{Z}$ have the same functionality or not; ${\\tt true}\\leftarrow {\\mathcal{O}}(z,z')$ if and only if $z$ and $z'$ have the same functionality. \n\\end{assumption}\nAssumption \\ref{assumption:measurability} below says that there is a way to measure the degree of manipulations by which one file \nis transformed to another.\n\\begin{assumption}[{\\sf Measurability} assumption ]\n\\label{assumption:measurability}\nThere is a function $\\Gamma(z,z'):\\mathcal Z \\times \\mathcal Z \\to \\mathbb R_+$ that measures the degree of manipulations according to which a file $z'\\in \\mathcal{Z}$ can be derived from the file $z \\in \\mathcal{Z}$.\n\\end{assumption}\nSince Assumption \\ref{assumption:measurability} is often difficult to validate, $\\Gamma(z,z')$ may be replaced by a function that quantifies the degree of manipulation that can turn feature representation $\\mathbf{x}$ into $\\mathbf{x}'$, where\n$\\mathbf{x}=\\phi(S,z)$ and $\\mathbf{x}'=\\phi(S,z')$. This leads to:\n\\begin{assumption}[{\\sf Smoothness} assumption ]\n\\label{assumption:smoothness}\n\\ignore{\nThis assumption says that for any $z,z'\\in \\mathcal{Z}$ and $\\delta \\in \\Delta$, it holds that $C(\\phi(z),\\phi(z'))\\approx 0$ when $(z'\\leftarrow \\mathcal{A}(z,\\delta)) \\wedge (\\Gamma(z,z')\\approx 0) \\wedge ({\\tt true}\\leftarrow {\\mathcal{O}(z,z')})$.\n}\nThere is a function $C(\\mathbf{x},\\mathbf{x}'):\\mathcal X \\times \\mathcal X \\to \\mathbb R_+$ such that $C(\\phi(S,z),\\phi(S,z'))\\approx 0$ when $ (\\Gamma(z,z')\\approx 0) \\wedge ({\\tt true}\\leftarrow {\\mathcal{O}(z,z')})$.\n\\end{assumption}\nAssumption \\ref{assumption:inverse} below says that the inverse of feature extraction, $\\phi^{-1}$, is solvable so that a perturbed representation $\\mathbf{x}'$ can be mapped back to a legitimate file. \n\\begin{assumption}[{\\sf Invertibility} assumption ] \\label{assumption:inverse}\nFeature extraction $\\phi$ is invertible, meaning that given $\\mathbf{x}'$, the function $\\phi^{-1}:\\mathcal{X}\\to\\mathcal{Z}$ produces $z'=\\phi^{-1}(\\mathbf{x}')$. \n\\end{assumption}\nRecall that the feature extraction function $\\phi$ may be composed of a hand-crafted $\\phi_c$ and an automated $\\phi_a$, where $\\phi_c$ may be neither differentiable nor invertible . This means $\\mathbf{x}'$ may not be mapped to a legitimate file. Researchers tend to relax the assumption by overlooking the interdependent features , while suffering from the side-effect $\\mathbf{x}'\\neq \\phi(\\phi^{-1}(\\mathbf{x}'))$ .", "cites": [318, 3857, 3862, 937, 7312, 9099, 3854], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key assumptions in the adversarial malware detection literature using a unified conceptual framework, integrating ideas from multiple papers into a structured analysis. It provides critical insight by discussing the limitations and relaxations of assumptions like invertibility and smoothness, showing how researchers have addressed these challenges. The abstraction is strong as it generalizes the assumptions into a meta-framework, revealing broader implications for the field."}}
{"id": "5f85312b-af7a-44ed-87ac-7e94a51dd8cc", "title": "Systematizing Attacks", "level": "subsubsection", "subsections": [], "parent_id": "8e32af6c-587d-43e1-aabc-dc5d8345628d", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Survey and Systematization Methodology"], ["subsection", "Framework"], ["subsubsection", "Systematizing Attacks"]], "content": "\\label{sec:sys-attack}\nWe systematize attacks from two perspectives: {\\em attacker's objective} (i.e., what the attacker attempts to accomplish) and {\\em attacker's input} (i.e., what leverages the attacker can use). Whenever possible, we seek rigorous definitions to specify the attacker's input, while noting that these definitions have been scattered in the literature. We believe this specification is important because it can serve as a common reference for future studies. To demonstrate this, we discuss how to apply it to formulate a partial-order structure for comparing attacks.\n\\smallskip\n\\noindent{\\bf Attacker's Objective}.\nThere are three kinds of objectives: (i)\n{\\sf Indiscriminate}, meaning $\\mathcal{A}$ attempts to cause as many false-negatives as possible ; \n(ii) {\\sf Targeted}, meaning $\\mathcal{A}$ attempts to cause specific false-negatives (i.e., making certain malicious files evade the detection );\n(iii) {\\sf Availability}, meaning $\\mathcal{A}$ attempts to frustrate defender $\\mathcal{I}$ by rendering $\\mathcal{I}$'s classifier $f$ unusable (e.g., causing substantially high false-positives ).\n\\begin{table}[!htbp]\n\\caption{Attributes for specifying $\\mathcal{A}$'s and $\\mathcal{I}$'s input.}\n\t\\begin{minipage}{\\columnwidth}\n\t\t\\centering\n\t\t\\begin{tabular}{l|c|c}\n\t\t\\hline\nAttributes & Attacker $\\mathcal{A}$'s input & Defender $\\mathcal{I}$'s input \\\\\\hline\\hline\n\\multicolumn{3}{l}{Attributes under $\\mathcal{I}$'s control but may be known to $\\mathcal{A}$ to some extent} \\\\\\hline\n$A_1$: Training set $D_{train}$ & $a_1\\in [0,1]$ & 1 \\\\\\hline\n$A_2$: Defense technique & $a_2\\in \\{0,1\\}$ & 1 \\\\\\hline\n$A_3$: Feature set $S$ & $a_3\\in [0,1]$ & 1\\\\\\hline\n$A_4$: Learning algorithm $F_\\theta$ & $a_4\\in [0,1]$ & 1 \\\\\\hline\n$A_5$: Response  & $a_5\\in \\{0,1\\}$ & 1 \\\\\\hline\\hline\n\\multicolumn{3}{l}{Attributes under $\\mathcal{A}$'s control but may be known to $\\mathcal{I}$ to some extent} \\\\\\hline\n$A_6$: Manipulation set $\\mathcal{M}$ & 1 & $a_6\\in [0,1]$ \\\\\\hline\n$A_7$: Attack tactic & 1 & $a_7\\in \\{0,1\\}$ \\\\\\hline\n$A_8$: Attack technique & 1& $a_8\\in \\{0,1\\}$ \\\\\\hline\n$A_9$: Adversarial examples & 1 & $a_9\\in [0,1]$ \\\\\\hline\n\t\t\\end{tabular}\n\t\\end{minipage}\n\t\\label{tab:attr}\n\\end{table}\n\\smallskip\n\\noindent{\\bf Attacker's Input}.\nTable \\ref{tab:attr} highlights the attributes we define to describe $\\mathcal{A}$'s input, including: five attributes $A_1,\\ldots,A_5$ that are under $\\mathcal{I}$'s control (indicated by 1) but may be known to $\\mathcal{A}$ at some extent $a_1,\\ldots,a_5$, respectively; and four attributes $A_6,\\ldots,A_9$ that are under $\\mathcal{A}$'s control (indicated by 1). These attributes are elaborated below.\n({\\bf i}) $A_1$: it describes $\\mathcal{I}$'s training set $D_{train}$ for learning classifier $f$. \nWe use $a_1\\in [0,1]$ to represent the extent at which $D_{train}$ is known to $\\mathcal{A}$.\nLet $\\hat{D}_{train}$ be the training files that are known to $\\mathcal{A}$. Then, $a_1=|\\hat{D}_{train}\\cap{D}_{train}|/|{D}_{train}|$.\n({\\bf ii}) $A_2$: it describes $\\mathcal{I}$'s techniques, which can be Ensemble Learning (${\\sf EL}$), Weight Regularization (${\\sf WR}$), Adversarial Training (${\\sf AT}$), Verifiable Learning (${\\sf VL}$), Robust Feature ({\\sf RF}), Input Transformation (${\\sf IT}$), Classifier ranDomization (${\\sf CD}$), Sanitizing Examples ({\\sf SE}). Let $A_2\\in\\{${\\sf EL},{\\sf WR},{\\sf AT},{\\sf VL},{\\sf RF},{\\sf IT},{\\sf CD},{\\sf SE}$\\}$ and $a_2\\in \\{0,1\\}$ such that $a_2=0$ means $\\mathcal{A}$ does not know $\\mathcal{I}$'s techniques and $a_2=1$ means $\\mathcal{A}$ knows $\\mathcal{I}$'s technique. The techniques are defined as follows.\nDefinition \\ref{definition:el} says that $\\mathcal{I}$ constructs multiple classifiers and uses them collectively in malware detection.\n\\begin{definition} [ensemble learning or {\\sf EL} ] \\label{definition:el}\nLet $\\mathcal{H}$ be $\\mathcal{I}$'s classifier space.\nGiven $K$ classifiers $\\{f_i\\}_{i=1}^{K}$ where $f_i\\in\\mathcal{H}$ and $f_i:\\mathcal{Z}\\to[0,1]$, let $f_i$ be assigned with weight $\\omega_i$ with $\\sum_{i=1}^K{\\omega_i}=1$ and $\\omega_i\\geq 0$. Then, $f=\\sum_{i=1}^{K}\\omega_i f_i$.\n\\end{definition}\nDefinition \\ref{definition:wr} says that $\\mathcal{I}$ uses regularization (e.g., $\\ell_2$ regularization  or {\\em dropout} ) to decrease model's sensitivity to adversarial examples.\n\\begin{definition} [weight regularization or {\\sf WR} ] \\label{definition:wr}\nGiven a regularization item $\\Omega$ (e.g., constraints imposed on the learnable parameters), the empirical risk is\n$\\min \\limits_{\\theta}~\\left[\\mathcal{L}(\\theta, D_{train})+\\Omega(\\theta)\\right]$, where $\\mathcal{L}$ is defined in Eq. \\eqref{eq:rsk_emp}.\n\\end{definition}\nDefinition \\ref{definition:at} says that $\\mathcal{I}$ proactively makes its classifier $f$ perceive some information about adversarial files. That is, $\\mathcal{I}$ augments the training set by incorporating adversarial examples that may be produced by $\\mathcal{I}$, $\\mathcal{A}$, or both.\n\\begin{definition} [adversarial training or {\\sf AT} ] \\label{definition:at}\nLet $D'$ denote a set of adversarial file-label pairs.\nThen, $\\mathcal{I}$ tunes model parameters by minimizing the empirical risk: $\\min \\limits_{\\theta}~\\left[\\mathcal{L}(\\theta, D_{train}) + \\beta \\mathcal{L}(\\theta, D')\\right]$, where $\\beta\\geq 0$ denotes a balance factor.\n\\end{definition}\nDefinition \\ref{definiton:vl} says that $\\mathcal{I}$ intentionally over-estimates the error incurred by $\\mathcal{A}$'s manipulations and then minimizes it.\n\\begin{definition}[verifiable learning or {\\sf VL} ] \\label{definiton:vl}\nGiven $(z,y)\\in D_{train}$ and a manipulation set $\\hat{\\mathcal{M}}$ known by $\\mathcal{I}$, let $z(\\hat{\\mathcal{M}})$ denote the upper and lower boundaries on $\\hat{\\mathcal{M}}$. Then, this defense technique minimizes the following loss function derived from Eq.\\eqref{eq:rsk_emp}:\n$L(F_\\theta(\\phi_c(S,z)), y)+\\beta L(F_\\theta(\\phi_c(S,z(\\hat{\\mathcal{M}}))), y).$\n\\end{definition}\nDefinition \\ref{eq:robust-feature-defense-technique} says that $\\mathcal{I}$ uses a set of features $S^*\\subseteq S$ that can lead to higher detection capability against adversarial example attacks.\n\\begin{definition} [robust feature or {\\sf RF}; adapted from ]\n\\label{eq:robust-feature-defense-technique}\nGiven a training set $D_{train} \\cup D'$ that contains (adversarial) file-label pairs, the set of robust feature set $S^*$ is  $$S^*=\\argmin_{\\tilde{S}\\subset S}\\sum_{(z,y)\\in {D_{train}\\cup D'}}L(\\widetilde{F}_\\theta(\\phi_c(\\tilde{S},z)),y),$$ \nwhere $\\widetilde{F}_\\theta$ is $F_\\theta$ or a simplified learning algorithm that is computationally faster than $F_\\theta$ .\n\\end{definition}\nDefinition \\ref{def:it} says that $\\mathcal{I}$ aims to use non-learning methods (e.g., de-obfuscation as shown in Proguard ) to offset $\\mathcal{A}$'s manipulations.\n\\begin{definition} [input transformation or {\\sf IT}, adapted from ] \\label{def:it}\nLet ${\\tt IT}:\\mathcal{Z}\\to\\mathcal{Z}$ denote an input transformation in the file space. Given file $z$ and transformation ${\\tt IT}$, the classifier is $f=\\varphi(\\phi({\\tt IT}(z)))$.\n\\end{definition}\nDefinition \\ref{definition:cd} says that $\\mathcal{I}$ randomly chooses $m$ classifiers and uses their results for prediction. That is, $\\mathcal{I}$ aims to randomize the feature representation used by $f$, the learning algorithm, and/or response to $\\mathcal{A}$'s queries (to prevent $\\mathcal{A}$ from inferring information about $f$). \n\\begin{definition} [classifier randomization or {\\sf CD}; adapted from ] \\label{definition:cd}\nGiven $\\mathcal{I}$'s classifier space $\\mathcal{H}$ and an input file $z$, $\\mathcal{I}$ randomly selects $m$ classifiers from $\\mathcal{H}$ with replacement, say $\\{f_i\\}_{i=1}^m$. Then,\n\t$f=\\frac{1}{m}\\sum_{i=1}^m~f_i(z)$. \n\\end{definition}\nInstead of enhancing malware detectors, Definition \\ref{definition:se} provides an alternative that detects the adversarial examples for further analysis.\n\\begin{definition}[sanitizing examples or ${\\sf SE}$; adapted from ] \\label{definition:se}\n$\\mathcal{I}$ aims to detect adversarial files by using function \n${\\sf flag}:\\mathcal{Z}\\to \\{{\\tt yes},{\\tt no}\\}$ to flag a file as adversarial ({\\tt yes}) or not ({\\tt no}).\n\\end{definition}\n({\\bf iii}) $A_3$: it describes $\\mathcal{I}$'s feature set $S$. We use $a_3\\in [0,1]$ to represent the extent at which $\\mathcal{A}$ knows about $S$. Let $\\hat{S}$\ndenote the features that are known to $\\mathcal{A}$. Then,\n$a_3=|\\hat{S}\\cap S|/|S|$.\n({\\bf iv}) $A_4$: it describes $\\mathcal{I}$'s learning algorithm $F_\\theta$, the set of trainable parameters $\\theta$, and hyperparameters (which are set manually, e.g., $\\beta$ in Definition \\ref{definition:at})\n. We use $a_4\\in [0,1]$ to represent that $\\mathcal{A}$ knows an $a_4$ degree about $A_4$, where $a_4=0$ means $\\mathcal{A}$ knows nothing and $a_4=1$ means $\\mathcal{A}$ knows everything.\n({\\bf v}) $A_5$: it describes $\\mathcal{I}$'s response to $\\mathcal{A}$'s query to $f$ (if applicable), which is relevant because $\\mathcal{A}$ can learn useful information about $f$ by observing $f$'s responses .\nWe define $a_5\\in \\{0,1\\}$ such that $a_5=0$ means there is a limit on the response that can be made by $\\mathcal{A}$ to $f$\n(referred as ${\\sf LQ}$) and $a_5=1$ means there is no limit (referred as ${\\sf FQ}$).\n({\\bf vi}) $A_6$: it describes $\\mathcal{A}$'s manipulation set in the problem space, which describes perturbations for generating adversarial files (adapted from {\\em perturbation set} in the AML literature ):\n\\begin{eqnarray*}\n\\mathcal{M}=\\{\\delta: (z'\\leftarrow \\mathcal{A}(z,\\delta)) \\wedge ({\\tt true}\\leftarrow \\mathcal{O}(z,z')\n) \\wedge (z \\in \\mathcal{Z}) \\wedge (z'\\neq z)\\}.\n\\end{eqnarray*}\n$\\mathcal{M}$ is application-specific. For instance, an Android Package Kit (APK) permits adding codes or renaming class names , a Windows Portable Executable (PE) permits adding codes or changing PE {\\em section} names , and a Portable Document Format (PDF) file permits appending dead-code at its end  or add new instructions . This means that a perturbation $\\delta\\in\\mathcal{M}$ can be a tuple specifying an operator (e.g., addition or removal), an object (e.g., a feature used by $\\mathcal{I}$), and other kinds of information (e.g., perturbation location in a file). \nSince it is often infeasible to enumerate the entire manipulation set, $\\mathcal{A}$ may leverage an empirical one $\\widetilde{\\mathcal{M}}$ , which can be defined in the problem or feature space. Manipulations in the problem space must not violate the relevant constraints (e.g., adding APIs in an APK should not cause the request of unauthorized permissions). Manipulations in the feature space facilitate efficient computing via  gradient-based methods as long as the inverse feature mapping $\\phi^{-1}$ is available.\nFurthermore, we can use the manipulation set $\\mathcal{M}$ to define a {\\em feature manipulation set} $\\mathbf{M}$:\n\\begin{eqnarray}\n\\mathbf{M}=\\{\\delta_{\\mathbf{x}}=\\mathbf{x}'-\\mathbf{x}: (\\mathbf{x}=\\phi(z)) \\land (\\mathbf{x}'=\\phi(z')) \\land (z'\\leftarrow \\mathcal{A}(z,\\delta)) \\land (\\delta \\in \\mathcal{M}) \\land (z \\in \\mathcal{Z})\n\\}. \\label{eq:manipulations}\n\\end{eqnarray}\nIn order to compute $\\mathbf{M}$ efficiently, one strategy is to estimate a feature-space analog of $\\widetilde{\\mathcal{M}}$, denoted by $\\widetilde{\\mathbf{M}}$ . This however demands resolving the invertibility Assumption \\ref{assumption:inverse}.\n({\\bf vii}) $A_7$: it describes $\\mathcal{A}$'s attack tactics. We consider two tactics: classifier {\\em evasion} and classifier {\\em poisoning}. For the evasion attack, we consider three variants: basic evasion (${\\sf BE}$), optimal evasion 1 (${\\sf OE1}$) and optimal evasion 2 (${\\sf OE2}$).\nFor the poisoning attack, we consider two variants: basic poisoning (${\\sf BP}$) and optimal poisoning (${\\sf OP}$). Correspondingly, we have $A_7\\in\\{{\\sf BE},{\\sf OE1},{\\sf OE 2},{\\sf BP},{\\sf OP}\\}$.\nThese tactics are elaborated below, while noting that they do not explicitly call oracle $\\mathcal{O}$ because definitions of manipulation sets $\\mathcal{M}$ already assure that manipulations preserve functionalities of non-adversarial files.\nAs shown in Definition \\ref{defintion:basic_evs}, the basic evasion attack is that $\\mathcal{A}$ uses a set of perturbations $\\delta_z\\subseteq\\mathcal{M}$ to manipulate a malicious file $z$, which is classified by $\\mathcal{I}$'s classifier $f$ as $+\\leftarrow f(z)$, to an adversarial file $z'$ such that $-\\leftarrow f(z')$.\n\\begin{definition}[basic evasion or ${\\sf BE}$ ] \\label{defintion:basic_evs}\n$\\mathcal{A}$ looks for $\\delta_z\\subseteq\\mathcal{M}$ to achieve the following for $z\\in \\mathcal{Z}$ with $+\\leftarrow f(z)$:\n\\begin{eqnarray*}\n-\\leftarrow f(z') ~\\text{where}~\n(z'\\leftarrow \\mathcal{A}(z,\\delta_z))   \\wedge\n(\\delta_z\\subseteq\\mathcal{M}).\n\\end{eqnarray*}\n\\end{definition}\nAs shown in Definition \\ref{def:opt_evasion1}, the attacker attempts to minimize the degree of perturbations. In other words, this attack tactic is the same as {\\sf BE}, except that $\\mathcal{A}$ attempts to minimize the manipulation when perturbing a non-adversarial file $z\\in \\mathcal{Z}$ into an adversarial file $z'\\in \\mathcal{Z}$.\n\\begin{definition}[optimal evasion 1 or ${\\sf OE1}$; adapted from ] \\label{def:opt_evasion1}\n$\\mathcal{A}$ attempts to achieve the following for $z\\in \\mathcal{Z}$ with $+\\leftarrow f(z)$:\n\\begin{eqnarray*}\n\\min \\limits_{z'}\n\\Gamma(z', z) \n~\\text{s.t.}~(z'\\leftarrow \\mathcal{A}(z,\\delta_z))\\land (\\delta_z\\subseteq\\mathcal{M}) \\land (-\\leftarrow f(z')). \n\\end{eqnarray*}\n\\end{definition}\nAs shown in Definition \\ref{def:opt_evasion2}, the attacker attempts to maximize $\\mathcal{I}$'s loss for waging high-confidence evasion attacks, while noting the small perturbations may be incorporated.\n\\begin{definition}[optimal evasion 2 or ${\\sf OE2}$; adapted from ] \\label{def:opt_evasion2}\n$\\mathcal{A}$ attempts to achieve the following for $z\\in \\mathcal{Z}$ with $+\\leftarrow f(z)$:\n\\begin{eqnarray*}\n\\max\\limits_{z'}\nL(F_\\theta(\\phi_c(S,z')),+)\n~\\text{s.t.}~(z'\\leftarrow \\mathcal{A}(z,\\delta_z))\\land (\\delta_z\\subseteq\\mathcal{M}) \\land (-\\leftarrow f(z')).\n\\end{eqnarray*}\n\\end{definition}\nLet $D'_{poison}\\subset \\mathcal{D}$ be a set of adversarial file-label pairs obtained by manipulating non-adversarial files in $D_{poison}$. Let $D'_{train}\\leftarrow D_{train} \\cup D'_{poison}$ be the contaminated training data for learning a classifier $f'$ with parameters $\\theta'$. As shown in Definition \\ref{def:basic poisoning attack}, the basic poisoning attack is that the attacker aims to make $f'$ mis-classify the files in a dataset $D_{target}$, while accommodating the attacks that $\\mathcal{A}$ manipulates labels of the files in $D_{poison}$ .\n\\begin{definition}[basic poisoning or ${\\sf BP}$ ]\n\\label{def:basic poisoning attack}\nGiven a set $D_{target}$ of files where $+\\leftarrow f(\\dot{z})$ for $\\dot{z}\\in D_{target}$ and a set $D_{poison}$ of non-adversarial files, $\\mathcal{A}$ attempts to perturb files in $D_{poison}$ to adversarial ones\n$D'_{poison}=\n\\{(\\mathcal{A}(z,\\delta_z), \\mathcal{A}(y)): ((z,y)\\in D_{poison})\\land (\\delta_z\\subseteq \\mathcal{M})\\land $ $ (\\mathcal{A}(y)\\in\\{+,-\\}) \\}$ such that classifier\n$f'$ learned from $D'_{train}\\leftarrow D_{train} \\cup D'_{poison}$\nmis-classifies the files in $D_{target}$.\nFormally, the attacker intents to achieve the following for $\\forall~\\dot{z} \\in D_{target}$: \n$-\\leftarrow f'(\\dot{z})$ where $f'$ is learned from $D'_{train}\\leftarrow D_{train}\\cup D'_{poison}$.\n\\end{definition}\nAs shown in Definition \\ref{def:optimal poisoning attack}, the optimal poisoning attack is the same as {\\sf BF}, except that $\\mathcal{A}$ attempts to\nmaximize the loss when using classifier $f'$ with parameter $\\theta'$ to classify\nfiles in $D_{target}$. \nDefinition \\ref{def:optimal poisoning attack} can have multiple variants by considering bounds on $|D'_{poison}|$  or bounds on the degree of perturbations $|\\delta_z|$ . \n\\begin{definition}[optimal poisoning or ${\\sf OP}$ ]\n\\label{def:optimal poisoning attack}\nGiven $D_{poison}$, $\\mathcal{A}$ perturbs $D_{poison}$ into $D'_{poison}$ for achieving:\n\\begin{align*}\n\\max \\limits_{D'_{poison}}~\\mathcal{L}(\\theta', D_{target})\n~\\text{where}~{\\theta'} \\leftarrow \\operatorname*{\\arg\\min}_{\\theta} \\mathcal{L}(\\theta, D_{train} \\cup D'_{poison}).\n\\end{align*}\n\\end{definition}\n{({\\bf viii})} $A_8$: it describes $\\mathcal{A}$'s attack techniques, such as Gradient-based Optimization ({\\sf GO}), Sensitive Features ({\\sf SF}), MImicry ({\\sf MI}), TRansferability ({\\sf TR}), Heuristic Search ({\\sf HS}), Generative Model ({\\sf GM}), and Mixture Strategy ({\\sf MS}). We denote this by $A_8\\in\\{${\\sf GO}, {\\sf SF}, {\\sf MI}, {\\sf TR}, {\\sf HS}, {\\sf GM}, {\\sf MS}$\\}$. Let $\\mathcal{A}$ have a classifier $\\hat{f}$, which consists of a hand-crafted feature extraction $\\hat{\\phi}_c$ and a parameterized model $\\hat{F}_{\\hat\\theta}$. Let $\\mathcal{A}$ also have an objective function $L_\\mathcal{A}:[0,1]\\times\\mathcal{Y}\\to\\mathcal{R}$, which measures $\\hat{f}$'s error or $\\mathcal{A}$' failure in evasion . Note that $\\hat{f}$ and $L_\\mathcal{A}$ can be the same as, or can mimic (by leveraging $\\mathcal{A}$'s knowledge about $\\mathcal{I}$'s attributes $A_1,\\ldots,A_5$), $\\mathcal{I}$'s classifier $f$ and loss function $L$, respectively. \nThe attack technique specified by Definition \\ref{definition:go} is that $\\mathcal{A}$ solves the feature-space optimization problems described in Definitions \\ref{def:opt_evasion1}, \\ref{def:opt_evasion2} and \\ref{def:optimal poisoning attack} by using some gradient-based optimization method and then leverages the invertibility Assumption \\ref{assumption:inverse} to generate adversarial malware examples.\n\\begin{definition}[Gradient-based Optimization or {\\sf GO}, adapted from ] \\label{definition:go}\nLet $\\mathbf{x}\\leftarrow\\hat{\\phi}_c(\\hat{S},z)$ and $\\mathbf{x}'\\leftarrow\\mathbf{x}+\\delta_\\mathbf{x}$. The feature-space optimization problem in Definition \\ref{def:opt_evasion1} can be written as\n\t\\begin{equation}\n\t\t\\min_{\\delta_\\mathbf{x}} C(\\mathbf{x}, \\mathbf{x}+\\delta_\\mathbf{x})~~~\\text{s.t.}~~~ (\\delta_\\mathbf{x}\\in[\\underline{\\mathbf{u}},\\overline{\\mathbf{u}}]) \\land (\\hat{F}_{\\hat{\\theta}}(\\mathbf{x}')<\\tau), \\label{eq:a8:evs1}\n\t\\end{equation}\nwhere $\\underline{\\mathbf{u}}$ and $\\overline{\\mathbf{u}}$ are respectively the lower and upper bounds on $\\mathbf{M}$ (e.g., $\\delta_\\mathbf{x}\\in[-\\mathbf{x}, 1-\\mathbf{x}]$ for binary representation $\\mathbf{x}$). \nThe feature-space optimization problem in Definition \\ref{def:opt_evasion2} can be written as \n    \\begin{equation}\n    \t\\max_{\\delta_\\mathbf{x}} L_\\mathcal{A}\\left(\\hat{F}_{\\hat{\\theta}}(\\mathbf{x}+\\delta_\\mathbf{x}), +\\right)~~\\text{s.t.}~~(\\delta_\\mathbf{x}\\in[\\underline{\\mathbf{u}},\\overline{\\mathbf{u}}]). \\label{eq:a8:evs2}\n    \\end{equation}\nThe feature-space optimization problem specified in Definition \\ref{def:optimal poisoning attack} can be written as \n    \\begin{align}\n    \t&\\max_{\\delta_\\mathbf{x}\\in[\\underline{\\mathbf{u}},\\overline{\\mathbf{u}}]}\\mathbb{E}_{(\\dot{z},\\dot{y})\\in D_{target}}L_\\mathcal{A}(\\hat{F}_{\\hat{\\theta}'}(\\hat{\\phi}_c(\\hat{S},\\dot{z}),\\dot{y})),~~ \\forall (z,y)\\in D_{poison} \\label{eq:a8:poi}\\\\\n    \t~~\\text{where}~~&{\\hat\\theta}'\\leftarrow\\argmin_{\\hat\\theta}\\mathbb{E}_{(z_t,y_t)\\in{\\hat{D}_{train}\\cup\\{(\\hat{\\phi}_c^{-1}(\\hat{\\phi}_c(z)+\\delta_\\mathbf{x}),y')\\}}}L_\\mathcal{A}\\left(\\hat{F}_{\\hat\\theta}(\\hat{\\phi}_c(\\hat{S},z_t),y_t)\\right). \\nonumber\n    \\end{align}\n\\end{definition}\nIn order to calculate the gradients of loss function $L_\\mathcal{A}$ with respect to $\\delta_\\mathbf{x}$ in Eqs.\\eqref{eq:a8:evs1} and \\eqref{eq:a8:evs2}, inequality constraints can be handled by appending penalty items to the loss function in question and box-constraints can be coped with by using gradient projection . Since $\\mathbf{x}+\\delta_\\mathbf{x}$ is continuous, the {\\sf GO} attack technique needs to map $\\delta_\\mathbf{x}$ to a discrete perturbation vector in $\\mathbf{M}$, for instance by using the nearest neighbor search .\nThe gradients of loss function $L_\\mathcal{A}$ with respect to $\\delta_\\mathbf{x}$ in Eq.  \\eqref{eq:a8:poi} are delicate to deal with. One issue is the indirect relation between $L_\\mathcal{A}$ and $\\delta_\\mathbf{x}$, which can be handled by the chain rule . \nAnother issue is the difficulty that is encountered when computing the partial derivatives $\\partial{\\hat\\theta'}/\\partial\\delta_{\\mathbf{x}}$ . \nFor dealing with this, researchers often relax the underlying constraints (e.g., by supposing that $\\hat{F}_{\\hat\\theta}$ is a linear model). \nThe attack technique specified by Definition \\ref{definition:sf} is that $\\mathcal{A}$ perturbs malware examples by injecting or removing a small number of features to decrease the classification error measured by the loss function ${L}_\\mathcal{A}$ as much as possible.\n\\begin{definition}[Sensitive Features or {\\sf SF}, adapted from ] \\label{definition:sf}\nFor evasion attacks, $\\mathcal{A}$ aims to maximize the following with respect to a given malware example-label pair $(z,+)$:  $$\\max_{z'} L_\\mathcal{A}\\left(\\hat{F}_{\\hat\\theta}(\\hat{\\phi}(\\hat{S},z')),+\\right)~~\\text{s.t.}~~(z'\\leftarrow \\mathcal{A}(z,\\delta_z)) \\land (\\delta_z\\subseteq \\mathcal{M}) \\land (|\\delta_z|\\leq m),$$ \nwhere $m$ is the maximum degree of manipulations.\nFor poisoning attacks, $\\mathcal{A}$ aims to maximize the following with respect to the given $D_{poison}$ and $D_{target}$, \n\\begin{align}\n    \\max_{D'_{poison}}\\mathbb{E}_{(\\dot{z},\\dot{y})\\in D_{target}}L_\\mathcal{A}(\\hat{F}_{\\hat{\\theta}'}(\\hat{\\phi}_c(\\hat{S},\\dot{z}),\\dot{y})),\n\\end{align}\nwhere $\\hat{\\theta}'$ is learned from $\\hat{D}_{train}\\cup D'_{poison}$ such that $\\forall z'\\in D'_{poison}$ is obtained via the perturbation $\\delta_z$ with obeying $(z'\\leftarrow \\mathcal{A}(z,\\delta_z)) \\land (\\delta_z\\subseteq \\mathcal{M}) \\land (z\\in D_{poison}) \\land (|\\delta_z|\\leq m)$.\n\\end{definition} \nThe attack technique specified by Definition \\ref{definition:mi} is that $\\mathcal{A}$ perturbs malware example $z$ by mimicking a benign example, while noting that this attack technique can be algorithm-agnostic.\n\\begin{definition}[MImicry or {\\sf MI}, adapted from ] \\label{definition:mi}\nGiven a set of benign examples $D_{ben}$ and a malware example $z$, $\\mathcal{A}$ aims to achieve the following minimization:\n\t\\begin{eqnarray}\n\t\t\\min_{\\delta_z\\in\\mathcal{M}} \\Gamma(\\mathcal{A}(z,\\delta_z),z_{ben})~~\\text{s.t.}~~ (\\exists~z_{ben}\\in D_{ben}). \\label{eq:mimicry}\n\t\\end{eqnarray}\n\\end{definition}\nThe attack technique specified by Definition \\ref{definition:mi} can be extended to accommodate the similarity between representations in the feature space .\nThe attack technique specified by Definition \\ref{definition:tr} is that $\\mathcal{A}$ generates adversarial examples against a surrogate model  $\\hat{f}$.\n\\begin{definition}[TRansferability or {\\sf TR}, adapted from ] \\label{definition:tr}\n$\\mathcal{A}$ learns a surrogate model $\\hat{f}$ of $f$ from $\\hat{D}_{train}$, $\\hat{S}$, $\\hat{\\phi_c}$ and $\\hat{F}$. For evasion attacks,\n$\\mathcal{A}$ achieves $-\\leftarrow\\hat{f}(z')$ by perturbing malware example $z$ to $z'$ and then attacks $f$ with $z'$. For poisoning attacks, $\\mathcal{A}$ contaminates $\\hat{f}$ to $\\hat{f}'$ such that $-\\leftarrow\\hat{f}'(\\dot{z})~\\text{for}~\\forall(\\dot{z},\\dot{y}) \\in D_{target}$, by poisoning the training set $\\hat{D}_{train}$ with $D'_{poison}$ and the attacks $f$ with $D'_{poison}$.\n\\end{definition}\nThe attack technique specified by\nDefinition \\ref{definition:hs} is that $\\mathcal{A}$ searches perturbations in $\\mathcal{M}$ via some heuristics, while leveraging oracle $\\mathcal{O}$'s responses to $\\mathcal{A}$'s queries and $f$'s responses to $\\mathcal{A}$'s queries. Since $\\mathcal{M}$ is defined with respect to the problem space, this attack technique does not need the invertibility Assumption \\ref{assumption:inverse}.\n\\begin{definition}[Heuristic Search or {\\sf HS}] \\label{definition:hs}\nLet $h$ be a function taking $\\mathcal{O}$'s response and $f$'s response as input. Given a malware example $z$, $\\mathcal{A}$ looks for an $m$-length manipulation path \n\\begin{align}\n\\langle z_{(0)}, z_{(1)},\\ldots,z_{(m)}\\rangle \n~~\\text{s.t.}~~z_{(i+1)}=\\mathcal{A}(z_{(i)},\\delta_{z,(i)}) \\land (\\delta_{z,(i)}& \\in \\mathcal{M}) \\land (h(\\mathcal{O}, f,z,z_{(i)})\\leq h(\\mathcal{O}, f,z,z_{(i+1)})) \\nonumber\n\\end{align}\nwhere $z_{(0)}=z$.\n\\end{definition}\nThe attack technique specified by Definition \\ref{def:generative-model} is that $\\mathcal{A}$ uses a generative model $G$ with parameters $\\theta_g$ to perturb malware representation vectors and then leverages the invertibility Assumption \\ref{assumption:inverse} to turn the perturbed vector into an adversarial malware example.\n\\begin{definition}[Generative Model or {\\sf GM}]\\label{def:generative-model}\nGiven a malware representation vector $\\mathbf{x}=\\hat{\\phi}_c(\\hat{S},z)$, $\\mathcal{A}$ achieves\n$$\\max_{\\theta_g} L_\\mathcal{A}\\left(\\hat{F}_{\\hat{\\theta}}(G_{\\theta_g}(\\mathbf{x})),+\\right)~~\\text{s.t.}~~ G_{\\theta_g}(\\mathbf{x})\\in [\\underline{\\mathbf{u}}-\\mathbf{x}, \\overline{\\mathbf{u}}-\\mathbf{x}]$$\nand leverages the invertibility Assumption \\ref{assumption:inverse} to obtain an adversarial example $z'=\\hat\\phi_c^{-1}(G_{\\theta_g}(\\mathbf{x}))$.\n\\end{definition}\nThe attack technique specified by\nDefinition \\ref{definition:ms} is that $\\mathcal{A}$ combines multiple perturbation methods to perturb an example. \n\\begin{definition}[Mixture Strategy or {\\sf MS} ] \\label{definition:ms}\nLet $\\mathcal{H}_A$ denote the space of generative methods and $\\mathcal{W}_a=\\{\\mathbf{w}_a:\\mathbf{w}_a=(w_{a,1},\\ldots,w_{a,K}),w_{a,i}\\geq 0\\}$ with $i=1,\\ldots,K$ denote the weights space. Given a malware example $z$, $\\mathcal{A}$ aims to achieve $$\\max_{\\mathbf{w}_a} L_\\mathcal{A}(\\hat{F}_{\\hat{\\theta}}(\\hat\\phi(z')),+) ~~\\text{s.t.}~~(z'=\\sum_{i=1}^K{w_{a,i}}g_i(z))\\land (\\mathcal{O}(z,z')={\\sf true})) \\land (g_i\\in \\mathcal{H}_A) \\land (\\mathbf{w}_a\\in \\mathcal{W}_a).$$\n\\end{definition}\n({\\bf ix}) $A_9$: it corresponds to $\\mathcal{A}$'s adversarial files. Given file manipulation set $\\mathcal{M}$, the corresponding set of adversarial files is defined as $\\mathcal{Z}_{\\mathcal M}=\\{\\mathcal{A}(z, \\delta_z):(z \\in \\mathcal{Z}) \\land (\\delta_z\\subseteq\\mathcal{M})\\}$. Given feature manipulation set $\\mathbf{M}$, the set of adversarial feature vectors is: $\\mathcal{X}_{\\mathbf{M}}=\\{\\mathbf{x}':(\\mathbf{x}'=\\mathbf{x}+\\delta_{\\mathbf{x}}) \\land (\\delta_\\mathbf{x}\\in\\mathbf{M})\\}.$\n\\begin{figure}[htbp!]\n\t\\centering\n\t\\begin{tikzpicture}[thick,scale=0.95, every node/.style={transform shape}]\n\t\\node(top) [label=right:{White-box} attack, label=left:{}] at (0,6) {$(1,1,1,1,1)$};\n\t\\node(l5n0)[label=below:{},align=center] at (0,5) {$(a_1,a_2,1,1,a_5)$};\n\t\\node(l4n0)[label=below:{},align=center] at (2.2,4) {$(a_1,a_2,a_3,1,a_5)$};\n\t\\node(l4n1)[label=below:{},align=center] at (0,4) {$(0,0,1,1,0)$};\n\t\\node(l4n2)[label=below:{},align=center] at (-2.2,4) {$(a_1,a_2,1,a_4,a_5)$};\n\t\\node(l3n0)[label=below:{},align=center] at (4.4,3) {$(a_1,a_2,0,1,a_5)$};\n\t\\node(l3n1)[label=below:{},align=center] at (2.2,3) {$(0,0,a_3,1,0)$};\n\t\\node(l3n2)[label=below:{},align=center] at (0,3) {$(a_1,a_2,a_3,a_4,a_5)$};\n\t\\node(l3n3)[label=below:{},align=center] at (-2.2,3) {$(0,0,1,a_4,0)$};\n\t\\node(l3n4)[label=below:{},align=center] at (-4.4,3) {$(a_1,a_2,1,0,a_5)$};\n\t\\node(l2n0)[label=below:{},align=center] at (4.4,2) {$(0,0,0,1,0 )$};\n\t\\node(l2n1)[label=below:{},align=center] at (2.2,2) {$(a_1,a_2,0,a_4,a_5)$};\n\t\\node(l2n2)[label=below:{},align=center] at (0,2) {$(0,0,a_3,a_4,0)$};\n\t\\node(l2n3)[label=below:{},align=center] at (-2.2,2) {$(a_1,a_2,a_3,0,a_5)$};\n\t\\node(l2n4)[label=below:{},align=center] at (-4.4,2) {$(0,0,1,0,0)$};\n\t\\node(l1n0)[label=below:{},align=center] at (2.2,1) {$(0,0,0,a_4,0 )$};\n\t\\node(l1n1)[label=below:{},align=center] at (0,1) {$(a_1,a_2,0,0,a_5)$};\n\t\\node(l1n2)[label=below:{},align=center] at (-2.2,1) {$(0,0,a_3,0,0 )$};\n\t\\node(bot)[label=right:{Black-box attack}, label=left:{}] at (0,0) {$ (0,0,0,0,0)$};\n\t\\draw(l5n0) -- (top);\n\t\\draw(l4n0) -- (l5n0);\n\t\\draw(l4n1) -- (l5n0);\n\t\\draw(l4n2) -- (l5n0);\n\t\\draw(l3n0) -- (l4n0);\n\t\\draw(l3n1) -- (l4n0);\n\t\\draw(l3n1) -- (l4n1);\n\t\\draw(l3n2) -- (l4n0);\n\t\\draw(l3n2) -- (l4n2);\n\t\\draw(l3n3) -- (l4n1);\n\t\\draw(l3n3) -- (l4n2);\n\t\\draw(l3n4) -- (l4n2);\n\t\\draw(l2n0) -- (l3n0);\n\t\\draw(l2n0) -- (l3n1);\n\t\\draw(l2n1) -- (l3n0);\n\t\\draw(l2n1) -- (l3n2);\n\t\\draw(l2n2) -- (l3n1);\n\t\\draw(l2n2) -- (l3n2);\n\t\\draw(l2n2) -- (l3n3);\n\t\\draw(l2n3) -- (l3n2);\n\t\\draw(l2n3) -- (l3n4);\n\t\\draw(l2n4) -- (l3n3);\n\t\\draw(l2n4) -- (l3n4);\n\t\\draw(l1n0) -- (l2n0);\n\t\\draw(l1n0) -- (l2n1);\n\t\\draw(l1n0) -- (l2n2);\n\t\\draw(l1n1) -- (l2n1);\n\t\\draw(l1n1) -- (l2n3);\n\t\\draw(l1n2) -- (l2n2);\n\t\\draw(l1n2) -- (l2n3);\n\t\\draw(l1n2) -- (l2n4);\n\t\\draw(bot) -- (l1n2);\n\t\\draw(bot) -- (l1n1);\n\t\\draw(bot) -- (l1n0);\n\t\\end{tikzpicture}\n\t\\caption{A portion of the partial order defined over $(a_1,\\ldots,a_5)$.\n\t} \n\t\\label{fig:attack_input}\n\\end{figure}\n\\smallskip\n\\noindent{\\bf On the Usefulness of the Preceding Specification}. The preceding specification can be applied to formulate a partial order in the attribute space, which allows to compare attacks unambiguously. Figure \\ref{fig:attack_input} depicts how vector $(a_1, \\cdots, a_5)$ formulates a partial order between the widely-used informal notions of {\\em black-box} attack, namely $(a_1,a_2,a_3,a_4,a_5)=(0,0,0,0,0)$, \nand {\\em white-box} attack, namely $(a_1,a_2,a_3,a_4,a_5)=(1,1,1,1,1)$;\nthere are many kinds of grey-box attacks in between.", "cites": [8695, 890, 3866, 7156, 888, 3868, 1193, 3867, 3865, 166, 6172, 3864, 7155, 3862, 937, 3853, 9099, 9100, 7754, 3863, 7153, 3869], "cite_extract_rate": 0.5111111111111111, "origin_cites_number": 45, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.4, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section provides a strong synthesis by integrating multiple attributes from cited works into a unified framework for attacks in adversarial malware detection. It abstracts key concepts such as attacker objectives and input attributes, enabling a structured comparison and analysis. While it offers a clear analytical structure, the critical analysis is moderate, occasionally lacking in nuanced critique of individual methods."}}
{"id": "b05fe1bc-da2e-4a87-8f17-d43f1acc81fc", "title": "Systematizing Security Properties", "level": "subsection", "subsections": [], "parent_id": "a1283bee-37c1-4371-bed3-ceeb8af64d75", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Survey and Systematization Methodology"], ["subsection", "Systematizing Security Properties"]], "content": "\\label{sec:ml_model_property}\nSince $f=\\varphi(\\phi(\\cdot))$, we decompose $f$'s security properties into $\\varphi$'s and $\\phi$'s. We consider: Representation Robustness ({\\sf RR}), meaning that two similar files have similar feature representations; Classification Robustness ({\\sf CR}), meaning that two similar feature representations lead to the same label; Detection Robustness ({\\sf DR}), meaning that feature extraction function $\\phi$ returns similar representations for two files with the same functionality; Training Robustness ({\\sf TR}), meaning that small changes to the training set does not cause any significant change to the learned classifier. \nWith respect to small perturbations, Definitions \\ref{definition:representation-robustness} and\n\\ref{definition:classification-robustness} below collectively say that when two files $z$ and $z'$ are similar, they would be classified as the same label with a high probability. Since the classification function $\\varphi$ is linear, we can obtain a $\\epsilon$-robust $\\varphi$ analytically, where $\\epsilon$ is a small scalar that bounds the perturbations applied to feature vectors . This means that the main challenge is to achieve robust feature extraction.\n\\begin{definition}[{\\sf RR} or $(\\epsilon,\\eta)$-robust feature extraction; adapted from ] \n\\label{definition:representation-robustness}\nGiven constants $\\epsilon,\\eta\\in [0,1]$, and files $z,z' \\in \\mathcal{Z}$ such that $(\\Gamma(z,z')\\approx 0) \\land ({\\tt true}\\leftarrow \\mathcal{O}(z,z'))$,\nwe say feature extraction function $\\phi$ is $(\\epsilon,\\eta)$-robust if\n$$\\mathbb{P}(C(\\mathbf{x},\\mathbf{x}')\\leq \\epsilon)=\\mathbb{P}(C(\\phi(z),\\phi(z'))\\leq \\epsilon)>1 - \\eta.$$\n\\end{definition}\n\\begin{definition}[{\\sf CR} or {$\\epsilon$-robust classification} ]\n\\label{definition:classification-robustness} \nGiven constant $\\epsilon\\in [0,1]$ as in Definition \n\\ref{definition:representation-robustness} and\nany feature vectors $\\mathbf{x},{\\bf x}'\\in \\mathcal{X}$, we say classification function $\\varphi$ is $\\epsilon$-robust if $$(C(\\mathbf{x},\\mathbf{x}') \\leq \\epsilon) \\rightarrow ((\\varphi({\\bf x})>\\tau)\\land (\\varphi({\\bf x}')>\\tau)).$$\n\\end{definition}\nDefinition \\ref{definition:representation-robustness2} \nspecifies detection robustness, which says that feature extraction function $\\phi$ returns similar representations for two different files as long as they have the same functionality.\nNote that Definitions \\ref{definition:classification-robustness} and\n\\ref{definition:representation-robustness2} collectively produce a malware detector with detection robustness.\n\\begin{definition}[{\\sf DR} or $(\\mathcal{O},\\eta)$-robust feature extraction; adapted from ] \n\\label{definition:representation-robustness2}\nGiven constant $\\eta\\in [0,1]$ and two files $z,z' \\in \\mathcal{Z}$ such that $(\\Gamma(z,z')>>0)\\land({\\tt true}\\leftarrow \\mathcal{O}(z,z'))$, we say feature extraction $\\phi$ is $(\\mathcal{O}, \\eta)$-robust if $\\mathbb{P}(C(\\phi(z),\\phi(z'))\\leq \\epsilon)>1 - \\eta.$\n\\end{definition}\nSuppose we impose a restriction on the adversarial files set $D'_{poison}$ such that $|D'_{poison}|\\leq \\gamma|D_{train}|$ for some constant $\\gamma\\in[0,1]$. Let classifier $f'$ be learned from $D_{train} \\cup D'_{poison}$. \nDefinition \\ref{def:robust-training} says that a classifier $f'$ learned from poisoned training set can predict as accurately as $f$ learned from $D_{train}$ with a high probability. \n\\begin{definition}[{\\sf TR} or $(\\gamma,\\zeta)$-robust training; adapted from ]\\label{def:robust-training}\nGiven classifiers $f$ learned from $D_{train}$ and $f'$ learned from $D_{train} \\cup D'_{poison}$ where\n$|D_{poison}'| \\leq \\gamma |D_{train}|$, and small constants $\\zeta\\in [0,1]$, we say\n$f'$ is $(\\gamma,\\zeta)$-robust if $\\forall z\\in\\mathcal{Z}$\n$$\\left((f(z)>\\tau) \\land (|D_{poison}'| \\leq \\gamma |D_{train}|)\\right) \\rightarrow \\left(\\mathbb{P}(f'(z) > \\tau)>1-\\zeta\\right).$$\n\\end{definition}", "cites": [917, 3866, 3870, 3854], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing security properties into a structured framework (RR, CR, DR, TR) and connecting them to concepts from adversarial machine learning. It abstracts the key challenges in adversarial malware detection, particularly emphasizing the importance of robust feature extraction. However, the critical analysis is limited—there is little evaluation of the strengths or weaknesses of the cited works or discussion of unresolved issues."}}
{"id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "title": "Systematizing Attack Literature", "level": "subsection", "subsections": ["f3db0c13-9b45-4545-9019-43245444f75d", "6dae1197-fcde-489c-835b-7117fc4737e8", "6bd1592d-a0b4-4061-b526-553c6ebc409a", "4e7e5878-d20a-4c28-acf4-a41004766a2c", "b8ab1fcb-f749-4c70-9893-28b772af455c", "33314056-9caf-48b3-90ff-0ca708a5a80c", "c51df6e6-79f5-400d-8643-cef009ff014f", "d4bb8301-3585-4f94-b9fa-12ca7a7b0164"], "parent_id": "e26ef155-9b9c-4a92-9ffa-e334fbe6ea87", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"]], "content": "\\label{sec:attack}\n\\begin{table}[htbp]\n\\caption{Summary of AMD attacks (\\checkmark means applicable, \\protect\\fullcirc means 0, \\protect\\emptycirc means 1, \\protect\\dotcirc means a value in $[0,1]$).}\n\t\\centering\n\t\\resizebox{0.9\\columnwidth}{!}{\n\t\\setlength{\\tabcolsep}{0.4em}\n\t\\begin{tabular}{l|ccc|ccccc|cccc|ccccc|cccc|ccc}\n\t\\hline\n\t\\multicolumn{1}{c|}{\\specialcell{Attack \\\\ (in chronological order)}}& \\multicolumn{3}{c|}{\\specialcell{Attack \\\\ Objective}} & \\multicolumn{9}{c|}{\\specialcell{Attack Input}} & \n\t\\multicolumn{5}{c|}{Assumptions} &\n\t\\multicolumn{4}{c|}{\\specialcell{Broken \\\\ Properties}} & \\multicolumn{3}{c}{\\specialcell{Malware \\\\ detector}} \\\\\n\t& \\vthead{\\sf Indiscriminate} \n\t& \\vthead{\\sf Targeted}\n\t& \\vthead{\\sf Availability}\n\t& \\vthead{$A_1$: Training set $D_{train}$}\n\t& \\vthead{$A_2$: Defense technique}\n\t& \\vthead{$A_3$: Feature set}\n\t& \\vthead{$A_4$: Learning algorithm}\n\t& \\vthead{$A_5$: Response}\n\t& \\vthead{$A_6$: Manipulation set}\n\t& \\vthead{$A_7$: Attack tactic}\n\t& \\vthead{$A_8$: Attack technique}\n\t& \\vthead{$A_9$: Adversarial example set}\n\t& \\vthead{{\\sf IID} assumption}\n\t& \\vthead{{\\sf Oracle} assumption}\n\t& \\vthead{{\\sf Measurability} assumption}\n\t& \\vthead{{\\sf Smoothness} assumption}\n\t& \\vthead{{\\sf Invertibility} assumption}\n\t& \\vthead{{\\sf RR}: Representation Robustness}\n\t& \\vthead{{\\sf CR}: Classification Robustness}\n\t& \\vthead{{\\sf DR}: Detection Robustness}\n\t& \\vthead{{\\sf TR}: Training Robustness}\n\t& \\vthead{Windows Program}\n\t& \\vthead{Android Package}\n\t& \\vthead{PDF}\n\t\\\\\\hline\\hline\n    \\rowcolor{lightgray!30}\n    Smutz and Stavrou  & \n    \\checkmark &&&\n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n    $\\mathbf{M}$ &\n    {\\sf OE2} &\n    {\\sf MI} &\n    $\\mathcal{X}_\\mathbf{M}$ &\n    & & \\checkmark & & &\n    & \\checkmark & & &\n    &  & \\checkmark\n    \\\\\n    Biggio et al.  &\n    \\checkmark &&& \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{0}} & \n    \\specialcell{\\pie{0} \\\\ \\pieg} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} &\n    $\\mathbf{M}$ &\n    ${\\sf OE2}$ &\n    ${\\sf GO}$ &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    & \\checkmark & \\checkmark && \\checkmark &\n    & \\checkmark & &&\n    & & \\checkmark \n    \\\\\n    \\rowcolor{lightgray!30}\n    Maiorca et al.  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{4}}  &\n \t$\\mathcal{M}$ &\n \t${\\sf BE}$ &\n \t{\\sf MI} &\n \t$\\mathcal{Z}_{\\mathcal{M}}$ &\n \t& \\checkmark & & & &\n \t& & \\checkmark & &\n \t&  & \\checkmark \n \t\\\\\n    {\\v{S}}rndi\\'{c} and Laskov  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{0} \\\\ \\pie{0}} & \n    \\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{4}} & \n    \\specialcell{\\pieg \\\\ \\pieg \\\\ \\pieg \\\\ \\pieg} & \n    \\specialcell{\\pie{4} \\\\ \\pieg \\\\ \\pie{4} \\\\ \\pieg} &\n    \\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{4}}  &\n \t\\specialcell{$\\mathbf{M}$ \\\\ $\\mathcal{M}$ \\\\ $\\mathbf{M}$ \\\\ $\\mathcal{M}$} &\n \t\\specialcell{${\\sf BE}$ \\\\ ${\\sf BE}$ \\\\ ${\\sf OE2}$ \\\\ ${\\sf OE2}$ } &\n \t\\specialcell{${\\sf TR}$ \\\\ ${\\sf TR}$ \\\\ ${\\sf TR}$ \\\\ ${\\sf TR}$} &\n \t\\specialcell{$\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$ \\\\ $\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$} &\n \t& \\checkmark &&&\\checkmark& \n \t&&\\checkmark&&\n \t& & \\checkmark \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Xu et al.~  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{0}} &\n \t$\\mathcal{M}$ &\n \t${\\sf BE}$ &\n \t{\\sf HS} &\n \t$\\mathcal{Z}_{\\mathcal{M}}$ &\n \t& & & & &\n \t& & \\checkmark &&\n \t& & \\checkmark \n \t\\\\\n \tCarmony et al.  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n \t\\specialcell{\\pie{4}} & \n \t\\specialcell{\\pie{4}} & \n \t\\specialcell{\\pie{4}} &\n \t\\specialcell{\\pie{4}}  &\n \t$\\mathcal{M}$ &\n \t${\\sf BE}$ &\n \t{\\sf MI} &\n \t$\\mathcal{Z}_{\\mathcal{M}}$ &\n \t& \\checkmark & & & &\n \t& & \\checkmark & &\n \t&  & \\checkmark \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Hu and Tan  & \n    \\checkmark &&& \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{0}}  &\n    $\\mathbf{M}$ &\n    ${\\sf BE}$ &\n    {\\sf GM} &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    \\checkmark& \\checkmark & & & \\checkmark& \n    & & \\checkmark &&\n    \\checkmark & & \n    \\\\\n    Hu and Tan  & \n    \\checkmark &&& \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{0}}  &\n    $\\mathbf{M}$ &\n    ${\\sf BE}$ &\n    {\\sf GM} &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    \\checkmark& \\checkmark & & & \\checkmark &\n    & & \\checkmark &&\n    \\checkmark & & \n    \\\\\n    \\rowcolor{lightgray!30}\n    Demontis et al.  &\n    \\checkmark &&& \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} &\n    \\specialcell{\\pie{0} \\\\ \\pie{0}} & \n    \\specialcell{\\pie{0} \\\\ \\pieg} &\n    \\specialcell{\\pie{0} \\\\ \\pie{4}} &\n    $\\mathbf{M}$ &\n    ${\\sf OE2}$ &\n    {\\sf SF} &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    & \\checkmark & \\checkmark & & \\checkmark &\n    & \\checkmark & &&\n    & \\checkmark &  \n    \\\\\n    Grosse et al.  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n \t$\\mathbf{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf SF} &\n \t$\\mathcal{X}_{\\mathbf{M}}$ &\n \t& \\checkmark &\\checkmark & & \\checkmark &\n \t\\checkmark & \\checkmark & &&\n \t& \\checkmark &  \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Chen et al.   &\n    \\checkmark &&& \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n    $\\mathbf{M}$ &\n    ${\\sf OE2}$ &\n    {\\sf SF} &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    & \\checkmark & \\checkmark & & \\checkmark & \n    & \\checkmark&&&\n    \\checkmark & & \n    \\\\\n    Khasawneh et al.   &\n    \\checkmark &&& \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pieg} &\n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}}  &\n    \\specialcell{ $\\mathbf{M}$ \\\\ $\\mathcal{M}$} &\n    \\specialcell{${\\sf BE}$ \\\\ ${\\sf BE}$} &\n    \\specialcell{{\\sf TR} \\\\ {\\sf TR}} &\n    \\specialcell{$\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$} &\n    & \\checkmark & & & \\checkmark & \n    & \\checkmark & &&\n    \\checkmark & &  \n    \\\\\n    \\rowcolor{lightgray!30}\n    Dang et al.   &\n    \\checkmark &&& \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{4}} &\n    $\\mathcal{M}$ &\n    ${\\sf BE}$ &\n    {\\sf HS} &\n    $\\mathcal{Z}_{\\mathcal{M}}$ &\n    & & & & &\n    & & \\checkmark &&\n    &  & \\checkmark \n    \\\\\n    Mu{\\~n}oz-Gonz{\\'a}lez et al.  & \n    &&\\checkmark& \n    \\specialcell{\\pie{0} \\\\ \\pie{0}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{0}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}} & \n    \\specialcell{\\pie{0} \\\\ \\pie{4}}  &\n    $\\mathbf{M}$ &\n    ${\\sf OP}$ &\n    {\\sf GO} &\n    $\\mathcal{X}_{\\mathbf{M}}$ &\n    & \\checkmark & & &\\checkmark &\n    & & & \\checkmark &\n    \\checkmark & & \n    \\\\\n    \\rowcolor{lightgray!30}\n    Yang et al.   &\n    \\checkmark &&& \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{0}} &\n    $\\mathcal{M}$ &\n    ${\\sf BE}$ &\n    ${\\sf HS}$ &\n    $\\mathcal{Z}_{\\mathcal{M}}$ &\n    & \\checkmark & \\checkmark & & &\n    & & \\checkmark &&\n    & \\checkmark &  \n    \\\\\n    Rosenberg et al.   &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} &\n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}}  &\n \t\\specialcell{$\\mathbf{M}$ \\\\ $\\mathcal{M}$} &\n \t\\specialcell{${\\sf BE}$ \\\\ ${\\sf BE}$} &\n \t\\specialcell{{\\sf TR} \\\\ {\\sf TR}} &\n \t\\specialcell{$\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$} &\n \t& \\checkmark & & & \\checkmark &\n \t\\checkmark & \\checkmark & &&\n \t\\checkmark & &  \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Anderson et al.   &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{4}} &\n    \\specialcell{\\pie{0}} &\n \t$\\mathcal{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf GM} &\n \t$\\mathcal{Z}_{\\mathcal{M}}$ \n \t&\\checkmark& \\checkmark& \\checkmark & & \n \t& \\checkmark & \\checkmark  &&\n \t& \\checkmark & &  \n \t\\\\\n \tKreuk et al.  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n \t\\specialcell{\\pie{0}} & \n \t\\specialcell{\\pie{0}} & \n \t\\specialcell{\\pie{0}} & \n \t\\specialcell{\\pie{0}} &\n \t$\\mathbf{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf GO} &\n \t$\\mathcal{X}_{\\mathbf{M}}$ \n \t& &\\checkmark& & & \\checkmark\n \t& \\checkmark & \\checkmark &&\n \t& \\checkmark & &  \n \t\\\\\n \t\\rowcolor{lightgray!30}\n \tChen et al.  & \n \t\\checkmark &&& \n \t\\specialcell{\\pie{0}\\\\\\pie{4}\\\\ \\pie{0}} & \n \t\\specialcell{\\pie{0} \\\\ \\pie{4} \\\\ \\pie{4}} & \n \t\\specialcell{\\pie{0} \\\\ \\pie{4} \\\\ \\pieg} & \n \t\\specialcell{\\pie{0} \\\\ \\pie{0} \\\\ \\pie{0}} & \n \t\\specialcell{\\pie{0}\\\\ \\pie{0} \\\\ \\pie{0}}  &\n \t$\\mathbf{M}$ &\n \t{\\sf BP} &\n \t{\\sf SF} &\n \t$\\mathcal{X}_\\mathbf{M}$ &\n \t& \\checkmark & \\checkmark & & \\checkmark & \n \t& & & \\checkmark &\n \t& \\checkmark & \n \t\\\\\n    Al-Dujaili et al.  & \n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}}  &\n \t$\\mathbf{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf GO} &\n \t$\\mathcal{X}_{\\mathbf{M}}$ &\n \t& \\checkmark & & & \\checkmark &\n \t&& \\checkmark &&\n \t\\checkmark & & \n \t\\\\\n \t\\rowcolor{lightgray!30}\n \tSuciu et al.  & \n \t&\\checkmark&& \n \t\\specialcell{\\pieg \\\\ \\pie{0} \\\\ \\pie{0} \\\\ \\pie{0}} & \n \t\\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{0}} & \n \t\\specialcell{\\pie{0} \\\\ \\pieg \\\\ \\pie{0} \\\\ \\pie{0}} & \n \t\\specialcell{\\pieg \\\\ \\pieg \\\\ \\pie{4} \\\\ \\pie{0}} & \n \t\\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{0}}   &\n \t\\specialcell{$\\mathbf{M}$ \\\\ $\\mathcal{M}$} &\n \t\\specialcell{${\\sf BP}$ \\\\ ${\\sf BP}$} &\n \t\\specialcell{{\\sf SF} \\\\ {\\sf SF}} &\n \t\\specialcell{$\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$} &\n \t& \\checkmark & \\checkmark & & \\checkmark & \n \t& & &\\checkmark&\n \t& \\checkmark & \n \t\\\\\n    Kolosnjaji et al.   &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n \t$\\mathbf{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf GO} &\n \t$\\mathcal{X}_{\\mathbf{M}}$ \n \t&& \\checkmark&& & \\checkmark\n \t& \\checkmark & \\checkmark &&\n \t& \\checkmark & &  \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Suciu et al.   &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} & \n    \\specialcell{\\pie{0}} &\n \t$\\mathbf{M}$ &\n \t${\\sf OE2}$ &\n \t{\\sf GO} &\n \t$\\mathcal{X}_{\\mathbf{M}}$ &\n \t& \\checkmark & & & \\checkmark &\n \t\\checkmark & \\checkmark &&&\n \t\\checkmark & &  \n \t\\\\\n    Chen et al.  &\n \t\\checkmark &&& \n \t\\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{0} \\\\ \\pie{0}} &\n    \\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{4}} &\n    \\specialcell{\\pie{0} \\\\ \\pie{0} \\\\ \\pie{0} \\\\ \\pie{0}} &\n    \\specialcell{\\pie{4} \\\\ \\pie{4} \\\\ \\pie{4} \\\\ \\pie{4}} &\n    \\specialcell{\\pie{4} \\\\ \\pie{0} \\\\ \\pie{4} \\\\ \\pie{0}} &\n \t\\specialcell{$\\mathbf{M}$ \\\\ $\\mathbf{M}$} &\n \t\\specialcell{${\\sf OE1}$ \\\\ ${\\sf OE2}$} &\n \t\\specialcell{{\\sf GO} \\\\ {\\sf SF}} &\n \t\\specialcell{$\\mathcal{X}_{\\mathbf{M}}$\\\\ $\\mathcal{X}_{\\mathbf{M}}$} &\n \t& \\specialcell{\\checkmark } & \\specialcell{\\checkmark } & & \\checkmark &\n \t& \\checkmark & &  &\n \t& \\checkmark &  \n \t\\\\\n\t\\rowcolor{lightgray!30}\n\tPierazzi et al.  &\n\t\\checkmark &&& \n\t\\specialcell{\\pie{0}} & \n\t\\specialcell{\\pie{0}} &\n\t\\specialcell{\\pie{0}} & \n\t\\specialcell{\\pie{0}} &\n\t\\specialcell{\\pie{0}} &\n\t$\\mathcal{M}$ &\n\t${\\sf OE2}$ &\n\t{\\sf MI} &\n\t$\\mathcal{Z}_{\\mathbf{M}}$ &\n\t& \\checkmark & & & &\n\t& & \\checkmark & & \n\t& \\checkmark &  \n\t\\\\ \t \n\tLi and Li  & \n\t\\checkmark &&& \n\t\\specialcell{\\pie{4}} & \n\t\\specialcell{\\pie{0}} & \n\t\\specialcell{\\pie{0}} & \n\t\\specialcell{\\pie{0}} & \n\t\\specialcell{\\pie{0}}  &\n\t\\specialcell{$\\mathbf{M}$ \\\\ $\\mathcal{M}$} &\n\t\\specialcell{${\\sf OE2}$ \\\\ ${\\sf OE2}$} &\n\t\\specialcell{{\\sf MS} \\\\ {\\sf MS}} &\n\t\\specialcell{$\\mathcal{X}_{\\mathbf{M}}$ \\\\ $\\mathcal{Z}_{\\mathcal{M}}$} &\n\t& \\checkmark & & & \\checkmark &\n\t&& \\checkmark &&\n\t& \\checkmark & \n\t\\\\\\hline\n\t\\end{tabular}\n}\n\t\\label{tbl:attacks}\n\\end{table}", "cites": [3867, 3857, 6172, 7756, 3864, 3862, 937, 3871, 9099, 3854, 7754], "cite_extract_rate": 0.4230769230769231, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing AMD attack literature into a structured table with consistent dimensions across all cited papers. It abstracts key concepts such as attack objectives, assumptions, and broken properties, revealing patterns like the importance of the defender’s feature set for transfer attacks. While it includes some critical insights (e.g., the attacker’s freedom affecting evasion success), it primarily focuses on analysis rather than deep evaluation of limitations or gaps."}}
{"id": "f3db0c13-9b45-4545-9019-43245444f75d", "title": "Attacks using Gradient-based Optimization ({\\sf GO", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using Gradient-based Optimization ({\\sf GO"]], "content": ")}\nBiggio et al.~ propose solving the problem of optimal evasion attacks by leveraging gradient-based optimization techniques. They focus on high-confidence evasion attacks with small perturbations (cf. Definition \\ref{def:opt_evasion2}). Given a malware representation-label pair $(\\mathbf{x},y=+)$, the optimization problem  specified in Eq.\\eqref{eq:a8:evs2} with respect to {\\sf GO} is instantiated as:\n\\begin{align*}\n\t\\max_{\\delta_{\\mathbf{x}}} L_\\mathcal{A}(\\hat{F}_{\\hat\\theta}(\\mathbf{x}+\\delta_{\\mathbf{x}}),y=+) & = \\min_{\\delta_{\\mathbf{x}}} \\left(L(F_\\theta(\\mathbf{x}+\\delta_{\\mathbf{x}}, y=-)) - \\beta_a\\mathcal{K}(\\mathbf{x} + \\delta_{\\mathbf{x}})\\right) \\\\ ~~\\text{s.t.}~~(\\delta_{\\mathbf{x}}\\in[\\mathbf{0},\\overline{\\mathbf{u}}]) &\\land (C(\\mathbf{x},\\mathbf{x}+\\delta_{\\mathbf{x}})\\leq m),\n\\end{align*}\nwhere $\\beta_a\\geq 0$ is a balance factor and $\\mathcal{K}$ is a density estimation function for lifting $\\mathbf{x}+\\delta_{\\mathbf{x}}$ to the populated region of benign examples. Since $\\delta_{\\mathbf{x}}\\geq \\mathbf{0}$, the manipulation only permits object injections to meet the requirement of preserving malicious functionalities. The attack is validated by using the PDF malware detector and the feature representation is the number of appearances of hand-selected keywords (e.g., JavaScript). Because the perturbation is continuous, the authors suggest searching a discrete point close to the continuous one and aligning the point with $\\nabla L_\\mathcal{A}(\\mathbf{x}+\\delta_{\\mathbf{x}},y=+)$. \nThis attack makes the invertibility Assumption \\ref{assumption:inverse} because it operates in the feature space. Experimental results show that when $\\mathcal{I}$ employs no countermeasures, knowing $\\mathcal{I}$'s feature set $S$ and learning algorithm $F$ are sufficient for $\\mathcal{A}$ to evade $\\mathcal{I}$'s detector. This attack and its variants have been shown to evade PDF malware detectors , PE malware detectors , Android malware detectors , and Flash malware detectors . The kernel density estimation item makes the perturbed representation $\\mathbf{x}+\\delta_\\mathbf{x}$ similar to the representations of benign examples, explaining the successful evasion. In summary, the attack works under the ${\\sf Oracle}$, ${\\sf Measurability}$, and {\\sf Invertibility} assumptions. $\\mathcal{A}$'s input is, or $\\mathcal{A}$ can be characterized as, $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) = (1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_{\\mathbf{M}}) \\lor (0,0,1,*,0|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_{\\mathbf{M}})$ and $\\mathcal{A}$ breaks the {\\sf CR} property.\nAl-Dujaili et al.~ propose evasion attacks against DNN-based malware detectors in the feature space. In this attack,\n$\\mathcal{A}$ generates adversarial examples  with possibly large perturbations in the feature space. More precisely, given a representation-label pair $(\\mathbf{x},y=+)$, the optimization problem of Eq.\\eqref{eq:a8:evs2} with respect to {\\sf GO} is instantiated as:\n\t$\\max_{\\delta_{\\mathbf{x}}} L(F_{\\theta}(\\mathbf{x}+\\delta_{\\mathbf{x}}),y=+) ~~\\text{s.t.}~~ \\delta_{\\mathbf{x}} \\in [\\mathbf{0}, \\mathbf{1}-\\mathbf{x}].$\nThe attack has four variants, with each perturbing the representation in a different direction (e.g., normalized gradient of the loss function using the $\\ell_\\infty$ norm). A ``random'' rounding operation is used to map continuous perturbations into a discrete domain. When compared with the basic rounding (which returns 0 if the input is smaller than 0.5, and returns 1 otherwise), the ``random'' rounding means that the threshold of rounding is sampled from the interval $[0,1]$ uniformly. For binary feature representation, the manipulation set $\\mathbf{M}_\\mathbf{x}=[\\mathbf{0}, \\mathbf{1}-\\mathbf{x}]$ assures the flipping of 0 to 1. The effectiveness of the attack is validated using Windows malware detector in the feature space. In summary, the attack works under the {\\sf Oracle} and {\\sf Invertibility} assumptions with $\\mathcal{A}$ input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) =(0,1,1,1,1|\\mathbf{M}, {\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$ and breaks the {\\sf DR} property.\nKreuk et al.  propose an evasion attack in the feature space against MalConv, which is an end-to-end Windows PE malware detector (as reviewed in Section \\ref{sec:ml}) . Given a malware embedding code $\\mathbf{x}$, the optimization problem of Eq.\\eqref{eq:a8:evs2} with respect to {\\sf GO} is instantiated as:\n\t$\\min_{\\delta_{\\mathbf{x}}} L(F_{\\theta}([\\mathbf{x}|\\delta_{\\mathbf{x}}]),y=-) ~~\\text{s.t.}~~ \\Vert\\delta_{\\mathbf{x}}\\Vert_p \\leq \\epsilon,$\nwhere | means concatenation and $\\Vert\\cdot\\Vert_p$ is the $p$ norm where $p\\geq 1$. Because MalConv is learned from sequential data, perturbation means appending some content to the end of a PE file. Perturbations are generated in a single step by following the direction of the $\\ell_\\infty$ or $\\ell_2$ normalized gradients of the loss function . For instance, the attack based on the $\\ell_{\\infty}$ norm is\n$\\tilde{\\mathbf{x}}'=\\mathbf{x} - \\epsilon\\cdot\\sign(\\nabla_{\\bf x}(L(F_\\theta(\\mathbf{x}), -))$, where $\\sign(x)=+1~(-1)$ if $x\\geq0~(x<0)$.\nSince the embedding operation uses a look-up table to map discrete values (0, 1, $\\ldots$, 255) to the learned real-value vectors, the attack uses a nearest neighbor search to look for the learned embedding code close to $\\tilde{\\mathbf{x}}'$.  \nIn summary, the attack works under the {\\sf Oracle} and {\\sf Invertibility} assumptions with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$ and breaks the {\\sf RR} and {\\sf CR} properties.\nKolosnjaji et al.  and Suciu et al.  independently propose gradient-based attacks in the feature space to evade MalConv . Both studies also use the loss function exploited by Kreuk et al. . Kolosnjaji et al.  use the manipulation set $\\mathcal{M}$ corresponding to appending instructions at the end of a file. This attack proceeds iteratively and starts with randomly initialized perturbations. In each iteration, continuous perturbations are updated in the direction of the $\\ell_2$ normalized gradient of the loss function with respect to the input, and then a nearest neighbor search is applied to obtain discrete perturbations. Suciu et al.  perturb embedding codes in the direction of the $\\ell_\\infty$ normalized gradient of the loss function, while adding instructions in the mid of a PE file (e.g., between PE {\\em sections}) while noting that appended content could be truncated by MalConv. Both attacks work under the {\\sf Oracle} and {\\sf Invertibility} assumptions with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$ and break the {\\sf RR} and {\\sf CR} properties.\nMu{\\~n}oz-Gonz{\\'a}lez et al.  propose the optimal poisoning {\\sf OP} attack in the feature space (Definition \\ref{def:optimal poisoning attack}), which is NP-hard. In this case, the optimization problem of Eq.\\eqref{eq:a8:poi} is relaxed by supposing that the classifier is linear to render the optimization problem tractable . The attack is waged against Windows PE malware detectors. Feature set includes API calls, actions and modifications in the file system; each file is represented by a binary vector. The attack has two variants: one uses white-box input, where $\\mathcal{A}$ derives $D_{poison}'$ from $\\mathcal{I}$'s detector $f$; the other uses grey-box input, where $\\mathcal{A}$ knows $\\mathcal{I}$'s training set as well as feature set and trains a surrogate detector. The attack works under the {\\sf Oracle} and {\\sf Invertibility} assumptions with input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) =(1,1,1,1,1|\\mathbf{M},{\\sf OP}, {\\sf GO}, \\mathcal{X}_\\mathbf{M})\\lor(1,0,1,0,0|\\mathbf{M},{\\sf OP}, {\\sf GO},\\mathcal{X}_\\mathbf{M})$ and breaks {\\sf TR}.", "cites": [892, 1193, 923, 3867, 3873, 3862, 3872, 937, 3871, 3863, 9099, 3855, 3854, 3861], "cite_extract_rate": 0.8235294117647058, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple gradient-based evasion attack papers into a structured and coherent framework, linking their methods to assumptions and properties. It also offers critical insights by highlighting the conditions under which these attacks succeed or fail (e.g., defender's feature set, attacker's manipulation freedom). The abstraction is evident in how it generalizes these attacks under a unified conceptual lens, identifying broader principles about the AMD arms race."}}
{"id": "6dae1197-fcde-489c-835b-7117fc4737e8", "title": "Attacks using Sensitive Features ({\\sf SF", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using Sensitive Features ({\\sf SF"]], "content": ")}\nDemontis et al.  propose the optimal evasion {\\sf OE2} in the feature space to perturb important features in terms of their weights in the linear function $\\varphi(\\mathbf{x})={\\mathbf{w}}^\\top\\mathbf{x} + b$, where $\\mathbf{w}=(w_1,w_2,\\cdots,w_d)$ is a weight vector, $b$ is the bias, and $d$ is the dimension of feature space. The attack is waged against Drebin malware detector (which is reviewed in Section \\ref{sec:ml}). $\\mathcal{A}$ manipulates the $x_i$'s with largest $|w_i|$'s as follows:\nflip $x_i=1$ to $x_i=0$ if $w_i > 0$, flip $x_i=0$ to $x_i=1$ if $w_i < 0$, and do nothing otherwise, while obeying the manipulation set $\\mathbf{M}$ corresponding to the injection or removal of features. The attack works under the {\\sf Oracle}, {\\sf Measurability}, and {\\sf Invertiblity} assumptions with\ninput $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (0,0,1,*,0|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$ and breaks the {\\sf CR} property.\nGrosse et al.  propose a variant of the Jacobian-based Saliency Map Attack (JSMA)  in the feature space against the Drebin  malware detector (which is reviewed in Section \\ref{sec:ml}). Instead of using SVM, Deep Neural Network (DNN) is used to build a detector. Important features are identified by leveraging the gradients of the softmax output of a malware example with respect to the input. A large gradient value indicates a high important feature. $\\mathcal{A}$ only injects manifest features to manipulate Android Packages and generates adversarial files from $\\mathcal{I}$'s detector $f$. The attack works under the {\\sf Oracle}, {\\sf Measurability}, and {\\sf Invertiblity} assumptions\nwith input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$ and breaks the {\\sf RR} and {\\sf CR} properties.\nChen et al.  propose an evasion attack in the feature space by perturbing the important features derived from a wrapper-based feature selection algorithm . The attacker's loss function $L_\\mathcal{A}$ has two parts: (i) the classification error in the mean squared loss and (ii) the manipulation cost $C(\\mathbf{x}, \\mathbf{x}')=\\sum_{i=1}^{d}c_i|x_i - x_i'|$, where $\\mathbf{x}=(x_1,\\ldots,x_d)$, $\\mathbf{x}'=(x_1', \\ldots,x_d')$, and $c_i$ is the hardness of perturbing the $i$th feature while preserving malware's functionality. The attack is waged against Windows PE malware detector that uses hand-crafted Windows API calls as features and the binary feature representation. However, there are no details about the composition of manipulation set. This attack works under the {\\sf Oracle}, {\\sf Measurability}, and {\\sf Invertibility} assumptions with input $(a_1,\\ldots,a_5|A_7,\\cdots,A_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF}, \\mathcal{X}_\\mathbf{M}))$ and breaks {\\sf CR}.\nChen et al.  propose evasion attacks in the feature space against two Android malware detectors, MaMaDroid  and Drebin . The manipulation set $\\mathbf{M}$ corresponds to the injection of manifest features (e.g., {\\em activities}) and \nAPI calls. $\\mathcal{A}$ evades MaMaDroid by using the optimal evasion ${\\sf OE1}$ (Definition \\ref{def:opt_evasion1}) and ${\\sf OE2}$ (Definition \\ref{def:opt_evasion2}), and evades Drebin by using ${\\sf OE2}$. The optimization problem of ${\\sf OE1}$ Eq.\\eqref{eq:a8:evs1} is solved using an advanced gradient-based method known as C\\&W . ${\\sf OE2}$ is solved using JSMA . Because JSMA perturbs sensitive features, we categorize this attack into the {\\sf SF} group. The ${\\sf OE2}$ attack works under the {\\sf Oracle}, {\\sf Measurability} and {\\sf Invertibility} assumptions,\nwith four kinds of input \n$(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,0,1,0,0|\\mathbf{M},{\\sf OE2}, {\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (0,0,1,0,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (1,0,1,0,0|\\mathbf{M},{\\sf OE2},{\\sf SF}, \\mathcal{X}_\\mathbf{M}) \\lor (1,0,1,0,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$,\nand breaks the {\\sf CR} property. The ${\\sf OE1}$ attack works under the same assumptions with the same input except using attack technique {\\sf GO}, and breaks the {\\sf CR} property.\nChen et al.  propose a basic poisoning {\\sf BP} attack in the feature space against Android malware detectors. The feature set contains syntax features (e.g., permission, hardware, {API}) and semantic features (e.g., sequence of pre-determined program behaviors such as {\\tt getDevicedID}$\\rightarrow$ URL$\\rightarrow${\\tt openConnection}). The ML algorithm used is SVM, random forest, or $K$-Nearest Neighbor (KNN) . The malware representations are perturbed using a JSMA variant  against the SVM-based classifier (while noting JSMA is applicable neither to random forests nor to KNN because they are gradient-free). Feature manipulation set ${\\bf M}$ corresponds to the injection of syntax features. $\\mathcal{A}$ poisons $\\mathcal{I}$'s training set by injecting perturbed perturbations with label $-$. The attack works under the {\\sf Oracle}, {\\sf Measurability}, and {\\sf Invertibility} assumptions with input\n$(a_1,\\ldots,a_5|A_6,\\cdots,A_9)=(1,1,1,1,1|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (0,0,0,1,1|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (1,0,*,1,1|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})$ and breaks {\\sf TR}.\nSuciu et al.  propose a basic poisoning attack in both feature and problem spaces. The authors obtain $D'_{poison}$ by applying small manipulations to non-adversarial benign files and then obtain their labels as given by VirusTotal service . \n$\\mathcal{A}$'s objective is to make $\\mathcal{I}$'s classifier $f$ mis-classify a targeted malware file $z_{mal}$ as benign. $\\mathcal{A}$ proceeds as follow: (i) obtain an initial benign file $z_{ben}$, where $z_{ben} \\approx z_{mal}$ in the feature space with respect to the $\\ell_1$ norm; (ii) use the JSMA method  to manipulate $z_{ben}$ to $z'_{ben}$ by making a small perturbation so that they have similar feature representations; (iii) add $z'_{ben}$ and its label obtained from VirusTotal to $D'_{poison}$ and use $D_{train} \\cup D'_{poison}$ to train classifier $f'$ (Definition \\ref{definition:sf}); (iv) undo the addition if $z'_{ben}$ lowers the classification accuracy significantly, and accept it otherwise. The attack is waged against the Drebin malware detector and the manipulation set corresponds to the feature injection of permission, API, and strings. This attack works under the {\\sf Oracle}, {\\sf Measurability}, and {\\sf Inversiability} assumptions with input \n$(a_1,\\ldots,a_5|A_6,\\ldots,A_9) =(*,0,1,*,0|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})\\lor(1,1,1,1,1|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})\\lor(1,0,*,*,0|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M}) \\lor (1,0,1,0,0|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})$\nand breaks the {\\sf TR} property. The study generates adversarial malware examples, but does not test their malicious functionalities.", "cites": [894, 890, 3864, 7754, 3863, 7154], "cite_extract_rate": 0.375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers under a unified framework by categorizing attacks based on feature manipulation and assumptions. It critically analyzes the effectiveness and limitations of different attack strategies, such as OE2 and JSMA, and identifies patterns in how attackers target sensitive features. The abstraction is strong, as it maps attacks to a conceptual model involving assumptions and properties, offering a generalized view of the AMD arms race."}}
{"id": "6bd1592d-a0b4-4061-b526-553c6ebc409a", "title": "Attacks using MImicry ({\\sf MI", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using MImicry ({\\sf MI"]], "content": ")} Smutz and Stavrou  propose a mimicry attack in the feature space to modify features of a malicious file to mimic benign ones, where $\\mathcal{A}$ knows $\\mathcal{I}$'s classifier $f$. Discriminative features are identified by observing their impact on classification accuracy. The attack perturbs features of malware examples by replacing their value with the mean of the benign examples. The attack is leveraged to estimate the robustness of PDF malware detectors without considering the preservation of malware functionality. The attack works under the {\\sf Measurability} assumption with input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) = (1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf MI},\\mathcal{X})$ and breaks the {\\sf CR} property.\nMaiorca et al.  propose a reverse mimicry attack against PDF malware detectors in the problem space. Instead of modifying malicious files to mimic benign ones, $\\mathcal{A}$ embeds malicious payload (e.g., JavaScript code) into a benign file. The attack can be enhanced by using parser confusion strategies, which make the injected objects being neglected by feature extractors when rendered by PDF readers . The attack works under the {\\sf Oracle} assumption with input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) = (0,0,0,0,0|$ $\\mathcal{M},{\\sf BE},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf DR} property. \nPierazzi et al.  propose a white-box evasion attack against the Drebin malware detector and then an enhanced version of the detector in the problem space . They intend to bridge the gap between the attacks in the problem space and the attacks in the feature space.\nIn addition, four realistic constraints are imposed on the manipulation set $\\mathcal{M}$, including available transformation, preserved semantics, robustness to preprocessing, and plausibility. In order to cope with the side-effect features when incorporating gradient information of $\\mathcal{I}$'s classifier, the attacker first harvests a set of manipulations from benign files; \nManipulations in the problem space are used to query $\\mathcal{I}$'s feature extraction for obtaining perturbations in the feature space;\nan adversarial malware example is obtained by using the manipulations corresponding to the perturbations that have a high impact on the classification accuracy. This attack works under the {\\sf Oracle} assumption with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(1,1,1,1,1|\\mathcal{M},{\\sf OE2},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf DR} property.", "cites": [7754], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates ideas from multiple cited works to discuss mimicry-based attacks, particularly in feature and problem spaces, and connects them under a conceptual framework. It shows moderate abstraction by identifying constraints on manipulation sets and their implications, but the critical evaluation is limited—few limitations or trade-offs of the cited methods are discussed. Synthesis is strong as it links attacks to assumptions and security properties."}}
{"id": "b8ab1fcb-f749-4c70-9893-28b772af455c", "title": "Attacks using Heuristic Search ({\\sf HS", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using Heuristic Search ({\\sf HS"]], "content": ")}\nXu et al.~ propose black-box evasion attacks in the problem space against two PDF malware detectors known as PDFrate  and Hidost , respectively. Given a malicious file $z$, $\\mathcal{A}$ uses a genetic algorithm to iteratively generate $z'$ from $z$ as follows: (i) $\\mathcal{A}$ manipulates a set of candidates (or $z$ in the initial iteration) via object deletion, insertion, or replacement. (ii) $\\mathcal{A}$ queries these variants to $\\mathcal{O}$ and $f$. (iii) $\\mathcal{A}$ succeeds when obtaining a successful adversarial example $z'$, namely $({\\tt True} \\leftarrow \\mathcal{O}(z,z')) \\land (-\\leftarrow f(z'))$; otherwise, $\\mathcal{A}$ uses a score function to select candidates for the next iteration or aborts after reaching a threshold number of iterations. The score function $h$ varies with classifiers; for PDFrate, $h(\\mathcal{O},f,z,z')=0.5-f(z')$ if $\\mathcal{O}(z,z')={\\sf true}$, and returns -0.5 if $\\mathcal{O}(z,z')={\\sf false}$. This attack models an {\\sf Oracle} and works with the input $(a_1,\\cdots, a_5|A_6,\\cdots,A_9) = (0,0,0,0,1|\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf DR} property.\nYang et al.  propose evasion attacks against Android malware detectors in the problem space. In this attack, $\\mathcal{A}$ also uses a genetic algorithm to perturb a malware example $z$ iteratively. In each iteration, $\\mathcal{A}$ extracts some features and calculates similarity scores between the malicious APKs in the feature space; the features that have high impact on the similarity scores are selected; the manipulations are to perturb the selected features.\nThe attack works under the {\\sf Oracle} and {\\sf Measurability} assumptions with input $(a_1,\\cdots, a_5|A_6,\\cdots,A_9) = (0,0,0,0,1|\\mathcal{M},\\mathcal{Z},{\\sf BE}, {\\sf HS},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf DR} property.\nDang et al.  propose a black-box evasion attack against malware detectors (e.g., PDFrate) in the problem space. Given a malicious file $z$, $\\mathcal{A}$ uses the hill-climbing algorithm to iteratively generate adversarial file $z'$ from $z$. In each iteration, $\\mathcal{A}$ generates a path of variants sequentially, each of which is perturbed from its predecessor using manipulations corresponding to object deletion, insertion, or replacement. A score function $h$ is leveraged to select candidates, such as $h(\\mathcal{O},f,z,z')={\\sf mal}_{z'}-{\\sf clf}_{z'}$ or $h(\\mathcal{O},f,z,z')={\\sf mal}_{z'}/{\\sf clf}_{z'}$, where ${\\sf mal}_{z'}$ denotes the length of the first example turned from malicious to benign (obtaining by using an oracle) on the manipulation path (cf. Definition \\ref{definition:hs}) and ${\\sf clf}_{z'}$ denotes the length of the first malware example that has successfully misled the classifier $f$. Both examples of interest are obtained by a binary search, effectively reducing the number of queries to oracle $\\mathcal{O}$ and $f$. The attack models an {\\sf Oracle} and works with input $(a_1,\\cdots, a_5|A_6,\\cdots,A_9) = (0,0,0,0,0|\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_{\\mathcal{M}})$ and breaks the {\\sf DR} property.", "cites": [3857], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.0, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section primarily describes three different papers on heuristic search-based attacks, explaining their methodologies, algorithms, and assumptions. While it uses a common framework (e.g., {\\sf Oracle} and {\\sf DR} properties) to structure the descriptions, it does not synthesize or connect these works into a broader narrative or trend. There is minim"}}
{"id": "33314056-9caf-48b3-90ff-0ca708a5a80c", "title": "Attacks using Generative Model ({\\sf GM", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using Generative Model ({\\sf GM"]], "content": ")}\nHu and Tan \\cite {Hu2017} propose an evasion attack against Windows malware detectors in the feature space, by using Generative Adversarial Networks (GAN) . In this attack, $\\mathcal{A}$ modifies the binary representation of Windows API calls made by malicious files, namely flipping some feature values from 0 to 1. $\\mathcal{A}$ learns a generator $G_{\\theta_{g}}$ and a discriminator from $\\mathcal{A}$'s training dataset. The discriminator is a surrogate detector learned from feature vectors corresponding to $\\mathcal{A}$'s benign files and those produced by $G_{\\theta_{g}}$, along with labels obtained by querying $\\mathcal{I}$'s detector $f$. An adversarial example feature vector is generated by using\n$\\mathbf{x}' = \\max(\\mathbf{x}, \\round(G_{\\theta_{g}}(\\mathbf{x}, \\mathbf{a})))$,\nwhere $\\mathbf{a}$ is a vector of noises, $\\round$ is the round function, and $\\max$ means element-wise maximum.\nHu and Tan~ also propose another evasion attack using the Seq2Seq model . Both attacks work under the {\\sf IID}, {\\sf Oracle} and {\\sf Invertibility} assumptions with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,0,1,0,1|\\mathbf{M},{\\sf BE},{\\sf GM},\\mathcal{X}_\\mathbf{M})$ and break the {\\sf DR} property.\nAnderson et al.  propose a Reinforcement Learning (RL)-based evasion attack against Windows PE malware detectors in the problem space. Manipulation set $\\mathcal{M}$ is the RL action space, which includes some bytecode injections (e.g., API insertion) and some bytecode deletion. Attacker $\\mathcal{A}$ learns an RL agent on $\\mathcal{A}$'s data, with labels obtained by querying defender $\\mathcal{I}$'s detector $f$. The learned agent predicts manipulations sequentially for a given malware example. Moreover, $\\mathcal{A}$ is restricted by only applying a small number of manipulations to a malicious PE file. Experimental results show that the attack is not as effective as others (e.g., gradient-based methods). The attack works under the {\\sf Oracle} and {\\sf Measurability} assumptions with input $(a_1,\\cdots, a_5|A_6,\\cdots,A_9) = (0,0,0,0,1|\\mathcal{M},{\\sf OE2},{\\sf GM},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf RR} and {\\sf CR} properties.", "cites": [243, 6172], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two distinct attack approaches—GAN-based and RL-based evasion attacks—into a structured framework, highlighting their assumptions and the security properties they break. It provides a critical comparison by noting the RL attack's lower effectiveness compared to gradient-based methods. However, it lacks deeper evaluation of the underlying strengths and weaknesses of generative models in adversarial settings, limiting its abstraction to a moderate level."}}
{"id": "c51df6e6-79f5-400d-8643-cef009ff014f", "title": "Attacks using Mixture Strategy ({\\sf MS", "level": "subsubsection", "subsections": [], "parent_id": "108d281b-7e1a-46ce-b750-a8b5a5ef8ec0", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Attack Literature"], ["subsubsection", "Attacks using Mixture Strategy ({\\sf MS"]], "content": ")} \nLi and Li  propose evasion attacks against DNN-based Android malware detectors in both feature and problem spaces. Given four gradient-based attack methods, the attack looks for the best one to perturb malware representations. $\\mathcal{A}$ can iteratively perform this strategy to modify the example obtained in the previous iteration. Experimental results show that the mixture of attacks can evade malware detectors effectively. The attack works under the {\\sf IID}, {\\sf Oracle} and {\\sf Invertibility} assumptions with input $(a_1,\\cdots, a_5|A_6,\\cdots,A_9) = (1,1,1,1,1|\\mathbf{M},{\\sf BE},{\\sf MS},\\mathcal{X}_\\mathbf{M})\\lor(1,1,1,1,1|\\mathcal{M},{\\sf BE},{\\sf MS},\\mathcal{Z}_\\mathcal{M})$ and breaks the {\\sf DR} property.", "cites": [9099], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes a specific type of attack proposed in one paper but lacks synthesis with other works. It provides minimal critical analysis or evaluation of the method's strengths and weaknesses. The abstraction level is limited to a surface-level generalization of the attack type without deeper conceptual insights."}}
{"id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "title": "Systematizing Defense Literature", "level": "subsection", "subsections": ["6f9fa41a-b356-42e4-b332-196f10febba0", "1e7bc569-0fe6-4f78-b486-391ed3ea051e", "272e13c4-c17a-4f6c-ab41-e2e83ef3c588", "89604438-0b71-470a-93c0-468dd4ea65af", "48b02090-502b-4798-92fb-c646a315077a", "a5b08aa1-d4f9-4077-b1e1-6b8c15efc187", "45225fbe-29bd-4000-8b60-553581208d2f", "362ab0d6-2e4b-4607-b141-4a89fdd71ebf", "7446c022-f8d7-4a15-911d-d2f735cce556"], "parent_id": "e26ef155-9b9c-4a92-9ffa-e334fbe6ea87", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"]], "content": "\\label{sec:defense}\n\\begin{table}[htbp]\n\t\\caption{Summary of AMD defenses (\\checkmark means applicable, \\protect\\fullcirc means 0, \\protect\\emptycirc means 1, \\protect\\dotcirc means a value in $[0,1]$)}\n\t\\resizebox{1.\\columnwidth}{!}{\n\t\\begin{threeparttable}\n\t\\centering\n\t\\setlength{\\tabcolsep}{0.4em}\n\t\\begin{tabular}{l|c|ccccc|cccc|ccccc|cccc|ccc}\n\t\\hline\n\t\\multicolumn{1}{c|}{\\specialcell{Defense \\\\ (in chronological order)}}& \n\t\\multicolumn{1}{c|}{\\specialcell{Defense \\\\ Objective}} &\n\t\\multicolumn{9}{c|}{\\specialcell{Defense Input}} & \n\t\\multicolumn{5}{c|}{\\specialcell{Assumptions}} & \n\t\\multicolumn{4}{c|}{\\specialcell{Achieved \\\\ Properties}} & \n\t\\multicolumn{3}{c}{\\specialcell{Malware\\\\Detector}}\n\t\\\\\n\t& \\vthead{malware detection} \n\t& \\vthead{$A_1$: Training set $D_{train}$}\n\t& \\vthead{$A_2$: Defense technique}\n\t& \\vthead{$A_3$: Feature set}\n\t& \\vthead{$A_4$: Learning algorithm}\n\t& \\vthead{$A_5$: Response}\n\t& \\vthead{$A_6$: Manipulation set}\n\t& \\vthead{$A_7$: Attack tactic}\n\t& \\vthead{$A_8$: Attack technique}\n\t& \\vthead{$A_9$: Adversarial example set}\n\t& \\vthead{${\\sf IID}$ assumption}\n\t& \\vthead{${\\sf Oracle}$ assumption}\n\t& \\vthead{${\\sf Measurability}$ assumption}\n\t& \\vthead{${\\sf Smoothness}$ assumption}\n\t& \\vthead{${\\sf Invertibility}$ assumption}\n\t& \\vthead{{\\sf RR}: Representation Robustness}\n\t& \\vthead{{\\sf CR}: Classification Robustness}\n\t& \\vthead{{\\sf DR}: Detection Robustness}\n\t& \\vthead{{\\sf TR}: Training Robustness}\n\t& \\vthead{Windows Program}\n\t& \\vthead{Android Package}\n\t& \\vthead{PDF}\n\t\\\\\n\t\\hline\\hline\n    \\rowcolor{lightgray!30}\n    Biggio et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf EL}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark& & & & &\n \t&\\checkmark&&&\n \t& &\\checkmark\n \t\\\\\n    Smutz and Stavrou  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf SE}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&&\\checkmark&&\n \t& & \\checkmark \n \t\\\\\n    \\rowcolor{lightgray!30}\n    Zhang et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf RF}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&\\checkmark&\\checkmark&&\n \t\\checkmark&\\checkmark&&&\n \t& & \\checkmark\n \t\\\\\n    Demontis et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf WR}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark& &\\checkmark& & &\n \t&\\checkmark&&&\n \t& \\checkmark &\n \t\\\\\n \t\\rowcolor{lightgray!30}\n \tWang et al.  &\n \t\\checkmark&\n \t$D_{train}$ &\n \t${\\sf IT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&\\checkmark&&&\n \t\\checkmark & &\n \t\\\\\n    Grosse et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf WR}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t\\checkmark&\\checkmark&&&\n \t& \\checkmark &\n \t\\\\\n    \\rowcolor{lightgray!30}\n    Grosse et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{0} &\n \t\\pie{4} &\n \t\\checkmark& &\\checkmark&&&\n \t\\checkmark&\\checkmark&&&\n \t& \\checkmark &\n \t\\\\\n    Chen et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{0} &\n \t\\pie{4} &\n \t\\checkmark& &\\checkmark&&&\n \t&\\checkmark&&&\n \t\\checkmark & &\n \t\\\\\n    \\rowcolor{lightgray!30}\n    Khasawneh et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf CD}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{0} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&\\checkmark&&&\n \t\\checkmark & &\n \t\\\\  \n \tDang et al.  &\n \t\\checkmark&\n \t$D_{train}$ &\n \t${\\sf SE}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf LQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&&\\checkmark&&\n \t& & \\checkmark \n \t\\\\  \n    \\rowcolor{lightgray!30}\n    Yang et al.  &\n    \\checkmark&\n \t\\specialcell{$D^\\ast_{train}$} &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pieg &\n \t\\checkmark&&&&&\n \t&&\\checkmark&&\n \t& \\checkmark &\n \t\\\\\n    Yang et al.  &\n    \\checkmark&\n \t\\specialcell{$D_{train}$} &\n \t${\\sf SE}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{0} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&&\\checkmark&&\n \t& \\checkmark &\n \t\\\\\n    \\rowcolor{lightgray!30}\n    Chen et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf RF}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark& &\\checkmark&\\checkmark& &\n \t\\checkmark&\\checkmark&&&\n \t& \\checkmark & \n \t\\\\\n    Incer et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf VL}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark& & & & &\n \t\\checkmark&\\checkmark&\\checkmark& & \n \t\\checkmark& & \n \t\\\\ \n \t\\rowcolor{lightgray!30}\n \tChen et al. ~ &\n \t\\checkmark&\n \t$D_{train}$ &\n \t${\\sf SE}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t&&\\checkmark&&&\n \t&&&\\checkmark&\n \t&\\checkmark &\n \t\\\\\n    Al-Dujaili et al.   &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&& &\n \t&&\\checkmark&&\n \t\\checkmark & &\n \t\\\\\n    \\rowcolor{lightgray!30}\n    Chen et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf IT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t&\\checkmark&&&\n \t& \\checkmark &\n \t\\\\\n    Jordan et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf RF}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t&&&&&\n \t\\checkmark&\\checkmark&\\checkmark&&\n \t& & \\checkmark\n \t\\\\ \n    \\rowcolor{lightgray!30}\n    Li et al.  &\n    \\checkmark&\n \t$D_{train}$ &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{4} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&\\checkmark&\\checkmark& &\n \t\\checkmark&\\checkmark&&&\n \t\\checkmark & &\n \t\\\\\n \tTong et al.  &\n \t\\checkmark&\n \t$D_{train}$ &\n \t${\\sf RF}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&&&\n \t\\checkmark&\\checkmark&\\checkmark&&\n \t& & \\checkmark\n \t\\\\ \n \t\\rowcolor{lightgray!30}\n \tLi and Li   &\n \t\\checkmark&\n \t$D_{train}$ &\n \t${\\sf AT}$ &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&&& &\n \t&&\\checkmark&&\n \t& \\checkmark &\n \t\\\\\n    Chen et al.  &\n    \\checkmark&\n    $D_{train}$ &\n    ${\\sf VL}$ &\n    $S$ &\n    $F_\\theta$ &\n    {\\sf FQ} &\n    \\pie{0} & \n    \\pie{0} & \n    \\pie{4} &\n    \\pie{4} &\n    \\checkmark& &\\checkmark & \\checkmark& &\n    \\checkmark&\\checkmark& & & \n    & & \\checkmark\n    \\\\\n    \\rowcolor{lightgray!30}\n    Li et al.  & \n \t\\checkmark &\n \t$D_{train}$ &\n \t\\small{{\\sf EL}+{\\sf AT}+{\\sf RF}} &\n \t$S$ &\n \t$F_\\theta$ &\n \t{\\sf FQ} &\n \t\\pie{0} & \n \t\\pie{0} & \n \t\\pie{4} &\n \t\\pie{4} &\n \t\\checkmark&&\\checkmark&& &\n \t&&\\checkmark&&\n \t& \\checkmark &\n \t\\\\\n    \\hline\n\t\\end{tabular}\n\t\\begin{tablenotes}\n\t\t\\small\n\t\t\\item $D^\\ast_{train}$ contains $D_{train}$ and a portion of $\\mathcal{A}$'s adversarial examples.\n\t\\end{tablenotes}\n    \\end{threeparttable}\n\t}\n\t\\label{tbl:defenses}\n\\end{table}", "cites": [3857, 3853, 9099, 3854, 9100, 7754, 3874, 3863], "cite_extract_rate": 0.38095238095238093, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section is primarily descriptive, summarizing various defenses using a structured table with categorized attributes. While it attempts to organize the literature systematically, it lacks deep synthesis across works or critical evaluation of their merits and limitations. The abstraction is moderate as the section uses a conceptual framework to categorize defenses, but it does not elevate these observations into broader meta-level principles or trends."}}
{"id": "6f9fa41a-b356-42e4-b332-196f10febba0", "title": "Defenses using Ensemble Learning ({\\sf EL", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Ensemble Learning ({\\sf EL"]], "content": ")} \\label{sec:defense-el}\nBiggio et al.  propose a one-and-a-half-class SVM classifier against evasion attacks, by leveraging an interesting observation (i.e., decision boundaries of one-class SVM classifiers are tighter than that of two-class SVM classifiers) to facilitate outlier detection. Specifically, the authors propose an ensemble of a two-class classifier and two one-class classifiers, and then combine them using another one-class classifier. \nThe defense can enhance PDF malware detectors against gradient-based attacks , which can be characterized as $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OP2}, {\\sf GO},\\mathcal{X}_\\mathbf{M})$. However, the defense cannot thwart attacks incurring large perturbations. Independent of this study, other researchers propose using the random subspace and bagging techniques to enhance SVM-based malware detectors, dubbed Multiple Classifier System SVM (MCS-SVM), which leads to evenly distributed weights .\nThese defenses work under the {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf EL},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and achieves the {\\sf CR} property.", "cites": [937, 7754], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two papers by discussing ensemble learning approaches to malware detection, connecting their use of SVM-based techniques. It provides some analytical framing by mapping the defenses to the unified conceptual framework (assumptions and properties). However, the critical evaluation is limited, as it does not deeply assess the strengths or weaknesses of the approaches or their broader implications."}}
{"id": "1e7bc569-0fe6-4f78-b486-391ed3ea051e", "title": "Defenses using Weight Regularization ({\\sf WR", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Weight Regularization ({\\sf WR"]], "content": ")} \\label{sec:attack-wr}\nDemontis et al.  propose enhancing the Drebin malware detector $\\varphi(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x} + b$ by using box-constraint weights. The inspiration is that the classifier's sensitivity to perturbations based on the $\\ell_1$ norm is bounded by the $\\ell_\\infty$ norm of the weights. This defense\nhardens the Drebin detector against a mimicry attack with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,0,1,0,0|\\mathbf{M},{\\sf BE},{\\sf MI},\\mathcal{X}_\\mathbf{M})$, obfuscation attack  with input $(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,0,0,0,$ $0|\\mathcal{M},{\\sf BE},-, \\mathcal{Z}_\\mathcal{M})$, and the attack that modifies important features   with input $(a_1,\\cdots,a_5|A_6,\\cdots,$ $A_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$, here `$-$' means inapplicable. Experimental results show this defense outperforms MSC-SVM . The defense works under the {\\sf IID}, {\\sf Oracle} and {\\sf Measurability} assumptions with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf WR},S,F_\\theta,{\\sf FQ}|1,1,0,0)$ and achieves {\\sf CR}.\nGrosse et al.  investigate how to apply two defense techniques known as distillation  and retraining  to enhance the DNN-based Drebin malware detector. The distillation technique can decrease a model's generalization error by leveraging a teacher to relabel the training data represented by real-value vectors (rather than one-hot encoding). It uses retraining to tune a learned model with respect to an augmented training set with adversarial examples. Both defenses are estimated against a variant of JSMA and can be characterized by their input as $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF}, \\mathcal{X}_\\mathbf{M})$. Experimental results show the two defenses achieve limited success. The defense based on the distillation technique works under the {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf WR},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and  achieves the {\\sf RR} and {\\sf CR} properties. The defense based on the retraining technique works under the {\\sf IID} and {\\sf Measurability} assumptions with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ}|1,$ $1,1,0)$ and achieves the {\\sf RR} and {\\sf CR} properties.", "cites": [314, 7754], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key contributions of two papers on weight regularization and distillation/retraining for adversarial malware detection, integrating them within a structured framework using inputs and assumptions. It provides some level of abstraction by mapping the defenses to generalized properties and assumptions, but the critical evaluation is limited to experimental success and failure without deeper analysis of methodological shortcomings or broader implications."}}
{"id": "272e13c4-c17a-4f6c-ab41-e2e83ef3c588", "title": "Defenses using Adversarial Training ({\\sf AT", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Adversarial Training ({\\sf AT"]], "content": ")}\nChen et al.  adapt a generic retraining framework proposed in the AML context  to enhance linear malware detectors. The defense uses a label smoothness regularization technique to mitigate the side-effect of adversarial training . The defense is evaluated using Windows malware detectors against ``feature selection''-based evasion attacks, which can be characterized as $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$. The defense works under the {\\sf IID} and {\\sf Measurability} assumptions and can be characterized as $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ}|1,1,1,0)$, while assuring the {\\sf CR} property.\nYang et al.  propose a defense against genetic algorithm-based evasion attacks that can be characterized as $(A_1,\\cdots,A_5|a_6,$ $\\cdots,a_9)=(0,0,0,0,1|\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$. The defense uses three techniques: adversarial training, sanitizing examples, and weight regularization . The adversarial training uses one half of $\\mathcal{A}$'s adversarial examples. The defense works under the {\\sf IID} assumption and can be characterized as\n$(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D'_{train},{\\sf AT},S,F_\\theta,{\\sf FQ}|0,1,0,*)$, where $D'_{train}$ is the union of $D_{train}$ and a portion (e.g., one half) of $\\mathcal{A}$'s adversarial examples. The defense of sanitizing examples is learned from manipulations used by the attacker and works under the IID assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf SE},S,F_\\theta,{\\sf FQ}|1,1,1,0)$. Both defenses achieve the {\\sf DR} property. The defense of wight regularization is reviewed in Section \\ref{sec:attack-wr}.\nAl-Dujaili et al.~ adapt the idea of minmax adversarial training (proposed in the AML context) to enhance DNN-based malware detectors. In this defense, the inner-layer optimization generates adversarial files by maximizing the classifier's loss function; the outer-layer optimization searches for the parameters $\\theta$ (of DNN $F_\\theta$) that minimize the classifier's loss with respect to the adversarial files.\nThe defense enhances Windows malware detectors against attacks with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$. \nExperimental results show that malware detectors that are hardened to resist one attack may not be able to defend against other attacks. By observing this phenomenon, researchers propose using a mixture of attacks to harden DNN-based malware detectors .\nThe defense works under the {\\sf IID} assumption and can be characterized as  $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ}|1,1,0,0)$. The defense assures the {\\sf DR} property. \nLi et al.  propose a DNN-based attack-agnostic framework to enhance adversarial malware detectors. The key idea is dubbed adversarial regularization, which enhances malware detectors via the (approximately) optimally small perturbation. The framework wins the AICS'2019 adversarial malware classification challenge organized by MIT Lincoln Lab researcher , without knowing anything about the attack. \nThe defense works under the {\\sf IID}, {\\sf Measurability}, and {\\sf Smoothness} assumptions with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and assures the {\\sf RR} and {\\sf CR} properties. In the extended study , the authors further enhance the framework with 6 defense principles, including ensemble learning, adversarial training, and robust representation learning. The enhanced defense is validated with 20 attacks (including 11 grey-box attacks and 9 white-box attacks) against Android malware detectors. The enhanced defense works under the {\\sf IID} and {\\sf Measurability} assumptions with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf AT}+{\\sf EL}+{\\sf RF},S,F_\\theta,{\\sf FQ}|1,1,0,0)$ and assures the {\\sf DR} property.", "cites": [3875, 9100, 7754, 9099, 3854], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on adversarial training for malware detection using a unified conceptual framework, which connects different approaches coherently. It includes critical observations, such as the limited effectiveness of defenses against varied attacks and the importance of attacker knowledge. The abstraction is strong, as the section identifies general patterns in the use of adversarial training, regularization, and ensemble learning across different malware detection systems."}}
{"id": "89604438-0b71-470a-93c0-468dd4ea65af", "title": "Defenses using Verifiable Learning ({\\sf VL", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Verifiable Learning ({\\sf VL"]], "content": ")}\nIncer et al.  propose using monotonic malware classifiers to defend  against evasion attacks, where {\\em monotonic} means\n$\\varphi(\\mathbf{x}) \\leq \\varphi(\\mathbf{x}')$ when ${\\mathbf{x}} \\leq {\\mathbf{x}}'$ . Technically, this can be achieved by using (i) robust features that can only be removed or added but not both and (ii) monotonic classification function (e.g., linear models with non-negative weights). The resulting classifier can thwart any attack that perturbs feature values monotonically. The defense works under the {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf VL},S,F_\\theta,{\\sf FQ}|1,1,0,0)$ and assures the {\\sf RR}, {\\sf CR} and {\\sf DR} properties.\nChen et al.  propose a defense to enhance PDF malware detectors against evasion attacks, by leveraging the observation that manipulations on PDF files are subtree additions and/or removals. They also propose new metrics for quantifying such structural perturbations. This allows to adapt the \n{\\em symbolic interval analysis} technique proposed in the AML context   to enhance the PDF malware detectors. The defense can cope with attacks leveraging small perturbations in the training phase. This defense works under the {\\sf IID}, {\\sf Measurability}, and {\\sf Smoothness} assumptions with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf VL},S,F_\\theta,{\\sf FQ}|1,1,0,0)$ and achieves the {\\sf RR} and {\\sf CR} properties.", "cites": [3877, 3876], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 2.5, \"abstraction\": 3.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section synthesizes two distinct approaches under the {\\sf VL} category, connecting their use of monotonicity and formal guarantees in adversarial contexts. It abstracts these methods into a general framework of assumptions and properties, providing some level of conceptual integration. However, it lacks deeper critical analysis of limitations or "}}
{"id": "48b02090-502b-4798-92fb-c646a315077a", "title": "Defenses using Robust Features ({\\sf RF", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Robust Features ({\\sf RF"]], "content": ")}\nZhang et al.  propose leveraging optimal adversarial attacks for feature selection. The defense enhances PDF malware detectors against gradient-based attacks, which can be characterized as $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_{\\mathbf{M}})$. The defense works under the {\\sf IID}, {\\sf Measurability}, and {\\sf Smoothness} assumptions and can be characterized as $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ}|1,1,0,0)$. The defense assures the {\\sf RR} and {\\sf CR} properties.\nTong et al.  propose refining features into invariant ones to defend against genetic algorithm-based evasion attacks with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,0,0,0,1|\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$. Experimental results show that adversarial training can be further leveraged to enhance the robustness of the defense. The defense works under {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)$ $=(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ}|1,1,0,0)$, and achieves {\\sf RR}, {\\sf CR} and {\\sf DR}.\nChen et al.  propose mitigating evasive attacks by filtering features according to their importance $|w_i|/c_i$  with respect to the linear function $\\varphi(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x} + b$, where $x_i$, $w_i$ and $c_i$ denote respectively the $i$th component of $\\mathbf{x}$, $\\mathbf{w}$ and the constraint on manipulation cost $\\mathbf{c}$. The defense enhances Android malware detectors against three attacks: a random attack with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,0,1,0,0|\\mathbf{M},{\\sf BE},-,\\mathcal{X}_\\mathbf{M})$, a variant of the mimicry attack with input $(0,0,1,0,0|\\mathbf{M},{\\sf BE},{\\sf MI},$ $\\mathcal{X}_\\mathbf{M})$, and the attack that modifies important features with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(1,1,1,1,1|\\mathbf{M},{\\sf BE},{\\sf SF},$ $\\mathcal{X}_\\mathbf{M})$, where `$-$' means inapplicable. The defense works under the {\\sf IID}, {\\sf Measurability}, and {\\sf Smoothness} assumptions with input $(A_1,\\cdots,A_5|a_6,$ $\\cdots,a_9)=(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ}|1,1,0,0)$ and achieves {\\sf RR} and {\\sf CR}.\nJordan et al.  propose a robust PDF malware detector against evasion attacks by interpreting JavaScript behaviors using static analysis. A PDF file is classified as malicious when it calls a vulnerable API method or when it exhibits potentially malicious or unknown behaviors. The defense is validated against the {\\em reverse mimicry} attack  with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,0,0,0,0|\\mathcal{M},{\\sf BE},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$. The defense has input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ}|1,$ $1,0,0)$ and achieves {\\sf RR}, {\\sf CR} and {\\sf DR}.", "cites": [3874, 3863], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the methods and assumptions of each cited paper using a structured format, but lacks deep synthesis or connections between the works. There is minimal critical analysis or identification of broader patterns or principles in the field."}}
{"id": "a5b08aa1-d4f9-4077-b1e1-6b8c15efc187", "title": "Defenses using Input Transformation ({\\sf IT", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Input Transformation ({\\sf IT"]], "content": ")}\nWang et al.  propose the {\\em random feature nullification} to enhance DNN-based malware detectors against the attack of Fast Gradient Sign Method (FGSM)  by nullifying (or dropping) features randomly in both training and testing phases. This offers a probabilistic assurance in preventing a white-box attacker from deriving adversarial files by using gradients of the loss function with respect to the input. \nThe defense enhances Windows malware detectors against the FGSM attack with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$. The defense works under {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,$ $\\cdots,a_9)=(D_{train},{\\sf IT},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and achieves {\\sf CR}.\nDroidEye  defends Android malware detectors against evasion attacks by quantizing binary representations, namely transforming binary representations into real values and then using compression to reduce the effect of adversarial manipulations. The defense enhances linear malware detectors against a ``feature selection''-based attack with input\n$(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(1,1,1,1,1|\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$  and the FGSM attack with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(0,1,1,1,$ $1|\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$ . The defense works under {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)$ $=(D_{train},{\\sf IT},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and  achieves {\\sf CR}.", "cites": [892, 3853], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes two specific input transformation-based defenses without offering substantial synthesis or connecting them to broader themes in adversarial malware detection. There is minimal critical evaluation or comparison of the approaches, and the analysis remains at a concrete level without abstracting general principles or patterns."}}
{"id": "362ab0d6-2e4b-4607-b141-4a89fdd71ebf", "title": "Defenses using Sanitizing Examples ({\\sf SE", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Defenses using Sanitizing Examples ({\\sf SE"]], "content": ")}\nSmutz and Stavrou  propose an ensemble classifier to defend against grey-box evasion attacks by returning classification results as benign, uncertain and malicious according to the voting result (e.g., $[0\\%,25\\%]$ classifiers saying malicious can be treated as benign, $[25\\%,75\\%]$ saying malicious can be treated as uncertain, and $[75\\%,100\\%]$ saying malicious can be treated as malicious). The defense enhances a PDF malware detector against three types of evasion attacks: gradient-based attack  with input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) =(1,0,*,*,0|\\mathcal{M},{\\sf OE2},{\\sf TR},\\mathcal{Z}_\\mathbf{M})$, mimicry attack with input $(1,0,*,*,0|\\mathcal{M},{\\sf BE},{\\sf TR},\\mathcal{Z}_\\mathbf{M})$, and reverse mimicry attack with input $(0,0,0,0,0|\\mathcal{M},{\\sf BE},{\\sf MI},\\mathcal{Z}_\\mathbf{M})$ .\nThe defense works under the {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)$ $=(D_{train},{\\sf SE},S,F_\\theta,{\\sf FQ}|0,1,0,0)$ and achieves {\\sf DR}.\nDang et al.  propose enhancing PDF malware detectors by lowering the classification threshold $\\tau$ and restricting the maximum query times, rendering genetic algorithm-based evasion attacks harder to succeed. This defense works under the {\\sf IID} assumption with input $(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf SE},S,F_\\theta,{\\sf LQ}|0,1,0,0)$ and achieves {\\sf DR}.\nChen et al.  investigate defending Android malware detectors against poisoning attacks with input $(a_1,\\ldots,a_5|A_6,\\cdots,A_9) =(1,1,1,1,1|\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})$\nThe idea is to filter adversarial files that are distant from non-adversarial ones, where distance is measured by the Jaccard index, Jaccard-weight similarity and cosine similarity.\nThe defense works under the {\\sf Measurability} assumption with input \n$(A_1,\\cdots,A_5|a_6,\\cdots,a_9)=(D_{train},{\\sf SE},S,F_\\theta,$ ${\\sf FQ}|0,1,0,0)$\nand achieves {\\sf TR}.", "cites": [3857], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes individual defenses from different papers, with minimal synthesis or integration of ideas across works. While it introduces a structured format with assumptions and security properties, it lacks deeper comparative or critical analysis, such as evaluating the relative strengths and weaknesses of the approaches. The abstraction remains limited to the specific methods discussed."}}
{"id": "7446c022-f8d7-4a15-911d-d2f735cce556", "title": "Drawing Observations and Insights", "level": "subsubsection", "subsections": [], "parent_id": "ecf055b9-207b-4f7f-9c11-8bb57d05480e", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing Defense Literature"], ["subsubsection", "Drawing Observations and Insights"]], "content": "Summarizing the preceding discussions, we draw the following observations.\n(i) Most studies focus on black-box defenses (i.e., the defender knows little about the attacker), which is against the principle of ``knowing yourself and knowing your enemy\". (ii) Most studies focus on defenses against evasion attacks rather than poisoning attacks. (iii) There is no silver bullet defense against evasion attacks or poisoning attacks, at least for now. (iv) Sanitizing adversarial files as outliers is effective against black-box and grey-box attacks, but not white-box attacks. (v) The security properties achieved by defenses have been evaluated empirically rather than rigorously proven (despite that provable security is emerging on the small degree of perturbations; see for example ). (vi) There is no theoretical evidence to support that the effectiveness of defense tactics on the training set (e.g., adversarial training and verifiable learning) can generalize to other adversarial examples.\nIn addition, we draw the following insights:\n\\begin{insight}\n(i) Effective defenses often require the defender to know the attacker's manipulation set. In the real world, it is hard to achieve this, explaining from one perspective why it is hard to design effective defenses.\n(ii) The effectiveness of adversarial training depends on the defender's capability in identifying the most powerful attack.\n\\end{insight}\n\\ignore{\nIn order to draw further insights, we train another {\\em random forest} model from the systematized structure data presented in Table \\ref{tbl:defenses}. We use attributes of defense objective, defense input, and assumptions as features and treat the achieved properties as the labels to learn. The data preprocessing is similar to insight learning from AMD attacks. This leads to a dataset of 22 defenses, each of which has 15 dimensions. \n\\begin{figure}[!htbp]\n\t\\centering\n\t\\scalebox{0.25}{\n\t\\includegraphics{figures/defense-important-feat.eps}\n\t}\n\\caption{Insights learning from systematized AMD defenses via attribute importance.\n}\n\t\\label{fig:defense-attribute}\n\\end{figure}\nFigure \\ref{fig:defense-attribute} shows the most important features on AMD defenses, from which we make the following observations. (i) Defense technique is the most important feature in determining the empirically achieved security property. This can be explained because ${\\sf EL},{\\sf IT},{\\sf CD}, {\\sf VL}, {\\sf WR}$ enhance the classifier against small manipulations to achieve the {\\sf CR} property; {\\sf SE} usually achieves the {\\sf DR} property against large manipulations, which could be detected as outliers; ${\\sf AT},{\\sf RF}$ enhance classifiers against certain attacks specified by the defender. (ii) The {\\sf Measurability} assumption is the second important feature.\n(iii) Manipulation set is the third important feature because knowing this attribute enables the defender to specify {\\sf RF} and {\\sf AT} techniques, which can enhance classifiers to achieve empirical {\\sf RR}, {\\sf CR} and {\\sf DR} properties.\n\\begin{insight}[Insights automatically learned]\nTechnique-wise, defense technique largely determines what security properties can be (empirically) achieved and knowing attacker's manipulation set is critical to defender's success.\n\\end{insight}\n}", "cites": [966], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes a range of defensive strategies in adversarial malware detection and connects them through a structured lens, including assumptions, attack types, and security properties. It offers some critical observations, such as the lack of a silver bullet and the limitations of empirical evaluations, but does not deeply critique the cited works. The section identifies broader patterns, such as the importance of the defender knowing the attacker’s manipulation set, indicating a moderate level of abstraction."}}
{"id": "76e4c68c-dec2-4af2-b3fa-ce426944cf52", "title": "Systematizing AMD Arms Race", "level": "subsection", "subsections": [], "parent_id": "e26ef155-9b9c-4a92-9ffa-e334fbe6ea87", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Systematizing AMD Arms Race"], ["subsection", "Systematizing AMD Arms Race"]], "content": "\\afterpage{\n\\begin{rotatepage}\n\\begin{figure*}[ht!]\n\\centering\n\\rotatebox{90}{\n\\scalebox{0.8}{\n\\begin{tikzpicture}\n\\node(rbot1)[label=below:{}, label=left:{},align=center] at (20,3.6) {$ (*,0,1,*,0)$\\\\$ (1,0,1,0,0)$\\\\$ (1,0,*,*,0)$};\n\\node(rbot1_attr)[label=below:{},align=center] at (20,2.6) {$(\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})$ \\\\ $(\\mathcal{M},{\\sf BP},{\\sf SF},\\mathcal{Z}_\\mathcal{M})$};\n\\node(rbot1_tag)[label=below:{},align=center] at (20,2.1) {};\n\\node(rbot)[label=below:{}, label=left:{}] at (20,1) {$ (1,1,1,1,1)$};\n\\node(rbot_attr)[label=below:{},align=center] at (20,0.6) {$(\\mathbf{M},{\\sf OE2},{\\sf MI},\\mathcal{X})$};\n\\node(rbot_tag)[label=below:{},align=center] at (20,0.2) {};\n\\node(rbot0)[label=below:{}, label=left:{}] at (20,-2) {$ (1,1,1,1,1)$};\n\\node(rbot0_attr)[label=below:{},align=center] at (20,-2.4) {$(\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$};\n\\node(rbot0_refer)[label=below:{},align=center] at (20,-2.8) {};\n\\node(lbot3)[label=below:{}, label=left:{}] at (0,3) {$ (0,0,0,0,1)$};\n\\node(lbot3_attr)[label=below:{},align=center] at (0,2.6) {$(\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$};\n\\node(lbot3_refer)[label=below:{},align=center] at (0,2.2) {};\n\\node(lbot2)[label=below:{}, label=left:{}] at (0,1) {$ (0,0,0,0,0)$};\n\\node(lbot2_attr)[label=below:{},align=center] at (0,0.6) {$(\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$};\n\\node(lbot2_refer)[label=below:{},align=center] at (0,0.2) {};\n\\node(lbot1)[label=below:{}, label=left:{}] at (0,-1) {$ (0,0,0,0,0)$};\n\\node(lbot1_attr)[label=below:{},align=center] at (0,-1.4) {$(\\mathcal{M},{\\sf BE},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$};\n\\node(lbot1_refer)[label=below:{},align=center] at (0,-1.8) {};\n\\node(lbot0)[label=below:{}, label=left:{}] at (0,-3) {$ (0,0,0,0,0)$};\n\\node(lbot0_attr)[label=below:{},align=center] at (0,-3.4) {$(\\mathcal{M},{\\sf BE},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$};\n\\node(lbot0_refer)[label=below:{},align=center] at (0,-3.8) {};\n\\node(above7_attr)[label=below:{},align=center] at (20,6) {$(\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$};\n\\node(above7)[label=below:{}, label=left:{}] at (20,6.4) {$ (0,1,1,1,1)$};\n\\node(above7_refer)[label=below:{},align=center] at (20,6.8) {};\n\\node(above6_attr)[label=below:{},align=center] at (16.8,6) {$(\\mathbf{M},{\\sf OE2},{\\sf MS},\\mathcal{X}_\\mathbf{M})$};\n\\node(above6)[label=below:{}, label=left:{}] at (16.8,6.4) {$ (0,1,1,1,1)$};\n\\node(above6_refer)[label=below:{},align=center] at (16.8,6.8) {};\n\\node(above5_attr)[label=below:{},align=center] at (14,6) {$(\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$};\n\\node(above5)[label=below:{}, label=left:{}] at (14,6.4) {$ (0,1,1,1,1)$};\n\\node(above5_refer)[label=below:{},align=center] at (14,6.8) {};\n\\node(above4_attr)[label=below:{},align=center] at (11.2,6) {$(\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$}; \n\\node(above4)[label=below:{}, label=left:{}] at (11.2,6.4) {$ (0,1,1,1,1)$};\n\\node(above4_refer)[label=below:{},align=center] at (11.2,6.8) {};\n\\node(above3_attr)[label=below:{},align=center] at (8.4,6) {$(\\mathcal{M},{\\sf OE2},{\\sf MI},\\mathcal{Z}_\\mathcal{M})$};\n\\node(above3)[label=below:{}, label=left:{}] at (8.4,6.4) {$ (1,1,1,1,1)$};\n\\node(above3_refer)[label=below:{},align=center] at (8.4,6.8) {};\n\\node(above2_attr)[label=below:{},align=center] at (5.6,6) {$(\\mathcal{M},{\\sf BE},{\\sf HS},\\mathcal{Z}_\\mathcal{M})$};\n\\node(above2)[label=below:{}, label=left:{}] at (5.6,6.4) {$ (0,0,0,0,1)$};\n\\node(above2_refer)[label=below:{},align=center] at (5.6,6.8) {};\n\\node(above1_attr)[label=below:{},align=center] at (2.8,6) {$(\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$};\n\\node(above1)[label=below:{}, label=left:{}] at (2.8,6.4) {$ (0,0,1,*,0)$};\n\\node(above1_refer)[label=below:{},align=center] at (2.8,6.8) {};\n\\node(above0_attr)[label=below:{},align=center] at (0,5.8) {$(\\mathbf{M},{\\sf OE1},{\\sf GO},\\mathcal{X}_\\mathbf{M})$ \\\\ $(\\mathbf{M},{\\sf OE2},{\\sf SF},\\mathcal{X}_\\mathbf{M})$ };\n\\node(above0)[label=below:{}, label=left:{}] at (0,6.4) {$ (0,0,1,0,0)$};\n\\node(above0_refer)[label=below:{},align=center] at (0,6.8) {};\n\\node(below7)[label=below:{}, label=left:{}] at (20,-6.4) {$ (1,0,1,0,0)$};\n\\node(below7_attr)[label=below:{},align=center] at (20,-6) {$(\\mathbf{M},{\\sf OP},{\\sf GO},\\mathcal{X}_\\mathbf{M})$};\n\\node(below7_refer)[label=below:{},align=center] at (20,-6.8) {};\n\\node(below6)[label=below:{}, label=left:{}] at (16.8,-6) {$ (0,0,0,1,1)$};\n\\node(below6_attr)[label=below:{},align=center] at (16.8,-6.4) {$(\\mathbf{M},{\\sf BP},{\\sf SF},\\mathcal{X}_\\mathbf{M})$};\n\\node(below6_tag)[label=below:{},align=center] at (16.8,-6.8) {};\n\\node(below5)[label=below:{}, label=left:{}] at (14,-6) {$ (0,0,1,0,1)$};\n\\node(below5_attr)[label=below:{},align=center] at (14,-6.4) {$(\\mathcal{M},{\\sf BE},{\\sf TR},\\mathcal{Z}_\\mathcal{M})$};\n\\node(below5_refer)[label=below:{},align=center] at (14,-6.8) {};\n\\node(below4)[label=below:{}, label=left:{}] at (11.2,-6) {$ (0,0,1,0,1)$};\n\\node(below4_attr)[label=below:{},align=center] at (11.2,-6.4) {$(\\mathbf{M},{\\sf BE},{\\sf GM},\\mathcal{X}_\\mathbf{M})$};\n\\node(below4_refer)[label=below:{},align=center] at (11.2,-6.8) {};\n\\node(below3)[label=below:{}, label=left:{}] at (8.4,-6) {$ (0,0,1,*,0)$};\n\\node(below3_attr)[label=below:{},align=center] at (8.4,-6.4) {$(\\mathbf{M},{\\sf OE2},{\\sf GO},\\mathcal{X}_\\mathbf{M})$};\n\\node(below3_refer)[label=below:{},align=center] at (8.4,-6.8) {};\n\\node(below2)[label=below:{}, label=left:{}] at (5.6,-6) {$ (0,0,*,0,1)$};\n\\node(below2_attr)[label=below:{},align=center] at (5.6,-6.4) {$(\\mathcal{M},{\\sf BE},{\\sf TR},\\mathcal{Z}_\\mathbf{M})$};\n\\node(below2_refer)[label=below:{},align=center] at (5.6,-6.8) {};\n\\node(below1)[label=below:{}, label=left:{}] at (2.8,-6) {$ (0,0,0,0,1)$};\n\\node(below1_attr)[label=below:{},align=center] at (2.8,-6.4) {$(\\mathcal{M},{\\sf OE2},{\\sf GO},\\mathcal{Z}_\\mathcal{M})$};\n\\node(below1_refer)[label=below:{},align=center] at (2.8,-6.8) {};\n\\node(below0)[label=below:{}, label=left:{}] at (0,-6) {$ (0,0,*,0,0)$};\n\\node(below0_attr)[label=below:{},align=center] at (0,-6.4) {$(\\mathcal{M},{\\sf BE},{\\sf TR},\\mathcal{Z}_\\mathbf{M})$};\n\\node(below0_refer)[label=below:{},align=center] at (0,-6.8) {};\n\\fill [lightgray!30] (1.5,5) rectangle (18.6,-5);\n\\node(mid4)[label=below:{},align=left] at (15.5,0) {:$(D_{train},{\\sf SE},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$\\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$\n};\n\\node(mid4-attr)[align=center] at (15.5,-0.85) {$(1,1,1,0)$};\n\\node(mid3)[label=below:{$(1,1,0,0)$},align=left] at (10,1.88) {\n:$(D_{train},{\\sf WR},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$  \\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf RF},S,F_\\theta,{\\sf FQ})$\n}; \n\\node(mid2)[label=below:{$(0,1,1,0)$},align=left] at (10,-1.5) {:$(D_{train},{\\sf CD},S,F_\\theta,{\\sf FQ})$};\n\\node(mid1)[label=below:{$(0,1,0,*)$},align=center] at (10,-3) {:$(D^\\ast_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$};\n\\node(mid0)[label=below:{$(0,1,0,0)$},align=left] at (4.5,0) {\n:$(D_{train},{\\sf SE},S,F_\\theta,{\\sf LQ})$\\\\\n:$(D_{train},{\\sf AT},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf IT},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf WR},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf SE},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf SE},S,F_\\theta,{\\sf FQ})$ \\\\\n:$(D_{train},{\\sf IT},S,F_\\theta,{\\sf FQ})$ \\\\ \n:$(D_{train},{\\sf EL},S,F_\\theta,{\\sf FQ})$\n};\n\\node(det3)[label=below:{},align=center, fill=blue!20,fill opacity=0.8] at (18.2,3) {};\n\\node(det2)[label=below:{},align=center, fill=blue!20,fill opacity=0.8] at (18.1,4.65) {};\n\\node(dnn1)[label=below:{},align=center, fill=blue!20,fill opacity=0.8] at (15.2, 3.7) {DNN-based malware \\\\ detector };\n\\node(dnn1_label)[align=center,color=blue!50] at (15.2,2.8) {$(D_{train},\\emptyset,S,F_\\theta,{\\sf FQ})$\\\\$(0,0,0,0)$};\n\\node(apk1)[label=right:{},align=center, fill=blue!20,fill opacity=0.8] at (6, 3.9) {Drebin };\n\\node(apk1_label) [align=center,color=blue!50] at (6.2,3.2) {$(D_{train},\\emptyset,S,F_\\theta,{\\sf FQ})$\\\\$(0,0,0,0)$};\n\\node(det1)[label=below:{},align=center, fill=blue!20,fill opacity=0.8] at (3.05,3.9) {};\n\\node(pdf2)[label=below:{},align=center, fill=blue!20,fill opacity=0.8] at (3.6, 3.2) {PDFrate };\n\\node(pdf2_label)[align=center,color=blue!50] at (3.6,2.4) {$(D_{train},\\emptyset,S,F_\\theta,{\\sf FQ})$\\\\$(0,0,0,0)$};\n\\node(pdf1)[label=below:{},label=below:{},align=left, fill=blue!20,fill opacity=0.8] at (3.6, -3.8) {PDFrate };\n\\node(pdf1_label)[align=center,color=blue!50] at (3.6,-4.6) {$(D_{train},\\emptyset,S,F_\\theta,{\\sf FQ})$\\\\$(0,0,0,0)$};\n{\n\\draw [-latex,line width=1] (3.3,-5.7) -- (3.3, -5) -- (2.3, -5) -- (2.3, -5.7);\n\\draw [-latex,line width=1] (11.7,-5.7) -- (11.7, -5) -- (10.7, -5) -- (10.7, -5.7);\n\\draw [-latex,line width=1] (14.5,-5.7) -- (14.5, -5) -- (13.5, -5) -- (13.5, -5.7);\n\\draw [-latex,line width=1] (20.5,-5.7) -- (20.5,-5) -- (19.5,-5) -- (19.5,-5.7);\n\\draw [-latex,line width=1] (19.5,1.2) -- (19.5,1.4) -- (18.6,1.4) -- (18.6,0) -- (19.5,0) -- (19.5,0.4); \n\\draw [-latex,line width=1] (above0_attr) -- (0, 3.9) -- (det1);\n\\draw [-latex,line width=1] (20,5.7) -- (20,4.65) -- (det2);\n\\draw [-latex,line width=1] (19,3.) -- (det3);\n\\draw [-latex,line width=1] (6.1,-5.7) -- (6.1, -5) -- (5.1, -5) -- (5.1, -5.7);\n\\draw [-latex,line width=1] (17.3,-5.7) -- (17.3, -5) -- (16.3, -5) -- (16.3, -5.7);\n\\ignore{\n\\draw [-latex,line width=1] (6.4,-0.2)node[red,right]{}--(5.8,-0.2);\n\\draw [-latex,line width=1] (6.4,0.2)node[red,right]{}--(5.9,0.2);\n\\draw [-latex,line width=1] (9.2,0.7)node[red,above]{}--(9.2,0.2);\n\\draw [-latex,line width=1] (6.4,-3.2)node[red,left]{}--(6.9,-3.2);\n\\draw [-latex,line width=1] (13.6,-3.2)node[red,left]{}--(14.1,-3.2);\n}\n\\draw [-latex,line width=1] (below3) -- (8.4,-5.0) -- (7.25,-5.0) -- (7.25,-1.5) -- (6.25,-1.5);\n\\draw [-latex,line width=1] (7.25,-1.5) -- (7.25,0.2) -- (8.1,0.2);\n\\draw [line width=1] (18.7,-2.4) -- (18.6,-2.4) -- (18.6,-1.05) -- (7.35,-1.05); \n\\draw [-latex,line width=1] (7.15,-1.05) --(6.25,-1.05);\n\\draw [-latex,line width=1] (12.55, -1.05) -- (12.55, 0.65) -- (11.75, 0.65);\n\\tkzDefPoint(7.25,-1.05){c1}\n\\tkzDefPoint(7.15,-1.05){c2}\n\\tkzDrawArc[rotate,color=black,line width=1](c1,c2)(180);\n\\draw [-latex,line width=1] (11.1,5.7) -- (11.1,4.3) -- (13.15,4.3) -- (13.15,3.7) -- (13.6, 3.7); \n\\draw [blue,densely dashdotted,-latex,line width=1] (16.8,3.7) -- (17.5,3.7) -- (17.5,0.1) -- (17.2,0.1); \n\\draw [red,dashed,-latex,line width=1] (11.3,5.7) -- (11.3,5.0) -- (13.9,5.0) -- (13.9,5.7);\n\\draw [-latex,line width=1] (14.1,5.7) -- (14.1,4.3) -- (17.7,4.3) -- (17.7,-0.1) -- (17.2,-0.1); \n\\draw [blue,densely dashdotted,-latex,line width=1] (13.55,0) -- (12.75,0) -- (12.75,1.8) -- (11.6,1.8);\n\\draw [red,dashed,-latex,line width=1] (14.3,5.7) -- (14.3,5.0) -- (16.7,5.0) -- (16.7,5.7);\n\\draw [line width=1] (17.3,5.7) -- (17.3,4.4);\n\\draw [-latex,line width=1] (17.3,4.2) -- (17.3,2) -- (11.6,2);\n\\draw [blue,densely dashdotted,-latex,line width=1] (8.05, 1.9) -- (7.25,1.9) -- (7.25,1.45) -- (8.1,1.45) ;\n\\tkzDefPoint(17.3,4.3){c3}\n\\tkzDefPoint(17.3,4.2){c4}\n\\tkzDrawArc[rotate,color=black,line width=1](c3,c4)(180);\n\\draw [-latex,line width=1] (2.7,5.7) -- (2.7,4.3) -- (4.7,4.3) -- (4.7,3.9) -- (5.2,3.9); \n\\draw [blue,densely dashdotted,-latex,line width=1] (6.9,3.9) -- (7.05,3.9) -- (7.05,3.58) -- (8.1,3.58); \n\\draw [red,dashed,-latex,line width=1] (2.9,5.7) -- (2.9,5.0) -- (8.5,5.0) --(8.5,5.7); \n\\draw [red,dashed,-latex,line width=1] (5.7,5.0) -- (5.7,5.7);\n\\draw [-latex,line width=1] (5.5,5.7) -- (5.5,4.3) -- (8.3,4.3) -- (8.3,3.7); \n\\draw [line width=1] (8.3,5.7) -- (8.3,4.3); \n\\draw [blue,densely dashdotted,-latex,line width=1] (11.9,3.7) -- (12.95,3.7) -- (12.95,0.43) -- (13.55,0.43);\n\\draw [blue,densely dashdotted,-latex,line width=1] (11.9,3.5) -- (12.75,3.5) -- (12.75,2.7) -- (11.8,2.7);\n\\draw [-latex,line width=1] (0.1, 3.1) -- (0.1,3.3) -- (2.5,3.3); \n\\draw [blue,densely dashdotted,-latex,line width=1] (2.4,3.1) -- (1.7,3.1) -- (1.7, 1.55) -- (2.5,1.55); \n\\draw [red,dashed,-latex,line width=1] (-0.1,2.1) -- (-0.1,1.1);\n\\draw [-latex,line width=1] (0.1,1.1) -- (0.1, 1.35) -- (2.5,1.35); \n\\draw [blue,densely dashdotted,-latex,line width=1] (4.7,3.2) -- (8.1,3.2);\n\\draw [-latex,line width=1] (0.1,-5.7) -- (0.1,-5) -- (1.5,-5) -- (1.5,-5) -- (1.5,-3.9) -- (2.5,-3.9); \n\\draw [blue,densely dashdotted,-latex,line width=1] (2.4,-3.7) -- (1.7,-3.7) -- (1.7, -0.75) -- (2.5, -0.75); \n\\draw [red,dashed,-latex,line width=1] (-0.1,-5.7) -- (-0.1,-3.9); \n\\draw [-latex,line width=1](0.1,-2.9) -- (0.1,-2) -- (1.5,-2) -- (1.5,-0.55) -- (2.5,-0.55); \n\\draw [blue,densely dashdotted,-latex,line width=1] (6.4,-0.6) -- (7.05,-0.6) -- (7.05,2.3) -- (8.1,2.3); \n\\draw [red,dashed,-latex,line width=1] (-0.1,-2.9) -- (-0.1,-2); \n\\draw [-latex,line width=1](0.1,-0.9) -- (0.1,-0.55) -- (2.5,-0.55);\n}\n\\draw [-latex,line width=1] (12.3,7.45) -- (12.9,7.45) node[right,align=center]{Waging \\\\ Attack};\n\\draw [blue,densely dashdotted,-latex,line width=1] (14.5,7.45) -- (15.1,7.45);\n\\node [right,align=center] at (15.1,7.45) {Defense \\\\ Escalation};\n\\draw [red,dashed,-latex,line width=1] (17,7.45) -- (17.6,7.45);\n\\node [right,align=center] at (17.6, 7.45) {Attack \\\\ Escalation};\n\\draw (11,7) -- (20,7) -- (20,7.9) -- (11,7.9) -- node [right]{Legend:}  (11,7);\n\\end{tikzpicture}\n}\n}\n\\caption{Arms race in AMD attack and defense escalations.}\n\\label{fig:attacks-defenses}\n\\end{figure*}\n\\end{rotatepage}\n}\nFigure \\ref{fig:attacks-defenses} displays AMD attack-defense arms race surrounding three malware detectors: PDFrate, Drebin, and DNN-based detector. For a better visual effect, we group papers that proposed defense methods in terms of a common input $(a_6,\\ldots,a_9)$. For example, we group\n, and  together because the defenders in both papers have input $(a_6,\\ldots,a_9)=(1,1,1,0)$, while noting that their input on $(A_1,\\ldots,A_5)$ may or may not be different.\nWe also simplify attack and defense inputs while preserving the {\\em critical information} when an attack (defense) works for multiple inputs. \nFor example, $(a_1,\\ldots,a_5|A_6,\\cdots,A_9)=(0,0,1,0,0|\\mathbf{M},{\\sf OE2}, {\\sf SF}, \\mathcal{X}_\\mathbf{M})$ is the {\\em critical information} for attack input  \n$(a_1,\\cdots,a_5|A_6,\\cdots,A_9)=(0,0,1,0,0|\\mathbf{M},{\\sf OE2}, {\\sf SF}, \\mathcal{X}_\\mathbf{M}) \\lor (0,0,1,0,1|\\mathbf{M},{\\sf OE2}, {\\sf SF}, \\mathcal{X}_\\mathbf{M}) \\lor (1,0,1,0,0|\\mathbf{M},{\\sf OE2}, {\\sf SF}, \\mathcal{X}_\\mathbf{M}) \\lor (1,0,1,0,1|\\mathbf{M},{\\sf OE2}, {\\sf SF},\\mathcal{X}_\\mathbf{M})$\nbecause it is the weakest attack input in the partial order formulated by these $(a_1,\\ldots,a_5)$'s. This suggests us to focus on attack input $(0,0,1,0,0|\\mathbf{M},{\\sf OE2}, {\\sf SF},\\mathcal{X}_\\mathbf{M})$ because it is already able to break some defense and automatically implies that a stronger input can achieve the same (while noting some special cases, see discussion in Section \\ref{sec:defense-el}). Multiple defense inputs are simplified in the same manner.\n{\\em Arms race in PDF malware detection}: We summarize two sequences of escalations caused by PDFrate . In one sequence, PDFrate is defeated by transfer attacks, which are realized by gradient-based and mimicry methods against surrogate models . These attacks trigger the defense escalation to an ensemble detector built on top of some diversified classifiers . This defense  triggers attack escalation to {\\em reverse mimicry attacks} , \nwhich trigger the defense escalation of using robust hand-crafted features . This defense represents the state-of-the-art PDF malware detector, but still incurs a high false-positive rate. In the other sequence of arms race, PDFrate is defeated by genetic algorithm-based attacks . These attacks trigger the defense escalation to  and . The former defense  restricts the responses to attacker queries, but can be defeated by the escalated attack that leverages the hill-climbing algorithm (also shown in ). The latter defense  uses invariant features to thwart the attacks and represents another state-of-the-art PDF malware detectors.\n{\\em Arms race in android malware detection}: Drebin is defeated by the attack that modifies a limited number of important features , which also proposes the new defense to defeat the escalated attack. This defense  triggers the attack escalation to, and is defeated by, the genetic algorithm-based attack  and the mimicry-alike attack . The former attack  triggers the escalated defense (also presented in ) that leverages attack mutations to detect adversarial examples . The latter attack  injects objects in APKs and (in principle) can be defeated by the monotonic classifier . These escalated defenses represent the state-of-the-art Android malware detectors, but still incur a high false-positive rate.\n{\\em Arms race in DNN-based malware detection}: The DNN-based detector  triggers four gradient-based evasion attacks presented in , which also hardens the DNN malware detector by using an {\\em minmax adversarial training} instantiation to incorporates the $\\ell_\\infty$ normalized gradient-based attack. This escalated defense  triggers the mixture of attacks presented in . The defense of {\\em minmax adversarial training} incorporating a mixture of attacks can defeat a broad range of attacks, but still suffers from the mimicry attack and other mixtures of attacks . \nAs a consequence, there are no effective defenses can thwart all kinds of attacks.\n{\\em Independent arms race}:\nThere are studies that have yet to trigger cascading arms races, including: (i) Studies  propose independent attacks and then show how to defeat these attacks.\n(ii) Studies  propose attacks to defeat naive malware detectors. (iii) Studies propose defenses to counter some attacks .", "cites": [3867, 3857, 6172, 3864, 7756, 3862, 937, 3871, 3853, 9099, 3854, 7754, 3874, 3863, 3861, 7154], "cite_extract_rate": 0.4, "origin_cites_number": 40, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an analytical perspective by mapping AMD attacks and defenses into a conceptual framework using tuples and attributes derived from the cited papers. It integrates multiple works to highlight broader patterns, such as the importance of the defender's feature set and the attacker's manipulation freedom. While the synthesis is strong and abstraction reaches a high level, the critical analysis is less explicit, with fewer direct evaluations of limitations or contradictions among the studies."}}
{"id": "965da080-653e-47f5-949b-de42288cf6a5", "title": "Future Research Directions (FDRs)", "level": "section", "subsections": [], "parent_id": "4fe25043-06f8-4577-bd62-3fe42c7ab899", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Future Research Directions (FDRs)"]], "content": "\\label{sec:cd}\n\\noindent{\\bf FRD 1}: {\\em Pinning down the root cause(s) of adversarial malware examples}.\nSpeculations on root cause(s) include: (i) invalidity of the {\\sf IID} assumption because of distribution drifting, namely that testing files and training files are drawn from different distributions   );\n(ii) incompetent feature extraction  ;\n(iii) high dimensionality of malware representations ;\n(iv) insufficient scale of training data ;\n(v) low-probability ``pockets'' in data manifolds ;\n(vi) linearity of DNNs ; and\n(vii) large curvature of decision boundaries .Although these speculations may be true, more studies are needed in order to (in)validate them. \n\\smallskip\n\\noindent{\\bf FRD 2}: {\\em Characterizing the relationship between transferability and vulnerability}.\nIn the AMD context, an attacker may use a surrogate model to generate adversarial examples and a defender may use a surrogate model for adversarial training. Transferability is related to the extent at which knowledge gained by a surrogate model may be the same as, or similar to, what is accommodated by a target model. The wide use of surrogate models in the AMD context suggests that there may be a fundamental connection between knowledge transferability and model vulnerability.\n\\smallskip\n\\noindent{\\bf FRD 3}: {\\em Investigating adversarial malware examples in the wild}. In the AMD context, it is challenging to generate practical adversarial malware examples to correspond to perturbations conducted in the feature space, owing to realistic constraints. On the other hand, an attacker can directly search for manipulations in the problem space. This may cause large perturbations, putting the value of studies on small perturbations in question. This represents a fundamental open problem that distinguishes the field of AMD from its counterparts in other application settings. \nThis issue is largely unaddressed by assuming that there is an oracle for telling whether manipulated or perturbed features indeed correspond to a malware sample or not.\n\\smallskip\n\\noindent{\\bf FRD 4}: {\\em Quantifying the robustness and resilience of malware detectors}. Robustness and resilience of malware detectors against adversarial examples need to be quantified, ideally with a provable guarantee. For this purpose, one may adapt the reduction-based paradigm underlying the provable security of cryptographic primitives and protocols.\n\\smallskip\n\\noindent{\\bf FRD 5}: {\\em Designing malware detectors with provable robustness and resilience guarantees}.\nHaving understood the root cause(s) of adversarial examples, characterized the effect of transferability, investigated the effectiveness of practical attacks, and designed metrics for quantifying the robustness and resilience of malware detectors, it is imperative to investigate robust malware detectors with provable robustness, ideally as rigorous as what has been achieved in the field of cryptography. In this regard, robust feature extraction, adversarial learning, and verifiable learning are promising candidates for making breakthroughs.\n\\smallskip\n\\noindent{\\bf FRD 6}: {\\em Forecasting the arms race in malware detection}. Arms race is a fundamental phenomenon inherent to the cybersecurity domain. In order to effectively defend against adversarial malware, one approach is to deploy proactive defense, which requires the capability to forecast the arms race between malware writers and defenders. For instance, it is important to predict how attacks will evolve and what kinds of information would be necessary in order to defeat such attacks.", "cites": [8399, 8696, 314, 892, 3878, 966, 937, 7754, 7312], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes key ideas from the cited papers to formulate insightful future research directions in adversarial malware detection. It abstracts from specific studies to highlight broader themes such as the arms race, the need for provable robustness, and the importance of understanding root causes and transferability. While it includes some critical elements by identifying open problems and limitations in current approaches, a deeper evaluation of conflicting perspectives or methodologies is less prominent."}}
{"id": "82494750-c0aa-44ab-add1-1d1d59481f4a", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "4fe25043-06f8-4577-bd62-3fe42c7ab899", "prefix_titles": [["title", "Arms Race in Adversarial Malware Detection: A Survey"], ["section", "Conclusion"]], "content": "\\label{sec:conc}\nWe have presented a framework for systematizing the field of AMD through the lens of assumptions, attacks, defenses and security properties. This paves the way for precisely relating attacks and defenses. We have also shown how to apply the framework to systematize the AMD literature, including the arms race between AMD attacks and defenses. We have reported a number of insights.\nThe study leads to a set of future research directions. In addition to the ones described in Section \\ref{sec:cd}, we mention the following two, which are discussed here because there are rarely studies on these aspects. (i) To what extent explainability (or interpretability) of ML models can be leveraged to cope with adversarial malware examples? It is intuitive that explainability could be leveraged to recognize adversarial examples because they may not be explainable . \n(ii) To what extent uncertainty quantification can be leveraged to cope with adversarial malware examples? If the uncertainty associated with detectors' predictions on adversarial malware examples are inherently and substantially higher than the uncertainty associated with non-adversarial malware examples, this fact can be leveraged to recognize adversarial malware examples.\nFinally, we reiterate that the research community should seek to establish a solid foundation for AMD. Although this foundation can leverage ideas and techniques from AML, the unique characteristics of AMD warrant the need of a unique foundation.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{mal_attacks,mal_defenses,aml,cyber_security,data_mining,mal_detection}\n\\end{document}", "cites": [7155], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides some analytical insights by identifying future research directions, such as the potential use of explainability and uncertainty quantification in AMD. It synthesizes the cited work to a limited extent by connecting it to broader themes in adversarial machine learning. However, the critical analysis is modest, as the section does not deeply evaluate or contrast the cited paper with others in the field. The abstraction is reasonable, pointing to general principles that could apply to AMD, but stops short of proposing a comprehensive meta-framework."}}
