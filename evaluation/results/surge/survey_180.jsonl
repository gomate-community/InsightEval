{"id": "9aecfbb3-eb2d-4acb-aa18-234de7f3c8fc", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "d39b8852-f438-4f5b-bfa6-286e79591ff4", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nUnderstanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. \nIt is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities~. \nIn particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed to biologically based intellect~.\nProper measurement helps to understand intelligence.\nFor instance, measures for general intelligence in human individuals often encompass IQ tests~.\n\\begin{figure*}[t!]\n\t\\centering\n\t\\resizebox{\\textwidth}{!}{\n\t\\begin{forest}\n  for tree={\n  grow=east,\n  reversed=true,\n  anchor=base west,\n  parent anchor=east,\n  child anchor=west,\n  base=left,\n  font=\\small,\n  rectangle,\n  draw,\n  rounded corners,align=left,\n  minimum width=2.5em,\n  inner xsep=4pt,\n  inner ysep=1pt,\n  },\n  where level=1{text width=5em,fill=blue!10}{},\n  where level=2{text width=5em,font=\\footnotesize,fill=pink!30}{},\n  where level=3{font=\\footnotesize,yshift=0.26pt,fill=yellow!20}{},\n  [\\llms \\\\ evaluation,fill=green!20\n        [What to evaluate\\\\(Sec.~\\ref{sec-what}),text width=7em\n            [Natural\\\\language\\\\processing,text width=4em\n              [\\emph{Natural language understanding:} \\\\\n              (1) Sentiment analysis: \n              / / / / /  \\\\ \n              (2) Text classification: \n               /  /    \\\\\n              (3) Natural language inference: \n               /   \\\\\n              (4) Others: \n               /  /   ]\n              [\\emph{Reasoning:} \n               /  /  /  /  /  /  \\\\  /   \n               /  /  /   /  /  \\\\  /  /  \\\\\n              ]\n              [\\emph{Natural language generation:} \\\\\n              (1) Summarization: \n               /  /  / \\\\\n              (2) Dialogue: \n               /  /  /   \\\\\n              (3) Translation: \n               /  /   \\\\\n              (4) Question answering: \n               /  /  /  /  /  \\\\\n              (5) Others: \n               /  / \n              ]\n              [\\emph{Multilingual:} \n               /  /  /  / ]\n              [\\emph{Factuality:} \n               /  /  /  /  / ]\n            ]\n            [Robustness / Ethics/ \\\\Biases/ Trustworthiness,text width=8em \n                [ \\emph{Robustness:} \n                 /  /  /  /  /  \\\\  / \n                ]\n                [ \\emph{Ethics and biases:} \n                 /  /  /  /  \n                 \\\\   / \n                 / \n                 /  / \n                 \\\\  /  /  /  \n                ]\n                [ \\emph{Trustworthiness:} \n                 /   /  /  /  \\\\ \n                 /  \n                ]\n            ]\n            [Social science, text width=5em\n                [ /  /  /  /  ]\n            ]    \n            [Natural science\\\\\\& engineering, text width=5em\n                [\\emph{Mathematics:} \n                 /  / /   /  / \n                 \\\\  / \n                ]\n                [\\emph{General science:} \n                 /  / ]\n                [\\emph{Engineering:} \n                 /  /  /  / \\\\  \n                 / \n                ]\n            ]\n            [Medical applications, text width=7em \n                [\\emph{Medical queries:} \n                 /  /  /  \\\\  /  \n                 /  /  ]\n                [\\emph{Medical examination:} \n                 / ]\n                [\\emph{Medical assistants:} \n                 /  /  / \n                 /  / ]\n            ]\n            [Agent applications, text width=6.5em\n            [\n             /  /  /  /  /  / \n            ]]\n            [Other\\\\applications, text width=4em\n                [\\emph{Education:} \n                 / citet / citet /  / ]\n                [\\emph{Search and recommendation:} \n                 /  /  /  /  \\\\  /  / ]\n                [\\emph{Personality testing:} \n                 /  /  /  /  / ] \n                [\\emph{Specific tasks:} \n                 /  / ]\n            ]\n        ]\n        [Where to evaluate\\\\(Sec.~\\ref{sec-where}),text width=7em\n          [General\\\\benchmarks,text width=4.2em\n            [Xiezhi~/MMLU~/ C-Eval~/OpenLLM~/DynaBench~/Chatbot Arena~/AlpacaEval~/HELM~/BIG-bench~\\\\ PandaLM~ / BOSS~ / GLUE-X~ \n            KoLA~ / AGIEval~/ PromptBench~ / MT-Bench~ / LLMEval²~]\n            ]\n          [Specific\\\\ benchmarks,text width=4.2em\n            [SOCKET~ / Choice-75~ / \n            CUAD~ / \n            TRUSTGPT~ / MATH~ / APPS~ / CELLO~ / EmotionBench~ / CMMLU~ \\\\ API-Bank~ /   M3KE~ / UHGEval~ / ARB~ / MultiMedQA~ /  CVALUES~ / ToolBench~ / FRESHQA~ \\\\ CMB~ / MINT~ / Dialogue CoT~ / M3Exam~ / GAOKAO-Bench~ / SafetyBench~]\n            ]\n             [Multi-modal\\\\ benchmarks,text width=4.2em\n            [MME~ / MMBench~ / SEED-Bench~ / MM-Vet~ / LAMM~ / LVLM-eHub~]\n            ]\n        ]\n        [How to evaluate\\\\(Sec.~\\ref{sec-how}),text width=7em\n          [Evaluation criterion, text width=7em\n              [\\emph{Automatic evaluation:} \n               /  /  /  /  \n              ]\n              [\\emph{Human evaluation:} \n               /  /  /  /  /   \n              ]\n          ]\n        ]\n        [Summary\\\\(Sec.~\\ref{sec-summary}),text width=4em\n          [Tasks: success and failure cases of \\llms,text width=14em\n          ]\n          [Benchmark and evaluations, text width=9.5em\n            [\\emph{Human-in-the-loop:} \n            AdaVision~ / AdaTest~\n            ]\n            [\\emph{Crowd-sourcing testing:} \n            DynaBench~ / DynaBoard~ / DynamicTempLAMA~ / \n            DynaTask~\n            ]\n            [\\emph{More challenging tasks:} \n            HELM~ / AdaFilter~ / CheckList~ / Big-Bench~ / \n            DeepTest~ / PromptBench~\n            ]\n          ]\n        ]\n        [Grand challenges\\\\(Sec.~\\ref{sec-challenge}),text width=7em\n          [Challenges,text width=4em\n            [(1) Designing AGI benchmarks (2) Complete behavioral evaluation (3) Robustness evaluation (4) Dynamic and evolving evaluation \\\\  (5) Principled and trustworthy evaluation (6) Unified evaluation that supports all \\llms tasks (7) Beyond evaluation: \\llms enhancement]\n            ]\n        ]\n    ]\n\\end{forest}\n\t}\n\t\\caption{Structure of this paper.}\n\t\\label{fig-main}\n\\end{figure*}\nWithin the scope of AI, the Turing Test~, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. \nIt is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. \nConsequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and algorithms. \nWith each emergence of a novel AI model or algorithm, researchers invariably scrutinize its capabilities in real-world scenarios through evaluation using specific and challenging tasks. \nFor instance, the Perceptron algorithm~, touted as an Artificial General Intelligence (AGI) approach in the 1950s, was later revealed as inadequate due to its inability to resolve the XOR problem. \nThe subsequent rise and application of Support Vector Machines (SVMs)~ and deep learning~ have marked both progress and setbacks in the AI landscape. \nA significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models.\nRecently, large language models (\\llms) have incited substantial interest across both academic and industrial domains~.\nAs demonstrated by existing work~, the great performance of \\llms has raised promise that they could be AGI in this era.\n\\llms possess the capabilities to solve diverse tasks, contrasting with prior models confined to solving specific tasks.\nDue to its great performance in handling different applications such as general natural language tasks and domain-specific ones, \n\\llms are increasingly used by individuals with critical information needs, such as students or patients.\nEvaluation is of paramount prominence to the success of \\llms due to several reasons.\nFirst, evaluating \\llms helps us better understand the strengths and weakness of \\llms.\nFor instance, the PromptBench~ benchmark illustrates that current \\llms are sensitive to adversarial prompts, thus a careful prompt engineering is necessary for better performance.\nSecond, better evaluations can provide better guidance for human-\\llms interaction, which could inspire future interaction design and implementation.\nThird, the broad applicability of \\llms underscores the paramount importance of ensuring their safety and reliability, particularly in safety-sensitive sectors such as financial institutions and healthcare facilities.\nFinally, as \\llms are becoming larger with more emergent abilities, existing evaluation protocols may not be enough to evaluate their capabilities and potential risks.\nTherefore, we aim to raise awareness in the community of the importance to \\llms evaluations by reviewing the current evaluation protocols and most importantly, shed light on future research about designing new \\llms evaluation protocols.\nWith the introduction of ChatGPT~ and GPT-4~, there have been a number of research efforts aiming at evaluating ChatGPT and other \\llms from different aspects (\\figurename~\\ref{fig-numbers}),  encompassing a range of factors such as natural language tasks, reasoning, robustness, trustworthiness, medical applications, and ethical considerations.\nDespite these efforts, a comprehensive overview capturing the entire gamut of evaluations is still lacking.\nFurthermore, the ongoing evolution of \\llms has also presented novel aspects for evaluation, thereby challenging existing evaluation protocols and reinforcing the need for thorough, multifaceted evaluation techniques.\nWhile existing research such as  claimed that GPT-4 can be seen as sparks of AGI, others contest this claim due to the human-crafted nature of its evaluation approach.\nThis paper serves as the first comprehensive survey on the evaluation of large language models.\nAs depicted in \\figurename~\\ref{fig-main}, we explore existing work in three dimensions: 1) What to evaluate, 2) Where to evaluate, and 3) How to evaluate. Specifically, ``what to evaluate\" encapsulates existing evaluation tasks for \\llms, ``where to evaluate\" involves selecting appropriate datasets and benchmarks for evaluation, while ``how to evaluate\" is concerned with the evaluation process given appropriate tasks and datasets. These three dimensions are integral to the evaluation of \\llms. We subsequently discuss potential future challenges in the realm of \\llms evaluation.\nThe contributions of this paper are as follows:\n\\begin{enumerate}\n    \\item We provide a comprehensive overview of \\llms evaluations from three aspects: what to evaluate, where to evaluate, and how to evaluate. Our categorization is general and encompasses the entire life cycle of \\llms evaluation.\n    \\item Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful conclusions on the success and failure case of \\llms (Sec.~\\ref{sec-summary}), providing experience for future research.\n    \\item As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to provide a profound understanding of current \\llms evaluations. In terms of how to evaluate, we explore current protocols and summarize novel evaluation approaches.\n    \\item We further discuss future challenges in evaluating \\llms. We open-source and maintain the related materials of \\llms evaluation at \\url{https://github.com/MLGroupJLU/LLM-eval-survey} to foster a collaborative community for better evaluations.\n\\end{enumerate}\nThe paper is organized as follows.\nIn Sec.~\\ref{sec-back}, we provide the basic information of \\llms and AI model evaluation.\nThen, Sec.~\\ref{sec-what} reviews existing work from the aspects of ``what to evaluate''.\nAfter that, Sec.~\\ref{sec-where} is the ``where to evaluate'' part, which summarizes existing datasets and benchmarks.\nSec.~\\ref{sec-how} discusses how to perform the evaluation.\nIn Sec.~\\ref{sec-summary}, we summarize the key findings of this paper.\nWe discuss grand future challenges in Sec.~\\ref{sec-challenge} and Sec.~\\ref{sec-con} concludes the paper.\n\\begin{figure}[t!]\n\\centering\n\\begin{tikzpicture}\n\\pgfplotstableread{\nYear Papers\n2020 0\n2021 3\n2022 3\n2023.01 2\n2023.02 7\n2023.03 12 \n2023.04 15\n2023.05 22\n2023.06+ 33\n}\\datatable\n\\begin{axis}[\n    ylabel={Number of papers},\n    xtick={1,2,3,4,5,6,7,8,9},\n    xticklabels={2020,2021,2022,2023.01,2023.02,2023.03,2023.04,2023.05,2023.06+},\n    xticklabel style={rotate=45, anchor=north east},\n    ymin=0,\n    ymax=35,\n    ytick distance=5,\n    legend pos=north west,\n    ymajorgrids=true,\n    grid=both,\n    grid style=dashed,\n    clip mode=individual,\n    font=\\small,\n]\n\\addplot[color=blue, mark=triangle, mark options={fill=blue}] table[x expr=\\coordindex+1, y=Papers] {\\datatable};\n\\legend{Number of papers}\n\\end{axis}\n\\end{tikzpicture}\n\\caption{Trend of \\llms evaluation papers over time (2020 - Jun. 2023, including Jul. 2023.).}\n\\label{fig-numbers}\n\\end{figure}", "cites": [3563, 8551, 3514, 3515, 8664, 9115, 3497, 2425, 7730, 3564, 7663, 3500, 431, 3557, 3529, 7573, 3519, 3525, 3560, 7667, 3490, 3549, 7574, 7638, 8535, 3566, 3527, 3561, 3543, 8663, 3554, 3546, 464, 2427, 3504, 3513, 3548, 3542, 7732, 2456, 3492, 3511, 366, 7648, 8665, 3533, 3505, 3532, 3521, 3537, 3517, 3540, 3558, 3503, 3524, 3562, 3550, 2205, 7731, 7122, 3083, 7141, 2218, 8667, 3565, 2958, 3526, 166, 3491, 3523, 3506, 3559, 3512, 7734, 7736, 3530, 3544, 3545, 2393, 7735, 3501, 3553, 3502, 7689, 7143, 3516, 3538, 3541, 3495, 8666, 3015, 3531, 3520, 3534, 1571, 3551, 7728, 7088, 7466, 3493, 1570, 8661, 1550, 3552, 3507, 3509, 3499, 3510, 7142, 3556, 3536, 7464, 8461, 3518, 3087, 3555, 3498, 3496, 3508, 8556, 3535, 7733, 440, 7729, 3522, 3092, 3547, 3080, 3528, 8662, 9151, 3539, 3494], "cite_extract_rate": 0.6767676767676768, "origin_cites_number": 198, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The introduction section primarily functions as a descriptive overview, listing various evaluation tasks, benchmarks, and methods without deeply synthesizing or critiquing the cited works. It provides a taxonomy of evaluation areas but lacks analysis of relationships between methods or evaluation of their strengths/weaknesses. Abstraction is minimal, with no clear meta-level insights or principles identified."}}
{"id": "e5f14a68-1b9f-4549-8f01-94dc66b8eac7", "title": "Large Language Models", "level": "subsection", "subsections": [], "parent_id": "3cb76e6c-44c4-4719-9f97-9f1e89bf5eea", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Background"], ["subsection", "Large Language Models"]], "content": "Language models (LMs)  are computational models that have the capability to understand and generate human language.\nLMs have the transformative ability to predict the likelihood of word sequences or generate new text based on a given input.\nN-gram models , the most common type of LM, estimate word probabilities based on the preceding context. \nHowever, LMs also face challenges, such as the issue of rare or unseen words, the problem of overfitting, and the difficulty in capturing complex linguistic phenomena. Researchers are continuously working on improving LM architectures and training methods to address these challenges. \nLarge Language Models (LLMs)  are advanced language models with massive parameter sizes and exceptional learning capabilities.\nThe core module behind many LLMs such as GPT-3 , InstructGPT , and GPT-4  is the self-attention module in Transformer\n that serves as the fundamental building block for language modeling tasks. Transformers have revolutionized the field of NLP with their ability to handle sequential data efficiently, allowing for parallelization and capturing long-range dependencies in text.\nOne key feature of \\llms is in-context learning , where the model is trained to generate text based on a given context or prompt. This enables LLMs to generate more coherent and contextually relevant responses, making them suitable for interactive and conversational applications.\nReinforcement Learning from Human Feedback (RLHF)  is another crucial aspect of \\llms. This technique involves fine-tuning the model using human-generated responses as rewards, allowing the model to learn from its mistakes and improve its performance over time.\nIn an autoregressive language model, such as GPT-3 and PaLM , given a context sequence \\(X\\), the LM tasks aim to predict the next token \\(y\\). The model is trained by maximizing the probability of the given token sequence conditioned on the context, i.e., $P(y | X) = P(y | x_1, x_2, ..., x_{t-1})$, where \\(x_1, x_2, ..., x_{t-1}\\) are the tokens in the context sequence, and \\(t\\) is the current position. By using the chain rule, the conditional probability can be decomposed into a product of probabilities at each position:\n\\begin{equation*}\nP(y | X) = \\prod_{t=1}^{T} P(y_t | x_1, x_2, ..., x_{t-1}),\n\\label{eq:autoregressive}\n\\end{equation*}\nwhere \\(T\\) is sequence length. In this way, the model predicts each token at each position in an autoregressive manner, generating a complete text sequence.\nOne common approach to interacting with \\llms is prompt engineering , where users design and provide specific prompt texts to guide LLMs in generating desired responses or completing specific tasks. This is widely adopted in existing evaluation efforts.\nPeople can also engage in question-and-answer interactions , where they pose questions to the model and receive answers, or engage in dialogue interactions, having natural language conversations with LLMs.\nIn conclusion, \\llms, with their Transformer architecture, in-context learning, and RLHF capabilities, have revolutionized NLP and hold promise in various applications.\n\\tablename~\\ref{tb-compare} provides a brief comparison of traditional ML, deep learning, and \\llms.\n\\input{tables/tb-related}", "cites": [679, 1354, 9115, 8461, 364, 3380, 38, 7465, 3568, 1554, 7, 3567], "cite_extract_rate": 0.631578947368421, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of LLMs, their architecture, and key features such as in-context learning and RLHF, drawing from multiple papers. It integrates some concepts, such as the role of the Transformer and prompt engineering, but does so in a descriptive manner without deep analysis or contrasting perspectives. While it touches on broader patterns like the shift from traditional ML to LLMs, it lacks critical evaluation of the cited works and deeper abstraction of underlying principles."}}
{"id": "d54f47dc-9dad-4afb-b0ff-962ccf011622", "title": "AI Model Evaluation", "level": "subsection", "subsections": [], "parent_id": "3cb76e6c-44c4-4719-9f97-9f1e89bf5eea", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Background"], ["subsection", "AI Model Evaluation"]], "content": "AI model evaluation is an essential step in assessing the performance of a model.\nThere are some standard model evaluation protocols, including \\(k\\)-fold cross-validation, holdout validation, leave one out cross-validation (LOOCV), bootstrap, and reduced set .\nFor instance, \\(k\\)-fold cross-validation divides the dataset into \\(k\\) parts, with one part used as a test set and the rest as training sets, which can reduce training data loss and obtain relatively more accurate model performance evaluation~; Holdout validation divides the dataset into training and test sets, with a smaller calculation amount but potentially more significant bias; LOOCV is a unique \\(k\\)-fold cross-validation method where only one data point is used as the test set~; Reduced set trains the model with one dataset and tests it with the remaining data, which is computationally simple, but the applicability is limited.\nThe appropriate evaluation method should be chosen according to the specific problem and data characteristics for more reliable performance indicators.\n\\figurename~\\ref{fig-eval-proc} illustrates the evaluation process of AI models, including \\llms.\nSome evaluation protocols may not be feasible to evaluate deep learning models due to the extensive training size.\nThus, evaluation on a static validation set has long been the standard choice for deep learning models.\nFor instance, computer vision models leverage static test sets such as ImageNet~ and MS COCO~ for evaluation.\n\\llms also use GLUE~ or SuperGLUE~ as the common test sets.\nAs \\llms are becoming more popular with even poorer interpretability, existing evaluation protocols may not be enough to evaluate the true capabilities of \\llms thoroughly.\nWe will introduce recent evaluations of \\llms in Sec.~\\ref{sec-how}.\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=.7\\textwidth]{Figures/fig-eval-proc.pdf}\n    \\caption{The evaluation process of AI models.}\n    \\label{fig-eval-proc}\n\\end{figure}", "cites": [1569, 486, 1568], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of AI model evaluation methods and their characteristics, but it lacks in-depth synthesis of the cited papers. While it mentions benchmarks like GLUE, SuperGLUE, and MS COCO, it does so in a general and non-integrated manner without connecting these to broader trends or insights in the field. There is minimal critical analysis or abstraction beyond the immediate methods discussed."}}
{"id": "a52655ad-ad2c-4d2f-81ee-e593fb9f1fbb", "title": "Natural language understanding", "level": "subsubsection", "subsections": [], "parent_id": "9627000a-5ef0-447d-8e16-8af6c358f8d1", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Language Processing Tasks"], ["subsubsection", "Natural language understanding"]], "content": "Natural language understanding represents a wide spectrum of tasks that aims to obtain a better understanding of the input sequence.\nWe summarize recent efforts in \\llms evaluation from several aspects.\n\\textbf{Sentiment analysis} is a task that analyzes and interprets the text to determine the emotional inclination.\nIt is typically a binary (positive and negative) or triple (positive, neutral, and negative) class classification problem.\nEvaluating sentiment analysis tasks is a popular direction.\n and  showed that the performance of the models on this task is usually high.\nChatGPT's sentiment analysis prediction performance is superior to traditional sentiment analysis methods  and comes close to that of GPT-3.5 .\nIn fine-grained sentiment and emotion cause analysis, ChatGPT also exhibits exceptional performance .\nIn low-resource learning environments, \\llms exhibit significant advantages over small language models , but the ability of ChatGPT to understand low-resource languages is limited .\nIn conclusion, \\llms have demonstrated commendable performance in sentiment analysis tasks. Future work should focus on enhancing their capability to understand emotions in under-resourced languages.\n\\textbf{Text classification} and sentiment analysis are related fields, text classification not only focuses on sentiment, but also includes the processing of all texts and tasks.\nThe work of  showed that GLM-130B was the best-performed model, with an overall accuracy of 85.8\\% for miscellaneous text classification.\n found that ChatGPT can produce credibility ratings for a wide range of news outlets, and these ratings have a moderate correlation with those from human experts.\nFurthermore, ChatGPT achieves acceptable accuracy in a binary classification scenario (AUC=0.89).\n discussed the problem of topic classification for public affairs documents and showed that using an LLM backbone in combination with SVM classifiers is a useful strategy to conduct the multi-label topic classification task in the domain of public affairs with accuracies over 85\\%.\nOverall, \\llms perform well on text classification and can even handle text classification tasks in unconventional problem settings as well.\n\\textbf{Natural language inference (NLI)} is the task of determining whether the given ``hypothesis'' logically follows from the ``premise''.\n showed that ChatGPT outperforms GPT-3.5 for NLI tasks.\nThey also found that ChatGPT excels in handling factual input that could be attributed to its RLHF training process in favoring human feedback. \nHowever,  observed \\llms perform poorly in the scope of NLI and further fail in representing human disagreement, which indicates that \\llms still have a large room for improvement in this field.\n\\textbf{Semantic understanding} refers to the meaning or understanding of language and its associated concepts. It involves the interpretation and comprehension of words, phrases, sentences, and the relationships between them. Semantic processing goes beyond the surface level and focuses on understanding the underlying meaning and intent.\n comprehensively evaluated the event semantic processing abilities of \\llms covering understanding, reasoning, and prediction about the event semantics. Results indicated that \\llms possess an understanding of individual events, but their capacity to perceive the semantic similarity among events is constrained. In reasoning tasks, \\llms exhibit robust reasoning abilities in causal and intentional relations, yet their performance in other relation types is comparatively weaker. In prediction tasks, \\llms exhibit enhanced predictive capabilities for future events with increased contextual information.\n explored the semantic proficiency of \\llms and showed that these models perform poorly in evaluating basic phrases.\nFurthermore, GPT-3.5 and Bard cannot distinguish between meaningful and nonsense phrases, consistently classifying highly nonsense phrases as meaningful.\nGPT-4 shows significant improvements, but its performance is still significantly lower than that of humans. \nIn summary, the performance of \\llms in semantic understanding tasks is poor. In the future, we can start from this aspect and focus on improving its performance on this application.\nIn \\textbf{social knowledge understanding},  evaluated how well models perform at learning and recognizing concepts of social knowledge and the results revealed that despite being much smaller in the number of parameters, finetuning supervised models such as BERT lead to much better performance than zero-shot models using state-of-the-art \\llms, such as GPT , GPT-J-6B  and so on.\nThis statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.", "cites": [1570, 1571, 3491, 1559, 3538, 7733, 3564, 3508, 3499, 8667], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on natural language understanding tasks, linking findings across sentiment analysis, text classification, NLI, and semantic understanding. It provides a coherent overview by grouping similar tasks and drawing some comparisons. However, the critical analysis is moderate—while some limitations are noted (e.g., poor performance in distinguishing nonsense phrases), deeper evaluation and nuanced critique are limited. The section identifies some general patterns but stops short of proposing overarching principles or a meta-level framework."}}
{"id": "fe78b183-7ba2-423c-b0ec-7d1ed567cd53", "title": "Reasoning", "level": "subsubsection", "subsections": [], "parent_id": "9627000a-5ef0-447d-8e16-8af6c358f8d1", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Language Processing Tasks"], ["subsubsection", "Reasoning"]], "content": "\\label{sec:reasoning}\nThe task of reasoning poses significant challenges for an intelligent AI model.\nTo effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inference to deduce answers when explicit responses are absent.\n\\tablename~\\ref{tb-what-nlptask} reveals that there is a growing interest in evaluating the reasoning ability of \\llms, as evidenced by the increasing number of articles focusing on exploring this aspect. \nCurrently, the evaluation of reasoning tasks can be broadly categorized into mathematical reasoning, commonsense reasoning, logical reasoning, and domain-specific reasoning.\nChatGPT exhibits a strong capability for arithmetic reasoning by outperforming GPT-3.5 in the majority of tasks . \nHowever, its proficiency in mathematical reasoning still requires improvement .\nOn symbolic reasoning tasks, ChatGPT is mostly worse than GPT-3.5, which may be because ChatGPT is prone to uncertain responses, leading to poor performance . \nThrough the poor performance of \\llms on task variants of counterfactual conditions,  showed that the current \\llms have certain limitations in abstract reasoning ability.\nOn abstract reasoning,  found that existing \\llms have very limited ability.\nIn logical reasoning,  indicated that ChatGPT and GPT-4 outperform traditional fine-tuning methods on most benchmarks, demonstrating their superiority in logical reasoning. \nHowever, both models face challenges when handling new and out-of-distribution data.\nChatGPT does not perform as well as other \\llms, including GPT-3.5 and BARD .\nThis is because ChatGPT is designed explicitly for chatting, so it does an excellent job of maintaining rationality.\nFLAN-T5, LLaMA, GPT-3.5, and PaLM perform well in general deductive reasoning tasks .\nGPT-3.5 is not good at keeping oriented for reasoning in the inductive setting .\nFor multi-step reasoning,  showed PaLM and Claude2 are the only two model families that achieve similar performance (but still worse than the GPT model family).\nMoreover, LLaMA-65B is the most robust open-source \\llms to date, which performs closely to code-davinci-002.\nSome papers separately evaluate the performance of ChatGPT on some reasoning tasks: ChatGPT generally performs poorly on commonsense reasoning tasks, but relatively better than non-text semantic reasoning .\nMeanwhile, ChatGPT also lacks spatial reasoning ability, but exhibits better temporal reasoning.\nFinally, while the performance of ChatGPT is acceptable on causal and analogical reasoning, it performs poorly on multi-hop reasoning ability, which is similar to the weakness of other \\llms on complex reasoning . \nIn professional domain reasoning tasks, zero-shot InstructGPT and Codex are capable of complex medical reasoning tasks, but still need to be further improved .\nIn terms of language insight issues,  demonstrated the potential of ChatGPT for solving verbal insight problems, as ChatGPT's performance was comparable to that of human participants. \nIt should be noted that most of the above conclusions are obtained for specific data sets.\nIn contrast, more complex tasks have become the mainstream benchmarks for assessing the capabilities of \\llms. These include tasks such as mathematical reasoning  and structured data inference .\nOverall, \\llms show great potential in reasoning and show a continuous improvement trend, but still face many challenges and limitations, requiring more in-depth research and optimization.", "cites": [1571, 3545, 3555, 3544, 7731, 3558, 8662, 3492, 7667, 3510, 3556, 3513, 3569], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes findings from multiple papers on different types of reasoning tasks (mathematical, logical, commonsense, etc.) and integrates them into a coherent overview of LLMs' current reasoning capabilities. It provides critical comparisons between model families and identifies limitations, such as poor performance on out-of-distribution and multi-hop reasoning tasks. The section also abstracts to broader patterns, such as the growing focus on complex reasoning benchmarks and the need for optimization, moving beyond individual results to a more general understanding of the field."}}
{"id": "f6ea555e-1ac8-415e-b632-dc9a7f043984", "title": "Natural language generation", "level": "subsubsection", "subsections": [], "parent_id": "9627000a-5ef0-447d-8e16-8af6c358f8d1", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Language Processing Tasks"], ["subsubsection", "Natural language generation"]], "content": "NLG evaluates the capabilities of \\llms in generating specific texts, which consists of several tasks, including summarization, dialogue generation, machine translation, question answering, and other open-ended generation tasks.\n\\textbf{Summarization} is a generation task that aims to learn a concise abstract for the given sentence.\nIn this evaluation,  found that TNLG v2 (530B)  achieved the highest score in both scenarios, followed by OPT (175B)  in second place.\nThe fine-tuned Bart  is still better than zero-shot ChatGPT.\nSpecifically, ChatGPT demonstrates comparable zero-shot performance to the text-davinci-002 , but performs worse than GPT-3.5 .\nThese findings indicate that \\llms, particularly ChatGPT, have a general performance in summarization tasks. \nEvaluating the performance of \\llms on \\textbf{dialogue} tasks is crucial to the development of dialogue systems and improving human-computer interaction.\nThrough such evaluation, the natural language processing ability, context understanding ability and generation ability of the model can be improved, so as to realize a more intelligent and more natural dialogue system.\nBoth Claude and ChatGPT generally achieve better performance across all dimensions when compared to GPT-3.5 .\nWhen comparing the Claude and ChatGPT models, both models demonstrate competitive performance across different evaluation dimensions, with Claude slightly outperforming ChatGPT in specific configurations.\nResearch by  underscores that fully fine-tuned models tailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue contexts. \nAdditionally,  have curated a comprehensive LLMs conversation dataset, LMSYS-Chat-1M, encompassing up to one million samples. This dataset serves as a valuable resource for evaluating and advancing dialogue systems.\nWhile \\llms are not explicitly trained for \\textbf{translation} tasks, they can still demonstrate strong performance.\n demonstrated that ChatGPT and GPT-4 exhibit superior performance in comparison to commercial machine translation (MT) systems, as evaluated by humans. \nAdditionally, they outperform most document-level NMT methods in terms of sacreBLEU scores.\nDuring contrastive testing, ChatGPT shows lower accuracy in comparison to traditional translation models. \nHowever, GPT-4 demonstrates a robust capability in explaining discourse knowledge, even though it may occasionally select incorrect translation candidates.\nThe findings from  indicated that ChatGPT performs X $\\to$ Eng translation well, but it still lacks the ability to perform Eng $\\to$ X translation.\n investigated several research directions in MT utilizing \\llms. \nThis study significantly contributes to the advancement of MT research and highlights the potential of LLMs in enhancing translation capabilities.\nIn summary, while \\llms perform satisfactorily in several translation tasks, there is still room for improvement, e.g., enhancing the translation capability from English to non-English languages.\n\\textbf{Question answering} is a crucial technology in the field of human-computer interaction, and it has found wide application in scenarios like search engines, intelligent customer service, and QA systems.\nThe measurement of accuracy and efficiency in QA models will have significant implications for these applications.\nAccording to , among all the evaluated models, InstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness, and fairness across the 9 QA scenarios.\nBoth GPT-3.5 and ChatGPT demonstrate significant advancements compared to GPT-3 in their ability to answer general knowledge questions. \nIn most domains, ChatGPT surpasses GPT-3.5 by more than 2\\% in terms of performance .\nHowever, ChatGPT performs slightly weaker than GPT-3.5 on the CommonsenseQA and Social IQA benchmarks. \nThis can be attributed to ChatGPT’s cautious nature, as it tends to decline to provide an answer when there is insufficient information available.\nFine-tuned models, such as Vícuna and ChatGPT, exhibit exceptional performance with near-perfect scores, surpassing models that lack supervised fine-tuning by a significant margin .\n evaluated the effectiveness of ChatGPT on a range of academic datasets, including various tasks such as answering questions, summarizing text, generating code, reasoning with commonsense, solving math problems, translating languages, detecting bias, and addressing ethical issues.\nOverall, \\llms showcase flawless performance on QA tasks and hold the potential for further enhancing their proficiency in social, event, and temporal commonsense knowledge in the future.\nThere are also other generation tasks to explore.\nIn the field of \\textbf{sentence style transfer},  demonstrated that ChatGPT surpasses the previous SOTA supervised model through training on the same subset for few-shot learning, as evident from the higher BLEU score. \nHowever, when it comes to controlling the formality of sentence style, ChatGPT’s performance still differs significantly from human behavior.\nIn \\textbf{writing tasks},  discovered that \\llms exhibit consistent performance across various categories such as informative, professional, argumentative, and creative writing. \nThis finding implies that \\llms possess a general proficiency in writing capabilities.\nIn \\textbf{text generation} quality,  revealed that ChatGPT excels in assessing text quality from multiple angles, even in the absence of reference texts, surpassing the performance of most existing automated metrics. \nEmploying ChatGPT to generate numerical scores for text quality emerged as the most reliable and effective approach among the various testing methods studied.", "cites": [3502, 1570, 1571, 3547, 7735, 7463, 3511, 3553], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of natural language generation tasks and summarizes the performance of various LLMs based on cited papers. While it mentions comparisons between models in some cases, it lacks a deeper synthesis of findings or critical evaluation of methodologies. The insights remain surface-level, focusing mainly on results without broader conceptual generalization or analysis of underlying trends."}}
{"id": "55dfbeaa-e75f-4607-9245-72ba54190ede", "title": "Multilingual tasks", "level": "subsubsection", "subsections": [], "parent_id": "9627000a-5ef0-447d-8e16-8af6c358f8d1", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Language Processing Tasks"], ["subsubsection", "Multilingual tasks"]], "content": "While English is the predominant language, many \\llms are trained on mixed-language training data. \nThe combination of multilingual data indeed helps \\llms gain the ability to process inputs and generate responses in different languages, making them widely adopted and accepted across the globe. \nHowever, due to the relatively recent emergence of this technology, \\llms are primarily evaluated on English data, leading to a potential oversight of evaluating their multilingual performance. \nTo address this, several articles have provided comprehensive, open, and independent evaluations of \\llms' performance on various NLP tasks in different non-English languages. \nThese evaluations offer valuable insights for future research and applications.\n evaluated the performance of ChatGPT in standard Arabic NLP tasks and observed that ChatGPT exhibits lower performance compared to SOTA models in the zero-shot setting for most tasks.\n utilized a greater number of languages across multiple datasets, encompassing a wider range of tasks, and conducted a more comprehensive evaluation of \\llms, including BLOOM, Vicuna, Claude, ChatGPT, and GPT-4. \nThe results indicated that these \\llms perform poorly when it came to non-Latin languages and languages with limited resources.\nDespite translating the input to English and using it as the query, generative \\llms still displays subpar performance across tasks and languages compared to SOTA models .\nFurthermore,  highlighted that ChatGPT still faces a limitation in translating sentences written in non-Latin script languages with rich linguistic resources.\nThe aforementioned demonstrates that there are numerous challenges and ample opportunities for enhancement in multilingual tasks for \\llms.\nFuture research should prioritize achieving multilingual balance and addressing the challenges faced by non-Latin languages and low-resource languages, with the aim of better supporting users worldwide. \nAt the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.", "cites": [3512, 7648], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers to highlight the challenges of multilingual evaluation in LLMs, particularly for non-Latin and low-resource languages. It offers some critical observations about the limitations of current models but does not deeply evaluate the methodologies or biases in the cited works. The section identifies general trends and suggests future directions, though the abstraction remains at a moderate level without proposing a new theoretical framework."}}
{"id": "d95c2630-781a-4097-ae6e-0156bf62bd22", "title": "Factuality", "level": "subsubsection", "subsections": [], "parent_id": "9627000a-5ef0-447d-8e16-8af6c358f8d1", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Language Processing Tasks"], ["subsubsection", "Factuality"]], "content": "Factuality in the context of \\llms refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts. Factuality in \\llms significantly impacts a variety of tasks and downstream applications, such as QA systems, information extraction, text summarization, dialogue systems, and automated fact-checking, where incorrect or inconsistent information could lead to substantial misunderstandings and misinterpretations. Evaluating factuality is of great importance in order to trust and efficiently use these models. This includes the ability of these models to maintain consistency with known facts, avoid generating misleading or false information (known as ``factual hallucination\"), and effectively learn and recall factual knowledge. A range of methodologies have been proposed to measure and improve the factuality of \\llms. \n assessed the internal knowledge capabilities of several large models, namely InstructGPT, ChatGPT-3.5, GPT-4, and BingChat , by examining their ability to answer open questions based on the Natural Questions  and TriviaQA  datasets. The evaluation process involved human assessment. The results of the study indicated that while GPT-4 and BingChat can provide correct answers for more than 80\\% of the questions, there is still a remaining gap of over 15\\% to achieve complete accuracy.\nIn the work of , they conducted a review of current factual consistency evaluation methods and highlighted the absence of a unified comparison framework and the limited reference value of related scores compared to binary labels. To address this, they transformed existing fact consistency tasks into binary labels, specifically considering only whether there is a factual conflict with the input text, without factoring in external knowledge. The research discovered that fact evaluation methods founded on natural language inference and question generation answering exhibit superior performance and can complement each other.\n proposed a novel metric, based on information theory, to assess the inclusion of specific knowledge in \\llms. The metric utilized the concept of uncertainty in knowledge to measure factualness, calculated by \\llms filling in prompts and examining the probability distribution of the answer. The paper discussed two methods for injecting knowledge into \\llms: explicit inclusion of knowledge in the prompts and implicit fine-tuning of the \\llms using knowledge-related data. The study demonstrated that this approach surpasses traditional ranking methods by achieving an accuracy improvement of over 30\\%.\n improved the method for evaluating fact consistency in summarization tasks. It proposed a novel approach that involved training student NLI models using summaries generated by multiple models and annotated by \\llms to ensure fact consistency. The trained student model was then used for summarization fact consistency evaluation.\n operated on two hypotheses regarding how \\llms generate factual or hallucinated responses. It proposed the use of three formulas (BERTScore , MQAG  and n-gram) to evaluate factuality and employed alternative \\llms to gather token probabilities for black-box language models. The study discovered that simply computing sentence likelihood or entropy helped validate the factuality of the responses.\n broke down text generated by \\llms into individual ``atomic\" facts, which were then evaluated for their correctness. The FActScore is used to measure the performance of estimators through the calculation of F1 scores. The paper tested various estimators and revealed that current estimators still have some way to go in effectively addressing the task.\n introduced the TruthfulQA dataset, designed to cause models to make mistakes. Multiple language models were tested by providing factual answers. The findings from these experiments suggest that simply scaling up model sizes may not necessarily improve their truthfulness, and recommendations are provided for the training approach. This dataset has become widely used for evaluating the factuality of \\llms .", "cites": [2393, 2425, 2217, 8551, 3543, 451, 2427, 9115, 8556, 1552, 2391], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple works into a cohesive narrative around the theme of factuality in LLMs. It goes beyond simple listing by connecting methods like NLI, probability-based metrics, and QA benchmarks to broader evaluation goals. The critical analysis is evident in pointing out limitations (e.g., lack of unified framework, estimator performance gaps), though it could provide deeper comparative insights. The abstraction level is moderate, identifying patterns in evaluation approaches and highlighting general issues like hallucinations and knowledge injection techniques."}}
{"id": "8264edb9-2dc5-4c16-84cd-722e9a2de5dd", "title": "Robustness", "level": "subsubsection", "subsections": [], "parent_id": "71cf6924-a7ea-4d0f-9bbe-c934c39f70c7", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Robustness, Ethic, Bias, and Trustworthiness"], ["subsubsection", "Robustness"]], "content": "Robustness studies the stability of a system when facing unexpected inputs.\nSpecifically, out-of-distribution (OOD)~ and adversarial robustness are two popular research topics for robustness.\n is an early work that evaluated ChatGPT and other \\llms from both the adversarial and OOD perspectives using existing benchmarks such as AdvGLUE~, ANLI~, and DDXPlus~ datasets.\n evaluated the robustness of semantic parsing.\n evaluated OOD robustness by extending the GLUE  dataset.\nThe results of this study emphasize the potential risks to the overall system security when manipulating visual input.\nFor vision-language models,  evaluated \\llms on visual input and transferred them to other visual-linguistic models, revealing the vulnerability of visual input.\n provided an overview of OOD evaluation for language models: adversarial robustness, domain generalization, and dataset biases.\nBridging these lines of research, the authors conducted a comparative analysis, unifying the three approaches. They succinctly outlined the data-generation processes and evaluation protocols for each line of study, all while emphasizing the prevailing challenges and future research prospects.\nAdditionally,  introduced a large-scale robust visual instruction dataset to enhance the performance of large-scale multi-modal models in handling relevant images and human instructions.\nFor adversarial robustness,  evaluated the robustness of \\llms to prompts by proposing a unified benchmark called PromptBench.\nThey comprehensively evaluated adversarial text attacks at multiple levels (character, word, sentence, and semantics). The results showed that contemporary \\llms are vulnerable to adversarial prompts, highlighting the importance of the models' robustness when facing adversarial inputs.\nAs for new adversarial datasets,  introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.", "cites": [3530, 8466, 1568, 3559, 7573, 3490, 3549, 7464, 7574, 1564, 3570], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis of robustness-related evaluation methods by linking adversarial and OOD robustness, and mentioning their relevance to multi-modal models. It includes some critical insights, such as the vulnerability of LLMs to adversarial prompts and the need for improved evaluation protocols. However, the abstraction level is moderate, as it focuses largely on summarizing individual benchmarks and evaluation approaches without fully articulating overarching principles."}}
{"id": "da1e5f90-f6b8-4248-9f96-ab5dc17f3826", "title": "Ethic and bias", "level": "subsubsection", "subsections": [], "parent_id": "71cf6924-a7ea-4d0f-9bbe-c934c39f70c7", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Robustness, Ethic, Bias, and Trustworthiness"], ["subsubsection", "Ethic and bias"]], "content": "LLMs have been found to internalize, spread, and potentially magnify harmful information existing in the crawled training corpora, usually, toxic languages, like offensiveness, hate speech, and insults~, as well as social biases like stereotypes towards people with a particular demographic identity (\\textit{e.g.}, gender, race, religion, occupation, and ideology)~. More recently,  used conventional testing sets and metrics~ to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step,  introduced role-playing into the model and observed an increase in generated toxicity up to 6x. Furthermore, such role-playing also caused biased toxicity towards specific entities. Different from simply measuring social biases,  investigated the sources, underlying mechanisms, and corresponding ethical consequences of these biases potentially produced by ChatGPT. Beyond social biases, LLMs have also been assessed by political tendency and personality traits~ based questionnaires like the Political Compass Test and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality type. In addition, LLMs like GPT-3 were found to have moral biases~ in terms of the Moral Foundation theory~;\nThe study conducted by~~reveals that existing LMs have potential in ethical judgment, but still need improvement. \n proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods.\nMoreover, in the assessment of GPT-4 alignment, ~ discovered a systematic bias.\nChatGPT is also observed to exhibit somewhat bias on cultural values~.\n also incorporated an evaluation dataset specifically aimed at gauging stereotype bias, using both targeted and untargeted system prompts.\nAll these ethical issues might elicit serious risks, impeding the deployment of LLMs and having a profound negative impact on society.", "cites": [3092, 7689, 3495, 2218, 3506, 3565, 3539, 8663, 3524, 3503, 7574, 3519, 3529], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to highlight the ethical challenges of LLMs, especially around toxicity, bias, and cultural alignment. It demonstrates analytical depth by connecting findings across different studies (e.g., role-playing increasing toxicity, bias evaluation datasets for non-English models) and abstracts to broader concerns about societal risks and model trustworthiness."}}
{"id": "c97a8e89-e56e-46d9-88a9-0c797309158a", "title": "Trustworthiness", "level": "subsubsection", "subsections": [], "parent_id": "71cf6924-a7ea-4d0f-9bbe-c934c39f70c7", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Robustness, Ethic, Bias, and Trustworthiness"], ["subsubsection", "Trustworthiness"]], "content": "\\label{sec:trustworthiness}\nSome work focuses on other trustworthiness problems in addition to robustness and ethics.\\footnote{The term `trustworthiness' in this section refers to other work that contains more than robustness and ethics.}\nIn their 2023 study, DecodingTrust,  offered a multifaceted exploration of trustworthiness vulnerabilities in the GPT models, especially GPT-3.5 and GPT-4.\nTheir evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics.\nThey revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more susceptible to attacks.\nIn another study by , \\llms with enhanced cognitive abilities were evaluated.\nThey found that these models can avoid common human intuitions and cognitive errors, demonstrating super-rational performance. By utilizing cognitive reflection tests and semantic illusion experiments, the researchers gained insights into the psychological aspects of \\llms.\nThis method offers new perspectives for evaluating model biases and ethical issues that may not have been previously identified.\nFurthermore, a study by  brings attention to a significant concern: the consistency of judgment in \\llms diminishes notably when faced with disruptions such as questioning, negation, or misleading cues, even if their initial judgments were accurate. The research delves into various prompting methods designed to mitigate this issue and successfully demonstrates their efficacy.\n\\llms are capable of generating coherent and seemingly factual text. However, the information generated can include factual inaccuracies or statements ungrounded in reality, a phenomenon known as \\textbf{hallucination} . \nEvaluating these issues helps improve the training methods of \\llms to reduce the occurrence of hallucinations. \nFor the evaluation of illusions in large-scale visual models, \n introduced a comprehensive and robust large-scale visual instruction dataset: LRV-Instruction. Through the GAVIE method, they fine-tuned the evaluation visual instructions, and experimental results demonstrated that LRV-Instruction effectively alleviates illusions in LLMs. \nIn addition, \n conducted an assessment of illusions in large-scale visual language models, revealing through experiments that the distribution of objects in visual instructions significantly impacts object illusions in LVLMs. To enhance the assessment of object illusions in LVLMs, they introduced a polling-based query method, known as POPE. This method provides an improved evaluation of object illusions in LVLMs.", "cites": [2456, 3559, 366, 3497, 7574, 7728], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on trustworthiness, particularly focusing on hallucination and judgment consistency, and connects them to broader evaluation themes such as robustness and ethics. While it offers some analytical perspective, such as highlighting how models can appear improved in standard tests yet be vulnerable to attacks, it lacks deeper comparative analysis or critique of the methodologies. The abstraction is moderate, as it begins to generalize issues like hallucination but does not fully articulate overarching principles."}}
{"id": "e09ebf9f-0996-4150-9f15-15a76b64b9ef", "title": "Social Science", "level": "subsection", "subsections": [], "parent_id": "bac4a200-8271-4f8a-9ed0-224da48fe68a", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Social Science"]], "content": "Social science involves the study of human society and individual behavior, including economics, sociology, political science, law, and other disciplines.\nEvaluating the performance of \\llms in social science is important for academic research, policy formulation, and social problem-solving.\nSuch evaluations can help improve the applicability and quality of models in the social sciences, increasing understanding of human societies and promoting social progress.\n evaluated the potential use of \\llms in addressing scaling and measurement issues in social science and found that \\llms can generate meaningful responses regarding political ideology and significantly improve text-as-data methods in social science.\nIn computational social science (CSS) tasks,  presented a comprehensive evaluation of \\llms on several CSS tasks.\nDuring classification tasks, \\llms exhibit the lowest absolute performance on event argument extraction, character tropes, implicit hate, and empathy classification, achieving accuracy below 40\\%. \nThese tasks either involve complex structures (event arguments) or subjective expert taxonomies with semantics that differ from those learned during LLM pretraining. \nConversely, \\llms achieve the best performance on misinformation, stance, and emotion classification. \nWhen it comes to generation tasks, \\llms often produce explanations that surpass the quality of gold references provided by crowd workers. \nIn summary, while \\llms can greatly enhance the traditional CSS research pipeline, they cannot completely replace it.\nSome articles also evaluate \\llms on legal tasks. \nThe zero-shot performance of \\llms is mediocre in legal case judgment summarization.\n\\llms have several problems, including incomplete sentences and words, meaningless sentences merge, and more serious errors such as inconsistent and hallucinated information .\nThe results showed that further improvement is necessary for \\llms to be useful for case judgment summarization by legal experts.\n indicated that \\llms, particularly when combined with prompting enhancements and the correct legal texts, could perform better but not yet at expert tax lawyer levels. \nLastly, within the realm of psychology,\n adopted an interdisciplinary approach and drew insights from developmental psychology and comparative psychology to explore alternative methods for evaluating the capabilities of \\llms. By integrating different perspectives, researchers can deepen their understanding of the essence of cognition and effectively leverage the potential of advanced technologies such as large language models, while mitigating potential risks.\nIn conclusion, the utilization of \\llms has significantly benefited individuals in addressing social science-related tasks, leading to improved work efficiency. The outputs produced by \\llms serve as valuable resources for enhancing productivity. However, it is crucial to acknowledge that existing \\llms cannot completely replace human professionals in this domain.", "cites": [3087, 3548, 3554], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes three distinct papers on LLMs in social science, law, and psychology, drawing connections between their findings and themes. It offers a critical view by highlighting limitations (e.g., zero-shot performance, hallucinations) and noting that LLMs cannot yet match expert-level capabilities. The section abstracts beyond individual studies to present broader implications for computational social science and the potential of interdisciplinary approaches for evaluating LLMs."}}
{"id": "91ca3083-7f93-4316-bc4f-ceed67c5bcaf", "title": "Mathematics", "level": "subsubsection", "subsections": [], "parent_id": "662cc18f-ad20-4e76-9d01-82a7331c3104", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Science and Engineering"], ["subsubsection", "Mathematics"]], "content": "For fundamental mathematical problems, most large language models (\\llms) demonstrate proficiency in addition and subtraction, and possess some capability in multiplication. However, they face challenges when it comes to division, exponentiation, trigonometry functions, and logarithm functions. On the other hand, \\llms exhibit competence in handling decimal numbers, negative numbers, and irrational numbers . \nIn terms of performance, ChatGPT and GPT-4 outperform other models significantly, showcasing their superiority in solving mathematical tasks . \nThese two models have a distinct advantage in dealing with large numbers (greater than 1e12) and complex, lengthy mathematical queries.\nGPT-4 outperforms ChatGPT by achieving a significant increase in accuracy of 10 percentage points and a reduction in relative error by 50\\%, due to its superior division and trigonometry abilities, proper understanding of irrational numbers, and consistent step-by-step calculation of long expressions.\nWhen confronted with complex and challenging mathematical problems, \\llms exhibit subpar performance. \nSpecifically, GPT-3 demonstrates nearly random performance, while GPT-3.5 shows improvement, and GPT-4 performs the best . \nDespite the advancements made in the new models, it is important to note that the peak performance remains relatively low compared to that of experts and these models lack the capability to engage in mathematical research~.\nThe specific tasks of algebraic manipulation and calculation continue to pose challenges for GPTs . \nThe primary reasons behind GPT-4's low performance in these tasks are errors in algebraic manipulation and difficulties in retrieving pertinent domain-specific concepts.\n evaluated the use of GPT-4 on difficult high school competition problems and GPT-4 reached 60\\% accuracy on half of the categories. \nIntermediate algebra and precalculus can only be solved with a low accuracy rate of around 20\\%.\nChatGPT is not good at answering questions on topics including derivatives and applications, Oxyz spatial calculus, and spatial geometry .\n showed that ChatGPT's performance worsens as task difficulty increases: it correctly answered 83\\% of the questions at the recognition level, 62\\% at the comprehension level, 27\\% at the application level, and only 10\\% at the highest cognitive complexity level.\nGiven those problems at higher knowledge levels tend to be more complex, requiring in-depth understanding and problem-solving skills, such results are to be expected.\nThese results indicate that the effectiveness of \\llms is highly influenced by the complexity of problems they encounter. \nThis finding holds significant implications for the design and development of optimized artificial intelligence systems capable of successfully handling these challenging tasks.", "cites": [3550, 3552, 3560, 7729, 7638, 3494], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes findings from multiple papers to present a coherent overview of LLMs' mathematical capabilities and limitations. It critically analyzes performance differences between GPT versions and highlights specific weaknesses, such as algebraic manipulation and domain-specific knowledge retrieval. While it identifies some broader trends (e.g., performance drops with complexity), it does not fully abstract to propose high-level principles or frameworks."}}
{"id": "688abbc7-81d0-4fc2-bb5c-2fd72ad3868d", "title": "General science", "level": "subsubsection", "subsections": [], "parent_id": "662cc18f-ad20-4e76-9d01-82a7331c3104", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Science and Engineering"], ["subsubsection", "General science"]], "content": "Further improvements are needed in the application of \\llms in the field of chemistry.\n presented five straightforward tasks from various subareas of chemistry to assess ChatGPT’s comprehension of the subject, with accuracy ranging from 25\\% to 100\\%.\n created a comprehensive benchmark that encompasses 8 practical chemistry tasks, which is designed to assess the performance of \\llms (including GPT-4, GPT-3.5, and Davinci-003) for each chemistry task. Based on the experiment results, GPT-4 demonstrates superior performance compared to the other two models.\n showed that \\llms perform worse on physics problems than chemistry problems, probably because chemistry problems have lower inference complexity than physics problems in this setting.\nThere are limited evaluation studies on \\llms in the field of general science, and the current findings indicate that further improvement is needed in the performance of \\llms within this domain.", "cites": [3550], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from a single cited paper and integrates it with other findings in the field, particularly contrasting performance across chemistry and physics. It offers some critical analysis by noting limitations and the need for improvement in general science evaluation. However, it lacks deeper abstraction and a more comprehensive framework, as the discussion remains relatively focused on task-specific results without broader theoretical generalization."}}
{"id": "543a0889-8872-4229-91c5-a75ecd375eec", "title": "Engineering", "level": "subsubsection", "subsections": [], "parent_id": "662cc18f-ad20-4e76-9d01-82a7331c3104", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Natural Science and Engineering"], ["subsubsection", "Engineering"]], "content": "Within engineering, the tasks can be organized in ascending order of difficulty, including code generation, software engineering, and commonsense planning.\nIn code generation tasks, the smaller \\llms trained for the tasks are competitive in performance, and CodeGen-16B  is comparable in performance to ChatGPT using a larger parameter setting, reaching about a 78\\% match .\nDespite facing challenges in mastering and comprehending certain fundamental concepts in programming languages, ChatGPT showcases a commendable level of coding level .\nSpecifically, ChatGPT has developed superior skills in dynamic programming, greedy algorithm, and search, surpassing highly capable college students, but it struggles in data structure, tree, and graph theory.\nGPT-4 demonstrates an advanced ability to generate code based on given instructions, comprehend existing code, reason about code execution, simulate the impact of instructions, articulate outcomes in natural language, and execute pseudocode effectively .\nIn software engineering tasks, ChatGPT generally performs well and provides detailed responses, often surpassing both human expert output and SOTA output. However, for certain tasks such as code vulnerability detection and information retrieval-based test prioritization, the current version of ChatGPT fails to provide accurate answers, rendering it unsuitable for these specific tasks .\nIn commonsense planning tasks, \\llms may not perform well, even in simple planning tasks where humans excel .  demonstrated that the fine-tuned CodeT5  performs the best across all considered domains, with the shortest inference time. Moreover, it explored the capability of \\llms for plan generalization and found that their generalization capabilities appear to be limited. It turns out that \\llms can handle simple engineering tasks, but they perform poorly on complex engineering tasks.", "cites": [3532, 3516, 7731, 2560, 3540, 7638], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss the capabilities and limitations of LLMs in engineering tasks, such as code generation and planning. It offers a structured overview of task difficulty and model performance, including specific findings about models like ChatGPT and GPT-4. However, while it makes some comparisons and highlights limitations, the analysis remains somewhat surface-level without deeper critique or a novel conceptual framework."}}
{"id": "8f4a3636-6232-469f-9b19-4b3c5ff5d662", "title": "Medical queries", "level": "subsubsection", "subsections": [], "parent_id": "ac019775-b72d-49dc-9857-819eea8ab110", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Medical Applications"], ["subsubsection", "Medical queries"]], "content": "The significance of evaluating \\llms on medical queries lies in providing accurate and reliable medical answers to meet the needs of healthcare professionals and patients for high-quality medical information. As shown in \\tablename~\\ref{table_3.2}, the majority of \\llms evaluations in the medical field concentrate on medical queries.\nChatGPT generated relatively accurate information for various medical queries, including genetics~, radiation oncology physics~, biomedicine~, and many other medical disciplines~, demonstrating its effectiveness in the field of medical queries to a certain extent.\nAs for the limitations,  assessed ChatGPT's performance in primary care and found that its average score in the student comprehensive assessment falls below the passing score, indicating room for improvement. \n highlighted that while ChatGPT can generate responses similar to existing sources in fertility-related clinical prompts, its limitations in reliably citing sources and potential for fabricating information restrict its clinical utility. \n\\input{tables/tb-medical}", "cites": [3528, 3537], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly summarizes the results of two cited papers but does not effectively synthesize or connect their findings. It lacks critical analysis of the limitations or implications of the results, and does not abstract beyond the specific examples to provide broader insights about evaluating LLMs in medical contexts."}}
{"id": "dfddd675-d7d8-4ff1-8bfc-9b17bf1457d8", "title": "Medical assistants", "level": "subsubsection", "subsections": [], "parent_id": "ac019775-b72d-49dc-9857-819eea8ab110", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Medical Applications"], ["subsubsection", "Medical assistants"]], "content": "In the field of medical assistance, \\llms demonstrate potential applications, including research on identifying gastrointestinal diseases , dementia diagnosis , accelerating the evaluation of COVID-19 literature , and their overall potential in healthcare~. However, there are also limitations and challenges, such as lack of originality, high input requirements, resource constraints, uncertainty in answers, and potential risks related to misdiagnosis and patient privacy issues.\nMoreover, several studies have evaluated the performance and feasibility of ChatGPT in the medical education field.\nIn the study by , ChatGPT, specifically GPT-3.5 and GPT-4 models, were evaluated in terms of their understanding of surgical clinical information and their potential impact on surgical education and training. The results indicate an overall accuracy of 46.8\\% for GPT-3.5 and 76.4\\% for GPT-4, demonstrating a significant performance difference between the two models. Notably, GPT-4 consistently performs well across different subspecialties, suggesting its capability to comprehend complex clinical information and enhance surgical education and training.\nAnother study by  explores the feasibility of utilizing ChatGPT in clinical education, particularly in translating radiology reports into easily understandable language.\nThe findings demonstrate that ChatGPT effectively translates radiology reports into accessible language and provides general recommendations.\nFurthermore, the quality of ChatGPT has shown improvement compared to GPT-4.\nThese findings suggest that employing \\llms in clinical education is feasible, although further efforts are needed to address limitations and unlock their full potential.", "cites": [3518, 7143], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 1.7}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of LLM applications in medical assistance, mentioning a few tasks and citing two papers. It lists some limitations and performance numbers but does not meaningfully synthesize or connect the cited works. Critical analysis is minimal, and abstraction is largely absent, focusing instead on specific examples and results."}}
{"id": "0134e598-91ce-4915-849c-f4fd56d3c558", "title": "Agent Applications", "level": "subsection", "subsections": [], "parent_id": "bac4a200-8271-4f8a-9ed0-224da48fe68a", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Agent Applications"]], "content": "Instead of focusing solely on general language tasks, \\llms can be utilized as powerful tools in various domains. Equipping LLMs with external tools can greatly expand the capabilities of the model~.\nToolLLM~ provides a comprehensive framework to equip open-source large language models with tool use capabilities.\n introduced KOSMOS-1, which is capable of understanding general patterns, following instructions, and learning based on context.\nThe study by MRKL~ emphasized the importance of understanding when and how to utilize external symbolic tools, as this knowledge is dependent on the capabilities of \\llms, particularly when these tools can reliably perform functions.\nAdditionally, two other studies, Toolformer~ and TALM~, explored the utilization of tools to enhance language models. Toolformer employs a training approach to determine the optimal usage of specific APIs and integrates the obtained results into subsequent token predictions. On the other hand, TALM combines indistinguishable tools with text-based methods to augment language models and employs an iterative technique known as ``self-play\", guided by minimal tool demonstrations.\nFurthermore,  proposed the HuggingGPT framework, which leverages \\llms to connect various AI models within the machine learning community (such as Hugging Face), aiming to address AI tasks.", "cites": [8661, 3514, 7663, 2958, 431, 3015], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of papers related to LLMs in agent applications, describing each paper's contribution and methods. However, it lacks deeper synthesis or comparative analysis between the approaches, and does not abstract the ideas into broader principles or frameworks. There is minimal critical evaluation of the cited works."}}
{"id": "d6510146-5406-4b94-b7c8-e30458ead3fc", "title": "Education", "level": "subsubsection", "subsections": [], "parent_id": "3ff34b9e-61c7-4c62-88b6-aaad76728700", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Other Applications"], ["subsubsection", "Education"]], "content": "\\llms have shown promise in revolutionizing the field of education. They have the potential to make significant contributions in several areas, such as assisting students in improving their writing skills, facilitating better comprehension of complex concepts, expediting the delivery of information, and providing personalized feedback to enhance student engagement. These applications aim to create more efficient and interactive learning experiences, offering students a broader range of educational opportunities. However, to fully harness the potential of \\llms in education, extensive research, and ongoing refinement are necessary.\nThe evaluation of \\llms for \\textbf{educational assistance} aims to investigate and assess their potential contributions to the field of education. Such evaluations can be conducted from various perspectives.\nAccording to , ChatGPT demonstrates the ability to generate detailed, fluent, and coherent feedback that surpasses that of human teachers. It can accurately assess student assignments and provide feedback on task completion, thereby assisting in the development of student skills.\nHowever, ChatGPT's responses may lack novelty or insightful perspectives regarding teaching improvement .\nAdditionally, the study conducted by  revealed that \\llms can successfully identify at least one actual problem in student code, although instances of misjudgment are also observed.\nIn conclusion, the utilization of \\llms shows promise in addressing program logic issues, although challenges remain in achieving proficiency in output formatting. It is important to note that while these models can provide valuable insights, they may still generate errors similar to those made by students.\nIn \\textbf{educational exams}, researchers aim to evaluate the application effectiveness of \\llms, including automatic scoring, question generation, and learning guidance.\n showed that ChatGPT achieves an average of 71.8\\% correctness, which is comparable to the average score of all participating students.\nSubsequently, the evaluation was conducted using GPT-4, and it achieved a score of 8.33. Furthermore, this evaluation showed the effectiveness of leveraging bootstrapping that combines randomness via the ``temperature'' parameter in diagnosing incorrect answers.\n claimed that GPT-3.5 can solve MIT math and EECS exams with GPT-4 achieving better performance.\nHowever, it turned out to be not fair since they accidentally included the correct answers into the prompts.\n\\input{tables/tb-others}", "cites": [8668, 3535], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two distinct studies to highlight the potential and limitations of LLMs in education, particularly in feedback and exam-solving. It provides a critical perspective by noting the lack of novelty in feedback and methodological issues in exam evaluations. While it captures some broader patterns, such as the dual nature of LLMs as both helpful and error-prone, it does not develop a meta-level framework that would elevate abstraction further."}}
{"id": "365d74c3-fe92-408f-86e6-f4fa6ba6efdb", "title": "Search and recommendation", "level": "subsubsection", "subsections": [], "parent_id": "3ff34b9e-61c7-4c62-88b6-aaad76728700", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Other Applications"], ["subsubsection", "Search and recommendation"]], "content": "The assessment of \\llms in search and recommendation can be broadly categorized into two areas.\nFirstly, in the realm of \\textbf{information retrieval},  investigated the effectiveness of generative ranking algorithms, such as ChatGPT and GPT-4, for information retrieval tasks. Experimental results demonstrate that guided ChatGPT and GPT-4 exhibit competitive performance on popular benchmark tests, even outperforming supervised methods. Additionally, the extraction of ChatGPT's ranking functionality into a specialized model shows superior performance when trained on 10K ChatGPT-generated data compared to training on 400K annotated MS MARCO data in the BEIR dataset .\nFurthermore,  conducted a randomized online experiment to investigate the behavioral differences of users when performing information retrieval tasks using search engines and chatbot tools.\nParticipants were divided into two groups: one using tools similar to ChatGPT and the other using tools similar to Google Search. The results show that the ChatGPT group spent less time on all tasks and the difference between these two groups is not significant.\nSecondly, moving to the domain of \\textbf{recommendation systems}, \n\\llms have emerged as essential components that leverage their natural language processing capabilities to comprehend user preferences, item descriptions, and contextual information . By incorporating LLMs into recommendation pipelines, these systems can offer more accurate and personalized recommendations, thereby improving user experience and overall recommendation quality.\nHowever, it is crucial to address the potential risks associated with using LLMs for recommendations. Recent research by  has highlighted the issue of unfair recommendations generated by ChatGPT. This emphasizes the importance of evaluating fairness when employing LLMs in recommendation scenarios.\n suggest that ChatGPT exhibits strong performance in recommender systems. The use of listwise ranking is found to strike the best balance between cost and performance. Furthermore, ChatGPT shows promise in addressing the cold-start problem and providing interpretable recommendations. \nMoreover, the research by  and  demonstrated the promising potential of the modality-based recommendation model (MoRec) and text-based collaborative filtering (TCF) in recommendation systems.", "cites": [3566, 3521, 3541, 3542, 8666, 3534], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.3, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on ChatGPT and LLMs in search and recommendation systems, linking findings across tasks and domains. It provides a critical view by highlighting both performance advantages and fairness concerns. The abstraction level is moderate, as it identifies broader implications for LLM usage in these areas, though it does not propose a novel overarching framework."}}
{"id": "c65d8b54-50e7-4728-9912-3323fb49fa0a", "title": "Personality testing", "level": "subsubsection", "subsections": [], "parent_id": "3ff34b9e-61c7-4c62-88b6-aaad76728700", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Other Applications"], ["subsubsection", "Personality testing"]], "content": "Personality testing aims to measure individuals' personality traits and behavioral tendencies, and \\llms as powerful natural language processing models have been widely applied in such tasks. \nResearch conducted by  investigated the personality features of using Davinci-003 as a chatbot and found variations in the consistency of its answers, despite exhibiting prosocial characteristics.\nHowever, there remains uncertainty regarding whether the chatbot's responses are driven by conscious self-reflection or algorithmic processes.\n examined the manifestation of personality in language models and discovered that many models perform unreliably in self-assessment tests and exhibit inherent biases.\nTherefore, it is necessary to develop specific machine personality measurement tools to enhance reliability.\nThese studies offer vital insights to better understand \\llms in personality testing.\n proposed a comprehensive approach to conduct effective psychometric testing for the personality traits in the text generated by \\llms.\nIn order to evaluate the emotional intelligence of \\llms,  developed a new psychometric assessment method. By referencing a framework constructed from over 500 adults, the authors tested various mainstream \\llms. The results showed that most \\llms achieve above-average scores in emotional quotient (EQ), with GPT-4 scoring 117, surpassing 89\\% of human participants. However, a multivariate pattern analysis indicated that certain \\llms achieve human-level performance without relying on mechanisms resembling those found in humans. This is evident from the distinct differences in the quality of their representational patterns, as compared to humans.\n employed the word guessing game to evaluate \\llms' language and theory of mind intelligences, a more engaging and interactive assessment method.\n discussed the challenges of incorporating humor into \\llms, particularly ChatGPT.\nThey found that while ChatGPT demonstrates impressive capabilities in NLP tasks, it falls short in generating humorous responses.\nThis study emphasizes the importance of humor in human communication and the difficulties that \\llms face in capturing the subtleties and context-dependent nature of humor.\nIt discusses the limitations of current approaches and highlights the need for further research on more sophisticated models that can effectively understand and generate humor.", "cites": [7122, 3083, 7736, 8665], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers on personality, emotional intelligence, and humor in LLMs, connecting their findings to highlight both progress and limitations in evaluating synthetic personality traits. It includes critical points such as the distinction between algorithmic and human-like mechanisms, as well as the unreliability of self-assessment in models. While it identifies some patterns (e.g., LLMs achieving human-level EQ scores), the abstraction remains moderate, focusing more on specific findings than deriving overarching principles."}}
{"id": "fc16c9dd-72d0-459a-89cd-2bf8e092a1ba", "title": "Specific applications", "level": "subsubsection", "subsections": [], "parent_id": "3ff34b9e-61c7-4c62-88b6-aaad76728700", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "What to Evaluate"], ["subsection", "Other Applications"], ["subsubsection", "Specific applications"]], "content": "Moreover, various research endeavors have been conducted to explore the application and evaluation of \\llms across a wide spectrum of tasks, such as \\textbf{game design} , \\textbf{model performance assessment} , and \\textbf{log parsing} .\nCollectively, these findings enhance our comprehension of the practical implications associated with the utilization of \\llms across diverse tasks. They shed light on the potential and limitations of these models while providing valuable insights for performance improvement.", "cites": [3523], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section briefly mentions several application areas of LLMs (game design, model performance assessment, log parsing) and cites one paper (PandaLM) without specific elaboration. It lacks synthesis of the cited paper with the broader context of evaluation methods and does not critically evaluate or compare the approaches. The discussion remains general and concrete, offering little abstraction or deeper insight into overarching evaluation principles."}}
{"id": "d95eb065-2f73-44ed-b48f-79dfe02aef18", "title": "Where to Evaluate: Datasets and Benchmarks", "level": "section", "subsections": ["629e485e-13aa-4b60-b583-cf4846e2ea8f", "d5d2ef2a-7eba-4be9-825d-4d9c193b7bba", "d075e665-5667-4c78-8e94-a8b364da307a"], "parent_id": "d39b8852-f438-4f5b-bfa6-286e79591ff4", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Where to Evaluate: Datasets and Benchmarks"]], "content": "\\label{sec-where}\n\\input{tables/tb-benchmarks}\n\\llms evaluation datasets are used to test and compare the performance of different language models on various tasks, as depicted in Sec.~\\ref{sec-what}.\nThese datasets, such as GLUE~ and SuperGLUE~, aim to simulate real-world language processing scenarios and cover diverse tasks such as text classification, machine translation, reading comprehension, and dialogue generation.\nThis section will not discuss any single dataset for language models but benchmarks for \\llms.\nA variety of benchmarks have emerged to evaluate their performance. \nIn this study, we compile a selection of 46 popular benchmarks, as shown in \\tablename~\\ref{tb-benchmarks}.\\footnote{Note that as the evaluation of \\llms is a hot research area, it is very likely that we cannot cover all benchmarks. We welcome suggestions and comments to make this list perfect.}\nEach benchmark focuses on different aspects and evaluation criteria, providing valuable contributions to their respective domains.\nFor a better summarization, we divide these benchmarks into three categories: benchmarks for general language tasks, benchmarks for specific downstream tasks, and benchmarks for multi-modal tasks.", "cites": [1568, 1569], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of LLM evaluation benchmarks and categorizes them into three groups, but it lacks integration of insights from the cited papers (GLUE and SuperGLUE). It does not critically analyze their strengths, weaknesses, or evolution, nor does it generalize these benchmarks into broader trends or frameworks."}}
{"id": "629e485e-13aa-4b60-b583-cf4846e2ea8f", "title": "Benchmarks for General Tasks", "level": "subsection", "subsections": [], "parent_id": "d95eb065-2f73-44ed-b48f-79dfe02aef18", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Where to Evaluate: Datasets and Benchmarks"], ["subsection", "Benchmarks for General Tasks"]], "content": "\\llms are designed to solve a vast majority of tasks.\nTo this end, existing benchmarks tend to evaluate the performance in different tasks.\nChatbot Arena~ and MT-Bench~ are two significant benchmarks that contribute to the evaluation and advancement of chatbot models and \\llms in different contexts.\nChatbot Arena provides a platform to assess and compare diverse chatbot models through user engagement and voting.\nUsers can engage with anonymous models and express their preferences via voting.\nThe platform gathers a significant volume of votes, facilitating the evaluation of models' performance in realistic scenarios.\nChatbot Arena provides valuable insights into the strengths and limitations of chatbot models, thereby contributing to the progress of chatbot research and advancement.\nMeanwhile, MT-Bench evaluates \\llms on multi-turn dialogues using comprehensive questions tailored to handling conversations.\nIt provides a comprehensive set of questions specifically designed for assessing the capabilities of models in handling multi-turn dialogues.\nMT-Bench possesses several distinguishing features that differentiate it from conventional evaluation methodologies.\nNotably, it excels in simulating dialogue scenarios representative of real-world settings, thereby facilitating a more precise evaluation of a model's practical performance.\nMoreover, MT-Bench effectively overcomes the limitations in traditional evaluation approaches, particularly in gauging a model's competence in handling intricate multi-turn dialogue inquiries.\nInstead of focusing on specific tasks and evaluation metrics, HELM  provides a comprehensive assessment of \\llms. It evaluates language models across various aspects such as language understanding, generation, coherence, context sensitivity, common-sense reasoning, and domain-specific knowledge. HELM aims to holistically evaluate the performance of language models across different tasks and domains.\nFor \\llms Evaluator,  introduces LLMEval², which encompasses a wide range of capability evaluations.\nIn addition, Xiezhi~ presents a comprehensive suite for assessing the knowledge level of large-scale language models in different subject areas.\nThe evaluation conducted through Xiezhi enables researchers to comprehend the notable limitations inherent in these models and facilitates a deeper comprehension of their capabilities in diverse fields.\nFor evaluating language models beyond their existing capacities, BIG-bench  introduces a diverse collection of 204 challenging tasks contributed by 450 authors from 132 institutions.\nThese tasks cover various domains such as math, childhood development, linguistics, biology, common-sense reasoning, social bias, physics, software development, etc.\nRecent work has led to the development of benchmarks for evaluating language models' knowledge and reasoning abilities. The Knowledge-Oriented Language Model Evaluation KoLA  focuses on assessing language models' comprehension and utilization of semantic knowledge for inference. As such, KoLA serves as an important benchmark for evaluating the depth of language understanding and reasoning in language models, thereby driving progress in language comprehension. \nTo enable crowd-sourced evaluations of language tasks, DynaBench~ supports dynamic benchmark testing. DynaBench explores new research directions including the effects of closed-loop integration, distributional shift characteristics, annotator efficiency, influence of expert annotators, and model robustness to adversarial attacks in interactive settings. \nFurthermore, to evaluate language models' ability to learn and apply multidisciplinary knowledge across educational levels, the Multidisciplinary Knowledge Evaluation M3KE ~ was recently introduced. M3KE assesses knowledge application within the Chinese education system.\nThe development of standardized benchmarks for evaluating \\llms on diverse tasks has been an important research focus. MMLU~ provides a comprehensive suite of tests for assessing text models in multi-task contexts.\nAlpacaEval~ stands as an automated evaluation benchmark, which places its focus on assessing the performance of \\llms across various natural language processing tasks. It provides a range of metrics, robustness measures, and diversity evaluations to gauge the capabilities of LLMs. AlpacaEval has significantly contributed to advancing LLMs in diverse domains and promoting a deeper understanding of their performance.\nFurthermore, AGIEval , serves as a dedicated evaluation framework for assessing the performance of foundation models in the domain of human-centric standardized exams. \nMoreover, OpenLLM~ functions as an evaluation benchmark by offering a public competition platform for comparing and assessing different LLM models' performance on various tasks. It encourages researchers to submit their models and compete on different tasks, driving progress and competition in LLM research.\nAs for tasks beyond standard performance, there are benchmarks designed for OOD, adversarial robustness, and fine-tuning.\nGLUE-X  is a novel attempt to create a unified benchmark aimed at evaluating the robustness of NLP models in OOD scenarios. This benchmark emphasizes the significance of robustness in NLP and provides insights into measuring and enhancing the robustness of models.\nIn addition,  presents BOSS, a benchmark collection for assessing out-of-distribution robustness in natural language processing tasks.\nPromptBench  centers on the importance of prompt engineering in fine-tuning \\llms. It provides a standardized evaluation framework to compare different prompt engineering techniques and assess their impact on model performance. PromptBench facilitates the enhancement and optimization of fine-tuning methods for \\llms.\nTo ensure impartial and equitable evaluation, PandaLM  is introduced as a discriminative large-scale language model specifically designed to differentiate among multiple high-proficiency LLMs through training. In contrast to conventional evaluation datasets that predominantly emphasize objective correctness, PandaLM incorporates crucial subjective elements, including relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality.", "cites": [1570, 2205, 3505, 3523, 440, 3520, 3501, 7466, 9151, 7142, 7573, 3493], "cite_extract_rate": 0.7058823529411765, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various benchmarks for general tasks and briefly describes each, but lacks a deep synthesis of ideas or a unifying framework. It provides minimal critical evaluation of the cited works and does not identify broader patterns or principles. While informative, it remains largely descriptive."}}
{"id": "d5d2ef2a-7eba-4be9-825d-4d9c193b7bba", "title": "Benchmarks for Specific Downstream Tasks", "level": "subsection", "subsections": [], "parent_id": "d95eb065-2f73-44ed-b48f-79dfe02aef18", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Where to Evaluate: Datasets and Benchmarks"], ["subsection", "Benchmarks for Specific Downstream Tasks"]], "content": "Other than benchmarks for general tasks, there exist benchmarks specifically designed for certain downstream tasks.\nQuestion-answering benchmarks have become a fundamental component in the assessment of \\llms and their overall performance. \nMultiMedQA  is a medical QA benchmark that focuses on medical examinations, medical research, and consumer healthcare questions. It consists of seven datasets related to medical QA, including six existing datasets and one new dataset. The goal of this benchmark is to evaluate the performance of \\llms in terms of clinical knowledge and QA abilities.\nTo assess the ability of LLMs in dynamic QA about current world knowledge,  introduced FRESHQA. By incorporating relevant and current information retrieved from search engines into prompts, there is a significant enhancement in the performance of LLMs on FRESHQA.\nTo effectively assess in-depth dialogue,  introduced the Dialogue CoT, incorporating two efficient dialogue strategies: Explicit CoT and CoT.\nThe assessment of \\llms in diverse and demanding tasks has garnered substantial attention in recent research. To this end, a range of specialized benchmarks have been introduced to evaluate \\llms' capabilities in specific domains and applications. Among these, ARB, as presented by , focuses on probing the performance of \\llms in advanced reasoning tasks spanning multiple domains.\nAdditionally, ethical considerations in \\llms have become an area of paramount importance. TRUSTGPT, as tailored by , addresses critical ethical dimensions, including toxicity, bias, and value alignment, within the context of \\llms.\nFurthermore, the simulation of human emotional reactions by \\llms remains an area with significant potential for improvement, as highlighted by the EmotionBench benchmark by .\nIn terms of security evaluation,  have introduced SafetyBench, a benchmark specifically designed to test the security performance of a range of popular Chinese and English \\llms. The results of this evaluation reveal substantial security flaws in current LLMs.\nTo evaluate the daily decision-making capabilities of intelligent systems,  introduced Choice-75.\nAdditionally, to assess \\llms' aptitude in understanding complex instructions,  have introduced CELLO. This benchmark encompasses the design of eight distinctive features, the development of a comprehensive evaluation dataset, and the establishment of four evaluation criteria alongside their respective measurement standards.\nOther specific benchmarks such as C-Eval , which is the first extensive benchmark to assess the advanced knowledge and reasoning capabilities of foundation models in Chinese.\nAdditionally,  introduces CMMLU as a comprehensive Chinese proficiency standard and evaluates the performance of 18 LLMs across various academic disciplines. The findings reveal that the majority of LLMs demonstrate suboptimal performance in Chinese language environments, highlighting areas for improvement.\nM3Exam  provides a unique and comprehensive evaluation framework that incorporates multiple languages, modalities, and levels to test the general capabilities of \\llms in diverse contexts.\nAdditionally, GAOKAO-Bench~ provides a comprehensive evaluation benchmark for gauging the proficiency of large language models in intricate and context-specific tasks, utilizing questions sourced from the Chinese Gaokao examination.\nOn the other hand, SOCKET  serves as an NLP benchmark designed to evaluate the performance of \\llms in learning and recognizing social knowledge concepts.\nIt consists of several tasks and case studies to assess the limitations of \\llms in social capabilities.\nMATH~ concentrates on assessing reasoning and problem-solving proficiencies of AI models within the domain of mathematics.\nAPPS~ is a more comprehensive and rigorous benchmark for evaluating code generation, measuring the ability of language models to generate python code according to natural language specifications.\nCUAD~ is an expert-annotated, domain-specific legal contract review dataset that presents a challenging research benchmark and potential for enhancing deep learning models' performance in contract understanding tasks.\nCVALUES~ introduces a humanistic evaluation benchmark to assess the alignment of LLMs with safety and responsibility standards.\nIn the realm of comprehensive Chinese medicine,  introduced CMB, a medical evaluation benchmark rooted in the Chinese language and culture. It addresses the potential inconsistency in the local context that may arise from relying solely on English-based medical assessments.\nIn the realm of hallucination assessment,  has developed UHGEval, a benchmark specifically designed to evaluate the performance of Chinese LLMs in text generation without being constrained by hallucination-related limitations.\nIn addition to existing evaluation benchmarks, there is a research gap in assessing the effectiveness of utilizing tools for \\llms. To address this gap,\nthe API-Bank benchmark  is introduced as the first benchmark explicitly designed for tool-augmented LLMs. It comprises a comprehensive Tool-Augmented LLM workflow, encompassing 53 commonly used API tools and 264 annotated dialogues, encompassing a total of 568 API calls.\nFurthermore, the ToolBench project  aims to empower the development of large language models that effectively leverage the capabilities of general-purpose tools. By providing a platform for creating optimized instruction datasets, the ToolBench project seeks to drive progress in language models and enhance their practical applications.\nTo evaluate \\llms in multi-turn interactions,  proposed MINT, which utilizes tools and natural language feedback.", "cites": [3546, 3507, 3563, 464, 3515, 8664, 3512, 3536, 3504, 7732, 7730, 3492, 3496, 3557, 3525, 8535, 3531, 3561], "cite_extract_rate": 0.782608695652174, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily lists various benchmarks and datasets for specific downstream tasks without synthesizing or connecting them into a broader narrative. It lacks critical evaluation of the cited works and does not identify overarching patterns or principles in the evaluation of LLMs."}}
{"id": "d075e665-5667-4c78-8e94-a8b364da307a", "title": "Benchmarks for Multi-modal task", "level": "subsection", "subsections": [], "parent_id": "d95eb065-2f73-44ed-b48f-79dfe02aef18", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Where to Evaluate: Datasets and Benchmarks"], ["subsection", "Benchmarks for Multi-modal task"]], "content": "\\label{sec:benm}\nFor the evaluation of Multimodal Large Language Models (MLLMs), MME~ serves as an extensive evaluative benchmark, aiming to assess their perceptual and cognitive aptitudes. It employs meticulously crafted instruction-answer pairs alongside succinct instruction design, thereby guaranteeing equitable evaluation conditions. \nTo robustly evaluate large-scale vision-language models,  introduced MMBench, which comprises a comprehensive dataset and employs a CircularEval assessment method.\nAdditionally, MMICL~ enhances visual language models for multimodal inputs and excels in tasks such as MME and MMBench.\nFurthermore, LAMM~ extends its research to encompass multimodal point clouds.\nLVLM-eHub~ undertakes an exhaustive evaluation of LVLMs using an online competitive platform and quantitative capacity assessments.\nTo comprehensively assess the generative and understanding capabilities of Multi-modal Large Language Models (MLLMs),  introduced a novel benchmark named SEED-Bench.\nThis benchmark consists of 19,000 multiple-choice questions that have been annotated by human assessors. Additionally, the evaluation covers 12 different aspects, including the models' proficiency in understanding patterns within images and videos.\nIn summary, recent works have developed robust benchmarks and improved models that advance the study of multimodal languages.", "cites": [7737, 3517, 7734, 3522, 7088], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a basic description of several benchmarks for multimodal tasks but lacks meaningful synthesis of the cited works into a cohesive narrative. It does not critically evaluate the methods or highlight their strengths and limitations. The content remains at a surface level, identifying specific tools and their features without abstracting broader trends or principles in multimodal evaluation."}}
{"id": "106ef5bf-a50c-4095-93cd-085fb93c9816", "title": "Automatic Evaluation", "level": "subsection", "subsections": [], "parent_id": "46c725d6-2804-4bd7-abe3-7dffb5722e70", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "How to Evaluate"], ["subsection", "Automatic Evaluation"]], "content": "Automated evaluation is a common, and perhaps the most popular, evaluation method that typically uses standard metrics and evaluation tools to evaluate model performance. \nCompared with human evaluation, automatic evaluation does not require intensive human participation, which not only saves time, but also reduces the impact of human subjective factors and makes the evaluation process more standardized.\nFor example, both  and  use automated evaluation methods to evaluate a large number of tasks.\nRecently, with the development of \\llms, some advanced automatic evaluation techniques are also designed to help evaluate.\n proposed LLM-EVAL, a unified multidimensional automatic evaluation method for open-domain conversations with \\llms.\nPandaLM~ can achieve reproducible and automated language model assessment by training an LLM that serves as the ``judge'' to evaluate different models.\nProposing a self-supervised evaluation framework,  enabled a more efficient form of evaluating models in real-world deployment by eliminating the need for laborious labeling of new data.\nIn addition, many benchmarks also apply automatic evaluation, such as MMLU~, HELM, C-Eval~, AGIEval~, AlpacaFarm~, Chatbot Arena~, etc.\n\\input{tables/tb-automatic}\nBased on the literature that adopted automatic evaluation, we summarized the main metrics in automatic evaluation in \\tablename~\\ref{tb-automatic}.\nThe key metrics include the following four aspects:\n\\begin{enumerate}[leftmargin=2em]\n\\setlength\\itemsep{0em}\n    \\item \\textbf{Accuracy} is a measure of how correct a model is on a given task. The concept of accuracy may vary in different scenarios and is dependent on the specific task and problem definition. It can be measured using various metrics such as Exact Match, F1 score, and ROUGE score.\n    \\begin{itemize}\n        \\item Exact Match (EM) is a metric used to evaluate whether the model's output in text generation tasks precisely matches the reference answer. In question answering tasks, if the model's generated answer is an exact match with the manually provided answer, the EM is 1; otherwise, it is 0.\n        \\item The F1 score is a metric for evaluating the performance of binary classification models, combining the model's precision and recall. The formula for calculation is as follows: \\(F1=\\frac{2\\times Precision \\times Recall }{Precision + Recall} \\).\n        \\item ROUGE is primarily employed to assess the performance of tasks such as text summarization and machine translation, involving considerations of overlap and matching between texts.\n    \\end{itemize}\n    \\item \\textbf{Calibrations} pertains to the degree of agreement between the confidence level of the model output and the actual prediction accuracy. \n    \\begin{itemize}\n        \\item Expected Calibration Error (ECE) is one of the commonly used metrics to evaluate model calibration performance~.  utilized ECE to study the calibration of RLHF-LMs, including ChatGPT, GPT-4, Claude 1, Claude 2 and Llama2. For the calculation of ECE, they categorize model predictions based on confidence and measure the average accuracy of the predictions within each confidence interval. \n         \\item Area Under the Curve of selective accuracy and coverage (AUC)~ is another commonly used metric. \n    \\end{itemize}\n    \\item \\textbf{Fairness} refers to whether the model treats different groups consistently, that is, whether the model's performance is equal across different groups. This can include attributes such as gender, race, age, and more. DecodingTrust~ employs the following two metrics for measuring fairness:\n    \\begin{itemize}\n        \\item Demographic Parity Pifference (DPD) measures whether the model's predictions are distributed equally across different population groups. If predictions differ significantly between groups, the DPD is high, indicating that the model may be unfairly biased against different groups. The calculation of DPD involves the prediction of the model and the true label, and the following formula can be used: \\(DPD=P(\\hat{y}|Z=1)-P(\\hat{y}|Z=0)\\), where \\(\\hat{y}\\) is the binary classification prediction of the model, Z is the identifier of the population group (usually binary, indicating two different groups, such as men and women), \\(P(\\hat{y}|Z=1)\\) and \\(P(\\hat{y}|Z=0)\\) respectively represent the probabilities of predicting the positive class in population \\(Z=1\\) and \\(Z=0\\).\n        \\item Equalized Odds Difference (EOD) aims to ensure that the model provides equal error rates across different populations, that is, the model's prediction error probability distribution is similar for different populations. The calculation of EOD involves probabilities related to true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. The formula for EOD is as follows:\n        \\(max \\{ P(\\hat{y}=1|Y=1,Z=1) - P(\\hat{y}=1|Y=1,Z=0), P(\\hat{y}=1|Y=0,Z=1) - P(\\hat{y}=1|Y=0,Z=0)\\} \\)\n        where \\(\\hat{y}\\) is the binary classification prediction of the model, \\(Y\\) is the true label, \\(Z\\) is the demographic group identifier (typically binary, representing two different groups), and \\(P(\\hat{y}=1|Y=1,Z=1)\\) denotes the probability of the model predicting a positive class when the true label is positive and belongs to group \\(Z=1\\).\n    \\end{itemize}\n    \\item \\textbf{Robustness} evaluates the performance of a model in the face of various challenging inputs, including adversarial attacks, changes in data distribution, noise, etc. \n    \\begin{itemize}\n        \\item Attack Success Rate (ASR) serves as a metric for evaluating the adversarial robustness of \\llms~.\n        Specifically, consider a dataset \\(\\mathcal{D} =\\left \\{ (x_{i},y_{i} ) \\right \\}_{i=1}^{N} \\) containing \\(N\\) pairs of samples \\(x_{i}\\) and ground truth \\(y_{i}\\). For an adversarial attack method \\(\\mathcal{A}\\), given an input \\(x\\), this method can produce adversarial examples \\(\\mathcal{A}(x)\\)  to  attack surrogate model \\(f\\), with the success rate is calculated as: \\(ASR=\\sum_{(x,y \\in D )} \\frac{\\mathcal{I}\\left [ f(\\mathcal{A}(x) )\\ne y \\right ]  }{\\mathcal{I} \\left [ f(x)=y \\right ] } \\), where \\(\\mathcal{I}\\) is the indicator function ~.\n        \\item Performance Drop Rate (PDR), a new unified metric, effectively assesses the robustness of prompt in \\llms~. PDR quantifies the relative performance degradation after a prompt attack, and the formula is as follows: \\(PDR=1-\\frac{\\sum_{(x,y)\\in D }\\mathcal{M}\\left [  f(\\left [ A(P),x \\right ] ),y\\right ] }{\\sum_{(x,y)\\in D }\\mathcal{M}\\left [ f(\\left [ P,x \\right ] ),y\\right ] } \\), where \\(A\\) represents the adversarial attack applied to prompt \\(P\\), and \\(M\\) denotes the evaluation function, which varies across different tasks~.\n    \\end{itemize}\n\\end{enumerate}", "cites": [1570, 1571, 759, 7464, 3570, 3523, 3509, 440, 7573, 1644, 7574, 8535, 3571, 3520], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of automatic evaluation methods and metrics, citing multiple papers but primarily listing their contributions without substantial synthesis, critical evaluation, or abstraction to broader principles."}}
{"id": "051d746a-338d-4297-945a-ad26168f0685", "title": "Human Evaluation", "level": "subsection", "subsections": [], "parent_id": "46c725d6-2804-4bd7-abe3-7dffb5722e70", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "How to Evaluate"], ["subsection", "Human Evaluation"]], "content": "The increasingly strengthened capabilities of \\llms have certainly gone beyond standard evaluation metrics on general natural language tasks.\nTherefore, human evaluation becomes a natural choice in some non-standard cases where automatic evaluation is not suitable.\nFor instance, in open-generation tasks where embedded similarity metrics (such as BERTScore) are not enough, human evaluation is more reliable .\nWhile some generation tasks can adopt certain automatic evaluation protocols, human evaluation in these tasks is more favorable as generation can always go better than standard answers.\nHuman evaluation is a way to evaluate the quality and accuracy of model-generated results through human participation. \nCompared with automatic evaluation, manual evaluation is closer to the actual application scenario and can provide more comprehensive and accurate feedback.\nIn the manual evaluation of \\llms, evaluators (such as experts, researchers, or ordinary users) are usually invited to evaluate the results generated by the model. For example,  used the annotations from experts for generation.\nBy human evaluation,  assessed on summarization and disinformation scenarios on 6 models and  evaluated analogical reasoning tasks.\n did a series of human-crafted tests using GPT-4 and they found that GPT-4 performs close to or even exceeds human performance on multiple tasks.\nThis evaluation requires human evaluators to actually test and compare the performance of the models, not just evaluate the models through automated evaluation metrics.\nNote that even human evaluations can have high variance and instability, which could be due to cultural and individual differences .\nIn practical applications, these two evaluation methods are considered and weighed in combination with the actual situation.\nExploring the human evaluation methods of \\llms requires thoughtful attention to various crucial factors to guarantee the dependability and precision of assessments . Table \\ref{tb-human} provides a concise overview of the essential aspects of human evaluation, including the number of evaluators, evaluation criteria, and evaluator's expertise level.\nPrimarily, the number of evaluators emerges as a crucial factor intricately intertwined with adequate representation and statistical significance. A judiciously chosen number of evaluators contributes to a more nuanced and comprehensive understanding of the \\llms under scrutiny, enabling a more reliable extrapolation of the results to a broader context. \nFurthermore, evaluation criteria are fundamental components of the human assessment process. Expanding upon the principles of the 3H rule (Helpfulness, Honesty, and Harmlessness) , we have elaborated them into the following 6 human assessment criteria. \nThese criteria include accuracy, relevance, fluency, transparency, safety, and human alignment. Through the application of these standards, a thorough analysis of LLMs' performance in syntax, semantics, and context is achieved, allowing for a more comprehensive evaluation of the quality of generated text.\n\\input{tables/tb-human}\n\\begin{enumerate}[leftmargin=2em]\n\\setlength\\itemsep{0em}\n    \\item \\textbf{Accuracy}  stands out as a pivotal criterion that assesses the precision and correctness of the generated text. It involves scrutinizing the extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies.\n    \\item \\textbf{Relevance}  focuses on the appropriateness and significance of the generated content. This criterion examines how well the text addresses the given context or query, ensuring that the information provided is pertinent and directly applicable.\n    \\item \\textbf{Fluency}  assesses the language model's ability to produce content that flows smoothly, maintaining a consistent tone and style. A fluent text is not only grammatically correct but also ensures readability and a seamless user experience. Analysts evaluate how well the model avoids awkward expressions and abrupt shifts in language or topic, contributing to effective communication with users.\n    \\item \\textbf{Transparency} delves into the clarity and openness of the language model's decision-making process. It involves assessing how well the model communicates its thought processes, enabling users to understand how and why certain responses are generated. A transparent model provides insights into its inner workings.\n    \\item \\textbf{Safety}  emerges as a critical criterion concerned with the potential harm or unintended consequences arising from the generated text. It examines the language model's ability to avoid producing content that may be inappropriate, offensive, or harmful, ensuring the well-being of users and avoiding misinformation.\n    \\item \\textbf{Human alignment} assesses the degree to which the language model's output aligns with human values, preferences, and expectations. It considers the ethical implications of the generated content, ensuring that the language model produces text that respects societal norms and user expectations, promoting a positive interaction with human users.\n\\end{enumerate}\nLastly, the expertise level of evaluators is a critical consideration, encompassing relevant domain knowledge, task familiarity, and methodological training. Delineating the requisite expertise level for evaluators ensures that they possess the necessary background knowledge to accurately comprehend and assess the domain-specific text generated by \\llms. This strategy adds a layer of rigor to the evaluation process, reinforcing the credibility and validity of the findings.", "cites": [1570, 3087, 3080, 3572, 3574, 3531, 7638, 3573], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple cited papers to present a structured framework for human evaluation of LLMs. It moves beyond mere description by abstracting the criteria into a 6-point system (accuracy, relevance, fluency, transparency, safety, human alignment), offering a high-level analytical perspective. While it provides some critical points (e.g., the variance and instability of human evaluations), deeper comparative critique across methodologies is limited."}}
{"id": "22d366ed-80ef-48c4-be3a-fe26b58be125", "title": "What can \\llms do well?", "level": "subsubsection", "subsections": [], "parent_id": "19660a20-c064-48d4-89c0-e1f39496206d", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Summary"], ["subsection", "Task: Success and Failure Cases of \\llms"], ["subsubsection", "What can \\llms do well?"]], "content": "\\begin{itemize}\n    \\item \\llms demonstrate proficiency in generating text  by producing fluent and precise linguistic expressions.\n    \\item \\llms obtain impressive performance in tasks involving language understanding, including sentiment analysis , text classification , as well as the handling of factual input .\n    \\item \\llms demonstrate robust arithmetic reasoning capabilities  and excel in logical reasoning . Moreover, they exhibit noteworthy proficiency in temporal reasoning . Furthermore, more intricate tasks such as mathematical reasoning  and structured data inference  have emerged as the prevailing benchmarks for evaluation.\n    \\item \\llms exhibit robust contextual comprehension, enabling them to generate coherent responses that align with the given input .\n    \\item \\llms also achieve satisfying performance across several natural language processing tasks, including machine translation , text generation , and question answering .\n\\end{itemize}", "cites": [1570, 679, 1550, 3510, 3513, 3545, 7735, 3553, 3538, 7733, 1554, 7667, 3558, 3492, 1571, 1553], "cite_extract_rate": 0.8421052631578947, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists several strengths of LLMs (e.g., text generation, language understanding, arithmetic reasoning) and associates them with relevant papers, but it lacks synthesis or deeper integration of these findings into a unified perspective. It offers minimal critical analysis or discussion of limitations and primarily functions as a descriptive overview of capabilities. There is little abstraction or generalization beyond the specific examples cited."}}
{"id": "8070ca6a-10a0-492d-be66-56d6ec0da88b", "title": "When can \\llms fail?", "level": "subsubsection", "subsections": [], "parent_id": "19660a20-c064-48d4-89c0-e1f39496206d", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Summary"], ["subsection", "Task: Success and Failure Cases of \\llms"], ["subsubsection", "When can \\llms fail?"]], "content": "\\begin{itemize}\n\\item Within the realm of NLI, \\llms exhibit subpar performance and encounter challenges in accurately representing human disagreements . \n\\item \\llms exhibit restricted proficiency in discerning semantic similarity between events  and demonstrate substandard performance in evaluating fundamental phrases .\n\\item \\llms have limited abilities on abstract reasoning , and are prone to confusion or errors in complex contexts .\n\\item In linguistic contexts featuring non-Latin scripts and limited resources, \\llms manifest suboptimal performance . Furthermore, generative \\llms consistently display proficiency levels below the expected standards across various tasks and languages .\n\\item \\llms demonstrate susceptibility when processing visual modal information . Furthermore, they have the capacity to assimilate, disseminate, and potentially magnify detrimental content found within the acquired training datasets, frequently encompassing toxic linguistic elements, including offensive, hostile, and derogatory language .\n\\item \\llms may exhibit social biases and toxicity  during the generation process, resulting in the production of biased outputs.\n\\item \\llms may manifest credibility deficits , potentially giving rise to fabricated information or erroneous facts within dialogues .\n\\item \\llms have limitations in incorporating real-time or dynamic information , making them less suitable for tasks that require up-to-date knowledge or rapid adaptation to changing contexts.\n\\item \\llms is sensitive to prompts, especially adversarial prompts , which trigger new evaluations and algorithms to improve its robustness.\n\\end{itemize}", "cites": [3564, 3508, 2218, 3512, 366, 3490, 3539, 8663, 7573, 3497, 7574, 7648, 3569], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section draws from multiple papers to identify common failure modes of LLMs, such as in event semantics, adversarial prompts, bias, hallucination, and multimodal processing. While it connects these ideas thematically, the synthesis remains largely descriptive without a deeper unifying framework. The analysis is somewhat critical in pointing out limitations but lacks in-depth evaluation or contrast between methodologies."}}
{"id": "6cd4e1ed-da39-402a-b8f9-2d17cd0025e6", "title": "Benchmark and Evaluation Protocol", "level": "subsection", "subsections": [], "parent_id": "c80f54fb-9337-4764-8cbe-700c1a613bba", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Summary"], ["subsection", "Benchmark and Evaluation Protocol"]], "content": "With the rapid development and widespread use of \\llms, the importance of evaluating them in practical applications and research has become crucial. This evaluation process should include not only task-level evaluation but also a deep understanding of the potential risks they pose from a societal perspective.\nIn this section, we summarize existing benchmarks and protocols in \\tablename~\\ref{tb-llm-eval}.\nFirst, a shift from objective calculation to human-in-the-loop testing, allowing for greater human feedback during the evaluation process.\nAdaVision , an interactive process for testing vision models, enables users to label a small amount of data for model correctness, which helps users identify and fix coherent failure modes.\nIn AdaTest , the user filters test samples by only selecting high-quality tests and organizing them into semantically related topics. \nSecond, a move from static to crowd-sourcing test sets is becoming more common.\nTools like DynaBench , DynaBoard , and DynaTask  rely on crowdworkers to create and test hard samples.\nAdditionally, DynamicTempLAMA  allows for dynamically constructed time-related tests.\nThird, a shift from a unified to a challenging setting in evaluating machine learning models.\nWhile unified settings involve a test set with no preference for any specific task, challenging settings create test sets for specific tasks.\nTools like DeepTest  use seeds to generate input transformations for testing, CheckList  builds test sets based on templates, and AdaFilter  adversarially constructs tests.\nHowever, it is worth noting that AdaFilter may not be entirely fair as it relies on adversarial examples.\nHELM  evaluates LLMs from different aspects, while the Big-Bench  platform is used to design hard tasks for machine learning models to tackle.\nPromptBench~ aims to evaluate the adversarial robustness of \\llms by creating adversarial prompts, which is more challenging and the results demonstrated that current \\llms are not robust to adversarial prompts.", "cites": [1570, 3498, 3527, 3551, 7141, 3500, 7142, 3526, 3533, 7573, 7466], "cite_extract_rate": 0.9166666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows strong synthesis by grouping cited papers into three clear trends: human-in-the-loop testing, dynamic/crowdsourced test sets, and challenging adversarial settings. It also provides some critical analysis by noting the potential fairness issues of adversarial approaches (e.g., AdaFilter). However, the abstraction remains moderate, as the broader implications and general principles are less emphasized in favor of describing specific benchmarks and protocols."}}
{"id": "6987d1f4-675d-483d-8d0a-671ff1a22bb2", "title": "Robustness Evaluation", "level": "subsection", "subsections": [], "parent_id": "059a0ae0-2dac-4937-860e-ca2897d17989", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Grand Challenges and Opportunities for Future Research"], ["subsection", "Robustness Evaluation"]], "content": "Beyond general tasks, it is crucial for \\llms to maintain robustness against a wide variety of inputs in order to perform optimally for end-users, given their extensive integration into daily life.\nFor instance, the same prompts but with different grammars and expressions could lead ChatGPT and other \\llms to generate diverse results, indicating that current \\llms are not robust to the inputs.\nWhile there are some prior work on robustness evaluation~, there are much room for advancement, such as including more diverse evaluation sets, examining more evaluation aspects, and developing more efficient evaluations to generate robustness tasks.\nConcurrently, the concept and definition of robustness are constantly evolving. It is thus vital to consider updating the evaluation system to better align with emerging requirements related to ethics and bias.", "cites": [7464, 7573], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the two cited papers, integrating their focus on adversarial and out-of-distribution robustness evaluation. It identifies some gaps, such as the need for more diverse evaluation sets and alignment with evolving ethical requirements, but does not offer in-depth comparative or evaluative analysis. The discussion moves toward broader considerations of robustness, indicating some abstraction but not at the meta-level."}}
{"id": "87bb1af9-eabb-4787-a74f-e91b785349f7", "title": "Unified Evaluation that Supports All \\llms Tasks", "level": "subsection", "subsections": [], "parent_id": "059a0ae0-2dac-4937-860e-ca2897d17989", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Grand Challenges and Opportunities for Future Research"], ["subsection", "Unified Evaluation that Supports All \\llms Tasks"]], "content": "There are many other research areas of \\llms and we need to develop evaluation systems that can support all kinds of tasks such as value alignment, safety, verification, interdisciplinary research, fine-tuning, and others.\nFor instance, PandaLM~ is an evaluation system that assists \\llms fine-tuning by providing an open-source evaluation model, which can automatically assess the performance of fine-tuning.\nWe expect that more evaluation systems are becoming more general and can be used as assistance in certain \\llms tasks.", "cites": [3523], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section is primarily descriptive, mentioning the need for unified evaluation systems and briefly introducing PandaLM without connecting it to broader trends or existing literature. It lacks critical evaluation of the system's strengths or weaknesses and does not abstract key principles or propose a framework for future development."}}
{"id": "9e2be6e1-62bf-408b-945c-903d1098c861", "title": "Beyond Evaluation: \\llms Enhancement", "level": "subsection", "subsections": [], "parent_id": "059a0ae0-2dac-4937-860e-ca2897d17989", "prefix_titles": [["title", "A Survey on Evaluation of Large Language Models "], ["section", "Grand Challenges and Opportunities for Future Research"], ["subsection", "Beyond Evaluation: \\llms Enhancement"]], "content": "Ultimately, evaluation is not the end goal but rather the starting point. \nFollowing the evaluation, there are undoubtedly conclusions to be drawn regarding performance, robustness, stability, and other factors. \nA proficient evaluation system should not only offer benchmark results but should also deliver an insightful analysis, recommendations, and guidance for future research and development. \nFor instance, PromptBench~ provides not only robustness evaluation results on adversarial prompts but also a comprehensive analysis through attention visualization, elucidating how adversarial texts can result in erroneous responses. \nThe system further offers a word frequency analysis to identify robust and non-robust words in the test sets, thus providing prompt engineering guidance for end users. \nSubsequent research can leverage these findings to enhance \\llms.\nAnother example is that  first explored the performance of large vision-language models on imbalanced (long-tailed) tasks, which demonstrates the limitation of current large models.\nThen, they explored different methodologies to enhance the performance on these tasks.\nIn summary, enhancement after evaluation helps to build better \\llms and much can be done in the future.", "cites": [3575, 7573], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly synthesizes the insights from two cited papers, showing how evaluation can inform model enhancement. However, the synthesis is limited and does not connect broader themes across multiple sources. It provides some analytical value by highlighting how evaluation results can lead to actionable improvements, but lacks deeper critical evaluation or abstraction to overarching principles."}}
