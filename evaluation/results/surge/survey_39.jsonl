{"id": "4ba7ff9e-302a-4cdd-bfa2-5253f5e42ce3", "title": "Introduction", "level": "section", "subsections": ["f8edc7dd-ff9f-48ed-b335-8cb95d65c581"], "parent_id": "f8de3762-f517-4130-b09c-c364b4e5ee85", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Introduction"]], "content": "Fueled by large amounts of available data and hardware advances, machine learning has experienced tremendous growth in academic research and real world applications. At the same time, the impact on the security, privacy, and fairness of machine learning is receiving increasing attention. In terms of privacy, our personal data are being harvested by almost every online service and are used to train models that power machine learning applications. However, it is not well known if and how these models reveal information about the data used for their training. If a model is trained using sensitive data such as location, health records, or identity information, then an attack that allows an adversary to extract this information from the model is highly undesirable. At the same time, if private data has been used without its owners' consent, the same type of attack could be used to determine the unauthorized use of data and thus work in favor of the user's privacy. \nApart from the increasing interest on the attacks themselves, there is a growing interest in uncovering what causes privacy leaks and under which conditions a model is susceptible to different types of privacy-related attacks. There are multiple reasons why models leak information. Some of them are structural and have to do with the way models are constructed, while others are due to factors such as poor generalization or memorization of sensitive data samples. Training for adversarial robustness can also be a factor that affects the degree of information leakage.  \nThe focus of this survey is the privacy and confidentiality attacks on machine learning algorithms. That is, attacks that try to extract information about the training data or to extract the model itself. Some existing surveys~ provide partial coverage of privacy attacks and there are a few other peer-reviewed works on the topic~. However, these papers are either too high level or too specialized in a narrow subset of attacks.\nThe security of machine learning and the impact of adversarial attacks on the performance of the models have been widely studied in the community, with several surveys highlighting the major advances in the area~. Based on the taxonomy proposed in~, there are three types of attacks on machine learning systems: i) attacks against integrity, e.g., evasion and poisoning backdoor attacks that cause misclassification of specific samples, ii) attacks against a system's availability, such as poisoning attacks that try to maximize the misclassification error and iii) attacks against privacy and confidentiality, i.e., attacks that try to infer information about user data and models. While all attacks on machine learning are adversarial in nature, the term \"adversarial attacks\" is commonly used to refer to security-related attacks and more specifically to adversarial samples. In this survey, we only focus on privacy and confidentiality attacks.\nAn attack that extracts information about the model's structure and parameters is, strictly speaking, an attack against model confidentiality. The decision to include model extraction attacks was made because in the existing literature, attacks on model confidentiality are usually grouped together with privacy attacks~. Another important reason is that stealing model functionality may be considered a privacy breach as well. Veale et al.~ made the argument that privacy attacks such as membership inference (Section~\\ref{subsec:mi_attacks}) increase the risk of machine learning models being classified as personal data under European Union's General Data Protection Regulation (GDPR) law because they can render a person identifiable. Although models are currently not covered by the GDPR, it may happen that they will be considered as personal data, and then attacks against them may fall under the same scope as attacks against personal data. This may be further complicated by the fact that model extraction attacks can be used as a stepping stone for other attacks. \nThis paper is, as far as we know, the first \\emph{comprehensive} survey of privacy-related attacks against machine learning. It reviews and systematically analyzes over 40 research papers. The papers have been published in top tier conferences and journals in the areas of security, privacy, and machine learning during 2014-2020. An initial set of papers was selected in Google Scholar using keyword searches related to \"privacy\", \"machine learning\", and the names of the attacks themselves (\"membership inference\", \"model inversion\", \"property inference\", model stealing\", \"model extraction\", etc.). After the initial set of papers was selected, more papers were added by backward search based on their references as well as by forward search based on the papers that cited them.\nThe main contributions of this paper are:\n\\begin{itemize}\n    \\item The first comprehensive study of attacks on privacy and confidentiality of machine learning systems. \n    \\item A unifying taxonomy of attacks against machine learning privacy.\n    \\item A discussion on the probable causes of privacy leaks in machine learning systems.\n    \\item An in-depth presentation of the implementation of the attacks.\n    \\item An overview of the different defensive measures tested to protect against the different attacks.\n\\end{itemize}", "cites": [5826, 6057, 3860, 6056], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to present a coherent narrative on privacy attacks in machine learning, particularly linking them to broader themes like GDPR implications. It critically evaluates the limitations of existing surveys and introduces a rationale for including model extraction attacks in the privacy category. The abstraction level is high, as it generalizes the causes of privacy leaks and situates the survey within the evolving legal and technical landscape of ML privacy."}}
{"id": "6e2f4e97-52a6-4f98-ae7d-1dcb1aa15811", "title": "Machine Learning", "level": "section", "subsections": ["a8f3000c-ecd8-4cf0-b0c3-1f3380ecf4d7", "d7363c29-40fb-479d-993b-b0a9cd5f3eb4", "bc4d976c-fa17-4a0b-9354-cb7f0ff7568a"], "parent_id": "f8de3762-f517-4130-b09c-c364b4e5ee85", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Machine Learning"]], "content": "Machine learning (ML) is a field that studies the problem of learning from data without being explicitly programmed. The purpose of this section is to provide a non-exhaustive overview of machine learning as it pertains to this survey and to facilitate the discussion in the subsequent chapters. We briefly introduce a high level view of different machine learning paradigms and categorizations as well as machine learning architectures. Finally, we present a brief discussion on model training and inference. For the interested reader, there are several textbooks such as  that provide a thorough coverage of the topic.", "cites": [166], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a general, high-level overview of machine learning and briefly mentions the cited paper without synthesizing its content or connecting it meaningfully to the survey’s focus on privacy attacks. It lacks critical analysis and abstraction, merely describing ML paradigms and referencing a textbook-style paper for further reading."}}
{"id": "5c74a20a-61d0-4105-8cb3-c02bbb6ca51e", "title": "Reinforcement Learning", "level": "subsubsection", "subsections": [], "parent_id": "a8f3000c-ecd8-4cf0-b0c3-1f3380ecf4d7", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Machine Learning"], ["subsection", "Types of Learning"], ["subsubsection", "Reinforcement Learning"]], "content": "Reinforcement learning concerns itself with agents that make observations of the environment and use these to take actions with the goal of maximizing a reward signal. In the most general formulation, the set of actions is not predefined and the rewards are not necessarily immediate but can occur after a sequence of actions~. To our knowledge, no privacy-related attacks against reinforcement learning have been reported, but it has been used to launch other privacy-related attacks~.", "cites": [6058], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis by only mentioning one paper and not integrating it into a broader discussion of privacy attacks in reinforcement learning. It lacks critical analysis of the cited work or identification of limitations, and offers little abstraction beyond stating that no direct attacks have been reported. The narrative remains descriptive and does not contribute to a deeper understanding of the topic."}}
{"id": "b19c53df-7423-4097-8931-0aa380293ac9", "title": "Generative and Discriminative Learning", "level": "subsubsection", "subsections": [], "parent_id": "a8f3000c-ecd8-4cf0-b0c3-1f3380ecf4d7", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Machine Learning"], ["subsection", "Types of Learning"], ["subsubsection", "Generative and Discriminative Learning"]], "content": "Another categorization of learning algorithms is that of \\textit{discriminative} vs \\textit{generative} algorithms. Discriminative classifiers try to model the conditional probability $p(y | \\mathbf{x})$, i.e., they try to learn the decision boundaries that separate the different classes directly based on the input data $\\mathbf{x}$. Examples of such algorithms are logistic regression and neural networks. Generative classifiers try to capture the joint distribution $p(\\mathbf{x}, y)$. An example of such a classifier is Naive Bayes. Generative models that do not require labels, but they try to model $p(\\mathbf{x})$, explicitly or implicitly. Notable examples are language models that predict the next word(s) given some input text or GANs and Variational Autoencoders (VAEs)~ that are able to generate data samples that match the properties of the training data.", "cites": [5680], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition and examples of generative and discriminative learning models but does not synthesize the cited papers to form a deeper narrative. It mentions a paper on Variational Autoencoders but does not integrate its insights into the discussion. There is no critical evaluation or abstraction to broader principles related to privacy attacks."}}
{"id": "04e70a3d-444d-4a78-943f-2462c45858c2", "title": "Distributed Learning", "level": "subsubsection", "subsections": [], "parent_id": "d7363c29-40fb-479d-993b-b0a9cd5f3eb4", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Machine Learning"], ["subsection", "Learning Architectures"], ["subsubsection", "Distributed Learning"]], "content": "The requirements that drive the need for distributed learning architectures are the handling and processing of large amounts of data, the need for computing and memory capacity, and even privacy concerns. From the existing variants of distributed learning, we present those that are relevant from a privacy perspective, namely \\textit{collaborative} or \\textit{federated learning} (FL), \\textit{fully decentralized} or \\textit{peer-to-peer} (P2P) learning and \\textit{split learning}.\nCollaborative or federated learning is a form of decentralized training where the goal is to learn one global model from data stored in multiple remote devices or locations~. The main idea is that the data do not leave the remote devices. Data are processed locally and then used to update the local models. Intermediate model updates are sent to the central server that aggregates them and creates a global model. The central server then sends the global model back to all participant devices.\nIn fully decentralized learning or Peer-to-Peer (P2P) learning, there is no central orchestration server. Instead, the devices communicate in a P2P fashion and exchange their updates directly with other devices. This setup may be interesting from a privacy perspective, since it alleviates the need to trust a central server. However, attacks on P2P systems are relevant in such settings and need to be taken into account. Up to now, there were no privacy-based attacks reported on such systems; although they may become relevant in the future. Moreover, depending on the type of information shared between the peers, several of the attacks on collaborative learning may be applicable.\nIn split learning, the trained model is split into two or more parts. The edge devices keep the initial layers of the deep learning model and the centralized server keeps the final layers~. The reason for the split is mainly to lower communication costs by sending intermediate model outputs instead of the input data. This setup is also relevant in situations where remote or edge devices have limited resources and are connected to a central cloud server. This scenario is common for Internet of Things (IoT) devices.", "cites": [602], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of distributed learning architectures and their privacy-related relevance, but it does not deeply synthesize or connect multiple cited papers. It lacks critical evaluation of the cited work, and while it introduces general terms and concepts, it does not offer meta-level insights or overarching principles. The content is primarily factual and explanatory."}}
{"id": "b35984e4-23f3-41a2-814c-94ee166af2be", "title": "Training in Distributed Settings", "level": "subsubsection", "subsections": [], "parent_id": "bc4d976c-fa17-4a0b-9354-cb7f0ff7568a", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Machine Learning"], ["subsection", "Training and Inference"], ["subsubsection", "Training in Distributed Settings"]], "content": "The most popular learning algorithm for federated learning is federated averaging~, where each remote device calculates one step of gradient descent from the locally stored data and then shares the updated model weights with the parameter server. The parameter server averages the weights of all remote participants and updates the global model which is subsequently shared again with the remote devices. It can be defined by:\n\\begin{equation}\n    \\label{eq:fed_avg}\n    \\theta_{t+1} = \\frac{1}{K} \\sum_{k=1}^{K} \\theta_{t}^{(k)} \n\\end{equation}\nwhere K is the number of remote participants and the parameters $\\theta_{t}^{(k)}$ of participant $k$ have been calculated locally based on Equations~\\ref{eq:theta_update} and \\ref{eq:sgd_grad}. \nAnother approach that comes from the area of distributed computing is downpour (or synchronized) SGD~, which proposes to share the loss gradients of the distributed devices with the parameter server that aggregates them and then performs one step of gradient descent. It can be defined by:\n\\begin{equation}\n    \\label{eq:SSGD}\n    \\theta_{t+1} = \\theta_{t} - \\eta \\sum_{k=1}^{K} \\frac{m^{(k)}}{M}\\mathbf{g}_t^{(k)}\n\\end{equation}\nwhere $\\mathbf{g}_t^{(k)}$ is the gradient computed by participant $k$ based on Equation~\\ref{eq:sgd_grad} using their local data, $m^{(k)}$ is the number of data points in the remote participant and $M$ is the total number of data points in the training data. After the calculation of Equation~\\ref{eq:SSGD}, the parameter server sends the updated model parameters $\\theta_{t+1}$ to the remote participants.", "cites": [582], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of two distributed learning algorithms—federated averaging and downpour SGD—but does not critically evaluate or compare them. It integrates minimal information from the cited paper and does not abstract broader patterns or principles related to privacy in distributed training. The content is primarily explanatory without offering deeper insights or analysis."}}
{"id": "3dd9a70b-72c5-4a28-aa0f-06d0d5de24b3", "title": "Membership Inference Attacks", "level": "subsection", "subsections": [], "parent_id": "a0d40d87-8b16-4b1e-a9a6-ccc5d529b0fe", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Attack Types"], ["subsection", "Membership Inference Attacks"]], "content": "\\label{subsec:mi_attacks}\nMembership inference tries to determine whether an input sample $\\mathbf{x}$ was used as part of the training set $\\mathcal{D}$. This is the most popular category of attacks and was first introduced by Shokri et al.~. The attack only assumes knowledge of the model's output prediction vector (black-box) and was carried out against supervised machine learning models. White-box attacks in this category are also a threat, especially in a collaborative setting, where an adversary can mount both passive and active attacks. If there is access to the model parameters and gradients, then this allows for more effective white-box membership inference attacks in terms of accuracy~. \nApart from supervised models, generative models such as GANs and VAEs are also susceptible to membership inference attacks~. The goal of the attack, in this case, is to retrieve information about the training data using varying degrees of knowledge of the data generating components.\nFinally, these types of attacks can be viewed from a different perspective, that of the data owner. In such a scenario, the owner of the data may have the ability to audit black-box models to see if the data have been used without authorization~.", "cites": [603, 6060, 609, 6059], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information across multiple papers on membership inference attacks, linking black-box and white-box scenarios as well as extending the discussion to generative models. It offers a moderate level of abstraction by framing the attacks from the perspective of data owners and auditing. However, it lacks deeper critical analysis of the cited works, such as evaluating their methodologies or identifying limitations in a nuanced way."}}
{"id": "041e4db7-9970-4f91-9c58-cafa305d06c4", "title": "Reconstruction Attacks", "level": "subsection", "subsections": [], "parent_id": "a0d40d87-8b16-4b1e-a9a6-ccc5d529b0fe", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Attack Types"], ["subsection", "Reconstruction Attacks"]], "content": "Reconstruction attacks try to recreate one or more training samples and/or their respective training labels. The reconstruction can be partial or full. Previous works have also used the terms \\textbf{attribute inference} or \\textbf{model inversion} to describe attacks that, given output labels and partial knowledge of some features, try to recover sensitive features or the full data sample. For the purpose of this survey, all these attacks are considered as part of the larger set of reconstruction attacks. The term \\textbf{attribute inference} has been used in other parts of the privacy related literature to describe attacks that infer sensitive \"attributes\" of a targeted user by leveraging publicly accessible data~. These attacks are not part of this review as they are mounted against the individual's data directly and not against ML models.\nA major distinction between the works of this category is between those that create an actual reconstruction of the data~ and the ones that create class representatives or probable values of sensitive features that do not necessarily belong to the training dataset~. In classification models, the latter case is limited to scenarios where classes are made up of one type of object, e.g., faces of the same person. While this limits the applicability of the attack, it can still be an interesting scenario in some cases.", "cites": [6061, 3477, 3469, 604, 6062], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of reconstruction attacks by unifying related terms such as attribute inference and model inversion under a coherent framework. It also abstracts broader patterns in the nature of these attacks, distinguishing between data reconstruction and class representative inference. While it provides some critical insights, particularly in delineating the scope and applicability of different attack types, a deeper comparative or evaluative critique of the cited papers is limited."}}
{"id": "f2b3105d-3ed7-45f2-b357-9f51b2787200", "title": "Property Inference Attacks", "level": "subsection", "subsections": [], "parent_id": "a0d40d87-8b16-4b1e-a9a6-ccc5d529b0fe", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Attack Types"], ["subsection", "Property Inference Attacks"]], "content": "The ability to extract dataset properties which were not explicitly encoded as features or were not correlated to the learning task, is called \\textbf{property inference}. An example of property inference is the extraction of information about the ratio of women and men in a patient dataset when this information was not an encoded attribute or a label of the dataset. Or having a neural network that performs gender classification and can be used to infer if people in the training dataset wear glasses or not. In some settings, this type of leak can have privacy implications. These types of properties can also be used to get more insight about the training data, which can lead to adversaries using this information to create similar models  or even have security implications when the learned property can be used to detect vulnerabilities of a system~. \nProperty inference aims to extract information that was learned from the model unintentionally and that is not related to the training task. Even well generalized models may learn properties that are relevant to the whole input data distribution and sometimes this is unavoidable or even necessary for the learning process. What is more interesting from an adversarial perspective, are properties that may be inferred from a specific subset of training data, or eventually about a specific individual.\nProperty inference attacks so far target either dataset-wide properties~ or the emergence of properties within a batch of data~. The latter attack was performed on the collaborative training of a model.", "cites": [614, 8974], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear definition of property inference attacks and integrates two cited papers to illustrate how such leaks can occur in collaborative learning and embedding models. It begins to synthesize concepts across these papers to highlight the general privacy implications of model training. However, the critical analysis is limited, as it does not deeply evaluate the methods or limitations of the cited works, and the abstraction is moderate, offering some general insight but not a comprehensive meta-framework."}}
{"id": "4878d969-ce11-4767-b4e9-016674b8d595", "title": "Model Extraction Attacks", "level": "subsection", "subsections": [], "parent_id": "a0d40d87-8b16-4b1e-a9a6-ccc5d529b0fe", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Attack Types"], ["subsection", "Model Extraction Attacks"]], "content": "\\label{subsec:model_extraction}\n\\textbf{Model extraction} is a class of black-box attacks where the adversary tries to extract information and potentially fully reconstruct a model by creating a substitute model $\\hat{f}$ that behaves very similarly to the model under attack $f$. There are two main focus for substitute models. First, to create models that match the accuracy of the target model $f$ in a test set that is drawn from the input data distribution and related to the learning task~. Second, to create a substitute model $\\hat{f}$ that matches $f$ at a set of input points that are not necessarily related to the learning task~. Jagielski et al.~ referred to the former attack as \\textbf{task accuracy} extraction and the latter as \\textbf{fidelity} extraction. In task accuracy extraction, the adversary is interested in creating a substitute that learns the same task as the target model equally well or better. In the latter case, the adversary aims to create a substitute that replicates the decision boundary of $f$ as faithfully as possible. This type of attack can be later used as a stepping stone before launching other types of attacks such as adversarial attacks~ or membership inference attacks~. In both cases, it is assumed that the adversary wants to be as efficient as possible, i.e., to use as few queries as possible. Knowledge of the target model architecture is assumed in some works, but it is not strictly necessary if the adversary selects a substitute model that has the same or higher complexity than the model under attack~. \nApart from creating substitute models, there are also approaches that focus on recovering information from the target model, such as hyper-parameters in the objective function~ or information about various neural network architectural properties such as activation types, optimisation algorithm, number of layers, etc~.", "cites": [2676, 2677, 6063, 6064, 6058, 7607], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by introducing a structured taxonomy (task accuracy vs. fidelity extraction) based on Jagielski et al. and integrating related works like PRADA and Copycat CNN. It shows critical analysis by discussing variations in attack objectives and assumptions, such as the necessity of architectural knowledge. The abstraction is strong, as it generalizes model extraction into broader principles and objectives, such as efficiency and replication of decision boundaries."}}
{"id": "0f6d8c4b-a341-48dd-bbf6-4777fd51f7ad", "title": "Causes of Privacy Leaks", "level": "section", "subsections": ["698d31d7-7e8e-4c0e-8e8a-f4fdf2146f8c", "60a4f30a-8e7d-417c-9a97-84ad3666c991", "ce601a44-2f2f-436a-884d-b82c700063f4", "2ba6d237-371c-4ffe-81ca-d4bb97f00d1b"], "parent_id": "f8de3762-f517-4130-b09c-c364b4e5ee85", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Causes of Privacy Leaks"]], "content": "\\label{sec:why_leak}\nThe conditions under which machine learning models leak is a research topic that has started to emerge in the past few years. Some models leak information due to the way they are constructed. An example of such a case is Support Vector Machines (SVMs), where the support vectors are data points from the training dataset. Other models, such as linear classifiers are relatively easy to \"reverse engineer\" and to retrieve their parameters just by having enough input / output data pairs~. Larger models such as deep neural networks usually have a large number of parameters and simple attacks are not feasible. However, under certain assumptions and conditions, it is possible to retrieve information about either the training data or the models themselves.", "cites": [2676], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of why machine learning models leak privacy, referencing a specific example from the cited paper. However, synthesis is limited as it only loosely connects one paper to general observations without deeper integration of multiple sources. Critical analysis is minimal, as the section does not evaluate or compare the cited work with others. It attempts some abstraction by categorizing model types (e.g., SVMs, linear classifiers, deep neural networks) and their vulnerability to leaks, but this is not extended to a broader, more comprehensive framework."}}
{"id": "698d31d7-7e8e-4c0e-8e8a-f4fdf2146f8c", "title": "Causes of Membership Inference Attacks", "level": "subsection", "subsections": [], "parent_id": "0f6d8c4b-a341-48dd-bbf6-4777fd51f7ad", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Causes of Privacy Leaks"], ["subsection", "Causes of Membership Inference Attacks"]], "content": "One of the conditions that has been shown to improve the accuracy of membership inference is the poor generalization of the model. The connection between overfitting and black-box membership inference was initially investigated by Shokri et al.~. This paper was the first to examine membership inference attacks on neural networks. The authors measured the effect of overfitting on the attack accuracy by training models in different MLaaS platforms using the same dataset. The authors showed experimentally that overfitting can lead to privacy leakage but also noted that it is not the only condition, since some models that had lower generalization error where more prone to membership leaks. The effect of overfitting was later corroborated formally by Yeom et al.~. The authors defined membership advantage as a measure of how well an attacker can distinguish whether a data sample belongs to the training set or not, given access to the model. They proved that the membership advantage is proportional to the generalization error of the model and that overfitting is a sufficient condition for performing membership inference attacks but not a necessary one. Additionally, Long et al.~ showed that even in well-generalized models, it is possible to perform membership inference for a subset of the training data which they named \\textit{vulnerable records}. \nOther factors, such as the model architecture, model type, and dataset structure, affect the attack accuracy. Similarly to ~ but in the white-box setting, Nasr et al.~ showed that two models with the same generalization error showed different degrees of leakage. More specifically, the most complex model in terms of number of parameters exhibited higher attack accuracy, showing that model complexity is also an important factor. \nTruex et al.~ ran different types of experiments to measure the significance of the model type as well as the the number of classes present in the dataset. They found that certain model types such as Naive Bayes are less susceptible to membership inference attacks than decision trees or neural networks. They also showed that as the number of classes in the dataset increases, so does the potential of membership leaks. This finding agrees with the results in ~.\nSecuring machine learning models against adversarial attacks can also have an adverse effect on the model's privacy as shown by Song et al.~. Current state of the art proposals for robust model training, such as projective gradient descent (PGD) adversarial training~, increase the model's susceptibility to membership inference attacks. This is not unexpected since robust training methods (both empirical and provable defenses) tend to increase the generalization error. As previously discussed, the generalization error is related to the success of the attack. Furthermore, the authors of~ argue that robust training may lead to increased model sensitivity to the training data, which can also affect membership inference.\nThe generalization error is easily measurable in supervised learning under the assumption that the test data can capture the nuances of the real data distribution. In generative models and specifically in GANs this is not the case, hence the notion of overfitting is not directly applicable. All three papers that deal with membership inference attacks against GANs mention overfitting as an important factor behind successful attacks~. In this case, overfitting means that the generator has memorized and replays part of the training data. This is further corroborated in the study in~, where their attacks are shown to be less successful as the training data size increases.", "cites": [603, 6065, 8094, 3482, 6060, 917, 6059], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a well-structured analytical overview of the causes of membership inference attacks, synthesizing findings from multiple papers and connecting the concept of overfitting to privacy risks. It identifies broader patterns such as the relationship between generalization error and privacy leakage, and extends the discussion to generative models. The section also includes some critical observations, such as the trade-off between adversarial robustness and privacy, though deeper critiques or limitations of the cited approaches are limited."}}
{"id": "60a4f30a-8e7d-417c-9a97-84ad3666c991", "title": "Causes of Reconstruction Attacks", "level": "subsection", "subsections": [], "parent_id": "0f6d8c4b-a341-48dd-bbf6-4777fd51f7ad", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Causes of Privacy Leaks"], ["subsection", "Causes of Reconstruction Attacks"]], "content": "Regarding reconstruction attacks, Yeom et al.~ showed that a higher generalization error can lead to a higher probability to infer data attributes, but also that the influence of the target feature on the model is an important factor. However, the authors assumed that the adversary has knowledge of the prior distribution of the target features and labels. Using weaker assumptions about the adversary's knowledge, Zhang et al.~ showed theoretically and experimentally that a model that has high predictive power is more susceptible to reconstruction attacks. Finally, similarly to vulnerable records in membership inference, memorization and retrieval of data which are \\textit{out-of-distribution} was shown to be the case even for models that do not overfit~.", "cites": [3482, 6061, 6066], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of reconstruction attacks by synthesizing key findings from three papers, highlighting the role of generalization error, model predictive power, and out-of-distribution memorization. It connects these factors to the susceptibility of models to attacks, offering a coherent explanation. While it includes some critical analysis (e.g., weaker assumptions in Zhang's work), it is limited and does not deeply critique the methodologies or limitations. The abstraction is moderate, identifying broader themes like the influence of model properties on privacy risks."}}
{"id": "ce601a44-2f2f-436a-884d-b82c700063f4", "title": "Causes of Property Inference Attacks", "level": "subsection", "subsections": [], "parent_id": "0f6d8c4b-a341-48dd-bbf6-4777fd51f7ad", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Causes of Privacy Leaks"], ["subsection", "Causes of Property Inference Attacks"]], "content": "Property inference is possible even with well-generalized models~ so overfitting does not seem to be a cause of property inference attacks. Unfortunately, regarding property inference attacks, we have less information about what makes them possible and under which circumstances they appear to be effective. This is an interesting avenue for future research, both from a theoretical and an empirical point of view.", "cites": [614], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly mentions the possibility of property inference attacks and cites one paper, but it lacks synthesis of multiple sources or a coherent narrative. It identifies a research gap and suggests future work, showing minimal critical perspective. The abstraction is limited, as it does not generalize findings or propose broader principles."}}
{"id": "2ba6d237-371c-4ffe-81ca-d4bb97f00d1b", "title": "Causes of Model Extraction", "level": "subsection", "subsections": [], "parent_id": "0f6d8c4b-a341-48dd-bbf6-4777fd51f7ad", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Causes of Privacy Leaks"], ["subsection", "Causes of Model Extraction"]], "content": "While overfitting increases the success of black-box membership inference attacks, the exact opposite holds for model extraction attacks. It is possible to steal model parameters when the models under attack have 98\\% or higher accuracy in the test set~. Also models with a higher generalization error are harder to steal, probably due to the fact that they may have memorized samples that are not part of the attacker's dataset~. Another factor that may affect model extraction success is the dataset used for training. Higher number of classes may lead to worse attack performance~.", "cites": [6067], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides some analytical connections, such as the inverse relationship between generalization and model extraction success, but lacks a strong synthesis of ideas from the cited paper. It does not offer detailed comparisons or critique of the approaches discussed, and the abstraction remains limited to surface-level observations without deeper generalization or identification of overarching principles."}}
{"id": "7ab2f30a-a1b0-4444-8247-d1882dfedd85", "title": "Shadow training", "level": "subsubsection", "subsections": [], "parent_id": "6609bda9-9718-4632-8ef3-bffbafd80a98", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Centralized Learning"], ["subsubsection", "Shadow training"]], "content": "A common design pattern for a lot of supervised learning attacks is the use of \\textbf{shadow models} and \\textbf{meta-models} or \\textbf{attack-models}~. The general shadow training architecture is depicted in Figure~\\ref{fig:shadow}. The main intuition behind this design is that models behave differently when they see data that do not belong to the training dataset. This difference is captured in the model outputs as well as in their internal representations. In most designs there is a target model and a target dataset. The adversary is trying to infer either membership or properties of the training data. They train a number of shadow models using shadow datasets $\\mathcal{D}_{shadow} = \\{\\textbf{x}_{shadow,i}, \\textbf{y}_{shadow,i}\\}_{i=1}^{n}$ that usually are assumed to come from the same distribution as the target dataset. After the shadow models' training, the adversary constructs an attack dataset  $\\mathcal{D}_{attack}=\\{f_i(\\textbf{x}_{shadow, i}), \\textbf{y}_{shadow, i}\\}_{i=1}^{n}$, where $f_i$ is the respective shadow model. The attack dataset is used to train the meta-model, which essentially performs inference based on the outputs of the shadow models. Once the meta-model is trained, it is used for testing using the outputs of the target model.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=12cm]{images/shadow_models.png}\n\\caption{Shadow training architecture. At first, a number of shadow models are trained with their respective shadow datasets in order to emulate the behavior of the target model. At the second stage, a meta-model is being trained from the outputs of the shadow models and the known labels of the shadow datasets. The meta-model is used to infer membership or properties of data or the model given the output of the target model.}\n\\label{fig:shadow}\n\\end{figure*}", "cites": [603, 8096, 8095], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of shadow training by integrating the ideas from the cited papers into a coherent framework. It abstracts the general intuition and methodology behind shadow models and meta-models, highlighting their role in membership inference attacks. While it provides some critical insight into the assumptions made (e.g., shadow datasets coming from the same distribution), it could offer deeper evaluation of limitations or variations in the approach across papers."}}
{"id": "524023f1-61f7-42eb-8811-7e57b6074cfe", "title": "Membership inference attacks", "level": "subsubsection", "subsections": [], "parent_id": "6609bda9-9718-4632-8ef3-bffbafd80a98", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Centralized Learning"], ["subsubsection", "Membership inference attacks"]], "content": "In \\textit{membership inference} black-box attacks, the most common attack pattern is the use of shadow models. The output of the shadow models is usually a prediction vector~.  The labels used for the attack dataset come from the test and training splits of the shadow data, where the data points that belong to the test set are labeled as non-members of the training set. The meta-model is trained to recognize patterns in the prediction vector output of the target model. These patterns allow the meta-model to infer whether a data point belongs to the training dataset or not. The number of shadow models affects the attack accuracy, but it also incurs cost to the attackers. Salem et al.~~ showed that membership inference attacks are possible with as little as one shadow model. \nShadow training can be further reduced to a threshold-based attack, where instead of training a meta-model, one can calculate a suitable threshold function that indicates whether a sample is a member of the training set. The threshold can be learned from multiple shadow models~ or even without using any shadow models~. Sablayrolles et al.~ showed that a Bayes optimal membership inference attack depends only on the loss and their attack outperforms previous attacks such as~. In terms of attack accuracy, they reported up to 90.8\\% on large neural network models such as VGG16~ that were performing classification on the Imagenet~ dataset.\nIn addition to relaxations on the number of shadow models, attacks have been shown to be data driven, i.e., an attack can be successful even if the target model is different than the shadow and meta-models~. The authors tested several types of models such as k-NN, logistic regression, decision trees and naive Bayes classifiers in different combinations on the role of the target model, shadow and meta model. The results showed that i) using different types of models did not affect the attack accuracy and ii) in most cases, models such as decision trees outperformed neural networks in terms of attack accuracy and precision.\nShadow model training requires a shadow dataset. One of the main assumptions of membership inference attacks on supervised learning models is that the adversary has no or limited knowledge of the training samples used. However, the adversary knows something about the underlying data distribution of the training data. If the adversary does not have access to a suitable dataset, they can try to generate one~. Access to statistics about the probability distribution of several features allows an attacker to create the shadow dataset using sampling techniques. If a statistics-based generation is not possible, a query-based approach using the target models' prediction vectors is another possibility. Generating auxiliary data using GANs was also proposed by Hayes et al.~. If the adversary manages to find input data that generate predictions with high confidence, then no prior knowledge of the data distribution is required for a successful attack~. Salem et al.~ went so far as to show that it is not even necessary to train the shadow models using data from the same distribution as the target, making the attack more realistic since it does not assume any knowledge of the training data.    \nThe previous discussion is mostly relevant to supervised classification or regression tasks. The efficacy of membership inference attacks against sequence-to-sequence models training for machine translation, was studied by~. The authors used shadow models that try to mimic the target model's behavior and then used a meta-model to infer membership. They found that sequence generation models are much harder to attack compared to other types of models such as image classification. However, membership of \\textit{out-of-domain} and \\text{out-of-vocabulary} data was easier to infer. \nMembership inference attacks are also applicable to deep generative models such as GANs and VAEs~. Since these models have more than one component (generator/discriminator, encoder/decoder), adversarial knowledge needs to take that into account. For these types of models, the taxonomy proposed by Chen et al.~ is partially followed. We consider black-box access to the generator as the ability to access generated samples and partial black-box access, the ability to provide inputs $z$ and generate samples. Having access to the generator model and its parameters is considered a white-box attack. The ability to query the discriminator is also  a white-box attack. \nThe full white-box attacks with access to the GAN discriminator are based on the assumption that if the GAN has \"overfitted\", then the data points used for its training will receive higher confidence values as output by the discriminator~. In addition to the previous attack, Hayes et al.~ proposed a set of attacks in the partial black-box setting. These attacks are applicable to both GANs and VAEs or any generative model. If the adversary has no auxiliary data, they can attempt to train an auxiliary GAN  whose discriminator distinguishes between the data generated by the target generator and the data generated by the auxiliary GAN. Once the auxiliary GAN is trained, its discriminator can be used for the white-box attack. The authors considered also scenarios where the adversary may have auxiliary information such as knowledge of training and test data. Using the auxiliary data, they can train another GAN whose discriminator would be able to distinguish between members of the original training set and non-members.  \nA distance-based attack over the nearest neighbors of a data point was proposed by Chen et al.~ for the full black-box model. In this case, a data point $\\mathbf{x}$ is a member of the training set if within its k-nearest neighbors there is at least one point that has a distance lower than a threshold $\\epsilon$. The authors proposed more complex attacks as the level of knowledge of the adversary increases, based on the idea that the reconstruction error between the real data point $x$ and a sample generated by the generator given some input $z$ should be smaller if the data point is coming from the training set.", "cites": [603, 8096, 3482, 6060, 8095, 6059], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers, integrating key concepts such as shadow models, threshold-based attacks, and generative model-specific techniques into a coherent explanation of membership inference attacks. It also provides critical insights by comparing the effectiveness of different model types in performing attacks and discussing the realism of assumptions (e.g., no need for same-distribution shadow data). The abstraction is strong, as it generalizes beyond specific papers to highlight broader patterns like the relationship between overfitting and privacy risk, as well as how attack strategies evolve with increasing adversarial knowledge."}}
{"id": "c20e108b-c9c0-4000-9f07-193d0a32320d", "title": "Reconstruction attacks", "level": "subsubsection", "subsections": [], "parent_id": "6609bda9-9718-4632-8ef3-bffbafd80a98", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Centralized Learning"], ["subsubsection", "Reconstruction attacks"]], "content": "The initial reconstruction attacks were based on the assumption that the adversary has access to the model $f$, the priors of the sensitive and nonsensitive features, and the output of the model for a specific input $x$. The attack was based on estimating the values of sensitive features, given the values of nonsensitive features and the output label~. This method used a maximum a posteriori (MAP) estimate of the attribute that maximizes the probability of observing the known parameters.\nHidano et al.~ used a similar attack but they made no assumption about the knowledge of the nonsensitive attributes. In order for their attack to work, they assumed that the adversary can perform a \\textit{model poisoning} attack during training.\nBoth previous attacks worked against linear regression models, but as the number of features and their range increases, the attack feasibility decreases. To overcome the limitations of the MAP attack, Fredrikson et al.~ proposed another inversion attack which recovers features using target labels and optional auxiliary information. The attack was formulated as an optimization problem where the objective function is based on the observed model output and uses gradient descent in the input space to recover the input data point. The method was tested on image reconstruction. The result was a class representative image which in some cases was quite blurry even after denoising. A formalization of the model inversion attacks in  was later proposed by Wu et al.~. \nSince the optimization problem in~ is quite hard to solve, Zhang et al.~ proposed to use a GAN to learn some auxiliary information of the training data and produce better results. The auxiliary information in this case is the presence of blurring or masks in the input images. The attack first uses the GAN to learn to generate realistic looking images from masked or blurry images using public data. The second step is a GAN inversion that calculates the latent vector $\\hat{z}$ which generates the most likely image: \n\\begin{equation}\n  \\hat{z}=\\arg\\min_z L_{prior}(z) + \\lambda L_{id}(z)  \n\\end{equation}\nwhere the prior loss $L_{prior}$ is ensuring the generation of realistic images and $L_{id}$ ensures that the images have a high likelihood in the target network. The attack is quite successful, especially on masked images.\nThe only black-box reconstruction attack until now was proposed by Yang et al.~. This attack employs an additional classifier that performs an inversion from the output of the target model $f(x)$ to a candidate output $\\hat{x}$. The setup is similar to that of an autoencoder, only in this case the target network that plays the role of the encoder is a black box and it is not trainable. The attack was tested on different types of target model outputs: the full prediction vector, a truncated vector, and the target label only. When the full prediction vector is available, the attack performs a good reconstruction, but with less available information, the produced data point looks more like a class representative.", "cites": [6061], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on reconstruction attacks, tracing the evolution from early MAP-based methods to more sophisticated GAN-driven approaches. It critically evaluates the limitations of prior methods (e.g., feasibility issues with increasing features) and highlights the trade-offs in reconstruction quality based on the amount of output information available. While it does not develop a completely novel framework, it offers a coherent narrative and some abstraction by identifying the use of optimization and auxiliary information as key principles in model inversion."}}
{"id": "e1af7043-30cd-47cc-8de3-c927774de72f", "title": "Property inference attacks", "level": "subsubsection", "subsections": [], "parent_id": "6609bda9-9718-4632-8ef3-bffbafd80a98", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Centralized Learning"], ["subsubsection", "Property inference attacks"]], "content": "In \\textit{property inference} the shadow datasets are labeled based on the properties that the adversary wants to infer, so the adversary needs access to data that have the property and data that do not have it. The meta-model is then trained to infer differences in the output vectors of the data that have the property versus the ones that they do not have it. In white-box attacks, the meta-model input can be other feature representations such as the support vectors of an SVM~ or transformations of neural network layer outputs~. When attacking language model embeddings, the embedding vectors themselves can be used to train a classifier to distinguish between properties such as text authorship~.", "cites": [8974], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of property inference attacks and mentions a single paper on information leakage in embedding models, but it does not synthesize insights from multiple sources, critically evaluate the approaches, or generalize to broader patterns or principles. It lacks depth and integration, functioning more as a summary than a reflective analysis."}}
{"id": "09d5c56e-6ebd-4684-b70a-dc55650330c2", "title": "Model extraction attacks", "level": "subsubsection", "subsections": [], "parent_id": "6609bda9-9718-4632-8ef3-bffbafd80a98", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Centralized Learning"], ["subsubsection", "Model extraction attacks"]], "content": "When the adversary has access to the inputs and prediction outputs of a model, it is possible to view these pairs of inputs and outputs as a system of equations, where the unknowns are the model parameters~ or hyper-parameters of the objective function~. In the case of a linear binary classifier, the system of equations is linear and only $d + 1$ queries are necessary to retrieve the model parameters, where $d$ is the dimension of the parameter vector $\\theta$. In more complex cases, such as multi-class linear regression or multi-layer perceptrons, the systems of equations are no longer linear. Optimization techniques such as Broyden–Fletcher–Goldfarb–Shanno (BFGS)~ or stochastic gradient descent are then used to approximate the model parameters~. \nLack of prediction vectors or a high number of model parameters renders equation solving attacks inefficient. A strategy is required to select the inputs that will provide the most useful information for model extraction. From this perspective, model extraction is quite similar to \\textit{active learning}~. Active learning makes use of an external \noracle that provides labels to input queries. The oracle can be a human expert or a system. The labels are then used to train or update the model. In the case of model extraction, the target model plays the role of the oracle.\nFollowing the active learning approach, several papers propose an adaptive training strategy. They start with some initial data points or \\textit{seeds} which they use to query the target model and retrieve labels or prediction vectors which they use to train the substitute model $\\hat{f}$. For a number of subsequent rounds, they extend their dataset with new synthetic data points based on some adaptive strategy that allows them to find points close to the decision boundary of the target model~. Chandrasekaran et al.~ provided a more query efficient method of extracting nonlinear models such as kernel SVMs, with slightly lower accuracy than the method proposed by Tramer et al.~, while the opposite was true for Decision Tree models.\nSeveral other strategies for selecting the most suitable data for querying the target model use: (i) data that are not synthetic but belong to different domains such as images from different datasets~, (ii) semi-supervised learning techniques such as rotation loss~ or MixMatch~ to augment the dataset~ or (iii) randomly generated input data~. In terms of efficiency, semi-supervised methods such as MixMatch require much fewer queries than fully supervised extraction methods to perform similarly or better in terms of task accuracy and fidelity, against models trained for classification using CIFAR-10 and SVHN datasets~. For larger models, trained for Imagenet classification, even querying a 10\\% of the Imagenet data, gives a comparable performance to the target model~. Against a deployed MLaaS service that provides facial characteristics, Orekondy et al.~ managed to create a substitute model that performs at 80\\% of the target in task accuracy, spending as little as \\$30.\nSome, mostly theoretical, work has demonstrated the ability to perform direct model extraction beyond linear models~. Full model extraction was shown to be theoretically possible against two-layer fully connected neural networks with rectified linear unit (ReLU) activations by Milli et al.~. However, their assumption was that the attacker has access to the loss gradients with respect to the inputs. Jagielski et al.~ managed to do a full extraction of a similar network without the need of gradients. Both approaches take into account that ReLUs transforms the neural network into a piecewise linear function of the inputs. By probing the model with different inputs, it is possible to identify where the linearity breaks and use this knowledge to calculate the network parameters. In a hybrid approach that uses both a learning strategy and direct extraction, Jagielski et al.~, showed that they can extract a model trained on MNIST with almost 100\\% fidelity by using an average of $2^{19.2}$ to $2^{22.2}$ queries against models that contain up to 400,000 parameters. However, this attack assumes access to the loss gradients similarly to~.\nFinally, apart from learning substitute models directly, there is also the possibility of extracting model information such as architecture, optimization methods and hyper-parameters using shadow models~. The majority of attacks were performed against neural networks trained on MNIST. Using the shadow models' prediction vectors as input, the meta-models managed to learn to distinguish whether a model has certain architectural properties. An additional attack by the same authors, proposed to generate adversarial samples which were created by models that have the property in question. The generated samples were created in a way that makes a classifier output a certain prediction if they have the attribute in question. The target model's prediction on this adversarial sample is then used to establish if the target model has a specific property. The combination of the two attacks proved to be the most effective approach. Some properties such as activation function, presence of dropout, and max-pooling were the most successfully predicted.", "cites": [2676, 7778, 8097, 6068, 6064, 6063, 7193, 6058, 2677, 7607], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers, connecting model extraction techniques with active learning and semi-supervised approaches, and identifies trends in query efficiency and fidelity. It also offers some critical analysis, such as noting assumptions and limitations (e.g., access to gradients). The abstraction is strong, generalizing to broader principles like the relationship between query strategy and attack success, and the feasibility of hybrid methods."}}
{"id": "2558e07f-5c8f-43b6-93b5-24f5ed0652b4", "title": "Membership inference attacks", "level": "subsubsection", "subsections": [], "parent_id": "c6118b29-b8f2-444e-994e-7af3d1792ae8", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Distributed Learning"], ["subsubsection", "Membership inference attacks"]], "content": "Nasr et al.~ showed that a membership inference attack is more effective than the black-box one, under the assumption that the adversary has some auxiliary knowledge about the training data, i.e., has access to some data from the training dataset, either explicitly or because they are part of a larger set of data that the adversary possesses. The adversary can use the model parameters and the loss gradients as inputs to another model which is trained to distinguish between members and non-members. The white-box attack accuracy with various neural network architectures was up to 75.1\\%, however, all target models had a high generalization error.\nIn the active attack scenario, the attacker, which is also a local participant, alters the gradient updates to perform a gradient ascent instead of descent for the data whose membership is under question. If some other participant uses the data for training, then their local SGD will significantly reduce the gradient of the loss and the change will be reflected in the updated model, allowing the adversary to extract membership information. Attacks from a local active participant reached an attack accuracy of 76.3\\% and in general, the active attack accuracy was higher than the passive accuracy in all tested scenarios. However, as the number of participants increases, it has adverse effects on the attack accuracy, which drops significantly after five or more participants. A global active attacker which is in a more favourable position, can isolate the model parameter updates they receive from each participant. Such an active attacker reached an attack accuracy of 92.1\\%.", "cites": [6069], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the core ideas from the cited paper on membership inference attacks, integrating them into the broader context of attacks against distributed learning. It also introduces variations such as passive and active attacks, and discusses how accuracy is affected by the number of participants and attacker position. However, the critical analysis is limited to surface-level observations (e.g., accuracy numbers), and the abstraction is moderate, identifying trends in attack effectiveness without deeper generalization or theoretical framing."}}
{"id": "bde78801-65b8-4592-b287-0e2e70c8b351", "title": "Property inference attacks", "level": "subsubsection", "subsections": [], "parent_id": "c6118b29-b8f2-444e-994e-7af3d1792ae8", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Distributed Learning"], ["subsubsection", "Property inference attacks"]], "content": "Passive property inference requires access to some data that possess the property and some that do not. The attack applies to both federated average and synchronized SGD settings, where each remote participant receives parameter updates from the parameter server after each training round~. The initial dataset is of the form $\\mathcal{D'}=\\{(\\mathbf{x}, \\mathbf{y}, \\mathbf{y'})\\}$, where $\\mathbf{x}$ and $\\mathbf{y}$  are the data used for training the distributed model and $\\mathbf{y}'$ are the property labels. Every time the local model is updated, the adversary calculates the loss gradients for two batches of data. One batch that has the property in question and one that does not. This allows the construction of a new dataset that consists of gradients and property labels $(\\nabla L, \\mathbf{y}')$. Once enough labeled data have been gathered, a second model, $f'$, is trained to distinguish between loss gradients of data that have the property versus those that do not. This model is then used to infer whether subsequent model updates were made using data that have the property. The model updates are assumed to be done in batches of data. The attack reaches an attack area under the curve (AUC) score of 98\\% and becomes increasingly more successful as the number of epochs increases. Attack accuracy also increases as the fraction of data with the property in question also increases. However, as the number of participants in the distributed model increases, the attack performance decreases significantly.", "cites": [614], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of property inference attacks in distributed learning and integrates concepts from the cited paper effectively. It synthesizes the methodological approach and results, such as the AUC score and factors affecting attack performance, but does not extend to a broader synthesis of multiple sources. There is some critical analysis regarding the impact of increasing participants on attack performance, but deeper limitations or comparisons with other attack types are not explored. The section abstracts the general mechanism of property inference but stops short of offering meta-level insights or overarching principles."}}
{"id": "222e2396-aa68-4486-afc2-6ecf44007c71", "title": "Reconstruction attacks", "level": "subsubsection", "subsections": [], "parent_id": "c6118b29-b8f2-444e-994e-7af3d1792ae8", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Attacks Against Distributed Learning"], ["subsubsection", "Reconstruction attacks"]], "content": "Some data reconstruction attacks in a federated learning setting use generative models and specifically GANs~. When the adversary is one of the participants, they can force the victims to release more information about the class they are interested in reconstructing~. This attack works as follows: The potential victim has data for a class \"A\" that the adversary wants to reconstruct. The adversary trains an additional GAN model. After each training round, the adversary uses the target model parameters for the GAN discriminator, whose purpose is to decide whether the input data come from the class \"A\" or are generated by the generator. The aim of the GAN is to create a generator that is able to generate faithful class \"A\" samples. In the next training step of the target model, the adversary generates some data using the GAN and labels them as class \"B\". This forces the target model to learn to discriminate between classes \"A\" and \"B\" which in turn improves the GAN training and its ability to generate class \"A\" representatives.\nIf the adversary has access to the central parameter server, they have direct access to the model updates of each remote participant. This makes it possible to perform more successful reconstruction attacks~. In this case, the GAN discriminator is again using the shared model parameters and learns to distinguish between real and generated data, as well as the identity of the participant. Once the generator is trained, the reconstructed samples are created using an optimization method that minimizes the distance between the real model updates and the updates due to the generated data. Both GAN based methods assume access to some auxiliary data that belong to the victims. However, the former method generates only class representatives.   \nIn a synchronized SGD setting, an adversary with access to the parameter server has access to the loss gradients of each participant during training. Using the loss gradients is enough to produce a high quality reconstruction of the training data samples, especially when the batch size is small~. The attack uses a second \"dummy\" model. Starting with random dummy inputs $x'$ and labels $y'$, the adversary tries to match the dummy model's loss gradients $\\nabla_{\\theta} \\mathcal{J'}$ to the participant's loss gradients $\\nabla_{\\theta} \\mathcal{J}$. This gradient matching is formulated as an optimization task that seeks to find the optimal $x'$ and $y'$ that minimize the gradients' distance:\n\\begin{equation}\n    \\label{eq:zhu}\n    x^*, y^* = \\arg\\min_{x',y'} \\| \\nabla_{\\theta} \\mathcal{J'}(\\mathcal{D'};\\theta) - \\nabla_{\\theta} \\mathcal{J}(\\mathcal{D};\\theta) \\|^2\n\\end{equation}\nThe minimization problem in Equation~\\ref{eq:zhu} is solved using limited memory BFGS (L-BFGS)~. \nThe size of the training batch is an important factor in the speed of convergence in this attack.\nData reconstruction attacks are also possible during the inference phase in the split learning scenario~. When the local nodes process new data, they perform inference on these initial layers and then send their outputs to the centralized server. In this attack, the adversary is placed in the centralized server and their goal is to try to reconstruct the data used for inference. He et al.~ cover a range of scenarios: (i) white-box, where the adversary has access to the initial layers and uses them to reconstruct the images, (ii) black-box where the adversary has no knowledge of the initial layers but can query them and thus recreate the missing layers and (iii) query-free where the adversary cannot query the remote participant and tries to create a substitute model that allows data reconstruction. The latter attack produces the worst results, as expected, since the adversary is the weakest. The split of the layers between the edge device and the centralized server is also affecting the quality of reconstruction. Fewer layers in the edge neural network allow for better reconstruction in the centralized server.", "cites": [3477, 3469, 604], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key mechanisms from multiple papers, integrating GAN-based attacks, gradient inversion, and split learning scenarios into a cohesive explanation of reconstruction attacks in distributed learning. It provides critical analysis by highlighting the importance of adversary strength, auxiliary data, and layer splits on reconstruction quality. The section abstracts these methods into a broader framework, emphasizing how access level and training dynamics influence privacy risks, which goes beyond individual paper descriptions to offer meaningful insights."}}
{"id": "55afc8ad-a48d-4a1f-9ec0-397a78e882aa", "title": "Summary of Attacks", "level": "subsection", "subsections": [], "parent_id": "d9a9b0d6-2920-445b-8cf1-e666bd2c8d85", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Implementation of the Attacks"], ["subsection", "Summary of Attacks"]], "content": "To summarize the attacks proposed against machine learning privacy, Table~\\ref{table:attack_summary} presents the 42 papers analyzed in terms of adversarial knowledge, model under attack, attack type, and timing of the attack.\n\\begin{table*}[t]\n\\centering\n\\scriptsize\t\n\\renewcommand{\\arraystretch}{1.1}\n\\caption{Summary of papers on privacy attacks on machine learning systems, including information of their assumptions about adversarial knowledge (black / white-box), the type of model(s) under attack, the attack type, and the timing of the attack (during training or during inference). The transparent circle in the Knowledge column indicates partial white-box attacks.}\n\\begin{tabular}{@{} l c | c c | c c c c c c c | c c  c c | c c @{} }\n\\toprule\n Reference & Year & \\multicolumn{2}{c}{Knowledge} & \\multicolumn{7}{c}{ML Algorithms} & \\multicolumn{4}{c}{Attack Type} & \\multicolumn{2}{c}{Timing}\\\\ \n \\midrule\n & & \\rotatebox[origin=c]{90}{Black-box} & \\rotatebox[origin=c]{90}{White-box} &\\rotatebox[origin=c]{90}{Linear regression} &\\rotatebox[origin=c]{90}{Logistic regression} &\\rotatebox[origin=c]{90}{Decision Trees} & \\rotatebox[origin=c]{90}{SVM} & \\rotatebox[origin=c]{90}{HMM} & \\rotatebox[origin=c]{90}{Neural network} & \\rotatebox[origin=c]{90}{GAN / VAE} & \\rotatebox[origin=c]{90}{Membership Inference} & \\rotatebox[origin=c]{90}{Reconstruction} & \\rotatebox[origin=c]{90}{Property Inference} & \\rotatebox[origin=c]{90}{Model Extraction} & \\rotatebox[origin=c]{90}{Training} & \\rotatebox[origin=c]{90}{Inference}  \\\\\n \\midrule\nFredrikson et al.~ & 2014  &   & $\\bullet$ & $\\bullet$ & & & & & & & $\\bullet$ & & & & $\\bullet$\\\\  \nFredrikson et al.~ & 2015 & $\\bullet$ & $\\bullet$  & & & $\\bullet$ & & & $\\bullet$& & & $\\bullet$ & & & & $\\bullet$\\\\\nAteniese et al.~ & 2015 &  & $\\bullet$   & & & & $\\bullet$ & $\\bullet$& & & & & $\\bullet$ & & & $\\bullet$ \\\\\nTramer et al.~& 2016  & $\\bullet$  & $\\bullet$  & & $\\bullet$ & $\\bullet$ & $\\bullet$ & &$\\bullet$ &  & & & & $\\bullet$ & & $\\bullet$\\\\\nWu et al.~ & 2016 & $\\bullet$ & $\\bullet$ &  & $\\bullet$ & & & & $\\bullet$ & &  & $\\bullet$ & & & & $\\bullet$\\\\\nHidano et al.~ & 2017 & & $\\bullet$ & $\\bullet$& & & & & & &  & $\\bullet$ & & & & $\\bullet$\\\\\nHitaj et al.~& 2017 &  & $\\bullet$  & & & &  & & $\\bullet$ & & & $\\bullet$ & & & $\\bullet$ & \\\\\nPapernot et al.~ & 2017 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nShokri et al.~  & 2017 & $\\bullet$  & & & & & & &$\\bullet$ & & $\\bullet$ & & &  & & $\\bullet$\\\\\nCorreia-Silva et al.~ & 2018 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nGanju et al.~ & 2018 &  & $\\bullet$ & & & & & & $\\bullet$& & & & $\\bullet$ & & & $\\bullet$ \\\\\nOh et al.~ & 2018 & $\\bullet$   & & & & & & & $\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nLong et al.~ & 2018 & $\\bullet$  & & & & & & & $\\bullet$ & & $\\bullet$ & & & & & $\\bullet$\\\\\nRahman et al.~ & 2018 &  & $\\bullet$ & & & & & & $\\bullet$ & & $\\bullet$ & & & & & $\\bullet$\\\\\nWang \\& Gong~ & 2018 &  & $\\bullet$   & $\\bullet$ & $\\bullet$ & & $\\bullet$ & & $\\bullet$ & & & & &  $\\bullet$ & & $\\bullet$\\\\\nYeom et al.~ & 2018 & $\\bullet$  & $\\circ$  & $\\bullet$ & & $\\bullet$ & & &$\\bullet$ & & & $\\bullet$ & &  & & $\\bullet$ \\\\\nCarlini et al.~ & 2019 & $\\bullet$ & & & & & & & $\\bullet$ &  &  & $\\bullet$ & & & & $\\bullet$\\\\\nHayes et al.~ & 2019 & $\\bullet$  & $\\bullet$  & & & & & & & $\\bullet$ & $\\bullet$ & & &  & & $\\bullet$\\\\\nHe et al.~ & 2019 & $\\bullet$   & $\\bullet$  & & & & & & $\\bullet$ &  & & $\\bullet$ & & & & $\\bullet$ \\\\\nHilprecht et al.~ & 2019 & $\\bullet$  & & & & & & & & $\\bullet$ & $\\bullet$ & & & & & $\\bullet$ \\\\\nJayaraman \\& Evans~ & 2019 &  $\\bullet$ & $\\bullet$  & & & & & & $\\bullet$ & & $\\bullet$ & $\\bullet$ & & & & $\\bullet$\\\\\nJuuti et al.~ & 2019 & $\\bullet$&   & & & & & & $\\bullet$ & &  & & &  $\\bullet$&  & $\\bullet$\\\\\nMilli et al.~  & 2019 & $\\bullet$& & & & & & & $\\bullet$ & & & & & $\\bullet$ &  & $\\bullet$ \\\\\nNasr et al.~ & 2019 &  & $\\bullet$ & & & & & & $\\bullet$& & $\\bullet$ & & &  & $\\bullet$ & \\\\\nMelis et al.~ & 2019 &  & $\\bullet$ & & & & & & $\\bullet$ & & $\\bullet$ & & $\\bullet$ & & $\\bullet$ &\\\\\nOrekondy et al.~ & 2019 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nSablayrolles et al.~ & 2019 & & $\\circ$ & & & & & & $\\bullet$ & & $\\bullet$ & & &  & &$\\bullet$ \\\\\nSalem et al.~ & 2019 & $\\bullet$   & & & & & & & $\\bullet$ &  & $\\bullet$ & & &  & & $\\bullet$\\\\\nSong L. et al.~ & 2019 & $\\bullet$ &  & & & & & & $\\bullet$&  & $\\bullet$ & & & & & $\\bullet$ \\\\\nTruex, et al.~ & 2019 & $\\bullet$ & & & $\\bullet$ & $\\bullet$ & & & $\\bullet$ & & $\\bullet$ & & & & & $\\bullet$\\\\\n Wang et al.~ & 2019 & & $\\bullet$ & & & & &  & $\\bullet$ &  & & $\\bullet$ & & & $\\bullet$ & \\\\\nYang et al.~ & 2019 & $\\bullet$ & & & & & & & $\\bullet$ &  & & $\\bullet$ & & & & $\\bullet$ \\\\\n Zhu et al.~ & 2019 &  & $\\bullet$   & & & & & & $\\bullet$ &  & & $\\bullet$ & & & $\\bullet$ & \\\\\nBarbalau et al.~ & 2020 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$ \\\\\n Chandrasekaran et al.~ & 2020 & $\\bullet$ & &  &  &$\\bullet$  & $\\bullet$ & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nChen et al.~ & 2020 & $\\bullet$ & $\\bullet$ & & & & & & & $\\bullet$ & $\\bullet$ & & & & & $\\bullet$ \\\\\nHishamoto et al.~ & 2020 & $\\bullet$ & & & & & & &$\\bullet$ & & $\\bullet$ & & & & & $\\bullet$\\\\\nJagielski et al.~ & 2020 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nKrishna et al.~ & 2020 & $\\bullet$ & & & & & & &$\\bullet$ & & & & & $\\bullet$ & & $\\bullet$\\\\\nPan et al.~ & 2020 & & $\\bullet$ & & & & & &$\\bullet$ & & & $\\bullet$ & &  & & $\\bullet$\\\\\nSong \\& Raghunathan~ & 2020 & $\\bullet$ & $\\bullet$ &  & & & & &$\\bullet$ & & $\\bullet$ & $\\bullet$ & $\\bullet$ & & & $\\bullet$\\\\\nZhang et al.~ & 2020 & & $\\bullet$ & & & & & &$\\bullet$ & & & $\\bullet$ & &  & & $\\bullet$\\\\\n\\bottomrule\n\\end{tabular}\n\\label{table:attack_summary}\n\\end{table*}\nIn terms of model types, 83.3\\% of the papers dealt with attacks against neural networks, with decision trees being the second most popular model to attack at 11.9\\% (some papers covered attacks against multiple model types). The concept of neural networks groups together both shallow and deep models, as well as multiple architectures, such as convolutional neural networks, recurrent neural networks, while under SVMs we group together both linear and nonlinear versions. \nThe most popular attack types are membership inference and reconstruction attacks (35.7\\% of the papers, respectively), with model extraction the next most popular (31\\%). The majority of the proposed attacks are performed during the inference phase (88\\%). Attacks during training are mainly on distributed forms of learning. Black-box and white-box attacks were studied in 66.7\\% and 54.8\\% of the papers, respectively (some papers covered both settings). In the white-box category, we also include partial white-box attacks.\nThe focus on neural networks in the existing literature as well as the focus on supervised learning is also apparent in Figure~\\ref{fig:map-attacks}. The figure depicts types of machine learning algorithms versus the types of attacks that have been studied so far based on the existing literature. The list of algorithms is indicative and not exhaustive, but it contains the most popular ones in terms of research and deployment in real-world systems. Algorithms such as random forests~ or gradient boosting trees~ have received little to no focus and the same holds for whole areas of machine learning such as reinforcement learning.  \n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\columnwidth]{images/map_of_attacks_on_ML_models.png}\n\\caption{Map of attack types per algorithm. The list of algorithm presented is not exhaustive but indicative. Underneath each algorithm or area of machine learning there is an indication of the attacks that have been studied so far. A red box indicates no attack.}\n\\label{fig:map-attacks}\n\\end{figure*}\nAnother dimension that is interesting to analyze is the types of learning tasks that have been the target of attacks so far. Figure~\\ref{fig:heatmap} presents information about the number of papers in relation to the learning task and the attack type. By learning task, we refer to the task in which the target model is initially trained. As the figure clearly shows, the majority of the attacks are on models that were trained for classification tasks, both binary and multiclass. This is the case across all four attack types. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=12cm]{images/heatmap3.png}\n\\caption{Number of papers used against each learning task and attack type. Classification includes both binary and multi-class classification. Darker gray means higher number of papers.}\n\\label{fig:heatmap}\n\\end{figure}\nWhile there is a diverse set of reviewed papers, it is possible to discern some high-level patterns in the proposed attacking techniques. Figure~\\ref{fig:heatmap2} shows the number of papers in relation to the attacking technique and attack type. Most notably, nine papers used shadow training mainly for membership and property inference attacks. Active learning was quite popular in model extraction attacks and was proposed by four papers. Generative models (mostly GANs) were used in five papers across all attack types and another three papers used gradient matching techniques. It should be noted here that the \"Learning\" technique includes a number of different approaches, spanning from using model parameters and gradients as inputs to classifiers~ to using input-output queries for substitute model creation~ and learning classifiers from language models for reconstruction~ and property inference~. In \"Threshold\" based attacks, we categorized the attacks proposed in~ and  and subsequent papers that used them for membership and property inference.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=12cm]{images/heatmap_technique.png}\n\\caption{Number of papers that used an attacking technique for each attack type. Darker gray means higher number of papers.}\n\\label{fig:heatmap2}\n\\end{figure}\nSome attacks may be applicable to multiple learning tasks and datasets, however, this is not the case universally. Dataset size, number of classes, and features might also be factors for the success of certain attacks, especially since most of them are empirical. Table ~\\ref{table:datasets_summary1} is a summary of the datasets used in all attack papers along with the data types of their features, the learning task they were used for, and the dataset size. The datasets were used during the training of the target models and in some cases as auxiliary information during the attacks. The table contains 51 unique datasets used across 42 papers, an indication of the variation of different approaches. \n\\begin{table*}[!t]\n\\scriptsize\n\\renewcommand{\\arraystretch}{1.0}\n\\caption{Summary of datasets used in the papers about privacy attacks on machine learning systems. The size of each dataset is measured by the number of samples unless otherwise indicated. A range in the size column indicates that different papers used different subsets of the dataset.}\n\\begin{tabular}{p{3cm} p{2cm} p{3.2cm} p{2.5cm} r}\n\\toprule\n\\textbf{Name} & \\textbf{Data Type} & \\textbf{Learning Task} & \\textbf{Reference(s)} & \\textbf{Size (Samples)}\\\\\n\\hline\n538 Steak Survey~ & mixed features & multi-class classification &  & 332 \\\\\nAT\\&T Faces~ & images & multi-class classification &  & 400 \\\\\nBank Marketing~ & mixed features & multi-class classification &  & 45,210 \\\\\nBitcoin prices & time series & regression &  & 1,076 \\\\\nBook Corpus~ & text & word-level language model &  & 14,000 sent. \\\\\nBreast Cancer~ & numerical feat. & binary classification &  & 699 \\\\\nCaltech 256~ & images & multi-class classification &  & 30,607 \\\\\nCaltech birds~ & images & multi-class classification &  & 6,033 \\\\\nCelebA~ & images & binary classification &  & 20-202,599 \\\\\nCIFAR-10~ & images & image generation, multi-class classification &  & 60,000 \\\\\nCIFAR-100~ & images & multi-class classification &  & 60,000 \\\\\nCLiPS stylometry~ & text & binary classification &  & 1,412 reviews \\\\\nChest X-ray~ &images & multi-class classification &  & 10,000\\\\\nDiabetes~ & time series & binary class., regression &  & 768 \\\\\nDiabetic ret.~ & images & image generation &  & 88,702 \\\\\nEnron emails & text & char-level language model &  & - \\\\  \nEyedata~ & numerical feat.& regression &  & 120 \\\\\nFaceScrub~ & images & binary classification &  & 18,809-48,579 \\\\ \nFashion-MNIST~ & images & multi-class classification &  & 60,000 \\\\\nFoursquare~ & mixed features & binary classification &  & 528,878 \\\\\nGeog. Orig. Music~ & numerical feat. & regression &  & 1,059 \\\\\nGerman Credit~ & mixed features & binary classification &  & 1,000 \\\\\nGSS marital survey~ & mixed features & multi-class classification &  & 16127 \\\\\nGTSRB~ & images & multi-class classification &  & 51839\\\\\nHW Perf. Counters (private) & numerical feat. & binary classification &  & 36,000 \\\\\nImagenet~ & images & multi-class classification &  & 14,000,000 \\\\\nInstagram~ & location data & vector generation &  & - \\\\\nIris~ & numerical feat. & multi-class classification &  & 150 \\\\\nIWPC~ & mixed features & regression &  & 3497 \\\\\nIWSLT Eng-Vietnamese~ & text & neural machine translation &  & - \\\\\nLFW~& images & image generation &  & 13233 \\\\ \nMadelon~ & mixed features & multi-class classification &  & 4,400 \\\\\nMIMIC-III~ & binary features & record generation &  & 41,307 \\\\\nMovielens 1M~ & numerical feat. & regression &  & 1,000,000 \\\\\nMNIST~ & images & multi-class classification &  & 70,000 \\\\\nMushrooms~ & categorical feat. & binary classification &  & 8,124 \\\\\nNetflix~ & binary features & binary classification &  & 2,416 \\\\\nNetflows (private) & network data & binary classification &  & - \\\\\nPTB~ & text & char-level language model &  & 5 MB \\\\\nPiPA~ & images & binary classification &  & 18,000 \\\\\nPurchase-100~ & binary features & multi-class classification &  & 197,324 \\\\\nSVHN~ & images & multi-class classification &  & 60,000 \\\\\nTED talks~ & text & machine translation &  & 100,000 pairs \\\\\nTexas-100~ & mixed features & multi-class classification &  & 67,330 \\\\ \nUJIndoor~ & mixed features & regression &  & 19,937 \\\\\nUCI / Adult~ & various & binary classification &  & 48,842 \\\\\nVoxforge~ & audio & speech recognition &  & 11,137 rec. \\\\\nWikipedia~ & text & language model &  & 150,000 articles \\\\\nWikitext-103~ & text & word-level language model &  & 500 MB \\\\ \nYale-Face~ & images & multi-class classification &  & 2,414 \\\\\nYelp reviews~ & text & binary classification &  & 16-40,000 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{table:datasets_summary1}\n\\end{table*}\nThis high variation is both a blessing and a curse. On the one hand, it is highly desirable to use multiple types of datasets to test different hypotheses and the majority of the reviewed research follows that approach. On the other hand, these many options make it harder to compare methods. As it is evident from Table~\\ref{table:datasets_summary1}, some of the datasets are quite popular. MNIST, CIFAR-10, CIFAR-100, and UCI Adult have been used by more than six papers, while 26 datasets have been used by only one paper.\nThe number of model parameters varies based on the model, task and datasets used in the experiments. As it can be seen in Table~\\ref{table:datasets_summary1}, most datasets are not extremely large, hence the models under attack are not extremely large. Given that most papers deal with neural networks, this might indicate that most attacks focused on smaller datasets and models which might not be representative of realistic scenarios. However, privacy attacks do not necessarily have to target large models with extreme amounts of data; and neural networks, however popular, are not necessarily the most used models in the \"real world\".", "cites": [6068, 8095, 3469, 2677, 8097, 6064, 6066, 614, 6059, 7607, 6071, 3482, 652, 461, 4883, 6070, 6061, 8096, 6060, 603, 604, 8974, 6058, 2676, 3477, 6069, 6065, 6063, 8094], "cite_extract_rate": 0.34523809523809523, "origin_cites_number": 84, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive summary of 42 papers through a well-structured table and additional textual commentary. It synthesizes basic attack characteristics like model type, attack category, and timing, which helps in organizing the literature. However, it lacks deeper critical analysis of the cited works and only marginally abstracts broader patterns, remaining primarily focused on factual representation."}}
{"id": "1540817d-bc2f-4e83-9f4f-5bda8ea81aa0", "title": "Differential Privacy", "level": "subsubsection", "subsections": [], "parent_id": "9355790c-dfe8-4d07-9d09-236edaae452f", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Membership Inference Attacks"], ["subsubsection", "Differential Privacy"]], "content": "Differential privacy started as a privacy definition for data analysis and it is based on the idea of \"learning nothing about an individual while learning useful information about a population\"~. Its definition is based on the notion that if two databases differ only by one record and are used by the same algorithm (or mechanism), the output of that algorithm should be similar. More formally,\n\\begin{definition}[($\\epsilon,\\delta$)-Differential Privacy]\nA randomized mechanism $\\mathcal{M}$ with domain $\\mathcal{R}$ and output $\\mathcal{S}$ is ($\\epsilon,\\delta$)-differentially private if for any adjacent inputs $D, D' \\in \\mathcal{R}$ and for any subsets of outputs $\\mathcal{S}$ it holds that:\n\\begin{equation}\n  Pr[\\mathcal{M}(D) \\in \\mathcal{S} ] \\leq e^{\\epsilon} Pr[\\mathcal{M}(D') \\in \\mathcal{S} ] +\\delta  \n\\end{equation}\nwhere $\\epsilon$ is the privacy budget and $\\delta$ is the failure probability. \n\\end{definition}\nThe original definition of DP did not include $\\delta$ which was introduced as a relaxation that allows some outputs not to be bounded by $e^{\\epsilon}$. \nThe usual application of DP is to add Laplacian or Gaussian noise to the output of a query or function over the database. The amount of noise is relevant to the \\textit{sensitivity} which gives an upper bound on how much we must perturb the output of the mechanism to preserve privacy~:\n\\begin{definition}\n$l_{1}$ (or $l_{2}$)-Sensitivity of a function $f$ is defined as  \n\\begin{equation}\n  \\Delta f = \\underset{D, D', \\|D-D'\\|=1}{\\max} \\|f(D) - f(D')\\|  \n\\end{equation}\n\\end{definition}\nwhere $\\|.\\|$ is the $l_1$ or the $l_2$-norm and the max is calculated over all possible inputs $D, D'$. \nFrom a machine learning perspective, $D$ and $D'$ are two datasets that differ by one training sample and the randomized mechanism $\\mathcal{M}$ is the machine learning training algorithm. In deep learning, the noise is added at the gradient calculation step. Because it is necessary to bound the gradient norm, gradient clipping is also applied~.\nDifferential privacy offers a trade-off between privacy protection and utility or model accuracy. Evaluation of differentially private machine learning models against membership inference attacks concluded that the models could offer privacy protection only when they considerably sacrifice their utility~. Jayaraman et al.~ evaluated several relaxations of DP in both logistic regression and neural network models against membership inference attacks. They showed that these relaxations have an impact on the utility-privacy trade-off. While they reduce the required added noise, they also increase the privacy leakage.\nDistributed learning scenarios require additional considerations regarding differential privacy. In a centralized model, the focus is on sample level DP, i.e., on protecting privacy at the individual data point level. In a federated learning setting where there are multiple participants, we not only care about the individual training data points they use, but also about ensuring privacy at the participant level. A proposal which applies DP at the participant level was introduced by McMahan et al.~ however, it requires a large number of participants. When it was tested with a number as low as 30, the method was deemed unsuccessful~.", "cites": [8096, 5419, 7608, 614], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers to present a coherent narrative on differential privacy in the context of machine learning privacy. It critically discusses the trade-offs between privacy and utility, and the effectiveness of DP in different learning scenarios, including identifying limitations (e.g., failure with small participant numbers). The abstraction is strong, moving from technical definitions to general principles such as the impact of DP relaxations and the need for participant-level privacy in federated learning."}}
{"id": "2094cb2b-8f83-41fd-bbb9-49919785e6c4", "title": "Regularization", "level": "subsubsection", "subsections": [], "parent_id": "9355790c-dfe8-4d07-9d09-236edaae452f", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Membership Inference Attacks"], ["subsubsection", "Regularization"]], "content": "Regularization techniques in machine learning aim to reduce overfitting and increase model generalization performance. Dropout~ is a form of regularization that randomly drops a predefined percentage of neural network units during training. Given that black-box membership inference attacks are connected to overfitting, it is a sensible approach to this type of attack and multiple papers have proposed it as a defense with varying levels of success~. Another form of regularization uses techniques that combine multiple models that are trained separately. One of those methods, model stacking, was tested in~ and produced positive results against membership inference. An advantage of model stacking or similar techniques is that they are model agnostic and do not require that the target model is a neural network.", "cites": [603, 6065, 614, 6059], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of regularization techniques as a defense against membership inference attacks but does not effectively synthesize or integrate the cited papers. It mentions that multiple papers have proposed these methods without elaborating on how they connect or differ. There is minimal critical evaluation or abstraction, making the section more of a factual summary than an insightful analysis."}}
{"id": "045d26c9-f871-41ba-a6f6-7ae03074caa4", "title": "Prediction vector tampering", "level": "subsubsection", "subsections": [], "parent_id": "9355790c-dfe8-4d07-9d09-236edaae452f", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Membership Inference Attacks"], ["subsubsection", "Prediction vector tampering"]], "content": "As many models assume access to the prediction vector during inference, one of the countermeasures proposed was the restriction of the output to the top k classes or predictions of a model~. However, this restriction, even in the strictest form (outputting only the class label) did not seem to fully mitigate membership inference attacks, since information leaks can still happen due to model misclassifications. Another option is to lower the precision of the prediction vector, which leads to less information leakage~. Adding noise to the output vector also affected membership inference attacks~.", "cites": [603, 6072], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the idea of tampering with prediction vectors as a defense against membership inference attacks, citing two relevant papers. While it touches on different techniques (e.g., restricting output, lowering precision, adding noise), it lacks deeper synthesis or comparison between these methods. The analysis remains surface-level without identifying broader patterns or offering a critical evaluation of their effectiveness."}}
{"id": "c02a249a-1a03-4943-a2ff-93a526dde506", "title": "Defenses Against Reconstruction Attacks", "level": "subsection", "subsections": [], "parent_id": "6fc43205-e8bc-4845-a22d-3a1f5a4462a8", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Reconstruction Attacks"]], "content": "Reconstruction attacks often require access to the loss gradients during training. Most of the defences against reconstruction attacks propose techniques that affect the information retrieved from these gradients. Setting all loss gradients which are below a certain threshold to zero, was proposed as a defence against reconstruction attacks in deep learning. This technique proved quite effective with as little as 20\\% of the gradients set to zero and with negligible effects on model performance~. On the other hand, performing quantization or using half-precision floating points for neural network weights did not seem to deter the attacks in~ and ~, respectively.", "cites": [6066, 604], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers to explain how defenses against reconstruction attacks manipulate gradient information. It provides a basic comparison between techniques like gradient thresholding and quantization, noting their effectiveness and limitations. While it begins to generalize by highlighting the relationship between gradient exposure and privacy risks, it lacks deeper abstraction or a comprehensive framework for understanding broader trends in defense mechanisms."}}
{"id": "e820744e-c453-452a-ba7c-03c0cbbfeeab", "title": "Defenses Against Property Inference Attacks", "level": "subsection", "subsections": [], "parent_id": "6fc43205-e8bc-4845-a22d-3a1f5a4462a8", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Property Inference Attacks"]], "content": "Differential privacy is designed to provide privacy guarantees in membership inference attack scenarios and it does not seem to offer protection against property inference attacks~. In addition to DP, Melis et al.~ explored other defenses against property inference attacks. Regularization (dropout) had an adverse effect and actually made the attacks stronger. Since the attacks in~ were performed in a collaborative setting, the authors tested the proposal in~, which is to share fewer gradients between training participants. Although sharing less information made the attacks less effective, it did not alleviate them completely.", "cites": [614], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides some analytical insight by discussing the effectiveness of different defenses, such as regularization and gradient sharing, but it lacks a more comprehensive synthesis of the cited papers. It identifies limitations, such as dropout increasing attack effectiveness, and evaluates the partial success of sharing fewer gradients, showing basic critical analysis. However, it does not generalize these findings into broader patterns or principles."}}
{"id": "4813f9d3-fbcc-4a44-987f-d29d06afe515", "title": "Protecting against DNN Model Stealing Attacks (PRADA)", "level": "subsubsection", "subsections": [], "parent_id": "91aa9441-4376-4f29-8b2f-9901f222b871", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Defending Machine Learning Privacy"], ["subsection", "Defenses Against Model Extraction Attacks"], ["subsubsection", "Protecting against DNN Model Stealing Attacks (PRADA)"]], "content": "Detecting model stealing attacks based on the model queries that are used by the adversary was proposed by Juuti et al.~. The detection is based on the assumption that model queries that try to explore decision boundaries will have a different distribution than the normal ones. While the detection was successful, the authors noted that it is possible to be evaded if the adversary adapts their strategy.", "cites": [7607], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 2.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited paper, merely paraphrasing its main idea without connecting it to other works or broader themes. It includes a slight critical note about the potential for evasion, but lacks deeper evaluation or comparison. There is no abstraction or generalization to broader principles or patterns in the field."}}
{"id": "a1c254ce-5af6-4a8b-a779-6cdff236917c", "title": "Discussion", "level": "section", "subsections": [], "parent_id": "f8de3762-f517-4130-b09c-c364b4e5ee85", "prefix_titles": [["title", "A Survey of Privacy Attacks in Machine Learning"], ["section", "Discussion"]], "content": "\\label{sec:discussion}\nAttacks on machine learning privacy have been increasingly brought to light. However, we are still at an exploratory stage. Many of the attacks are applicable only under specific sets of assumptions or do not scale to larger training data sets, number of classes,  number of participants, etc. The attacks will keep improving and to successfully defend against them, the community needs to answer fundamental questions about why they are possible in the first place. While progress has been made in the theoretical aspects of some of the attacks, there is still a long way to go to achieve a better theoretical understanding of privacy leaks in machine learning.\nAs much as we need answers about why leaks happen at a theoretical level, we also need to know how well privacy attacks work on real deployed systems. Adversarial attacks on realistic systems bring to light the issue of additional constraints that need to be in place for the attacks to work. When creating glasses that can fool a face recognition system, Sharif et al.~, they had to pose constraints that had to do with physical realizations, e.g., that the color of the glasses should be printable. In privacy-related attacks, the most realistic cases come from the model extraction area, where attacks against MLaaS systems have been demonstrated in multiple papers. For the majority of other attacks, it is certainly an open question of how well they would perform on deployed models and what kind of additional requirements need to be in place for them to succeed.\nAt the same time, the main research focus up to now has been supervised learning. Even within supervised learning, there are areas and learning tasks that have been largely unexplored, and there are few attacks reported on popular algorithms such as random forests or gradient boosting trees despite their wide application. In unsupervised and semi-supervised learning, the focus is mainly on generative models and only just recently, papers started exploring areas such as representation learning and language models. Some attacks on image classifiers do not transfer that well to natural language processing tasks~ while others do, but may require different sets of assumptions and design considerations~.     \nBeyond expanding the focus on different learning tasks, there is the question of datasets. The impact of datasets on the attack success has been demonstrated by several papers. Yet, currently, we lack a common approach as to which datasets are best suited to evaluate privacy attacks, or constitute the minimum requirement for a successful attack. Several questions are worth considering: do we need standardized datasets and if yes, how do we go about and create them? Are all data worth protecting and if some are more interesting than others, should we not be testing attacks beyond popular image datasets?\nFinally, as we strive to understand the privacy implications of machine learning, we also realize that several research areas are connected and affect each other. We know, for instance, that adversarial training affects membership inference~ and that model censoring can still leak private attributes~. Property inference attacks can deduce properties of the training dataset that were not specifically encoded or were not necessarily correlated to the learning task. This can be understood as a form of bias detection, which means that relevant literature in the area of model fairness should be reviewed as potentially complementary. Furthermore, while deep learning models are considered black-boxes in terms of explainability, work that sheds light on what kind of data make neurons activate~ can be relevant to discovering information about the training dataset and can therefore lead to privacy leaks. All these are examples of potential inter-dependencies between different areas of machine learning research, therefore, a better understanding of privacy attacks calls for an interdisciplinary approach.", "cites": [6073, 5934, 6074], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by connecting multiple cited papers to broader themes like theoretical understanding, practical deployment constraints, and interdisciplinary links. It offers critical analysis by pointing out the limitations of current attacks and the need for standardized evaluation. The abstraction is high as it identifies overarching research challenges and general principles in privacy attacks and their relationships to other ML areas."}}
