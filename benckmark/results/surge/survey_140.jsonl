{"id": "aba2383f-bffd-45ed-b1dc-31ffa78e89b6", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "a6c9d773-425e-48b4-9e0e-9613ad655a8e", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\label{sec:intro}\n\\IEEEPARstart{V}{ideo} segmentation --- identifying the key\nobjects with some specific properties or semantics in a\nvideo scene --- is a fundamental and challenging problem in computer vision, with\nnumerous potential applications including autonomous driving, robotics, automated surveillance, social media,  augmented reality, movie production, and video conferencing.\nThe problem has been addressed  using various traditional computer vision and machine learning techniques, including hand-crafted features (\\eg, histogram statistics, optical flow, \\etc), heuristic prior knowledge (\\eg, visual attention mechanism$_{\\!}$~, motion boundaries$_{\\!}$~, \\etc), low/mid-level visual representations (\\eg, super-voxel$_{\\!}$~, trajectory$_{\\!}$~, object proposal$_{\\!}$~, \\etc), and classical machine learning models (\\eg, clustering$_{\\!}$~, graph models$_{\\!}$~, random walks$_{\\!}$~, support vector machines$_{\\!}$~, random decision forests$_{\\!}$~, markov random fields$_{\\!}$~, conditional random fields$_{\\!}$~, \\etc).\nRecently, deep neural networks, and Fully Convolutional Networks (FCNs)$_{\\!}$~ in particular, have led to remarkable advances in video segmentation. These deep learning-based video segmentation algorithms are significantly more accurate (and sometimes even more efficient) than traditional approaches.\n\\begin{figure}[!htb]\n\t\\vspace{-26pt}\n\t\\centering\n\t\\includegraphics[width=0.99\\linewidth]{figs/top-right}\n\t\\vspace{-10pt}\n\t\\caption{Video segmentation tasks reviewed in this survey: (a) object-level automatic video object segmentation (object-level AVOS), (b) instance-level automatic video object segmentation (instance-level AVOS), (c) semi-automatic video object segmentation (SVOS), (d) interactive video object segmentation (IVOS), (e) language-guided video object segmentation (LVOS),  (f) video semantic segmentation (VSS), (g) video instance segmentation (VIS), and (h) video panoptic segmentation (VPS).\n\t}\n\t\\vspace{-12pt}\n\t\\label{fig:overview}\n\\end{figure}\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\includegraphics[width=0.99\\linewidth]{figs/overview}\n\t\\put(-492,219){\\scriptsize \\S\\ref{sec:introduction}}\n\t\\put(-443.5,219){\\scriptsize \\S\\ref{sec:2}}\n\t\\put(-348,219){\\scriptsize \\S\\ref{sec:3}}\n\t\\put(-279.5,228){\\scriptsize \\S\\ref{sec:3.1}}\n\t\\put(-280,210){\\scriptsize \\S\\ref{sec:3.2}}\n\t\\put(-210.5,218.5){\\scriptsize \\S\\ref{sec:4}}\n\t\\put(-138,218.5){\\scriptsize \\S\\ref{sec:5}}\n\t\\put(-77.5,218.5){\\scriptsize \\S\\ref{sec:6}}\n\t\\put(-35,218.5){\\scriptsize \\S\\ref{sec:7}}\n\t\\put(-448.5,102){\\tiny \\S\\ref{sec:2.1}}\n\t\\put(-451.5,79){\\tiny \\S\\ref{sec:2.2}}\n\t\\put(-461,56){\\tiny \\S\\ref{sec:2.3}}\n\t\\put(-353,162){\\tiny \\S\\ref{sec:3.1.1}}\n\t\\put(-340.5,90){\\tiny \\S\\ref{sec:3.1.2}}\n\t\\put(-353,43){\\tiny \\S\\ref{sec:3.1.3}}\n\t\\put(-340.5,13){\\tiny \\S\\ref{sec:3.1.4}}\n\t\\put(-134.5,98){\\tiny \\S\\ref{sec:vss}}\n\t\\put(-135.5,50){\\tiny \\S\\ref{sec:vis}}\n\t\\put(-134.5,6){\\tiny \\S\\ref{sec:vps}}\n\t\\vspace{-12pt}\n\t\\caption{{Overview of this survey.}}\n\t\\vspace{-10pt}\n\t\\label{fig:overview2}\n\\end{figure*}\nWith the rapid advance of this field, there is a huge body of new literature being produced. However, {most existing surveys predate the modern deep learning era$_{\\!}$~,} and often take a narrow view, such as focusing only on video foreground/background segmentation$_{\\!}$~. \nIn this paper, we offer\na  state-of-the-art review that addresses the\nwide area$_{\\!}$ of$_{\\!}$ video$_{\\!}$ segmentation,$_{\\!}$ especially$_{\\!}$ to$_{\\!}$ help$_{\\!}$ new$_{\\!}$ researchers$_{\\!}$\nenter this rapidly-developing field. We systematically introduce\nrecent advances in video segmentation, spanning from task formulation to taxonomy, from algorithms to datasets, and from unsolved issues to future research directions. We cover crucial aspects including task categories (\\ie, foreground/background separation \\textit{vs} semantic segmentation), {inference modes} (\\ie, automatic, semi-automatic, and interactive), and learning paradigms (\\ie, supervised, unsupervised, and weakly supervised), and we try to clarify  terminology (\\eg, background subtraction, motion segmentation, \\etc). \\red{We hope that this survey helps accelerate progress in this field.}\nThis survey mainly focuses on recent progress in two major branches of video segmentation, namely video object segmentation (Fig.~\\ref{fig:overview}(a-e)) and video semantic segmentation  (Fig.$_{\\!}$~\\ref{fig:overview}(f-h)),$_{\\!}$ which$_{\\!}$ are$_{\\!}$ further$_{\\!}$ divided$_{\\!}$ into$_{\\!}$ eight$_{\\!}$ sub-fields. {Even\n\tafter restricting our focus to deep learning-based video segmentation, there are still hundreds of papers in this fast-growing field. We select influential work published in prestigious journals and conferences. We also  include some non-deep learning video segmentation models and relevant literature in other areas, \\eg, visual tracking, to give necessary background.} \\red{Moreover, in order to promote the development of this field, we provide an accompanying webpage which catalogs algorithms and datasets addressing\n\tvideo segmentation: \\url{https://github.com/tfzhou/VS-Survey}.}\n{Fig.~\\ref{fig:overview2} shows the structure of this survey.}  Section \\S\\ref{sec:2} gives some brief background on taxonomy, terminology, study history, and related research areas. We review representative papers on deep learning algorithms and video segmentation datasets in \\S\\ref{sec:3} and \\S\\ref{sec:4}, respectively.  Section \\S\\ref{sec:5} conducts performance evaluation and analysis, {while \\S\\ref{sec:6} raises open questions and directions.} Finally, we make concluding remarks in \\S\\ref{sec:7}.", "cites": [810, 6685], "cite_extract_rate": 0.11764705882352941, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers by distinguishing between traditional and deep learning-based approaches, and it introduces the survey's focus on modern deep learning techniques in video segmentation. It abstracts to some extent by categorizing the field into major branches and sub-fields, but critical analysis is limited to general observations about the shortcomings of prior surveys rather than specific critiques of the cited works. Overall, it provides a structured overview with moderate analytical depth."}}
{"id": "d5eabe09-98ad-47f2-b006-ef58f8217be3", "title": "{Inference Modes", "level": "subsubsection", "subsections": [], "parent_id": "da022227-1ea2-4cd7-bff7-62b55077f65e", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Background"], ["subsection", "Problem Formulation and Taxonomy"], ["subsubsection", "{Inference Modes"]], "content": "for Video Segmentation}\\label{sec:2.1.2}\nVOS methods can be further classified into three types: automatic, semi-automatic, and interactive, according to how much human intervention is involved during inference.\n\\noindent$\\bullet$~\\textbf{Automatic Video Object Segmentation (AVOS)}. AVOS, or \\textit{unsupervised video segmentation} or \\textit{zero-shot video segmentation}, performs VOS in an automatic manner, without any manual initialization  (Fig.~\\ref{fig:overview}(a-b)). The input space $\\bm{\\mathcal{X}}$ refers to the video domain $\\bm{\\mathcal{V}}$ only. AVOS is suitable for video analysis but not for video editing that requires segmenting arbitrary objects or their parts flexibly; a typical application is virtual background creation in video conferencing. \n\\noindent$\\bullet$~\\textbf{Semi-automatic Video Object Segmentation (SVOS)}. SVOS, also known as \\textit{semi-supervised video segmentation} or \\textit{one-shot video segmentation}~, involves limited human inspection (typically provided in the first frame) to specify the desired objects  (Fig.~\\ref{fig:overview}(c)). For SVOS,  $\\bm{\\mathcal{X}}_{\\!}\\!=_{\\!}\\!\\bm{\\mathcal{V}}\\!\\times\\!\\bm{\\mathcal{M}}$, where $\\bm{\\mathcal{V}}_{\\!}$ indicates the video space and~$\\bm{\\mathcal{M}}$ refers to human input. Typically the human input is an object mask in the  first video frame, in which case SVOS is also called \\textit{pixel-wise} \\textit{tracking} or \\textit{mask propagation}.\n{Other forms of human input include bounding boxes and scribbles~.} From this perspective, \\textbf{language-guided video object segmentation (LVOS)} is a sub-branch of SVOS, in which the human input is given as linguistic descriptions about the desired objects  (Fig.~\\ref{fig:overview}(e)).\nCompared to AVOS, {SVOS is more flexible in defining target objects, but requires human input.} SVOS is typically applied in a user-friendly setting (without specialized equipment), such as video content creation in mobile phones. One of the core challenges in SVOS is how to fully utilize target information from limited human intervention.\n\\noindent$\\bullet$~\\textbf{Interactive Video Object Segmentation (IVOS)}. SVOS models are designed to operate automatically once the target has been identified, while systems for IVOS incorporate user guidance throughout the analysis process  (Fig.~\\ref{fig:overview}(d)). IVOS can obtain high-quality segments and works well for computer-generated imagery and video post-production, {where tedious human supervision is possible. IVOS is also studied in the graphics community as \\textit{video cutout}.} The input space $\\bm{\\mathcal{X}}$ for IVOS is $\\bm{\\mathcal{V}}\\!\\times\\!\\bm{\\mathcal{S}}$, where $\\bm{\\mathcal{S}}$ typically refers to human scribbling.  Key challenges include: 1) allowing users to easily specify segmentation constraints; 2) incorporating human specified constraints into the segmentation algorithm; and 3) giving quick response to the constraints.\nIn contrast to VOS,\nVSS methods typically work in an automatic mode (Fig.~\\ref{fig:overview}(f-h)), \\ie, $\\bm{\\mathcal{X}}\\!\\equiv\\!\\bm{\\mathcal{V}}$. Only a few early methods  address the semi-automatic setting, called \\textit{label propagation}~.\n\\noindent\\textbf{Remark}. The terms ``unsupervised'' and ``semi-supervised'' are conventionally used in VOS to specify the amount of human interaction involved during inference. But they are easily confused with ``unsupervised$_{\\!}$ learning''$_{\\!}$ and$_{\\!}$ ``semi-supervised$_{\\!}$ learning.''$_{\\!}$ We$_{\\!}$ urge$_{\\!}$ the$_{\\!}$ community$_{\\!}$ to$_{\\!}$ replace$_{\\!}$ these$_{\\!}$  ambiguous$_{\\!}$ terms with ``automatic'' and ``semi-automatic.''\n\\vspace{-3pt}", "cites": [3806], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates information about different inference modes in video segmentation, clearly distinguishing between automatic, semi-automatic, and interactive methods. It also introduces a sub-branch (LVOS) and connects the terminology to broader learning paradigms, showing some synthesis and abstraction. However, critical evaluation is limited to pointing out the limitations of terminology rather than deeply analyzing the cited papers or their technical shortcomings."}}
{"id": "2840bd6c-3d23-4b59-9cc4-1ead1415d4ac", "title": "Learning Paradigms for Video Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "da022227-1ea2-4cd7-bff7-62b55077f65e", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Background"], ["subsection", "Problem Formulation and Taxonomy"], ["subsubsection", "Learning Paradigms for Video Segmentation"]], "content": "\\label{sec:2.1.3}\nDeep learning-based video segmentation models can be grouped into three categories\naccording to the learning strategy they use to approximate $f^{*}$:\nsupervised, unsupervised, and weakly supervised.\n\\noindent$\\bullet$~\\textbf{Supervised Learning Methods.} Modern video segmentation models are typically learned in a fully supervised manner, requiring $N$ input training samples and their desired outputs $y_n\\!\\!:=\\!f^{*\\!}(x_n)$, where $\\{(x_n, y_n)\\}_{n\\!}\\!\\subset$\\bm{$\\mathcal{X}$}$\\times$\\bm{$\\mathcal{Y}$}. The standard method for evaluating learning outcomes follows an \\textit{empirical} risk/loss minimization formulation:\\footnote{\\scriptsize{We omit the regularization term for brevity.}}\n\\vspace{-3pt}\n\\begin{equation*}\\small\n\t\\tilde{f}\\in\\mathop{\\arg\\min}_{f\\in\\bm{\\mathcal{F}}} \\frac{1}{N}\\sum\\nolimits_n\\varepsilon(f(x_n),z(x_n)),\n\t\\vspace{-3pt}\n\\end{equation*}\nwhere \\bm{$\\mathcal{F}$} denotes the hypothesis (solution) space, and $\\varepsilon_{\\!\\!}:$ $\\bm{\\mathcal{X}}_{\\!}\\times\\bm{\\mathcal{Y}}_{\\!}\\mapsto\\!\\mathbb{R}$ is an error function that evaluates the estimate $f(x_n)$ against video segmentation related prior knowledge $z(x_n)\\!\\!\\in$\\bm{$\\mathcal{Z}$}. To make $\\tilde{f}$  a good approximation of $f^{*\\!}$, current supervised video segmentation methods directly use the desired output $y_n$, \\ie, $z(x_n)\\!\\!:=\\!\\!f^{*\\!}(x_n)$, as the prior knowledge, {with$_{\\!}$ the$_{\\!}$ price$_{\\!}$ of$_{\\!}$ requiring$_{\\!}$ vast$_{\\!}$ amounts$_{\\!}$ of$_{\\!}$ well-labeled$_{\\!}$ data.}\n\\noindent\\red{$\\bullet$~\\textbf{Unsupervised (\\red{Self-supervised}) Learning Methods.} When only data samples $\\{x_n\\}_{n\\!\\!}\\!\\subset$\\bm{$\\mathcal{X}$} are given, the problem of approximating $f^{*\\!}$ is known as unsupervised learning. Unsupervised learning includes fully unsupervised learning methods in which\n\tthe methods do not need any labels at all, as well as self-supervised learning methods in which networks are explicitly trained with automatically-generated pseudo labels without any human annotations~. Almost all existing unsupervised learning-based video segmentation models are self-supervised learning methods, where the prior knowledge \\bm{$\\mathcal{Z}$} refers to pseudo labels derived from intrinsic properties of video data (\\eg, cross-frame consistency). We thus use ``unsupervised learning'' and ``self-supervised learning''  interchangeably. }\n\\noindent$\\bullet$~\\textbf{Weakly-Supervised Learning Methods.} In this case, \\bm{$\\mathcal{Z}$} is typically a more easily-annotated domain, {such as tags, bounding boxes, or scribbles,} and  $f^{*}$ is approximated using a finite number of samples from \\bm{$\\mathcal{X}$}$\\times$\\bm{$\\mathcal{Z}$}.\n\\noindent\\textbf{Remark}. So far, deep supervised learning-based methods are dominant in the field of video segmentation. However, exploring the task in an unsupervised or weakly supervised\nsetting is more appealing, not only because it alleviates the annotation burden of\n\\bm{$\\mathcal{Y}$}, but because it inspires an in-depth$_{\\!}$ understanding$_{\\!}$ of$_{\\!}$ the$_{\\!}$ nature$_{\\!}$ of$_{\\!}$ the$_{\\!}$ task$_{\\!}$ by$_{\\!}$ exploring$_{\\!}$ \\bm{$\\mathcal{Z}$}.\n\\vspace{-3pt}", "cites": [322], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section describes the three main learning paradigms in video segmentation—supervised, unsupervised (self-supervised), and weakly supervised—with minimal integration of cited papers. It provides definitions and distinctions but does not synthesize insights from multiple sources or present a novel framework. The analysis remains at a descriptive level without strong evaluation or deeper critical or abstract insights."}}
{"id": "0e658e90-4825-4e82-8a76-76eef59cf11f", "title": "History and Terminology", "level": "subsection", "subsections": [], "parent_id": "1ecf3274-8ab2-4640-be6d-ea1fb36fd264", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Background"], ["subsection", "History and Terminology"]], "content": "\\label{sec:2.2}\nDigital image segmentation has been studied for at least 50 years, starting with the Roberts\noperator~ for identifying object boundaries.\nSince then, numerous algorithms~for image segmentation have been proposed, and many are extended to the video domain.  The field of video segmentation has evolved quickly and undergone great change.\nEarlier attempts focus on \\textbf{video over-segmentation}, \\ie, partitioning a video into\nspace-time homogeneous, perceptually distinct-regions. Typical approaches include hierarchical video\nsegmentation~, temporal superpixel~, and super-voxels~, based on the discontinuity and similarity of pixel intensities in a particular location, \\ie, separating pixels according to abrupt changes in intensity or grouping pixels with similar intensity together.  These methods are instructive for early stage video preprocessing, but cannot solve the problem of object-level pattern modeling, as they do not provide any principled approach to flatten the hierarchical video decomposition into a binary segmentation~.\nTo extract foreground objects from video sequences, \\textbf{background subtraction} techniques emerged {beginning in the late 70s~}, and became popular following the work of~. They assume that the background is known a priori, and that the camera is stationary\\!~ or undergoes a predictable, parametric 2D\\!~ or 3D motion with 3D parallax\\!~. These geometry-based methods fit well for specific application scenarios such as surveillance systems~, but they are sensitive to model selection (2D or 3D), and cannot handle non-rigid camera movements.\nAnother group of video segmentation solutions tackled the task of \\textbf{motion segmentation}, \\ie, finding objects in motion. Background subtraction can also be viewed as a specific case of motion segmentation. However, most motion segmentation models are built upon motion analysis~, factorization\\!~, and/or statistical\\!~ techniques that comprehensively model the characteristics of moving scenes without prior knowledge of camera motion. Among the big family of motion segmentation algorithms, \\textbf{trajectory segmentation} attained particular attention~. Trajectories are generated through tracking points over multiple frames and can represent long-term motion patterns, serving as an informative cue for segmentation.  Though impressive, motion-based methods heavily rely on the accuracy of optical flow estimation and can fail when different parts of an object exhibit heterogeneous motions.\nTo overcome these limitations, the task of extracting generic objects from unconstrained video sequences, \\ie, AVOS, has drawn increasing research interest~. Several methods  explored object hypotheses or proposals~ as middle-level object representations. They generate a large number of object candidates in every frame and cast the task of segmenting video objects as an object region selection problem. The main drawbacks of the proposal-based algorithms are the high computational cost   and complicated object inference schemes. Some others explored heuristic hypotheses such as visual attention~ and motion boundary~, but  easily fail in scenarios where the heuristic assumptions do not hold.\nAs argued earlier, an alternative to the above unattended solutions is {to incorporate human-marked initialization}, \\ie, SVOS. Older SVOS methods often rely on optical flow  and share a similar spirit with \\textit{object tracking}~. In addition, some pioneering IVOS methods were proposed to address high-quality video segmentation under extensive human guidance, including rotoscoping~, scribble~, contour~, and points~. Significant engineering is typically needed to allow IVOS systems to operate at interactive speeds. In short, SVOS and IVOS pay for the improved flexibility and accuracy: they are infeasible at large scale due to their human-in-the-loop nature.\nIn the pre-deep learning era, {relatively few papers$_{\\!}$~  considered VSS due to the complexity of the task. The approaches typically relied on supervised classifiers such as SVMs and video over-segmentation} techniques.  \nOverall, traditional approaches for video segmentation, though giving interesting results, are  constrained by hand-crafted features and heavy engineering. {But deep learning brought the performance of video segmentation to a new level, as we will review in \\S\\ref{sec:3}.}\n\\vspace{-4pt}", "cites": [9062], "cite_extract_rate": 0.020833333333333332, "origin_cites_number": 48, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a historical overview of video segmentation, synthesizing various traditional approaches and connecting them to the emergence of deep learning methods. It identifies limitations in earlier techniques and abstracts them into broader categories like video over-segmentation, background subtraction, and motion segmentation. While it includes some critical discussion of drawbacks, it is largely analytical in nature and offers a coherent narrative, though it does not deeply evaluate or compare specific cited works in detail."}}
{"id": "a6648382-257f-4567-9895-8f5d7a034483", "title": "Related Research Areas", "level": "subsection", "subsections": [], "parent_id": "1ecf3274-8ab2-4640-be6d-ea1fb36fd264", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Background"], ["subsection", "Related Research Areas"]], "content": "\\label{sec:2.3}\nThere are several research fields {closely related to video segmentation, which we now  briefly describe.}\n\\noindent$\\bullet$~\\textbf{Visual Tracking.} To infer the location of a target object over time, current tracking methods usually assume that the target is determined by a bounding box in the first frame~. However, in more general tracking scenarios, and in particular the cases studied in early tracking methods, diverse object representations are explored~, including centroids, skeletons, and contours. Some video segmentation techniques, such as background subtraction, are also merged into older trackers~. Hence, visual tracking and video segmentation encounter some common challenges (\\eg, object/camera motion, appearance change, occlusion, \\etc), fostering their mutual collaboration.\n\\noindent$\\bullet$~\\textbf{Image Semantic Segmentation.} The success of end-to-end image semantic segmentation~ {has sparked the rapid development of VSS}. Rather than directly applying image semantic segmentation techniques frame by frame, recent VSS systems explore temporal continuity to increase both accuracy and efficiency. Nevertheless, image semantic segmentation techniques continue to serve as a foundation for advancing segmentation in video.\n\\begin{table*}\n\\centering\n\\caption{\n\t{\\textbf{Summary of essential characteristics for reviewed AVOS methods} (\\S\\ref{sec:3.1.1}).\n\t\t\\protect\\\\ \\textbf{Instance}: instance- or object-level segmentation; \\textbf{Flow}: if optical flow is used.\n\t}\n}\n\\vspace{-6pt}\n\\label{table:AVOS_methods}\n\\begin{threeparttable}\n\t\\resizebox{0.89\\textwidth}{!}{\n\t\t\\setlength\\tabcolsep{6pt}\n\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\\begin{tabular}{|c|r||c|c|c|c|c|c|}\n\t\t\t\\hline\\thickhline\n\t\t\t\\rowcolor{mygray}\n\t\t\tYear &Method~~~~  &Pub. &Core Architecture  \n\t\t\t& Instance & Flow \n\t\t\t& Training Dataset \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{5}{*}{\\rotatebox{90}{2017}}\n\t\t\t& FSEG~ & CVPR & Two-Stream FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& ImageNet VID~ + DAVIS$_{16}$~ \\\\\n\t\t\t& SFL~ & ICCV & Two-Stream FCN \n\t\t\t& Object& \\checkmark \n\t\t\t& DAVIS$_{16}$~ \\\\\n\t\t\t& LVO~ & ICCV & Two-Stream FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& DAVIS$_{16}$~ \\\\\n\t\t\t&LMP~  &ICCV &FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& FT3D~\\\\\n\t\t\t&NRF~ &ICCV &FCN \n\t\t\t& Object& \\checkmark  \n\t\t\t&Youtube-Objects~ \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{6}{*}{\\rotatebox{90}{2018}}\n\t\t\t&IST~  &CVPR &FCN \n\t\t\t&Object & \\checkmark \n\t\t\t& DAVIS$_{16}$~ \\\\\n\t\t\t&FGRNE~&CVPR &FCN + RNN \n\t\t\t& Object & \n\t\t\t&SegTrackV2~ + DAVIS$_{16}$~ + FBMS~ \\\\\n\t\t\t&MBN~ &ECCV &FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& DAVIS$_{16}$~\\\\\n\t\t\t&PDB~&ECCV &RNN \n\t\t\t& Object & \n\t\t\t&DAVIS$_{16}$~ \\\\\n\t\t\t& MOT~ &ICRA& Two-Stream FCN \n\t\t\t& Object & \\checkmark\n\t\t\t&DAVIS$_{16}$~  \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{9}{*}{\\rotatebox{90}{2019}}\n\t\t\t&RVOS~ &CVPR &RNN \n\t\t\t& Instance &  \n\t\t\t& DAVIS$_{17}$~/YouTube-VIS~\\\\\n\t\t\t&COSNet~ & CVPR & Siamese FCN + Co-attention \n\t\t\t& Object & \n\t\t\t&  MSRA10K~ + DUT~ + DAVIS$_{16}$~\\\\\n\t\t\t&UMOD~ & CVPR & Adversarial Network \n\t\t\t& Object & \\checkmark \n\t\t\t& SegTrackV2~ + DAVIS$_{16}$~ + FBMS~ \\\\\n\t\t\t&AGS~ & CVPR & FCN \n\t\t\t& Object &  \n\t\t\t& SegTrackV2~ + DAVIS$_{16}$~ + DUT~ + PASCAL-S~ \\\\\n\t\t\t&AGNN~ &ICCV &FCN + GNN \n\t\t\t& Object & \n\t\t\t& MSRA10K~ + DUT~ + DAVIS$_{16}$~\\\\\n\t\t\t&MGA~ &ICCV &Two-Stream FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& DUTS~ + DAVIS$_{16}$~ + FBMS~\\\\\n\t\t\t&AnDiff~ & ICCV & Siamese FCN + Co-attention \n\t\t\t& Object & \n\t\t\t& DAVIS$_{16}$~\\\\\n\t\t\t&LSMO~ &IJCV &Two-Stream FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& FT3D~ + DAVIS$_{16}$~\\\\\t\n\t\t\t\\hline\n\t\t\t\\multirow{7}{*}{\\rotatebox{90}{2020}}\n\t\t\t&MATNet~ &AAAI &Two-Stream FCN \n\t\t\t& Object &\\checkmark \n\t\t\t& Youtube-VOS~ + DAVIS$_{16}$~\\\\\n\t\t\t&PyramidCSA~ &AAAI &Siamese FCN + Co-attention \n\t\t\t& Object & \n\t\t\t& DUTS~ + DAVIS$_{16}$~ + DAVSOD~\\\\\n\t\t\t& MuG~ & CVPR & FCN \n\t\t\t& Object & \n\t\t\t& OxUvA~ \\\\\n\t\t\t&EGMN~ & ECCV & FCN + Episodic Memory \n\t\t\t& Object & \n\t\t\t& MSRA10K~ + DUT~ + DAVIS$_{16}$~ \\\\\n\t\t\t&WCSNet~ & ECCV & Siamese FCN \n\t\t\t& Object & \n\t\t\t& SALICON~ + PASCAL VOC 2012~  + DUTS~ + DAVIS$_{16}$~ \\\\\n\t\t\t&DFNet~ & ECCV & Siamese FCN \n\t\t\t& Object & \n\t\t\t& MSRA10K~ + DUT~ + DAVIS$_{16}$~ \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{5}{*}{\\rotatebox{90}{2021}}\n\t\t\t&F2Net~ &AAAI &Siamese FCN \n\t\t\t& Object &\n\t\t\t& MSRA10K~ + DAVIS$_{16}$~\\\\\n\t\t\t&TODA~ & CVPR & Siamese FCN \n\t\t\t& Instance & \n\t\t\t& DAVIS$_{17}$~/YouTube-VIS~ \\\\\n\t\t\t&RTNet~ & CVPR & Two-Stream FCN \n\t\t\t& Object & \\checkmark \n\t\t\t& DUTS~ + DAVIS$_{16}$~ \\\\\n\t\t\t&DyStab~ & CVPR & Adversarial Network \n\t\t\t& Object & \\checkmark \n\t\t\t& SegTrackV2~ + DAVIS$_{16}$~ + FBMS~ \\\\\n\t\t\t& \\red{MotionGrouping } & \\red{ICCV} & \\red{Transformer} & \\red{Object} &  \\red{\\checkmark} & \\red{DAVIS16~/SegTrackV2~/FBMS59~/MoCA~} \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\\end{threeparttable}\n\\vspace{-10pt}\n\\end{table*}\n\\noindent$\\bullet$~\\textbf{Video Object Detection.} To generalize object detection in the video domain~, video object detectors incorporate temporal cues over the box- or feature- level. There are many key technical steps and challenges, such as object proposal generation, temporal information aggregation, and cross-frame object association, that are shared between video object detection and (instance-level) video segmentation.\n\\vspace{-4pt}", "cites": [6703, 5637, 6705, 895, 6688, 6696, 2651, 6693, 6697, 6692, 3805, 6699, 6704, 6689, 6694, 6698, 5630, 6686, 6701, 6690, 6700, 9063, 6687, 6252, 6702, 6691, 6695, 5799], "cite_extract_rate": 0.5, "origin_cites_number": 56, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by listing related research areas and mentioning how they connect to video segmentation, but it largely remains descriptive, summarizing individual methods and their components without deeply analyzing their relationships or differences. Critical analysis is limited to superficial observations (e.g., 'tend to favour short-term temporal dependencies'), and abstraction is minimal, with no clear generalization of principles or trends across the cited works."}}
{"id": "3b0f0ea5-4ea5-4714-986b-cf7c5ebf4974", "title": "Automatic Video Object Segmentation (AVOS)", "level": "subsubsection", "subsections": [], "parent_id": "e8f77e8a-feaa-4ad7-9d91-c8bda372d9d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VOS Models"], ["subsubsection", "Automatic Video Object Segmentation (AVOS)"]], "content": "\\label{sec:3.1.1}\nInstead of using heuristic priors and hand-crafted features to automatically execute VOS, modern AVOS methods learn {generic video object patterns in a data-driven fashion. We group landmark efforts based on their key techniques.}\n\\noindent$\\bullet$~\\textbf{Deep Learning Module based Methods.} In 2015, Fragkiadaki \\etal~ made an early effort that learns a multi-layer perceptron to rank proposal segments and infer foreground objects. In 2016, Tsai \\etal~ proposed a joint optimization framework for AVOS and optical flow estimation with a na\\\"{i}ve use of deep features from a pre-trained classification network. Later methods~ learn FCNs to predict initial, pixel-level foreground estimates from frame images~ or optical flow fields~, while several post-processing steps are still needed. Basically, these primitive solutions largely rely on traditional AVOS techniques; the learning ability of neural networks is under-explored.\n\\noindent$\\bullet$~\\textbf{Pixel Instance Embedding based Methods.}  A group of AVOS models has been developed to make use of stronger deep learning descriptors~ -- instance embeddings -- learned from image instance segmentation data~. \\red{They first generate pixel-wise instance embeddings, and select representative embeddings which are clustered into foreground and background. Finally, the labels of the sampled embeddings are propagated to the other ones. The clustering and  propagation can be achieved without video specific supervision. Though using fewer annotations, these methods suffer from a fragmented and complicated pipeline.}\n\\noindent$\\bullet$~\\textbf{End-to-end Methods with Short-term Information Encoding.} End-to-end model designs became the mainstream in this field. For example, convolutional recurrent neural networks (RNNs) were used to learn spatial and temporal visual patterns jointly~. Another big family is built upon two-stream networks~, wherein two parallel streams\nare built to extract features from raw image and optical flow, which are further fused for segmentation prediction. Two-stream methods make explicit use of appearance and motion cues, at the cost of optical flow computation and vast learnable parameters. {These end-to-end methods improve accuracy and show the advantages of applying neural\nnetworks to this task. However, they only consider local content within very limited time span; they stack appearance and/or motion information from a few successive frames as input, ignoring relations among distant frames. Although RNNs are usually adopted, their internal hidden memory creates the inherent limits in modeling longer-term dependencies~.}\n\\begin{table*}\n\t\\centering\n\t\\caption{\n\t\t{\\textbf{Summary of essential characteristics for reviewed  SVOS methods} (\\S\\ref{sec:3.1.2}).\n\t\t\t\\textbf{Flow}: if optical flow is used.\n}\n\t}\n\t\\vspace{-5pt}\n\t\\label{table:SVOS_methods}\n\t\\begin{threeparttable}\n\t\t\\resizebox{0.99\\textwidth}{!}{\n\t\t\t\\setlength\\tabcolsep{6pt}\n\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\\begin{tabular}{|c|r||c|c|c|c|c|}\n\t\t\t\t\\hline\\thickhline\n\t\t\t\t\\rowcolor{mygray}\n\t\t\t\tYear &Method~~~~  &Pub. &Core Architecture \n& Flow \n& Technical Feature & Training Dataset \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{7}{*}{\\rotatebox{90}{2017}}\n\t\t\t\t&OSVOS~ &CVPR & FCN \n& \n&  Online Fine-tuning&DAVIS$_{16}$~ \\\\\n\t\t\t\t& MaskTrack~ & CVPR & FCN \n& \\checkmark \n& Propagation-based & ECSSD~ + MSRA10K~ + PASCAL-S~ + DAVIS$_{16}$~  \\\\\n\t\t\t\t& CTN~ & CVPR & FCN \n& \\checkmark \n& Propagation-based & PASCAL VOC 2012~  \\\\\n\t\t\t\t& VPN~ & CVPR & Bilateral Network \n& \n& Propagation-based  &DAVIS$_{16}$~ \\\\\n\t\t\t\t& PLM~& CVPR & Siamese FCN \n& \n& Matching-based & DAVIS$_{16}$~\\\\\n\t\t\t\t&OnAVOS~ &BMVC & FCN \n& \n& Online Fine-tuning &PASCAL VOC 2012~ + COCO~  + DAVIS~ \\\\\n\t\t\t\t& Lucid~ & IJCV & Two-Stream FCN \n& \\checkmark \n& Propagation-based & DAVIS$_{16}$~\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{12}{*}{\\rotatebox{90}{2018}}\n\t\t\t\t& CINM~ & CVPR & Spatio-temporal MRF  \n& \\checkmark \n& Propagation-based  & DAVIS$_{17}$~ \\\\\n\t\t\t\t& FAVOS~ & CVPR & FCN \n& \n& Propagation-based & DAVIS$_{16}$~/DAVIS$_{17}$~ \\\\\n\t\t\t\t& RGMP~ & CVPR & Siamese FCN \n& \n&Propagation-based & PASCAL VOC 2012~ + ECSSD~ + MSRA10K~ + DAVIS$_{17}$~ \\\\\n\t\t\t\t& OSMN~ & CVPR & FCN + Meta Learning  \n&  \n& Online Fine-tuning & ImageNet VID~  + DAVIS$_{16}$~ \\\\\n\t\t\t\t& MONet~ & CVPR & FCN  \n& \\checkmark   \n& Online Fine-tuning&PASCAL VOC 2012~ + DAVIS$_{16}$~ \\\\\n\t\t\t\t& CRN~ & CVPR & FCN + Active Contour \n& \\checkmark \n& Propagation-based & PASCAL VOC 2012~ + DAVIS$_{16}$~ \\\\\n\t\t\t\t& RCAL~ & CVPR & FCN + RL \n& \n& Propagation-based & MSRA10K~ + PASCAL-S + SOD + ECSSD~ + DAVIS$_{16}$~ \\\\\n\t\t\t\t&OSVOS-S~  &PAMI & FCN \n&  \n& Online Fine-tuning& DAVIS$_{16}$~/DAVIS$_{17}$~ \\\\\n\t\t\t\t& Videomatch~ & ECCV & Siamese FCN \n&  \n& Matching-based & DAVIS$_{16}$~/DAVIS$_{17}$~ \\\\\n\t\t\t\t& Dyenet~ & ECCV & Re-ID \n& \n& Propagation-based& DAVIS$_{17}$~ \\\\\n\t\t\t\t& LSE~ & ECCV & FCN \n& \n& Propagation-based& PASCAL VOC 2012~ \\\\\n\t\t\t\t& Colorization~ & ECCV & Siamese FCN \n& \n& Unsupervised Learning& Kinetics~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{13}{*}{\\rotatebox{90}{2019}}\n\t\t\t\t&MVOS~ & PAMI & Siamese FCN + Meta Learning \n& \n&Online Fine-tuning   &  PASCAL VOC 2012~ + DAVIS$_{16}$~/DAVIS$_{17}$~\\\\\n\t\t\t\t& FEELVOS~ & CVPR & FCN \n& \n& Matching-based& COCO~  + DAVIS$_{17}$~ + YouTube-VOS~ \\\\\n\t\t\t\t& MHP-VOS~ & CVPR & Graph Optimization \n& \n& Propagation-based & COCO~  + DAVIS$_{16}$~/DAVIS$_{17}$~\\\\\n\t\t\t\t& AGSS~ & CVPR & FCN \n& \\checkmark\n&  Propagation-based&  DAVIS$_{17}$~/YouTube-VOS~ \\\\\n\t\t\t\t& AGAME~ & CVPR & FCN \n& \n&   Propagation-based&  MSRA10K~ + PASCAL VOC 2012~ + DAVIS$_{17}$~/YouTube-VOS~ \\\\\n\t\t\t\t& SiamMask~ & CVPR & Siamese FCN \n& \n&  Box-Initialization&  DAVIS$_{16}$~/DAVIS$_{17}$~/YouTube-VOS~ \\\\\n\t\t\t\t&RVOS~ &CVPR & RNN \n& \n&  Propagation-based & DAVIS$_{17}$~/YouTube-VIS~\\\\\n\t\t\t\t&BubbleNet~ &CVPR & Siamese Network\n&  \n&  Bubble Sorting & DAVIS$_{17}$~\\\\\n\t\t\t\t&RANet~ & ICCV & Siamese FCN \n& \n& Matching-based& MSRA10K~ + ECSSD~+ HKU-IS~ + DAVIS$_{16}$~/DAVIS$_{17}$~ \\\\\n\t\t\t\t& DMM-Net~ & ICCV & Mask R-CNN \n& \n&Differentiable Matching &  DAVIS$_{17}$~/YouTube-VOS~ \\\\\n\t\t\t\t& DTN~ & ICCV & FCN \n& \\checkmark \n&  Propagation-based& COCO~  + PASCAL VOC 2012~ + DAVIS$_{16}$/DAVIS$_{17}$~\\\\\n\t\t\t\t& STM~ & ICCV &  Memory Network \n& \n& Matching-based&  PASCAL VOC 2012~ + COCO~  + ECSSD~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& TimeCycle~ & ECCV & Siamese FCN \n& \n& Unsupervised Learning& VLOG~ \\\\\n\t\t\t\t&{UVC}  &{NeurIPS} & {Siamese FCN} \n& \n& {Unsupervised Learning} & {COCO~ + Kinetics~}\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{16}{*}{\\rotatebox{90}{2020}}\n\t\t\t\t&e-OSVOS~ & NeurIPS &  Mask R-CNN + Meta Learning\n&   \n& Online Fine-tuning&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& AFB-URR~ & NeurIPS & Memory Network \n& \n& Matching-based& PASCAL VOC 2012~ + COCO~ + ECSSD~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& Fasttan~ & CVPR & Faster R-CNN \n&  \n& Propagation-based &  COCO~  + DAVIS$_{17}$~\\\\\n\t\t\t\t& Fasttmu~ & CVPR  & FCN + RL \n& \n& Box-Initialization & PASCAL VOC 2012~ + DAVIS$_{17}$~\\\\\n\t\t\t\t& SAT~ & CVPR & FCN + RL \n& \n& Propagation-based& COCO~  + DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& FRTM-VOS~ & CVPR & FCN  \n& \n& Matching-based&  DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& TVOS~ & CVPR & FCN  \n& \n&  Matching-based& DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& MuG~ & CVPR & Siamese FCN  \n& \n&  Unsupervised Learning& OxUvA~\\\\\n\t\t\t\t& MAST~ & CVPR & Memory Network  \n& \n&  Unsupervised Learning& OxUvA~ + YouTube-VOS~\\\\\n\t\t\t\t& GCNet~ & ECCV & Memory Network \n& \n& Matching-based& MSRA10K~ + ECSSD~ + HKU-IS~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& KMN~ & ECCV & Memory Network \n& \n&Matching-based & PASCAL VOC 2012~ + COCO~  + ECSSD~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& CFBI~ & ECCV & FCN \n& \n& Matching-based& COCO~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& LWL~ & ECCV & Siamese FCN + Meta Learning \n& \n& Matching-based&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& MSN~ & ECCV & Memory Network \n& \n& Matching-based&  DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& EGMN~ & ECCV & Memory Network \n& \n& Matching-based&  MSRA10K~ + COCO~ + DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& STM-Cycle~ & NeurIPS & Memory Network \n& \n& Matching-based&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{8}{*}{\\rotatebox{90}{2021}}\n\t\t\t\t& QMA~ & AAAI & Memory Network \n& \n& Box-Initialization & DUT~ + HKU-IS~ + MSRA10K~ + YouTube-VOS~\\\\\n\t\t\t\t& SwiftNet~ & CVPR & Memory Network \n& \n& Matching-based&  COCO~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& G-FRTM~ & CVPR & FCN + RL \n& \n& Matching-based&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& SST~ & CVPR & Transformer \n& \n& Matching-based&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& GIEL~ & CVPR & Siamese FCN \n& \n& Matching-based&  DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t& LCM~ & CVPR & Memory Network  \n& \n&Matching-based &  PASCAL VOC 2012~ + COCO~  + ECSSD~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& RMNet~ & CVPR & Memory Network  \n& \\checkmark \n& Matching-based&  PASCAL VOC 2012~ + COCO~  + ECSSD~ + DAVIS$_{17}$~/YouTube-VOS~\\\\\n\t\t\t\t& {CRW}~ & {NeurIPS} & {FCN}\n& \n& {Unsupervised Learning} & {Kinetics}  \\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{threeparttable}\n\t\\vspace{-10pt}\n\\end{table*}\n\\noindent$\\bullet$~\\textbf{End-to-end Methods with Long-term Context Encoding.} Current leading AVOS models use global context over long time spans. \\red{In a seminal work~, Lu \\etal proposed~a Siamese architecture-based  model that extracts features for arbitrary frame pairs and captures cross-frame context by calculating pixel-wise feature correlations. During inference, for each test frame, context from several other frames (within the same\nvideo) is aggregated to locate objects. A contemporary work~ exploited a similar idea but only used the first frame as reference. Several papers~ extended  by making better use of information from multiple frames , encoding spatial context , and incorporating temporal consistency to improve representation power and computation efficiency~.}\n\\noindent$\\bullet$~$_{\\!}$\\textbf{Un-/Weakly-supervised$_{\\!}$ based$_{\\!}$ Methods.} Only a handful of methods learn to perform AVOS from unlabeled or weakly labeled data. In~, static image salient object segmentation and dynamic eye fixation data, which are more easily acquired compared with video segmentation data, are used to learn video generic object patterns.  In~, visual patterns are learned through exploring several intrinsic properties of video data at multiple granularities, \\ie, intra-frame saliency, short-term visual coherence, long-range semantic correspondence, and video-level discriminativeness. In~, an adversarial contextual model is developed to segment  moving objects without any manual\nannotation, achieved by minimizing the mutual information between the motions of an object and its context. This method is further enhanced in~ by adopting a bootstrapping strategy and enforcing temporal consistency. \\red{In~, motion is exclusively exploited to discover moving objects, and a Transformer-based model is designed and trained by self-supervised flow reconstruction using unlabeled video data.}\n\\noindent$\\bullet$~\\textbf{Instance-level AVOS Methods.} Instance-level AVOS, also referred as \\textit{multi-object unsupervised video segmentation}, was introduced with the launch of the DAVIS$_{19}$ challenge~. This task setting is more challenging as it requires not only separating the foreground objects from the background, but also discriminating different object instances. To tackle this task, current solutions typically work in a {top-down fashion}, \\ie, generating object candidates for each frames, and associating instances over different frames. In an early attempt~, Ventura~\\etal delivered a recurrent network-based model that consists of a spatial LSTM for {per-frame instance discovery} and a temporal LSTM for cross-frame instance association. {This method features an elegant model design}, while its representation ability is too weak to enumerate all the object instances and to capture complex interactions between instances over the temporal domain. \\red{Thus later methods~ strengthen the two-step pipeline through: \\textbf{i)} employing image instance segmentation models (\\eg, Mask R-CNN~) to detect object candidates, and \\textbf{ii)} leveraging  tracking/re-identification techniques and manually designed rules for instance association. Foreground/background AVOS techniques~ are also used to filter out nonsalient candidates~. More recent methods, \\eg, , generate object candidates first and obtain corresponding tracklets via advanced SVOS techniques.} Overall, current instance-level AVOS models follow the classic tracking-by-detection paradigm, involving several ad-hoc designs. There is still considerable room for further improvement in accuracy and efficiency.", "cites": [5637, 895, 6729, 6720, 2652, 6696, 6692, 6708, 3757, 6726, 6725, 6230, 6686, 6700, 8262, 5633, 520, 6733, 6722, 6731, 2651, 486, 6693, 6697, 3805, 6735, 6706, 6716, 6698, 6728, 5629, 6687, 6702, 6736, 6714, 6715, 9066, 8263, 5799, 6703, 6705, 9065, 6732, 6719, 3819, 6699, 6713, 6718, 5630, 6707, 9063, 6709, 6710, 9064, 6721, 6711, 6734, 6737, 6724, 6704, 6689, 3806, 6727, 6712, 9109, 1283, 5632, 3818, 6701, 5625, 6690, 6252, 6723, 6717, 6695, 6730], "cite_extract_rate": 0.7037037037037037, "origin_cites_number": 108, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes a wide range of AVOS methods by grouping them into three main categories, connecting their technical features and evolution. It critically evaluates the strengths and weaknesses of each category, such as the limitations of RNNs in modeling long-term dependencies. The section also identifies broader trends like the shift toward end-to-end models and the trade-offs between accuracy and efficiency."}}
{"id": "b28b9caf-373d-4361-b942-d79e1015b842", "title": "Semi-automatic Video Object Segmentation (SVOS)", "level": "subsubsection", "subsections": [], "parent_id": "e8f77e8a-feaa-4ad7-9d91-c8bda372d9d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VOS Models"], ["subsubsection", "Semi-automatic Video Object Segmentation (SVOS)"]], "content": "\\label{sec:3.1.2}\nDeep learning-based SVOS methods mainly focus on the first-frame \\textit{mask} propagation setting.  They are categorized by their utilization of the test-time provided object masks.\n\\noindent$\\bullet$~\\textbf{Online Fine-tuning based Methods.} Following the one-\n\\noindent shot principle, this family of methods~ trains a segmentation model separately on each given object mask in an online fashion. Fine-tuning methods essentially exploit the transfer learning capabilities of neural networks and often follow a two-step training procedure: i) \\textit{offline pre-training}: learn general segmentation features from images and video sequences, and ii)\n\\textit{online fine-tuning}: learn target-specific representations from test-time supervision. The idea of fine-tuning was first introduced in~, where only the initial image-mask pair is used for {training an online, one-shot, but merely appearance-based FCN model.} Then, in~, more pixel samples in the unlabeled frames are mined as online training samples to better adapt to further changes over time.  As  have no notion of individual objects,   further incorporates instance segmentation models (\\eg, Mask R-CNN~) during inference.  \nWhile elegant through their simplicity, {fine-tuning methods have several weaknesses}: i) pre-training is fixed and not optimized for\nsubsequent fine-tuning, ii) hyperparameters of online fine-tuning are often excessively\nhand-crafted and fail to generalize between test cases, iii) the common existing fine-tuning setups suffer from high test runtimes (up to 1,000 training iterations per segmented object online~). The root cause is that these approaches choose to encode all the target-related cues (\\ie, appearance, mask) into network parameters implicitly. Towards efficient and automated fine-tuning, some recent methods~ turn to meta learning techniques, \\ie, optimize the fine-tuning policies~ (\\eg, generic model initialization, learning rates, \\etc) or even directly modify network weights~.\n\\noindent$\\bullet$~\\textbf{Propagation-based Methods.} {Two recent lines of research\\\\\n-- built upon mask propagation and template matching~techniques respectively -- try to refrain from the online~optimization to deliver compact, end-to-end SVOS solutions.} In particular, propagation-based methods use the previous frame mask to infer the current mask$_{\\!}$~. For example, Jampani \\etal~ propose a bilateral network for long-range\nvideo-adaptive mask propagation. \\red{Perazzi \\etal\\\\ } \\red{approach SVOS by employing a modified FCN, where the previous frame mask is considered as an extra input channel. Follow-up work adopts optical flow guided mask alignment~,  heavy first-frame data augmentation$_{\\!}$~, and  multi-step segmentation refinement$_{\\!}$~. Others apply\\\\ re-identification to retrieve missing objects after prolonged occlusions$_{\\!}$~, design a reinforcement learning agent that tackles$_{\\!}$ SVOS as$_{\\!}$ a$_{\\!}$ conditional$_{\\!}$ decision-making$_{\\!}$ process$_{\\!}$~, or propagate masks in a spatiotemporal MRF model~to~improve temporal coherency~. Some researchers leverage location-aware embeddings to sharpen the feature$_{\\!}$~, or directly learn sequence-to-sequence mask propagation~.} Advanced tracking techniques are also exploited in~. Propagation-based methods are found~to easily suffer from error accumulation due to occlusions~and drifts during mask propagation. Conditioning propagation on the initial frame-mask pair$_{\\!}$~ seems a feasible$_{\\!}$ solution$_{\\!}$ to$_{\\!}$ this.$_{\\!}$ Although$_{\\!}$ target-specific$_{\\!}$ mask$_{\\!}$ is$_{\\!}$ exp-\\\\ licitly encoded into the segmentation network, making up for$_{\\!}$ the$_{\\!}$ deficiencies$_{\\!}$ of$_{\\!}$ fine-tuning$_{\\!}$ methods$_{\\!}$ to$_{\\!}$ a$_{\\!}$ certain$_{\\!}$ extent,\\\\ propagation-based$_{\\!}$ methods still embed object appearance into hidden network weights. Clearly, {such implicit target-appearance modeling strategy hurts flexibility and adaptivity (\\red{while$_{\\!}$~ is an exception -- a generative model of target\\\\ and background is explicitly built to aid mask propagation}).}\n\\begin{figure*}\n\t\\begin{minipage}{\\textwidth}\n\t\t{\n\t\t\t\\begin{minipage}[t]{0.3\\textwidth}\n\t\t\t\t{\\normalsize tionally considers first-frame annotations, but easily fails in challenging scenes {without human feedback}. Moreover, the first-frame annotations are typically detailed masks, which are tedious to acquire: 79 seconds per instance for coarse polygon annotations of COCO~, and much more for higher quality. Thus performing~ VOS~ in~ the~ interactive}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\begin{minipage}[t]{0.7\\textwidth}\n\t\t\t\\vspace{-3ex}\n\t\t\t\\makeatletter\\def\\@captype{table}\\makeatother\n\t\t\t\\caption{\n\t\t\t\t{\\textbf{Summary of essential characteristics for reviewed  IVOS methods} (\\S\\ref{sec:3.1.3}).}\n\t\t\t}\n\t      \\vspace{-5pt}\n\t\t\t\\begin{threeparttable}[t]\n\t\t\t\t\\resizebox{\\textwidth}{!}{\n\t\t\t\t\t\\setlength\\tabcolsep{3pt}\n\t\t\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\t\t\\begin{tabular}{|c|r||c|c|c|c|c|c|}\n\t\t\t\t\t\t\\hline\\thickhline\n\t\t\t\t\t\t\\rowcolor{mygray}\n\t\t\t\t\t\tYear &Method~~~~  &Pub. &Core Architecture  \n\t\t\t\t\t\t& Technical Feature & Training Dataset \\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t2017\n\t\t\t\t\t\t& IIW~ & - & FCN \n\t\t\t\t\t\t& Interaction-Propagation  &PASCAL VOC 2012~  \\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t2018\n\t\t\t\t\t\t& BFVOS~ & CVPR & FCN \n\t\t\t\t\t\t& Pixel-wise Retrieval  &DAVIS$_{16}$~  \\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t2019\n\t\t\t\t\t\t&IVS~ &CVPR & FCN \n\t\t\t\t\t\t& Interaction-Propagation & DAVIS$_{17}$~+YouTube-VOS~\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multirow{3}{*}{{2020}}\n\t\t\t\t\t\t& MANet~ & CVPR & Siamese FCN \n\t\t\t\t\t\t& Interaction-Propagation & DAVIS$_{17}$~ \\\\\n\t\t\t\t\t\t&ATNet~ &ECCV & FCN \n\t\t\t\t\t\t&  Interaction-Propagation& SBD + DAVIS$_{17}$~+YouTube-VOS~\\\\\n\t\t\t\t\t\t& ScribbleBox~ & ECCV & GCN\n\t\t\t\t\t\t& Interaction-Propagation & COCO~ + ImageNet VID~  + YouTube-VOS~ \\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multirow{3}{*}{{2021}}\n\t\t\t\t\t\t& IVOS-W~ & CVPR & FCN + RL \n\t\t\t\t\t\t& Keyframe Selection & DAVIS$_{17}$~ \\\\\n\t\t\t\t\t\t& GIS~ & CVPR &  FCN \n\t\t\t\t\t\t& Interaction-Propagation & DAVIS$_{17}$~+YouTube-VOS~ \\\\\n\t\t\t\t\t\t& MiVOS~ & CVPR & Memory Network \n\t\t\t\t\t\t& Interaction-Propagation & BL30K~+DAVIS$_{17}$~ + YouTube-VOS~\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\\end{tabular}}\n\t\t\t\t\\vfill\n\t\t\t\t\\vspace{-8pt}\n\t\t\t\\end{threeparttable}\n\t\t\\end{minipage}\n\t\\end{minipage}\n\t\\vspace*{-20pt}\n\\end{figure*}\n\\noindent$\\bullet$$_{\\!}$~\\textbf{Matching-based$_{\\!}$ Methods.$_{\\!}$} \\red{This type of methods, might the most promising SVOS solution so far, constructs an embedding space to memorize the initial object embeddings, and classifies each pixel's label according to their similarities to the target object in the embedding space.} Thus the initial object appearance is explicitly modeled, and test-time fine-tuning is not needed. The earliest effort in this direction can be tracked back to$_{\\!}$~. Inspired by the advance in visual tracking~, Yoon \\etal~ proposed a Siamese network to perform pixel-level matching between the first frame and\\\\ upcoming frames. Later,  proposed to learn an embedding space from the first-frame supervision and pose VOS as a task of pixel retrieval: pixels are simply their respective nearest neighbors in the learned embedding space. The idea of~ is also explored in~, while it computes two ma-\\\\ tching maps for each upcoming frame, with respect to the foreground and background annotated in the first frame.~In , pixel-level similarities computed from the first frame and from the previous frame are used as a guide to segment succeeding frames. {Later, many matching-based solutions were proposed~, perhaps most\nnotably Oh~\\etal, who propose a space-time memory (STM) model to explicitly store previously computed segmentation information in an external memory~.} The memory facilitates learning the evolution of objects over time and allows for comprehensive use of past segmentation cues even over long period of time. Almost all current top-leading SVOS solutions~ are built upon STM; they improve the target adaption ability~, incorporate local temporal continuity~, explore instance-aware cues~, and develop more efficient memory designs~. Recently,  introduced a Transformer~ based model, which performs matching-like computation through attending over a\nhistory of multiple frames. In general, matching-based solutions enjoy the advantage of flexible and differentiable model design as well as long-term correspondence modeling.  On the\nother hand, feature matching relies on a powerful and generic feature embedding, which may limit its performance in challenging scenarios.\nIt is also worth mentioning that, as an effective technique for target-specific model learning, online learning is applied by many propagation~ and matching~ methods to boost performance.\n\\noindent$\\bullet$~\\textbf{Box-initialization based Methods.} As pixel-wise annotations are time-consuming or even impractical to acquire in realistic scenes, {some work has considered the situation where the first-frame annotation is provided in the form of a bounding box.  Specifically,} in~, Siamese trackers~are augmented with a mask prediction branch. In~, reinforcement learning is introduced to make decisions for target updating and matching. Later, in~, an outside memory is utilized to build a stronger Siamese track-segmenter.\n\\noindent$\\bullet$~\\textbf{Un-/Weakly-supervised based Methods.} To alleviate the demand for large-scale, pixel-wise annotated training samples, several un-/weakly-supervised learning-based SVOS solutions were recently developed. {They are typically built as$_{\\!}$ a$_{\\!}$ \\textit{reconstruction}$_{\\!}$ scheme$_{\\!}$ (\\ie,$_{\\!}$ each$_{\\!}$ pixel$_{\\!}$ from$_{\\!}$ a$_{\\!}$ `query' frame is reconstructed by finding and assembling related pixels from adjacent frame(s)) , and/or adopt a \\textit{cycle-consistent tracking} paradigm (\\ie, pixels/patches are encouraged to fall into the same location after one cycle of forward and backward tracking) .}\n\\noindent$\\bullet$~\\textbf{Other Specific Methods.} Other papers make specific contributions that deserve a separate look. In~, Zeng~\\etal extract mask proposals per frame and formulate the matching between object templates and proposals in a \\textit{differentiable} manner. Instead of using only the first frame annotation,  learns to select \\textit{the best frame} from the whole video for user interaction, so as to boost mask propagation. In~, Li~\\etal introduce a forward-backward data flow based cycle consistency mechanism to improve both traditional SVOS training and offline inference protocols, through mitigating the error propagation problem. To accelerate processing speed, a dynamic network~ is proposed to selectively allocate computation source for each frame according to the similarity to the previous frame.", "cites": [6721, 5637, 6711, 895, 6733, 9065, 6729, 6722, 2652, 6720, 6740, 6738, 6741, 6731, 2651, 6732, 486, 6719, 6742, 6692, 6735, 3819, 6706, 6737, 6724, 6716, 9068, 3806, 38, 6727, 6713, 6712, 6743, 6728, 5632, 6708, 6726, 9067, 6739, 6725, 5629, 6718, 3818, 6701, 6707, 5625, 6736, 6714, 6723, 6744, 8262, 6710, 6717, 8263, 5633, 9069, 9064, 520], "cite_extract_rate": 0.7631578947368421, "origin_cites_number": 76, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a structured analytical overview of SVOS methods, synthesizing key ideas across multiple papers and categorizing them into online fine-tuning, propagation-based, and matching-based approaches. It critically evaluates the limitations of each category, such as the inefficiency of fine-tuning and error accumulation in propagation. The section also abstracts these methods by identifying broader principles, such as the distinction between implicit and explicit encoding of target appearance, and highlights promising directions like memory-based and embedding space strategies."}}
{"id": "d4820182-4cc1-4796-aad2-52ca8c11e0cf", "title": "Interactive Video Object Segmentation (IVOS)", "level": "subsubsection", "subsections": [], "parent_id": "e8f77e8a-feaa-4ad7-9d91-c8bda372d9d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VOS Models"], ["subsubsection", "Interactive Video Object Segmentation (IVOS)"]], "content": "\\label{sec:3.1.3}\nAVOS, without any human involvement, loses flexibility in segmenting arbitrary objects of user interest. SVOS addi-  { setting has gained increasing attention.  Unlike classic models  requiring extensive and professional user intervention, recent deep learning-based IVOS solutions usually work with multiple rounds of scribble supervision, to minimize the user's effort.} In this scenario~, the user draws scribbles on a selected frame\nand an algorithm computes the segmentation maps for all video frames in a batch process. For refinement, user intervention and segmentation are repeated. This \\textit{round-based interaction}~ is useful~for consumer-level applications and rapid prototyping for professional usage, where efficiency is the main concern. {One can control the segmentation quality at the expense of time, as more rounds of interaction will provide better results.}\n\\noindent$\\bullet$~\\textbf{Interaction-propagation based Methods.} The majority of current studies~ follow an \\textit{interaction-propagation} scheme. \\red{In the preliminary attempt~, IVOS is achieved by a simple combination of two separate modules: an interactive image segmentation model~ for producing segmentation based on user annotations; and a SVOS model~ for propagating masks from the user-annotated frames to the others. Later,  devised a more compact solution, with also two modules for interaction and propagation, respectively.  However, the two modules are internally connected through intermediate feature exchanging, and also externally connected, \\ie, each of them is conditioned on the other's output. In~, a similar model design is also adopted, however, the propagation part is specifically designed to address both local mask tracking (over adjacent frames) and global propagation (among distant frames), respectively.} {However, these\ntechniques  have to start a new feed-forward computation in each interaction round, making them inefficient as the number of rounds grows.} A more efficient solution was developed in~. The  critical idea is to build a common encoder for discriminative pixel embedding learning, upon which two small network branches are added for interactive segmentation and mask propagation, respectively.  Thus the model extracts pixel embeddings for all frames only once (in the first round). In the following rounds, the feed-forward computation is only made within the two shallow branches.\n\\noindent$\\bullet$~\\textbf{Other Methods.}  Chen \\etal~ propose a pixel embedding learning-based model, applicable to both SVOS and IVOS. With a similar idea of~, IVOS is formulated as a pixel-wise retrieval problem, \\ie, transferring labels to each pixel according to its nearest reference pixel. This model supports different kinds of user input, such as masks, clicks and scribbles, and can provide immediate feedback after user interaction. In~, an interactive annotation tool is proposed for VOS. The annotation has two phases: annotating objects with tracked boxes, and labeling masks inside these tracks.  Box tracks are annotated efficiently by approximating the trajectory using a parametric curve with a small number of control points which the annotator can interactively correct. Segmentation masks are corrected via scribbles which are propagated through time. In~, a reinforcement learning framework is exploited to automatically determine the most valuable frame for interaction.\n\\begin{table}\n\t\\centering\n\\vspace{-5pt}\n\t\\caption{\n\t\t\\!\\!{\\textbf{Summary of characteristics for reviewed  LVOS methods} (\\S\\ref{sec:3.1.4}).}\\!\\!\\!\\!\n\t}\n\t\\vspace{-7pt}\n\t\\label{table:LVOS_methods}\n\t\\begin{threeparttable}\n\t\t\\resizebox{0.49\\textwidth}{!}{\n\t\t\t\\setlength\\tabcolsep{3pt}\n\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\\begin{tabular}{|c|r||c|c|c|c|c|c|}\n\t\t\t\t\\hline\\thickhline\n\t\t\t\t\\rowcolor{mygray}\n\t\t\t\tYear &Method~~~~  &Pub. &\\tabincell{c}{Visual + Language\\\\Encoder}  &Technical Feature & Training Dataset \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{{2018}}\n\t\t\t\t& A2DS~ & CVPR & I3D + CNN &   Dynamic Conv. &A2D Sentences~ \\\\\n\t\t\t\t& LangVOS~ & ACCV & CNN + CNN &   Cross-modal Att. &DAVIS$_{17}$-RVOS~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{1}{*}{{2019}}\n\t\t\t\t& AAN~ & ICCV & I3D + CNN &Cross-modal Att. & A2D Sentences~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{4}{*}{{2020}}\n\t\t\t\t& CDNet~ & AAAI & I3D +  GRU & Dynamic Conv. & A2D Sentences~ \\\\\n\t\t\t\t& PolarRPE~ & IJCAI & I3D + LSTM  & Dynamic Conv. & A2D Sentences~ \\\\\n\t\t\t\t& VT-Capsule~ & CVPR & I3D + CNN  & Capsule Routing & A2D Sentences~ \\\\\n\t\t\t\t& URVOS~ & ECCV & CNN + MLP  & Cross-modal Att. & Refer-YouTube-VOS~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{{2021}}\n\t\t\t\t& CST~ & CVPR & I3D + GRU  & Cross-modal Att. & A2D Sentences~ \\\\\n\t\t\t\t& CMSANet~ & PAMI & CNN + Word embed.  & Cross-modal Att. & A2D Sentences~ \\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{threeparttable}\n\t\\vspace{-12pt}\n\\end{table}\n\\begin{table*}\n\t\\vspace{2pt}\n\t\\centering\n\t\\caption{\n\t\t{\\textbf{Summary of essential characteristics for reviewed VSS methods} (\\S\\ref{sec:3.2}). \\textbf{Flow} indicates whether optical flow is used.}\n\t}\n\t\\vspace{-5pt}\n\t\\label{table:VSS_methods}\n\t\\begin{threeparttable}\n\t\t\\resizebox{0.94\\textwidth}{!}{\n\t\t\t\\setlength\\tabcolsep{4pt}\n\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\\begin{tabular}{|c|r||c|c|c|c|c|c|c|}\n\t\t\t\t\\hline\\thickhline\n\t\t\t\t\\rowcolor{mygray}\n\t\t\t\tYear &Method~~~~  &Pub. & Seg. Level  &Core Architecture \n& Flow &  Technical Feature & Training Dataset \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{3}{*}{\\rotatebox{90}{2016}}\n\t\t\t\t& Clockwork~ & ECCV & Semantic & FCN  \n&  \\checkmark& Faster Segmentation & Cityscapes~/YouTube-Objects~  \\\\\n\t\t\t\t& FSO~ & CVPR & Semantic& FCN + Dense CRF \n& \\checkmark & Temporal Feature Aggregation & Cityscapes~/CamVid~ \\\\\n\t\t\t\t& JFS~ & ECCV & Semantic& FCN \n& \\checkmark & Temporal Feature Aggregation & KITTI MOTS~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{\\rotatebox{90}{2017}}\n\t\t\t\t& BANet~ \t\t\t\t\t& CVPR & Semantic& FCN + LSTM \t\t\t\t\n& \t\t\t\t& Keyframe Selection \t\t&  CamVid~/KITTI\\\\\n\t\t\t\t& PEARL~                 & ICCV  & Semantic& FCN                \n&\t\\checkmark\t\t & Flow-guided Feature Aggregation  \t\t\t&  Cityscapes~/CamVid~\\\\\n\t\t\t\t& NetWarp~ & ICCV & Semantic& Siamese FCN  \n& \t\\checkmark\t\t& Flow-guided Feature Aggregation \t\t\t&  Cityscapes~/CamVid~\\\\\n\t\t\t\t& DFF~ \t\t\t\t\t& ICCV & Semantic& FCN  \t\t\t\t\n& \t\t\t\t& Flow-guided Feature Aggregation\t&  Cityscapes~\\\\\n\t\t\t\t& BBF~ & ICCV & Semantic& Two-Stream FCN \n& \\checkmark & Weakly-Supervised Learning & Cityscapes~/CamVid~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{\\rotatebox{90}{2018}}\n\t\t\t\t& GRFP~ \t\t& CVPR & Semantic& FCN + GRU \t\t\n&\\checkmark & Temporal Feature Aggregation \t\t&Cityscapes~/CamVid~ \\\\\n\t\t\t\t& LVS~  \t\t\t\t\t\t\t& CVPR & Semantic& FCN \t\t\t\t\n& \t\t\t\t\t& Keyframe Selection \t\t&Cityscapes~/CamVid~\\\\\n\t\t\t\t& DVSN~  \t\t\t\t& CVPR & Semantic& FCN+RL \t\t\n& \\checkmark & Keyframe Selection  \t\t&Cityscapes~\\\\\n\t\t\t\t& EUVS~ \t\t\t& ECCV & Semantic& Bayesian CNN \n& \\checkmark & Flow-guided Feature Aggregation & CamVid~ \\\\\n\t\t\t\t& GCRF~ \t\t\t& CVPR & Semantic& FCN+CRF \n & \\checkmark & Gaussian CRF & CamVid~ \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{4}{*}{\\rotatebox{90}{2019}}\n\t\t\t\t& Accel~ \t\t\t\t& CVPR \t\t& Semantic& FCN \t\t\t\t\n&\\checkmark   & Keyframe Selection\t\t\t&  KITTI\\\\\n\t\t\t\t& SSeg~ \t\t\t\t& CVPR \t\t& Semantic& FCN \t\t\t\t\n&   & Weakly-Supervised Learning&  Cityscapes~/CamVid~ \\\\\n\t\t\t\t& MOTS~ \t\t\t\t& CVPR \t\t& Instance& Mask R-CNN \t\t\t\t\n&   & \tTracking by Detection\t&  KITTI MOTS~ /MOTSChallenge~\\\\\n\t\t\t\t& MaskTrack R-CNN~ & ICCV & Instance& Mask R-CNN \n& \t\t\t\t\t\t& Tracking by Detection\t\t\t\t& YouTube-VIS~\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{10}{*}{\\rotatebox{90}{2020}}\n\t\t\t\t& EFC~\t\t\t\t\t& AAAI \t\t& Semantic& FCN \t\t\t\t\n& \\checkmark\t&Temporal Feature Aggregation \t\t\t& Cityscapes~/CamVid~\\\\\n\t\t\t\t& TDNet~\t\t\t& CVPR \t& Semantic\t& Memory Network \n&\t\t\t\t& Attention-based Feature Aggregation \t\t&  Cityscapes~/CamVid~/NYUDv2~\\\\\n\t\t\t\t& MaskProp~& CVPR & Instance & Mask R-CNN \n& & Instance Feature Propagation &  YouTube-VIS~\\\\\n\t\t\t\t& VPS~ \t\t& CVPR & Panoptic& Mask R-CNN \n& & Spatio-Temporal Feature Alignment &VIPER-VPS~/Cityscapes-VPS~ \\\\\n\t\t\t\t& MOTSNet~ \t\t& CVPR & Instance& Mask R-CNN\n& & Unsupervised Learning & KITTI MOTS~ /BDD100K~ \\\\\n\t\t\t\t& MVAE~ \t\t& CVPR & Instance& Mask R-CNN+VAE \n  & & Variational Inference  & KITTI MOTS~ /YouTube-VIS~ \\\\\n\t\t\t\t& ETC~\t\t\t\t& ECCV & Semantic& FCN + KD \n& \\checkmark& Knowledge Distillation & Cityscapes~/CamVid~\\\\\n\t\t\t\t& Sipmask~\t\t\t& ECCV & Instance& FCOS \n  & & Single-Stage Segmentation & YouTube-VIS~ \\\\\n\t\t\t\t& STEm-Seg~\t\t& ECCV & Instance& FCN \n&& Spatio-Temporal Embedding Learning & DAVIS$_{17}$~/YouTube-VIS~/KITTI-MOTS~  \\\\\n\t\t\t\t& Naive-Student~\t\t& ECCV & Semantic& FCN+KD \n&& Semi-Supervised Learning & Cityscapes~ \\\\\t\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{11}{*}{\\rotatebox{90}{2021}}\n\t\t\t\t& CompFeat~& AAAI& Instance & Mask R-CNN \n& & Spatio-Temporal Feature Alignment & YouTube-VIS~\\\\\n\t\t\t\t& TraDeS~ & CVPR & Instance & Siamese FCN \n& & Tracking by Detection & MOT/nuScenes/KITTI MOTS~ /YouTube-VIS~\\\\\n\t\t\t\t& SG-Net~ & CVPR & Instance & FCOS \n& & Single-Stage Segmentation & YouTube-VIS~\\\\\n\t\t\t\t& VisTR~ & CVPR & Instance & Transformer \n& & Transformer-based Segmentation & YouTube-VIS~\\\\\n\t\t\t\t& SSDE~ & CVPR & Semantic & FCN \n& & Semi-Supervised Learning & Cityscapes~ \\\\\n\t\t\t\t& SiamTrack~ & CVPR & Panoptic& Siamese FCN \n& & Supervised Contrastive Learning & VIPER-VPS~/Cityscapes-VPS~ \\\\\n\t\t\t\t& ViP-DeepLab~ & CVPR & Panoptic& FCN \n& & Depth-Aware Panoptic Segmentation & Cityscapes-VPS~  \\\\\n\t\t\t\t& fIRN~ & CVPR & Instance& Mask R-CNN \n& \\checkmark & Weakly-Supervised Learning & YouTube-VIS~/Cityscapes~ \\\\\n\t\t\t\t& SemiTrack~ & CVPR& Instance & SOLO \n&  & Semi-Supervised Learning & YouTube-VIS~/Cityscapes~ \\\\\n& Propose-Reduce~& ICCV & Instance  &  Mask R-CNN  \n& & Propose and Reduce & DAVIS$_{17}$~/YouTube-VIS~\\\\\n\t& {CrossVIS}~ & {ICCV} & {Instance} & {FCN} & & {Dynamic Convolution} &  {YouTube-VIS~/OVIS~} \\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{threeparttable}\n\t\\vspace{-8pt}\n\\end{table*}\n\t\\vspace{-5pt}", "cites": [5637, 6745, 6756, 6765, 6770, 6738, 6741, 6749, 6768, 6767, 8838, 3807, 6754, 6751, 5798, 6742, 6772, 6771, 6766, 6750, 6748, 6759, 6752, 1733, 3806, 6774, 6755, 6743, 8219, 9067, 6739, 6757, 6753, 778, 6758, 5625, 6746, 6769, 6760, 6763, 6744, 4169, 6773, 6761, 6747, 5799, 6764, 6762, 9069, 3799], "cite_extract_rate": 0.7285714285714285, "origin_cites_number": 70, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a coherent narrative on interactive video object segmentation (IVOS), discussing different methodologies like interaction-propagation based models and others. It critically evaluates the limitations of prior approaches, such as inefficiency in multiple interaction rounds. While it identifies patterns like the use of scribbles and reinforcement learning for interaction, it could offer more abstracted principles or broader theoretical insights for a higher abstraction score."}}
{"id": "8073e711-f9e6-4e60-a22c-3ad16756be48", "title": "Language-guided$_{\\!", "level": "subsubsection", "subsections": [], "parent_id": "e8f77e8a-feaa-4ad7-9d91-c8bda372d9d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VOS Models"], ["subsubsection", "Language-guided$_{\\!"]], "content": "$ Video$_{\\!}$ Object$_{\\!}$ Segmentation$_{\\!}$ (LVOS)$_{\\!\\!\\!}$}\\label{sec:3.1.4}\nLVOS is an emerging area, dating back to 2018~. Although there have already existed some efforts~ in the intersection of language and video understanding, none of them addresses pixel-level video-language reasoning. Most efforts in LVOS are made around the theme of efficient alignment between visual and linguistic modalities. According to the multi-modal information fusion strategy, existing models can be divided into three groups.\n\\noindent$\\bullet$~\\textbf{Dynamic Convolution-based Methods.} The first initiate was proposed in\\!~ that applies dynamic networks\\!~ for visual-language relation modeling. Specifically, convolution filters, dynamically generated from linguistic query, are used to adaptively transform visual features into desired segments. In the same line of work,  incorporate spatial context into filter generation. However, as indicated by~, {linguistic variation of input description may greatly impact sentence representation and subsequently make dynamic filters unstable, causing inaccurate segmentation.} For example, ``car in blue is parked on the grass'' and ``blue car standing on the grass'' have the same meaning but different generated filters, leading to poor performance.\n\\noindent$\\bullet$~\\textbf{Capsule Routing-based Methods.} In~, both video and textual inputs are encoded through capsules~, which are considered effective in modeling visual/textual entities. Then,  dynamic routing is applied over the video and text capsules  for visual-textual information integration.\n\\noindent$\\bullet$~\\textbf{Attention-based Methods.} Neural attention technique is\\\\ also widely adopted in the filed of LVOS$_{\\!}$~, for fully capturing global visual/textual context. \\red{In~, vision-guided language attention and language-guided vision attention were developed to capture visual-textual correlations. In~, two different attentions are learned to ground spatial and temporal relevant linguistic cues to static and dynamic visual embeddings, respectively. \n}\n\t\\vspace{-15pt}", "cites": [9070, 6753, 6766, 6775, 6764, 6765, 6776], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers by grouping them into three methodological categories (Dynamic Convolution, Capsule Routing, and Attention-based), which provides a structured narrative. It includes some critical analysis, such as pointing out the instability of dynamic filters due to linguistic variation. The abstraction level is moderate, with the identification of common themes in visual-linguistic alignment but not fully moving to overarching theoretical principles."}}
{"id": "46dd768e-f3e3-44e5-9063-9ec5caf1a997", "title": "$_{\\!\\!\\!", "level": "subsubsection", "subsections": [], "parent_id": "dc07ce5f-79a0-4c05-9868-2a3c21ee8891", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VSS Models"], ["subsubsection", "$_{\\!\\!\\!"]], "content": "$(Instance-agnostic)$_{\\!}$  Video$_{\\!}$ Semantic$_{\\!}$ Segmentation$_{\\!}$ (VSS)$_{\\!\\!\\!\\!\\!\\!\\!}$}\\label{sec:vss}\nExtending the success of deep learning-based image semantic segmentation techniques to the video domain has become a research focus in computer vision recently. To achieve this, the most straightforward strategy is the na\\\"{i}ve application of an image semantic segmentation model in a frame-by-frame manner. But this strategy completely ignores temporal continuity and coherence cues  provided in videos. To make better use of temporal information, research efforts in this field are mainly made along two lines.\n\\noindent$\\bullet$~\\textbf{Efforts towards More Accurate Segmentation.} A major stream of methods exploits cross-frame relations to boost the prediction accuracy. They typically first apply the very same segmentation algorithms to each frame independently. Then they add extra modules on top, \\eg, optical flow-guided feature aggregation~, and sequential network based temporal information propagation~, to gather multi-frame context and get better results. For example, in some pioneer work~, after performing static semantic segmentation for each frame individually, optical$_{\\!}$ flow$_{\\!}$~$_{\\!}$ or$_{\\!}$ 3D$_{\\!}$ CRF$_{\\!}$~$_{\\!}$ based$_{\\!}$ post$_{\\!}$ processing is applied for gaining temporally consistent segments. Later,  jointly learns CNN-based per-frame segmentation and CRF-based spatio-temporal reasoning. In~, features warped from previous frames with optical flow are combined with the current frame features for prediction. These methods require additional feature aggregation modules, which increase the computational costs during the inference. Recently,  proposes to only incorporate flow-guided temporal consistency into the training phase, without bringing any extra inference cost. But its processing speed is still bounded to the adopted per-frame segmentation algorithms, as all\nfeatures must be recomputed at each frame. {For these methods, the utility in time-sensitive application areas, such as mobile and autonomous driving, is limited.}\n\\noindent$\\bullet$~\\textbf{Efforts towards Faster Segmentation.}  Yet another complementary line of work tries to leverage temporal information to accelerate computation. They approximate the expensive per-frame forward pass with cheaper alternatives, \\ie, reusing the features in  neighbouring frames. In~, parts of segmentation networks are adaptively executed across frames, thus reducing the computation cost. Later methods use keyframes to avoid processing of each frame, and then propagate the outputs or the feature maps to other frames. For instance,~ employs optical flow to warp the features between the keyframe and non-key frames. Adaptive keyframe selection is later exploited in~, further enhanced by adaptive feature propagation~. In~, Jain \\etal use a large, strong model to predict the keyframe and use a compact one in non-key frames. Keyframe-based methods have different computational\nloads between keyframes and non-key frames, causing high maximum latency and unbalanced occupation of\ncomputation resources that may decrease system efficiency~. Additionally,  the spatial misalignment of other frames with respect to the keyframes is challenging to compensate for and\noften leads to different quantity results between keyframes and non-key frames. In~, a temporal consistency guided knowledge distillation technique is proposed to train a compact network, which is applied to all frames. In~, several weight-sharing sub-networks are distributed over sequential frames, whose extracted shallow features are composed for final segmentation. This trend of methods indeed speeds up inference, but still with the cost of reduced accuracy.\n\\noindent$\\bullet$~\\textbf{Semi-/Weakly-supervised based Methods.} Away from these main battlefields, some researchers made efforts to learn VSS under annotation efficient settings. In~, classifier heatmaps are used to learn VSS from image tags~only.  use both labeled and unlabeled video frames to learn VSS. They propagate annotations from labeled frames to other unlabeled, neighboring frames~, or alternatively train teacher and student networks with groundtruth annotations and iteratively generated pseudo labels~.", "cites": [6745, 6770, 3807, 6754, 6751, 6759, 6755, 6774, 8219, 778, 6758, 6769, 4169, 6747, 6762], "cite_extract_rate": 0.7894736842105263, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers into a coherent narrative by categorizing methods into three distinct lines of research (accuracy, speed, and annotation efficiency). It provides critical analysis by highlighting trade-offs, limitations, and performance implications of each approach. The abstraction is strong in identifying broader trends like the use of optical flow and keyframes, though deeper meta-level insights could be further emphasized."}}
{"id": "cdff8a04-7c92-40d0-b506-da936cb8ad05", "title": "{Video Instance Segmentation (VIS)", "level": "subsubsection", "subsections": [], "parent_id": "dc07ce5f-79a0-4c05-9868-2a3c21ee8891", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VSS Models"], ["subsubsection", "{Video Instance Segmentation (VIS)"]], "content": "}\\label{sec:vis}\n In 2019, Yang \\etal extended image instance segmentation to the video domain~, which requires simultaneous detection, segmentation and tracking of instances in videos. This task is also known as \\textit{multi-object tracking\nand segmentation} (MOTS)~. Based on the patterns of generating instance sequences, existing frameworks can be\nroughly categorized into four paradigms: i) \\textit{track-detect}, ii) \\textit{clip-match}, iii) \\textit{propose-reduce}, iv) \\textit{segment-as-a-whole}. \\red{Track-detect methods detect and segment instances for each individual frame, {followed by frame-by-frame instance tracking}~. For example, in~, Mask R-CNN~ is adapted for\nVIS/MOTS by adding a tracking branch for cross-frame instance association. Alternatively,  models spatial attention to describe instances, tackling the task from a novel single-stage yet elegant perspective. Clip-match methods  divide an entire video into multiple  overlapped clips, and\nperform VIS independently for each clip through mask propagation~  or spatial-temporal embedding~. Final instance sequences are generated by  merging neighboring clips.} Both of the two paradigms need two independent steps to generate a complete sequence. They both generate multiple incomplete sequences (\\ie, frames or clips) from a video, and merge (or complete) them by tracking/matching at the second stage. Intuitively, these paradigms are vulnerable to error accumulation in the process of merging sequences, especially when occlusion or fast motion exists. \\red{To address these limitations, a propose-reduce paradigm is proposed in~. It first samples several key frames and obtains instance sequences by propagating the instance segmentation results from each key frame to the entire video. Then, the redundant sequence proposals of the same instances are removed.} {This paradigm not\nonly discards the step of merging incomplete sequences, but also achieves robust results considering multiple key frames. However, these three types of methods still need complex heuristic rules to associate instances and/or multiple steps to generate instance sequences.} \\red{The segment-as-a-whole paradigm~ is more elegant; it poses the task as a direct sequence prediction problem using Transformer~.}\nAlmost all VIS models are built upon fully supervised learning, while  are the exceptions. Specifically, in~, motion and temporal consistency cues are leveraged to generate pseudo-labels from tag labeled videos for weakly supervised VIS learning. In , a semi-supervised  embedding learning approach is proposed to learn VIS from pixel-wise annotated images and unlabeled videos.\n\\vspace{-4pt}", "cites": [6768, 6767, 6760, 8838, 520, 5798, 6761, 6772, 38, 6771, 6757, 6756, 5799, 6777, 6750, 6778, 6746], "cite_extract_rate": 0.9444444444444444, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to categorize and explain different paradigms in Video Instance Segmentation (VIS), establishing a coherent narrative. It provides critical insights by highlighting limitations such as error accumulation and reliance on complex heuristics. The abstraction level is strong, as it generalizes the VIS approaches into four distinct paradigms and identifies trends in model design and supervision levels."}}
{"id": "5cf8144b-b322-4d03-b52d-c80388bdb0e8", "title": "Video Panoptic Segmentation (VPS)", "level": "subsubsection", "subsections": [], "parent_id": "dc07ce5f-79a0-4c05-9868-2a3c21ee8891", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Deep Learning-based Video Segmentation"], ["subsection", "Deep Learning-based VSS Models"], ["subsubsection", "Video Panoptic Segmentation (VPS)"]], "content": "\\label{sec:vps}\nVery recently, Kim \\etal extended image panoptic segmentation to the video domain~, which aims at a holistic segmentation of all foreground instance tracklets and background regions, and assigning a semantic label to each video pixel. They adapt an image panoptic segmentation model~ for VPS, by adding two modules for temporal feature fusion and cross-frame instance association, respectively. Later, temporal correspondence was explored in~ through learning coarse segment-level and fine pixel-level matching. Qiao \\etal~ propose to learn monocular depth estimation and video panoptic segmentation jointly.\n\\begin{figure}[t]\n  \\centering\n      \\includegraphics[width=0.99\\linewidth]{figs/dataset}\n       \\put(-247,87.5){\\tiny Youtube-Objects~}\n      \\put(-192,87.5){\\tiny FBMS$_{59}\\!$~}\n      \\put(-147.5,87.5){\\tiny DAVIS$_{16}\\!$~}\n      \\put(-94,87.5){\\tiny DAVIS$_{17}\\!$~}\n      \\put(-47,87.5){\\tiny YouTube-VOS~}\n      \\put(-245,55){\\tiny A2D Sentence~}\n      \\put(-192,55){\\tiny J-HMDB-S~}\n      \\put(-150,55){\\tiny DAVIS$_{17}$-RVOS~}\n      \\put(-97,55){\\tiny Refer-Youtube-VOS\\!~}\n      \\put(-36,55){\\tiny CamVid~}\n      \\put(-244,30){\\tiny {CityScapes}~}\n      \\put(-201,30){\\tiny NYUDv2\\!~}\n      \\put(-159,30){\\tiny VSPW~}\n      \\put(-121,30){\\tiny YouTube-VIS~}\n      \\put(-60,30){\\tiny KITTI MOTS~}\n      \\put(-250,1.9){\\tiny {MOTSChallenge}\\!~}\n      \\put(-193.5,1.9){\\tiny {BDD100K}~}\n      \\put(-140,1.9){\\tiny {OVIS}~}\n      \\put(-98,1.9){\\tiny {VIPER-VPS}~ }\n      \\put(-50,1.9){\\tiny Cityscapes-VPS~}\n      \t\\vspace{-10pt}\n\\caption{{Example frames from twenty famous video segmentation benchmark datasets. The ground-truth segmentation annotation is overlaid.}}\n     \t\\vspace{-10pt}\n\\label{fig:overviewdataset}\n\\end{figure}\n\\begin{table*}\n\\centering\n\\caption{\\textbf{Statistics of representative video segmentation datasets.}\n        See \\S\\ref{sec:4.1} and \\S\\ref{sec:4.2} for more detailed descriptions.}\n\\vspace{-7pt}\n\\begin{threeparttable}\n\\resizebox{0.99\\textwidth}{!}{\n\\setlength\\tabcolsep{8pt}\n\\renewcommand\\arraystretch{1.0}\n\\begin{tabular}{|r||c|c|c|c|c|c|c|c|}\n\\hline\\thickhline\n\\rowcolor{mygray}\nDataset~~~~~~~~~&Year &Pub. &\\#Video &\\#Train/Val/Test/Dev &Annotation &Purpose &\\#Class &Synthetic\\\\%&C\n\\hline\n\\hline\nYoutube-Objects~ &2012 &CVPR  &1,407\n(126) &-/-/-/- &Object-level AVOS,  SVOS &Generic &10&\\\\%&\\checkmark\n\\rowcolor{mygray2}\nFBMS$_{59}$~\\!~~ &2014 &PAMI &59 &29/30/-/- &Object-level AVOS,  SVOS &Generic&-&\\\\%&\\checkmark\nDAVIS$_{16}$~\\!~~ &2016 &CVPR &50 &30/20/-/- &Object-level AVOS,  SVOS &Generic&-&\\\\\n\\rowcolor{mygray2}\nDAVIS$_{17}$~ &2017 &- &150 &60/30/30/30 &Instance-level\nAVOS, SVOS, IVOS &Generic&-&\\\\\nYouTube-VOS~ &2018 &- &4,519 &3,471/507/541/- &SVOS &Generic &94&\\\\\n\\rowcolor{mygray2}\nA2D Sentence~ &2018 &CVPR &3,782 &3,017/737/-/- &LVOS &Human-centric&-&\\\\\nJ-HMDB Sentence~&2018 &CVPR  &928 &-/-/-/- &LVOS &Human-centric&-&\\\\%&\\checkmark\n\\rowcolor{mygray2}\nDAVIS$_{17}$-RVOS~ &2018 &ACCV  &90 &60/30/-/- &LVOS &Generic&-&\\\\\nRefer-Youtube-VOS~ &2020 &ECCV  &3,975 &3,471/507/-/- &LVOS &Generic&-&\\\\\n\\rowcolor{mygray2}\nCamVid~  &2009 &PRL    &4  &(frame: 467/100/233/-) &VSS &Urban&11&\\\\%&\\checkmark\n{CityScapes}~   &2016  &CVPR    &5,000  &2,975/500/1,525 &VSS&Urban&19&\\\\\n\\rowcolor{mygray2}\nNYUDv2~ &2012 &ECCV &518 &(frame: 795/654/-/-) &VSS &Indoor&40&\\\\\nVSPW~ &2021 &CVPR &3,536 &2,806/343/387/- &VSS &Generic&124&\\\\\n\\rowcolor{mygray2}\nYouTube-VIS~ &2019  &ICCV &3,859 &2,985/421/453/- &VIS &Generic&40&\\\\%$\\max(w)\\!=\\!3888,\\max(h)\\!=\\!3264,\\min(w)\\!=\\!135,\\min(h)\\!=\\!120$\nKITTI MOTS~ &2019 &CVPR &21 &12/9/-/- &VIS &Urban &2&\\\\ \n\\rowcolor{mygray2}\n{MOTSChallenge}~ &2019 &CVPR &4 &-/-/-/- &VIS &Urban &1&\\\\\n{BDD100K}~ &2020 &ECCV &100,000 &7,000/1,000/2,000/- &VSS, VIS &Driving &40 (VSS), 8 (VIS)&\\\\ \n\\rowcolor{mygray2}\n{OVIS}~ &2021 &- &901 & 607/140/154/-  &VIS &Generic&25&\\\\\n{VIPER-VPS}~ &2020 &CVPR &124 &(frame: 134K/50K/70K/-) &VPS &Urban&23 &\\checkmark\\\\\n\\rowcolor{mygray2}\nCityscapes-VPS~&2020 &CVPR & 500 &400/100/-/- &VPS &Urban&19&\\\\\n\\hline\n\\end{tabular}\n}\n\\end{threeparttable}\n\\label{table:dataset}\n\\vspace{-10pt}\n\\end{table*}\n\\vspace{-7pt}", "cites": [5637, 6692, 6772, 6779, 6748, 1733, 6753, 6763, 5799, 6764, 6749], "cite_extract_rate": 0.55, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the concept of Video Panoptic Segmentation and briefly mentions a few related papers. While it introduces the task and some representative methods, it lacks deep synthesis, critical evaluation, or abstraction to broader principles. The inclusion of a dataset figure and table suggests a descriptive rather than analytical approach."}}
{"id": "66c755ef-84c1-4a33-a25c-892fac82a0b3", "title": "AVOS/SVOS/IVOS Datasets", "level": "subsubsection", "subsections": [], "parent_id": "6806158b-933c-483f-9339-2d38de61a72f", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Video Segmentation Datasets"], ["subsection", "VOS Datasets"], ["subsubsection", "AVOS/SVOS/IVOS Datasets"]], "content": "\\noindent$\\bullet$~\\textbf{Youtube-Objects}  is a large dataset of $1,\\!407$ videos collected from 155 web videos belonging to 10 object\ncategories (\\eg, dog, cat, plane, \\etc). VOS models typically test the generalization ability on a subset~ having totally 126 shots with $20,\\!647$ frames that provides coarse pixel-level fore-/background annotations on every 10$^{th}$ frames. \n\\noindent$\\bullet$~\\textbf{FBMS$_{59}$}\\!~ consists of  59 video sequences with $13,\\!860$ frames in total. However, only 720 frames are annotated for fore-/background separation. The dataset is split into 29 and 30 sequences for training and evaluation, respectively. \n\\noindent$\\bullet$~\\textbf{DAVIS$_{16}$}\\!~ has 50 videos (30 for \\textit{train} set and 20 for \\textit{val}\nset) with $3,\\!455$ frames in total. For each frame, in addition to high-quality fore-/background segmentation annotation, a set of attributes (\\eg, deformation, occlusion, motion blur, \\etc) are also provided to highlight the main challenges.\n\\noindent$\\bullet$~\\textbf{DAVIS$_{17}$}\\!~ contains 150 videos, \\ie,  60/30/30/30 videos for \\textit{train}/\\textit{val}/\\textit{test-dev}/\\textit{test-challenge} sets. Its \\textit{train} and \\textit{val} sets are extended from the respective sets in DAVIS$_{16}$. There are 10,459 frames in total. DAVIS$_{17}$ provides instance-level annotations to support SVOS. Then, DAVIS$_{18}$ challenge\\!~ provides scribble annotations to support IVOS. Moreover, as the original annotations of DAVIS$_{17}$ are biased towards the SVOS scenario, DAVIS$_{19}$ challenge\\!~ re-annotates \\textit{val} and \\textit{test-dev} sets of DAVIS$_{17}$ to support AVOS. \n\\noindent$\\bullet$~\\textbf{YouTube-VOS}\\!~ is a large-scale dataset, which is split into a  \\textit{train} ($3,\\!471$ videos), \\textit{val} (507 videos), and \\textit{test} (541 videos) set, in its newest 2019 version. Instance-level precise annotations are provided every five frames in a 30FPS frame rate. There are 94 object categories  (\\eg, person, snake, \\etc) in total, of which 26 are unseen in \\textit{train} set.\n\\noindent \\textbf{Remark.} Youtube-Objects, FBMS$_{59}$ and DAVIS$_{16}$ are used\n\\noindent for instance-agnostic AVOS and SVOS evaluation. DAVIS$_{17}$ is unique in comprehensive annotations for instance-level AVOS, SVOS as well as IVOS, but its scale is relatively small. YouTube-VOS is the largest one but only supports SVOS benchmarking. There also exist some other VOS datasets, such as SegTrack$_{V1}$~ and SegTrack$_{V2}$~, but they were less used recently, due to the limited scale and difficulty.", "cites": [5637, 6692, 6730], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of various VOS datasets, including their sizes, annotations, and use in different segmentation tasks. While it integrates basic information from the cited papers, it lacks deeper synthesis or a novel framework. It includes some minimal comparison (e.g., YouTube-VOS being the largest but only supporting SVOS), but primarily summarizes rather than critically analyzing or identifying broader trends in dataset design or limitations."}}
{"id": "afffa9ed-1337-476b-a98a-9166b7fb9387", "title": "LVOS Datasets", "level": "subsubsection", "subsections": [], "parent_id": "6806158b-933c-483f-9339-2d38de61a72f", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Video Segmentation Datasets"], ["subsection", "VOS Datasets"], ["subsubsection", "LVOS Datasets"]], "content": "\\noindent$\\bullet$~\\textbf{A2D Sentence}\\!~ augments A2D~ with phrases. It contains $3,\\!782$ videos, with $8$ action classes performed by $7$ actors. In each video,  $3$ to $5$ frames are provided with segmentation masks. It contains $6,\\!655$ sentences describing actors and their actions. The dataset is split into $3,\\!017$/$737$ for \\texttt{train}/\\texttt{test}, and $28$ unlabeled videos are ignored~.\n\\noindent$\\bullet$~\\textbf{J-HMDB Sentence}\\!~ is built upon J-HMDB . It  is comprised of $928$ short videos with $928$ corresponding sentences describing $21$ different action categories.\n\\noindent$\\bullet$~\\textbf{DAVIS$_{17}$-RVOS}\\!~ extends DAVIS$_{17}$ by collecting referring expressions for the annotated objects. 90 videos from \\texttt{train} and \\texttt{val} sets are annotated with more than 1,500 referring expressions. They provide two  types of annotations, which describe the highlighted\nobject: 1) based on the entire video (\\ie, full-video expression) and 2) using only  the first frame of the video (\\ie, first-frame expression).\n\\noindent$\\bullet$~\\textbf{Refer-Youtube-VOS}\\!~ includes  3,975 videos from {YouTube-VOS}\\!~,  with 27,899 language descriptions of target objects.\nSimilar to DAVIS$_{17}$-RVOS~, both full-video and first-frame expression annotations  are provided.\n\\noindent \\textbf{Remark.} To date, {A2D Sentence} and {J-HMDB Sentence} are the main test-beds.  However, the phrases are not produced with the aim of reference, but description, and limited to only a few object categories corresponding to the dominant `actors' performing a salient `action'~. But newly introduced DAVIS$_{17}$-RVOS and Refer-Youtube-VOS show improved difficulties in both visual and linguistic modalities.\n\\vspace{-2pt}", "cites": [6692, 6753, 6764], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of LVOS datasets but lacks substantial synthesis of the cited papers. It mentions the purpose and features of each dataset without connecting them to broader themes or trends. The critical evaluation is minimal, and there is little abstraction or generalization to identify overarching principles in language-based video segmentation."}}
{"id": "d43e1f90-ccf9-49f4-9510-bc11ec6b9dfb", "title": "VSS Datasets", "level": "subsection", "subsections": [], "parent_id": "893a236a-80f8-41b7-a23c-6c86f755158d", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Video Segmentation Datasets"], ["subsection", "VSS Datasets"]], "content": "\\label{sec:4.2}\n\\noindent$\\bullet$~\\textbf{CamVid}\\!~ is composed of 4 urban scene videos with 11-class pixelwise annotations. Each video is annotated every 30 frames.  The\nannotated frames are usually grouped into 467/100/233 for \\texttt{train}/\\texttt{val}/\\texttt{test}~.\n\\noindent$\\bullet$~\\textbf{CityScapes}$_{\\!~}$~$_{\\!~}$ is$_{\\!~}$ a$_{\\!~}$ large-scale$_{\\!~}$ VSS$_{\\!~}$ dataset$_{\\!~}$ for$_{\\!~}$ street\n\\noindent  views. It has\n2,975/500/1,525 snippets for \\texttt{train}/\\texttt{val}/ \\texttt{test}, captured at 17FPS. Each snippet contains 30 frames, and only the 20$^{th}$ frame is densely labelled with 19 semantic classes. 20,000 coarsely annotated frames are also provided.\n\\noindent$\\bullet$~\\textbf{NYUDv2}\\!~ contains 518 indoor RGB-D videos with  high-quality ground-truths (every 10$^{th}$  video frame is labeled). There are 795 training frames and 654 testing frames being rectified and annotated with 40-class semantic labels.\n\\noindent$\\bullet$~\\textbf{VSPW}\\!~ is a recently  proposed large-scale VSS dataset. It addresses video scene parsing in the wild by considering diverse scenarios.  It consists of 3,536 videos, and provides  pixel-level annotations for 124 categories at 15FPS. The \\texttt{train}/\\texttt{val}/\\texttt{test} sets contain 2,806/343/387 videos with 198,244/24,502/28,887 frames, respectively.\n\\noindent$\\bullet$~\\textbf{YouTube-VIS}\\!~ is built upon YouTube-VOS~ with instance-level annotations. Its newest 2021 version has 3,859 videos (2,985/421/453 for \\texttt{train}/\\texttt{val}/\\texttt{test}) with 40 semantic categories. It provides 232K high-quality annotations for 8,171 unique video instances.\n\\noindent$\\bullet$~\\textbf{KITTI MOTS}\\!~ extends the 21 training sequences of KITTI tracking dataset~ with VIS annotations -- 12 for training and 9 for validation, respectively.  The dataset contains 8,008 frames with a resolution of $375\\times1242$, 26,899 annotated cars and 11,420 annotated pedestrians.\n\\noindent$\\bullet$~\\textbf{MOTSChallenge}\\!~ annotates 4 of 7 training sequences of MOTChallenge$_{2017}$~. It has 2,862 frames with 26,894 annotated pedestrians and presents many occlusion cases.\n\\noindent$\\bullet$~\\textbf{BDD100K}\\!~ is a large-scale dataset with 100K driving videos  (40 seconds and 30FPS each)  and supports  various tasks, including VSS and VIS. For VSS, 7,000/1,000/2,000 frames are densely labelled with 40 semantic classes for  \\texttt{train}/\\texttt{val}/\\texttt{test}. For VIS, 90 videos with 8 semantic categories are annotated by 129K instance masks -- 60 training videos, 10 validation videos, and 20 testing videos.\n\\noindent$\\bullet$~\\textbf{OVIS}\\!~ is a new challenging\nVIS dataset, where object occlusions usually occur. It has 901 videos and 296K high-quality instance masks for 25 semantic categories. It is split into 607 training, 140 validation and 154 test videos.\n\\noindent$\\bullet$~\\textbf{VIPER-VPS}\\!~ re-organizes VIPER~ into the video panoptic format. VIPER, extracted from the GTA-V game engine, has annotations of semantic and instance segmentations for 10 thing and 13 stuff classes on 254K frames of ego-centric driving scenes at $1080\\!\\times\\!1920$  resolution.\n\\noindent$\\bullet$~\\textbf{Cityscapes-VPS}\\!~ is built upon CityScapes\\!~. Dense panoptic annotations for 8 thing and 11 stuff classes for 500 snippets in Cityscapes \\texttt{val} set are provided every five frames and temporally consistent instance ids to the thing objects are also given, leading to 3000 annotated frames in total. These videos are split into 400/100 for \\texttt{train}/\\texttt{val}.\n\\noindent \\textbf{Remark.} CamVid, CityScapes, NYUDv2, and VSPW are built for VSS benchmarking.  YouTube-VIS, OVIS, KITTI MOTS, and MOTSChallenge are VIS datasets, but the diversity of the last two are limited. BDD100K has both VSS and VIS  annotations. VIPER-VPS and Cityscapes-VPS are aware of VPS evaluation, but VIPER-VPS is a synthesized dataset.", "cites": [3781, 6692, 6772, 5799, 6748, 8264, 1733], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of various video segmentation datasets, listing their contents, splits, and labeling details. It integrates minimal information from the cited papers beyond basic definitions and origins. There is little critical evaluation of the datasets or identification of broader trends or principles in the field."}}
{"id": "50d67499-d1be-4b7e-bbfd-43f892a4a59f", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "5186dbe8-d3f8-4469-ba91-2f2a7f8e85b8", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "Object-level AVOS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "Presently, three metrics are frequently used~ to measure how object-level AVOS methods perform on this task:\n\\begin{table}[t]\n\t\\centering\n\t\\caption{{\\textbf{Quantitative object-level AVOS results on\n\t\t\t\tDAVIS$_{16}$}~ \\texttt{val}\n\t\t\t(\\S\\ref{sec:OAVOSeva}) in terms of region similarity $\\mathcal{J}$, boundary\n\t\t\taccuracy $\\mathcal{F}$ and time stability $\\mathcal{T}$. We\n\t\t\talso report the recall and the decay performance over time\n\t\t\tfor both $\\mathcal{J}$ and $\\mathcal{F}$. (FPS denotes \\textit{frames per second}.  \\red{$^\\dagger$: FPS is borrowed from the original paper.} The three best scores are marked in \\textcolor{red}{\\textbf{red}}, \\textcolor{blue}{\\textbf{blue}}, and \\textcolor{green}{\\textbf{green}}, respectively.  These notes also apply to the other\n\t\t\ttables.)}}\n\\vspace{-5pt}\n\t\\begin{threeparttable}\n\t\t\\resizebox{0.48\\textwidth}{!}{\n\t\t\t\\setlength\\tabcolsep{2.5pt}\n\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\\begin{tabular}{|r||ccc|ccc|c|c|}\n\t\t\t\t\\hline\\thickhline\n\t\t\t\t\\rowcolor{mygray}\n&\\multicolumn{3}{c|}{{$\\mathcal{J}$}} &\\multicolumn{3}{c|}{{$\\mathcal{F}$}}&$\\mathcal{T}$ &\\\\\n\\rowcolor{mygray}\n\t\t\t\t\\multirow{-2}{*}{{Method}}  &\\textit{mean}$\\uparrow$ &\\textit{recall}$\\uparrow$ &\\textit{decay}$\\downarrow$ &\\textit{mean}$\\uparrow$ &\\textit{recall}$\\uparrow$ &\\textit{decay}$\\downarrow$ &\\textit{mean}$\\downarrow$ & \\multirow{-2}{*}{{FPS$\\uparrow$}} \\\\ \\hline \\hline\n\t\t\t\tMuG~ & 58.0 & 65.3 & 2.0 & 51.5 & 53.2 & 2.1 & 30.1 & 2.5 \\\\\n\t\t\t\t SFL~ &67.4 &81.4 &6.2 & 66.7& 77.1 &5.1 &28.2 & 3.3 \\\\\n\t\t\t\t MotionGrouping~ & 68.3 & - & - & 67.6 & -  & - & - & 83.3 \\\\\n\t\t\t\t  LVO~ &75.9 &89.1  &\\textcolor{red}{\\textbf{0.0}} &72.1  &83.4 &1.3 &26.5 & {13.5}\\\\\n\t\t\t\t  LMP~ & 70.0 &85.0 &\\textcolor{green}{\\textbf{1.3}} & 65.9 & 79.2 &2.5&57.2 & {18.3}\\\\\n\t\t\t      FSEG~ &70.7   &83.0 &1.5 &65.3 &73.8  &1.8  &32.8 & {7.2} \\\\\n\t\t\t\tPDB~&77.2 &93.1 &\\textcolor{blue}{\\textbf{0.9}}  &74.5  &84.4  &\\textcolor{red}{\\textbf{-0.2}} &29.1 & 1.4 \\\\\n\t\t\t\tMOT~&77.2 &87.8 &5.0 &77.4 &84.4 &3.3 &27.9 & {1.0}\\\\\n\t\t\t\tLSMO~&78.2 &91.1  &4.1 &75.9 &84.7 &3.5  &21.2 & 0.4  \\\\\n\t\t\t\tIST~ & 78.5 & - & - & 75.5 & - & - & - & - \\\\\n\t\t\t\tAGS~&79.7 &89.1 &1.9 &77.4 &85.8 &\\textcolor{blue}{\\textbf{0.0}} &26.7 & 1.7 \\\\\n\t\t\t\tMBN~ & 80.4 & 93.2 & 4.8 & 78.5 & 88.6 & 4.4 & 27.8 & 1.0 \\\\\n\t\t\t\tCOSNet~&80.5 &93.1  &4.4 &79.4 &89.5 &5.0 &\\textcolor{blue}{\\textbf{18.4}} & 2.2\\\\\n\t\t\t\tAGNN~ &81.3 &93.1 &\\textcolor{red}{\\textbf{0.0}} &79.7 &88.5 &5.1  &33.7&1.9\\\\\n\t\t\t\tMGA~ & 81.4 & - & - & 81.0 & - & - & - & 1.1 \\\\\n\t\t\t\tAnDiff~ &81.7  &90.9 &2.2 &80.5 &85.1 &\\textcolor{green}{\\textbf{0.6}} &21.4& 2.8\\\\\n\t\t\t\tPyramidCSA~ &78.1  &90.1 &- &78.5  &88.2&-&-& $^\\dagger$110 \\\\\n\t\t\t\tWCSNet~ & 82.2 & - & - & 80.7 & - & - & - & 25 \\\\\n\t\t\t\tMATNet~ &82.4 &\\textcolor{green}{\\textbf{94.5}} &5.5 &80.7 &90.2 &4.5 &21.6&1.3\\\\\n\t\t\t\tEGMN~ & 82.5 &94.3  &4.2 &81.2 &\\textcolor{green}{\\textbf{90.3}} &5.6 &\\textcolor{green}{\\textbf{19.8}}& {5.0} \\\\\n\t\t\t\tDFNet~ &\\textcolor{blue}{\\textbf{83.4}} &-&-&\\textcolor{green}{\\textbf{81.8}}&-&- &\\textcolor{red}{\\textbf{15.9}} & $^\\dagger$3.6\\\\\n\t\t\t\tF2Net~ &\\textcolor{green}{\\textbf{83.1}} & \\textcolor{blue}{\\textbf{95.7}} &\\textcolor{red}{\\textbf{0.0}} &\\textcolor{blue}{\\textbf{84.4}} &\\textcolor{blue}{\\textbf{92.3}} &0.8 &20.9 & $^\\dagger$10\\\\\n\t\t\t\tRTNet~ &\\textcolor{red}{\\textbf{85.6}} &\\textcolor{red}{\\textbf{96.1}} & 0.5 &\\textcolor{red}{\\textbf{84.7}} &\\textcolor{red}{\\textbf{93.8}} &0.9 & 19.9 & {6.7}\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{threeparttable}\n\t\\label{table:OAVOS}\n\\vspace{-10pt}\n\\end{table}\n\\begin{table}[t]\n\t\\centering\n\\renewcommand\\thetable{8}\n\t\\caption{{\\textbf{Quantitative instance-level AVOS results on DAVIS$_{17}$}~ \\texttt{val} (\\S\\ref{sec:IAVOSeva}) in terms of region similarity $\\mathcal{J}$ and boundary accuracy $\\mathcal{F}$.}}\n\\vspace{-7pt}\n\t\\begin{threeparttable}\n\t\t\\resizebox{0.48\\textwidth}{!}{\n\t\t\t\\setlength\\tabcolsep{2.5pt}\n\t\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\t\\begin{tabular}{|r||c|ccc|ccc|c|}\n\t\t\t\t\\hline\\thickhline\n\\rowcolor{mygray}\n&$\\mathcal{J}\\!\\&\\mathcal{F}$ &\\multicolumn{3}{c|}{{$\\mathcal{J}$}} &\\multicolumn{3}{c|}{{$\\mathcal{F}$}}&\\\\\n\\rowcolor{mygray}\n\t\t\t\t\\multirow{-2}{*}{{Method}} &\\textit{mean}$\\uparrow$ &\\textit{mean}$\\uparrow$ &\\textit{recall}$\\uparrow$ &\\textit{decay}$\\downarrow$ &\\textit{mean}$\\uparrow$ &\\textit{recall}$\\uparrow$ &\\textit{decay}$\\downarrow$ & \\multirow{-2}{*}{{FPS$\\uparrow$}} \\\\ \\hline \\hline\n\t\t\t\t PDB~    & 55.1 & 53.2 & 58.9 & 4.9 & 57.0 & 60.2 & \\textcolor{green}{\\textbf{6.8}} & 0.7\\\\\n\t\t\t\t RVOS~ & 41.2 & 36.8 & 40.2 & \\textcolor{green}{\\textbf{0.5}} & 45.7 & 46.4 & 1.7 & 14.3\\\\\n\t\t\t\t  AGS~  & 57.5 & 55.5 & 61.6 & 7.0 & 59.5 & 62.8 & 9.0 & 1.1 \\\\\n\t\t\t\t AGNN~ & 61.1 & 58.9 & 65.7 & 11.7 & 63.2 & 67.1 & 14.3 & {0.9} \\\\\n\t\t\t\t STEm-Seg~  & \\textcolor{green}{\\textbf{64.7}} & \\textcolor{green}{\\textbf{61.5}} & \\textcolor{green}{\\textbf{70.4}} & \\textcolor{red}{\\textbf{-4.0}} & \\textcolor{blue}{\\textbf{67.8}} & \\textcolor{blue}{\\textbf{75.5}} & \\textcolor{blue}{\\textbf{1.2}} & {9.3} \\\\\n\t\t\t\t  UnOVOST~ & \\textcolor{red}{\\textbf{67.9}} & \\textcolor{red}{\\textbf{66.4}} & \\textcolor{red}{\\textbf{76.4}} & \\textcolor{blue}{\\textbf{-0.2}} & \\textcolor{red}{\\textbf{69.3}} & \\textcolor{red}{\\textbf{76.9}} & \\textcolor{red}{\\textbf{0.0}} & $^\\dagger$1.0 \\\\\n\t\t\t\tTODA~ & \\textcolor{blue}{\\textbf{65.0}} & \\textcolor{blue}{\\textbf{63.7}} & \\textcolor{blue}{\\textbf{71.9}} & 6.9 & \\textcolor{green}{\\textbf{66.2}} & \\textcolor{green}{\\textbf{73.1}} & 9.4 & 9.1 \\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{threeparttable}\n\t\\label{table:IAVOS}\n\\vspace{-7pt}\n\\end{table}\n\\noindent$\\bullet$~\\textbf{Region Jaccard} $\\mathcal{J}$ is calculated by the intersection-over-union (IoU) between the segmentation results ${\\hat{Y}}\\!\\in\\!\\{0,1\\}^{w\\times h}$ \\red{and the ground-truth ${Y}\\!\\in\\!\\{0,1\\}^{w\\times h}$:\n$\\mathcal{J} = {|\\hat{Y} \\cap  Y}|/|{\\hat{Y}\\cup Y|}$,}\nwhich computes the number of pixels of the intersection between $\\hat{Y}$ and ${Y}$, and divides it by the size of the union.\n\\noindent$\\bullet$~\\textbf{Boundary Accuracy}  $\\mathcal{F}$ is the harmonic mean of the boundary precision $\\text{P}_c$ and recall $\\text{R}_c$. The value of $\\mathcal{F}$ reflects how well the segment contours $c(\\hat{Y})$ match the ground-truth contours $c(Y)$. Usually, the value of $\\text{P}_c$ and $\\text{R}_c$ can be computed via bipartite graph matching~, then the boundary accuracy $\\mathcal{F}$ \\red{can be computed as: $\\mathcal{F} = {2 \\text{P}_c \\text{R}_c}/({\\text{P}_c + \\text{R}_c})$}.\n\\noindent$\\bullet$~\\textbf{Temporal Stability}  $\\mathcal{T}$ is informative of the stability of segments. It is computed as the pixel-level cost of matching two successive segmentation boundaries. The match is achieved by minimizing the shape context descriptor~ distances between matched points while preserving the order in which the points are present in the boundary polygon. Note that $\\mathcal{T}$\nwill compensate motion and small deformations, but not penalize inaccuracies of the contours~.\n\\begin{figure*}\n    \\begin{minipage}{\\textwidth}\n        \\begin{minipage}[t]{0.31\\textwidth}\n        \\centering\\small\n  \\makeatletter\\def\\@captype{table}\n  \\renewcommand\\thetable{10}\n  \\caption{{\\textbf{Quantitative IVOS results on DAVIS$_{17}$}  \\texttt{val} (\\S\\ref{sec:IVOSeva}) in terms of AUC and $\\mathcal{J}$@60.}}\\label{table:IVOS}\n  \\vspace{-5pt}\n\t\\resizebox{1.\\linewidth}{!}{\n    \\setlength\\tabcolsep{15pt}\n    \\renewcommand\\arraystretch{1.05}\n    \\begin{tabular}{|r||c|c|}\n     \\hline\\thickhline\n     \\rowcolor{mygray}\n     Method &AUC $\\uparrow$ &$\\mathcal{J}$@60 $\\uparrow$\\\\ \\hline \\hline\n     IVS~ &69.1 &73.4 \\\\\n     MANet~ &74.9 &76.1 \\\\\n     IVOS-W~ & 74.1 & - \\\\\n     ATNet~ &\\textcolor{green}{\\textbf{77.1}} &\\textcolor{green}{\\textbf{79.0}}\\\\\n     GIS~ &\\textcolor{blue}{\\textbf{82.0}}&\\textcolor{blue}{\\textbf{82.9}}\\\\\n     MiVOS~  &\\textcolor{red}{\\textbf{84.9}}  &\\textcolor{red}{\\textbf{85.4}}\\\\\n     \\hline\n    \\end{tabular}\n   }\n    \\end{minipage}\n\\begin{minipage}[t]{0.69\\textwidth}\n\\centering\\small\n\\makeatletter\\def\\@captype{table}\n\\renewcommand\\thetable{11}\n\\caption{{\\textbf{Quantitative LVOS results on A2D Sentence}~ \\texttt{test}  (\\S\\ref{sec:LVOSeva}) \\protect\\\\ in terms of {Precision@$K$}, {mAP} and {IoU}.}}\t\\label{table:a2dsota}\n\\vspace{-5pt}\n\t\\resizebox{1.\\linewidth}{!}{\n\t\t\\setlength\\tabcolsep{8pt}\n\t\t\\renewcommand\\arraystretch{1.05}\n\t\t\\begin{tabular}{|r||ccccc|c|cc|c|}\n\t\t\t\\hline\\thickhline\n\t\t\t\\rowcolor{mygray}\n\t\t\t   & \\multicolumn{5}{c|}{{Overlap}} & {mAP$\\uparrow$} & \\multicolumn{2}{c|}{{IoU}} & \\\\\\cline{2-6} \\cline{8-9}\n\t\t\t\\rowcolor{mygray}\n\t\t\t \\multirow{-2}{*}{{Method}} & P@0.5$\\uparrow$ & P@0.6$\\uparrow$ & P@0.7$\\uparrow$ & P@0.8$\\uparrow$ & P@0.9$\\uparrow$ & 0.5:0.95 & \\textit{overall}$\\uparrow$ & \\textit{mean}$\\uparrow$ & \\multirow{-2}{*}{{FPS$\\uparrow$}} \\\\\n\t\t\t\\hline \\hline\n\t\t\t A2DS~ & 50.0 & 37.6 & 23.1 & 9.4 & 0.4 & 21.5 & 55.1 & 42.6 & - \\\\\n\t\t\t CMSANet~ & 46.7 & 38.5 & 27.9 & 13.6 & 1.7 & 25.3 & 61.8 & 43.2 & 6.5 \\\\\n\t\t\tAAN~ & 55.7 & 45.9 & 31.9 & 16.0 & 2.0 & 27.4 & 60.1 & 49.0 & 8.6 \\\\\n           VT-Capsule~ & 52.6 & 45.0 & 34.5 & 20.7 & 3.6 & 30.3 & 56.8 & 46.0 & - \\\\\n\t\t\tCDNet~ & \\textcolor{green}{\\textbf{60.7}} & \\textcolor{green}{\\textbf{52.5}} & \\textcolor{green}{\\textbf{40.5}} & \\textcolor{green}{\\textbf{23.5}} & \\textcolor{green}{\\textbf{4.5}} & \\textcolor{green}{\\textbf{33.3}} & \\textcolor{green}{\\textbf{62.3}} & \\textcolor{blue}{\\textbf{53.1}} & 7.2 \\\\\n\t\t\t PolarRPE~& \\textcolor{blue}{\\textbf{63.4}} & \\textcolor{blue}{\\textbf{57.9}} & \\textcolor{blue}{\\textbf{48.3}} & \\textcolor{blue}{\\textbf{32.2}} & \\textcolor{blue}{\\textbf{8.3}} & \\textcolor{blue}{\\textbf{38.8}} & \\textcolor{blue}{\\textbf{66.1}} & \\textcolor{green}{\\textbf{52.9}} & 5.4 \\\\\n            CST~ &\\textcolor{red}{\\textbf{65.4}} &\\textcolor{red}{\\textbf{58.9}} &\\textcolor{red}{\\textbf{49.7}} &\\textcolor{red}{\\textbf{33.3}} &\\textcolor{red}{\\textbf{9.1}} &\\textcolor{red}{\\textbf{39.9}} &\\textcolor{red}{\\textbf{66.2}} &\\textcolor{red}{\\textbf{56.1}} & {8.1} \\\\\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-14pt}\n    \\end{minipage}\n    \\end{minipage}\n\\end{figure*}\n\\vspace{-4pt}", "cites": [6703, 5637, 6756, 6765, 6696, 6738, 6741, 2651, 6697, 3805, 6699, 6766, 6704, 6689, 6743, 5630, 6701, 9063, 6686, 6700, 6690, 6687, 6252, 6744, 9066, 6695, 6764, 9069], "cite_extract_rate": 0.6222222222222222, "origin_cites_number": 45, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section offers a comparative view of object-level AVOS methods through a well-structured table with multiple metrics (region similarity, boundary accuracy, temporal stability). It synthesizes key performance indicators from cited papers, enabling readers to compare results. However, it lacks deeper critical analysis of the methods or broader abstraction to overarching trends, remaining largely focused on numerical comparison."}}
{"id": "bf0da4b5-72be-403d-a38c-b9ba7f24c4ee", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "5186dbe8-d3f8-4469-ba91-2f2a7f8e85b8", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "Object-level AVOS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:OAVOSeva}\nWe select DAVIS$_{16}$\\!~, the most widely used dataset in AVOS, for performance benchmarking. Table~\\ref{table:OAVOS} presents the results of those reviewed AVOS methods DAVIS$_{16}$ \\texttt{val} set. The current best solution, RTNet~, reaches 85.6 region similarity $\\mathcal{J}$, significantly outperforming the earlier deep learning-based methods, such as SFL~, proposed in 2017.\n\\vspace{-5pt}", "cites": [3805], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a brief performance comparison by citing results on the DAVIS$_{16}$ dataset but lacks integration with the cited paper (SegFlow) beyond mentioning its existence. There is no critical analysis of methods or limitations, and no generalization or abstraction of trends in the field. It primarily functions as a factual summary of results."}}
{"id": "174c0124-9f8b-436b-98d9-81de503fe137", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "58928916-e587-4f70-9390-30c1fc2e979a", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "Instance-level AVOS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "\\label{sec:IAVOSm}\nIn instance-level AVOS setting, region Jaccard $\\mathcal{J}$, boundary accuracy $\\mathcal{F}$, and $\\mathcal{J}\\&\\mathcal{F}$ -- the mean of $\\mathcal{J}$ and $\\mathcal{F}$ -- are used for evaluation~. Each of the annotated object tracklets will be matched with one of predicted tracklets according to $\\mathcal{J}\\&\\mathcal{F}$, using bipartite graph matching. For a certain criterion, the final score will be computed between each ground-truth object and its optimal assignment.", "cites": [6730], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the evaluation metrics used in instance-level AVOS but does not synthesize broader insights from the cited paper. It lacks critical analysis of these metrics and does not abstract them into a conceptual framework or evaluate their strengths and limitations in the context of video segmentation research."}}
{"id": "cdf27b5a-a77f-45aa-a0a9-040b3a1172ec", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "58928916-e587-4f70-9390-30c1fc2e979a", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "Instance-level AVOS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:IAVOSeva}\nRegarding instance-level AVOS, we take into account DAVIS$_{17}$~ in which the vast majority of methods are evaluated. From Table~\\ref{table:IAVOS} we can find that UnOVOST~ is the top scorer, with 67.9 $\\mathcal{J}$  at the time of this writing.", "cites": [5637, 9066], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis, merely stating that UnOVOST is the top-performing method on DAVIS$_{17}$ without contextualizing it within the broader landscape of AVOS approaches. There is no critical evaluation or comparison of strengths/weaknesses of methods, nor is there any abstraction or identification of overarching patterns or principles in the field."}}
{"id": "fd582738-4fff-4995-a61e-27242438d4e5", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "ad33a83a-a37b-4737-b0e9-afc2a4f00f73", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "SVOS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "Region Jaccard $\\mathcal{J}$, boundary accuracy $\\mathcal{F}$, and $\\mathcal{J}\\&\\mathcal{F}$ are also widely adopted for SVOS performance evaluation~.\n\\begin{table}[t]\n  \\centering\n  \t\\vspace{-5pt}\n  \\renewcommand\\thetable{9}\n  \\caption{{\\textbf{Quantitative SVOS results on DAVIS$_{17}$}~ \\texttt{val} (\\S\\ref{sec:SVOSeva}) \\protect\\\\ in terms of region similarity $\\mathcal{J}$ and boundary accuracy $\\mathcal{F}$.} }\n  \\vspace{-5pt}\n  \\begin{threeparttable}\n   \\resizebox{0.48\\textwidth}{!}{\n    \\setlength\\tabcolsep{1pt}\n    \\renewcommand\\arraystretch{1.04}\n    \\begin{tabular}{|r||c|cc|c||r||c|cc|c|}\n     \\hline\\thickhline\n     \\rowcolor{mygray}\n     Method &\\tabincell{c}{$\\mathcal{J}\\!\\&\\mathcal{F}$\\\\\\textit{mean}$\\uparrow$} & \\tabincell{c}{$\\mathcal{J}$\\\\\\textit{mean}$\\uparrow$} &\\tabincell{c}{$\\mathcal{F}$\\\\\\textit{mean}$\\uparrow$} & FPS$\\uparrow$&Method &\\tabincell{c}{$\\mathcal{J}\\!\\&\\mathcal{F}$\\\\\\textit{mean}$\\uparrow$} & \\tabincell{c}{$\\mathcal{J}$\\\\\\textit{mean}$\\uparrow$} &\\tabincell{c}{$\\mathcal{F}$\\\\\\textit{mean}$\\uparrow$} & FPS$\\uparrow$\\\\ \\hline \\hline\n     OnAVOS~ &67.9 &64.5 &70.5 &0.08 &STM~ &81.8 &79.2 &84.3 &6.3\\\\\n     OSVOS~ &60.3 &56.7 &63.9 &0.22 &e-OSVOS~ &77.2 &74.4 &80.0 &0.5\\\\\n     CINM~ &67.5 &64.5 &70.5 &$^\\dagger$0.01 &AFB-URR~ &74.6 &73.0  &76.1 &3.8\\\\\n     FAVOS~ &58.2 &54.6 &61.8 &0.56\n      &Fasttan~ &75.9 &72.3 &79.4 &$^\\dagger$7\\\\\n      MAST~ & 65.5 & 63.3 & 67.6 & 5.1 & STM-Cycle~&  71.7 & 68.7 & 74.7 & 38 \\\\\n      CRW~ & 67.6 & 64.5 & 70.6 & 7.3 &   QMA  & 71.9 & - & - & 6.3 \\\\\n     RGMP~ &66.7 &64.8 &68.6 &7.7 &Fasttmu~ &70.6 &69.1 &72.1 &9.7\\\\\n     OSMN~ &54.8 &52.5 &57.1 &7.7 &SAT~ &72.3 &68.6 &76.0 &$^\\dagger$39\\\\\n     OSVOS-S~ &68.0 &64.7 &71.3 &0.22 &TVOS~ &72.3 &69.9 &74.7 &$^\\dagger$37\\\\\n     Videomatch~ &61.4  &-&- &$^\\dagger$0.38 &GCNet~ &71.4 &69.3 &73.5 &$^\\dagger$25\\\\\n     Dyenet~ &69.1 &67.3 &71.0 &2.4 &KMN~ &76.0 &74.2 &77.8 &8.3\\\\\n     MVOS~ &59.2  &56.3   &62.1 &1.5 &CFBI~ &81.9 &79.3 &84.5&2.2\\\\\n     FEELVOS~ &71.5 &69.1 &74.0 &2.2 &LWL~ &70.8 &68.2 &73.5 &15.6\\\\\n     MHP-VOS~ &75.3 &71.8 &78.8 &$^\\dagger$0.01 &MSN~ &74.1 &71.4 &76.8&$\\dagger$10\\\\\n     AGSS~ &67.4 &64.9 &69.9 &$^\\dagger$10 &EGMN~ &\\textcolor{blue}{\\textbf{82.8}} &80.0 &85.2 &5.0 \\\\\n     AGAME~ &70.0 &67.2 &72.7 &$^\\dagger$14 &SwiftNet~ &81.1 &78.3 &83.9 &$^\\dagger$25\\\\\n     SiamMask~ &56.4 &64.3 &58.5 &$^\\dagger$35 &G-FRTM~&76.4& - &- &$^\\dagger$18.2\\\\\n     RVOS~ &60.6&57.5 &63.6 &0.56 &SST~ & 82.5 &79.9 &85.1&-\\\\\n     RANet~ &65.7 &63.2 &68.2 &$^\\dagger$30 &GIEL~ &\\textcolor{green}{\\textbf{82.7}} &\\textcolor{green}{\\textbf{80.2}} &\\textcolor{green}{\\textbf{85.3}} &6.7\\\\\n     DMM-Net~ &70.7 &68.1 &73.3 &0.37 &LCM~ &\\textcolor{red}{\\textbf{83.5}} &\\textcolor{blue}{\\textbf{80.5}} &\\textcolor{red}{\\textbf{86.5}} &8.5\\\\\n     DTN~ &67.4 &64.2 &70.6 &14.3 &RMNet~ &\\textcolor{red}{\\textbf{83.5}}  &\\textcolor{red}{\\textbf{81.0}} &\\textcolor{blue}{\\textbf{86.0}} &$^\\dagger$11.9\\\\\n     \\hline\n    \\end{tabular}\n   }\n  \\end{threeparttable}\n \\label{table:SVOS}\n \\vspace{-11pt}\n\\end{table}\n\\vspace{-4pt}", "cites": [5637, 6711, 6733, 9065, 6729, 6722, 6720, 2652, 6731, 2651, 6732, 6719, 6697, 6735, 6706, 6737, 3806, 6727, 6713, 6712, 6728, 5632, 6708, 6726, 6725, 6718, 5629, 6707, 5625, 6736, 6714, 6723, 6717, 8263, 5633, 9064], "cite_extract_rate": 0.8409090909090909, "origin_cites_number": 44, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"comparative\",\n    \"scores\": {\"synthesis\": 2.5, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section provides a clear comparative overview of SVOS methods using standardized metrics ($\\mathcal{J}$, $\\mathcal{F}$, $\\mathcal{J}\\&\\mathcal{F}$, and FPS) on the DAVIS 2017 dataset, but lacks deeper synthesis of ideas across the cited works. It presents numerical results and highlights top-performing methods (e.g., LCM and RMNet), but does not "}}
{"id": "4bb3da56-0025-4c44-b5b9-84d781f0b928", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "ad33a83a-a37b-4737-b0e9-afc2a4f00f73", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "SVOS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:SVOSeva}\nDAVIS$_{17}$~ is also one of the most important SVOS dataset.  Table~\\ref{table:SVOS} shows the results of recent SVOS methods   on DAVIS$_{17}$ \\texttt{val} set. In this case, all the top-leading solutions, such as EGMN~, LCM~, and RMNet~, are built upon the memory augmented architecture -- STM~.\n\t\\vspace{-7pt}", "cites": [2651, 5637, 6717, 6706, 2652], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly presents the results of recent SVOS methods on the DAVIS$_{17}$ dataset and mentions that top methods use the memory-augmented STM architecture. However, it lacks synthesis of the underlying ideas from the cited papers and does not provide a critical evaluation or abstraction of trends. It remains largely descriptive without deeper insight or comparative analysis."}}
{"id": "0d112b11-cb0f-4fbf-b18c-0d2897b66b01", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "f96ac66a-354c-4fc2-8eeb-90a572fec2e9", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "IVOS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "\\vspace{-2pt}\nArea under the curve (AUC) and Jaccard at 60 seconds ($\\mathcal{J}$@60s) are two widely used IVOS evaluation criteria~.\n\\noindent$\\bullet$~\\textbf{AUC} is designed to measure the overall accuracy of the evaluation. It is computed over the plot Time \\textit{vs} Jaccard. Each sample in the plot is computed considering the average time and the average Jaccard for a certain interaction.\n\\noindent$\\bullet$~\\textbf{$\\mathcal{J}$@60} measures the accuracy with a limited time budget (60 seconds). It is achieved by interpolating the Time \\textit{vs} Jaccard plot at 60 seconds. This evaluates which quality an IVOS method can obtain in 60 seconds.\n\\vspace{-8pt}", "cites": [5637], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.0, \"critical\": 1.5, \"abstraction\": 1.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a basic description of the AUC and $\\mathcal{J}$@60s metrics used in IVOS evaluation, citing Paper 1 for context. It lacks synthesis of multiple sources, critical evaluation of the metrics, or broader abstraction beyond their definitions and purposes. The explanation is minimal and primarily serves as a factual summary without deepe"}}
{"id": "020c8c1d-cd6b-4b8e-b464-6838e4b10e8e", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "f96ac66a-354c-4fc2-8eeb-90a572fec2e9", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "IVOS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:IVOSeva}\nDAVIS$_{17}$~ is also widely used for IVOS performance benchmarking. Results summarized in Table~\\ref{table:IVOS} show that the method proposed by Cheng \\etal~ is the top one.\n\\vspace{-11pt}", "cites": [5637], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis, merely stating that the DAVIS$_{17}$ dataset is used for benchmarking and citing a single paper to note a top-performing method. It lacks critical analysis and does not evaluate the strengths or limitations of the cited work. There is no abstraction or identification of broader trends or principles related to video segmentation techniques."}}
{"id": "3a605e66-ce96-4280-86fc-1dc3ed5c9c6e", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "67b8f993-eb63-4d38-a948-68d41db76a44", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "LVOS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "\\vspace{-2pt}\n\\red{As~, {overall IoU}, {mean IoU} and precision are adopted.}\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\\renewcommand\\thetable{12}\n\t\\caption{{\\textbf{Quantitative VSS results on Cityscapes}~ \\texttt{val} (\\S\\ref{sec:VSSeva}) in terms of IoU$_{\\text{class}}$ and IoU$_{\\text{category}}$ (Max Latency: \\textit{maximum per-frame time cost}).}}\n\t\\vspace{-5pt}\n\\resizebox{0.45\\textwidth}{!}{\n\t\t\\setlength\\tabcolsep{5pt}\n\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\\begin{tabular}{|r||cc|cc|}\n\t\t\t\\hline\\thickhline\n\t\t\t\\rowcolor{mygray}\n\t\t\t{{Method}} &IoU$_{\\text{class}}$$\\uparrow$ &IoU$_{\\text{category}}$ $\\uparrow$ & {{FPS}$\\uparrow$}  & {{Max Latency (ms)}$\\downarrow$} \\\\\n\t\t\t\\hline \\hline\n\t\t\tClockwork~ & 66.4 & 88.6 & 6.4 &  198\\\\\n\t\t\tDFF~ & 69.2 & 88.9 & 5.6  & 575\\\\\n\t\t\tPEARL~ & 75.4 & 89.2 & 1.3 & 800 \\\\\n\t\t\tNetWarp~ & 80.5 & \\textcolor{blue}{\\textbf{91.0}} & -  & -\\\\\t\n\t\t\tDVSN~ & 70.3 &  -& $^\\dagger$19.8  & - \\\\\n\t\t\tLVS~ & 76.8 & 89.8 & 5.8 & 380 \\\\\n\t\t\tGRFP~ & \\textcolor{green}{\\textbf{80.6}} & \\textcolor{green}{\\textbf{90.8}} & 3.9 & 255\\\\\t\t\t\t\t\n\t\t\t Accel~ & 75.5 & - & $^\\dagger$1.1 & -\\\\\n\t\t\t VPLR~ & \\textcolor{blue}{\\textbf{81.4}} & - & $^\\dagger${5.9} & -\\\\\n\t\t\t TDNet~ & 79.9 & 90.1 & 5.6 & 178\\\\\n\t\t\t EFC~ & \\textcolor{red}{\\textbf{83.5}} & \\textcolor{red}{\\textbf{92.2}} & $^\\dagger${16.7} & - \\\\\n\t\t\t Lukas~ & 71.2 & - & $^\\dagger${1.9}& -\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-10pt}\n\t\\label{table:YVSSsota}\n\\end{table}\n\\begin{table*}[t]\n\t\\centering\n  \\renewcommand\\thetable{14}\n\t\\small\n\\caption{{\\textbf{Quantitative VPS results on Cityscapes-VPS}~ (\\S\\ref{sec:VPSeva}) \\texttt{test} in term of VPQ. Each cell shows\nVPQ$^k$ / VPQ$^k$-Thing / VPQ$^k$-Stuff.}}\n\\vspace{-8pt}\n\t\\resizebox{0.85\\textwidth}{!}{\n\t\t\\setlength\\tabcolsep{9pt}\n\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\\begin{tabular}{|r||cccc|c|c|}\n\t\t\t\\hline\\thickhline\n\t\t\t\\rowcolor{mygray}\n\t\t\t  & \\multicolumn{4}{c|}{{Temporal window size}} & & \\\\\\cline{2-5}\n\t\t\t\\rowcolor{mygray}\n\t\t\t  \\multirow{-2}{*}{{Method}} & $k=0$$\\uparrow$ & $k=5$$\\uparrow$ & $k=10$$\\uparrow$ & $k=15$$\\uparrow$ &\\multirow{-2}{*}{VPQ$\\uparrow$} & \\multirow{-2}{*}{{FPS$\\uparrow$}} \\\\\n\t\t\t\\hline \\hline\n\t\t\t VPS~ &64.2 / 59.0 / 67.7 &57.9 / 46.5 / 65.1 &54.8 / 41.1 / 63.4 &52.6 / 36.5 / 62.9 &57.4 / 45.8 / 64.8 &1.3 \\\\\n\t\t\t SiamTrack~ &63.8 / 59.4 / 66.6 &58.2 / 47.2 / 65.9 &56.0 / 43.2 / 64.4 &54.7 / 40.2 / 63.2 &57.8 / 47.5 / 65.0 &4.5\\\\\n                ViP-DeepLab~  &68.9 / 61.6 / 73.5 &62.9 / 51.0 / 70.5 &59.9 / 46.0 / 68.8 &58.2 / 42.1 / 68.4 &62.5 / 50.2 / 70.3  & {10.0}\\\\\n\\hline\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-15pt}\n\t\\label{table:vps}\n\\end{table*}\n\\noindent$\\bullet$~\\textbf{IoU}:  \\textit{overall IoU} is computed as total intersection area of all test data over the total union area, while \\textit{mean IoU} refers to average over IoU of each test sample.\n\\noindent\\red{$\\bullet$~\\textbf{Precision}: Precision@$K$ is computed as the percentage of test samples whose IoU scores are higher than a threshold $K$. Precision at five thresholds ranging from 0.5 to 0.9 and\\\\ mean$_{\\!}$ average$_{\\!}$ precision$_{\\!}$ (mAP)$_{\\!}$ over$_{\\!}$ 0.5:0.05:0.95$_{\\!}$ are$_{\\!}$ reported.}\n\\vspace{-6pt}", "cites": [6763, 6762, 3807, 6745, 6754, 6751, 8219, 4169, 6773, 6759, 778, 6758, 6764, 1733, 6749, 6748], "cite_extract_rate": 0.9411764705882353, "origin_cites_number": 17, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section primarily compares different methods using evaluation metrics like IoU and FPS, as presented in tables. It synthesizes basic performance data from cited papers but lacks deeper integration of their conceptual contributions. There is minimal critical analysis or abstraction to broader principles beyond the numerical comparison."}}
{"id": "f0631c3c-caef-4279-a0ad-421760cd78cf", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "67b8f993-eb63-4d38-a948-68d41db76a44", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "LVOS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:LVOSeva}\nA2D Sentence~ is arguably the most popular dataset~in\n\\noindent LVOS. Table~\\ref{table:a2dsota} gives the results of six recent methods on A2D Sentence \\texttt{test} set. It shows clear improvement trend from the first LVOS model~ proposed in 2018, to recent complicated solution~. {For runtime comparison, all the methods are tested on a video clip of 16 frames with resolution $512\\!\\times\\!512$ and a textual sequence of {20} words.}\n\\vspace{-5pt}", "cites": [6764, 6765], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions two papers and provides factual results from a benchmark on the A2D Sentence dataset. It lacks deeper synthesis of the ideas, critical evaluation of the methods, and abstraction to broader trends or principles. The narrative remains descriptive with minimal analytical depth."}}
{"id": "82043a70-172d-4b27-8dfd-b1d924c41bb6", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "d4eb58f7-8cd1-49bd-a920-f463ffee5c89", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VSS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "IoU metric is the most widely used metric in VSS. Moreover, in Cityscapes~ -- the gold-standard benchmark dataset in this field, two IoU scores, IoU$_{\\text{category}}$ and IoU$_{\\text{class}}$, defined over two semantic granularities, are reported.\nHere, `category' refers to high-level semantic categories (\\eg, vehicle, human), while `class' indicates more fine-grained semantic classes (\\eg, car, bicycle, person, rider). In total,~ considers $19$ classes, which are further grouped into $8$ categories.\n\\vspace{-3pt}", "cites": [1733], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of evaluation metrics used in video semantic segmentation, particularly highlighting the IoU metric and its use in the Cityscapes dataset. However, it lacks synthesis across multiple sources, critical analysis of the metrics or their limitations, and abstraction to broader trends or principles in the field."}}
{"id": "12d5f93a-9a33-472d-a3d1-aac079689ba7", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "d4eb58f7-8cd1-49bd-a920-f463ffee5c89", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VSS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:VSSeva}\nTable~\\ref{table:YVSSsota} summarizes the results of eleven VSS approaches on Cityscapes~ \\texttt{val} set.  \nAs seen, EFC~ performs the best currently, with $83.5\\%$ in terms of IoU$_{\\text{class}}$.\n\\vspace{-3pt}", "cites": [1733, 6762], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis by merely listing a single top-performing method (EFC) and its score without contextualizing it in relation to other cited works. There is no critical evaluation of the approaches or their limitations, nor any abstraction to broader trends or principles in video semantic segmentation. The narrative remains superficial and descriptive."}}
{"id": "61815096-8d4c-48a1-8880-3f9412d1c8ac", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "ad84ffd2-495e-40ca-9701-faff2d18197f", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VIS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "As in~, precision and recall metrics are used for VIS performance evaluation. Precision at IoU thresholds 0.5 and 0.75, as well as mean average precision (mAP) over 0.50:0.05:0.95 are reported.  Recall@$N$ is defined as the maximum recall given $N$ segmented instances per video. These two metrics are first evaluated per category and then\naveraged over the category set. The IoU metric is similar to region Jaccard $\\mathcal{J}$ used in instance-level AVOS (\\S\\ref{sec:IAVOSm}).\n\\vspace{-3pt}", "cites": [5799], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes the evaluation metrics used for VIS, primarily referencing one cited paper. It lacks synthesis of multiple sources, critical evaluation of the metrics or their implications, and abstraction to broader trends or principles. The content is mostly a factual summary without deeper analysis or comparative insight."}}
{"id": "3a4a8749-805b-4dc8-b068-d1956cc088c5", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "ad84ffd2-495e-40ca-9701-faff2d18197f", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VIS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:VISeva}\nTable~\\ref{table:YVISsota} gathers VIS results for on  YouTube-VIS~ \\texttt{val}  set, showing that Transformer-based architecture, \\ie,  VisTR , and redundant sequence proposal based solution Propose-Reduce , greatly improve the state-of-the-art.\n\\vspace{-6pt}", "cites": [6750, 8838, 5799], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the performance of two specific methods (VisTR and Propose-Reduce) on the YouTube-VIS dataset but does not provide substantial synthesis, critical evaluation, or abstraction. It lacks comparison with other methods, analysis of strengths/weaknesses, or identification of broader trends in the field."}}
{"id": "bfecf310-45c1-433a-9224-165fea491e9f", "title": "Evaluation Metrics", "level": "subsubsection", "subsections": [], "parent_id": "eb6ba0ec-694d-457c-9e10-058f7925d0d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VPS Performance Benchmarking"], ["subsubsection", "Evaluation Metrics"]], "content": "In~, the panoptic quality (PQ) metric used in image panoptic segmentation is modified as video panoptic quality (VPQ) to adapt to video panoptic segmentation.\n\\noindent\\red{$\\bullet$$_{\\!}$~\\textbf{VPQ}:$_{\\!}$ Given$_{\\!}$ a$_{\\!}$ snippet$_{\\!}$ $V^{t:t+k\\!}$ with$_{\\!}$ time$_{\\!}$ window$_{\\!}$ $k$,$_{\\!}$ true$_{\\!}$~po-$_{\\!}$ sitive (TP) is defined by $\\text{TP}\\!=\\!\\{(u, \\hat{u})_{\\!}\\!\\in\\!U_{\\!}\\!\\times\\!\\hat{U}_{\\!}\\!:_{\\!} \\text{IoU}(u,\\hat{u})\\!>\\!0.5\\}$ where $U$ and $\\hat{U}$ are\nthe set of the ground-truth and predicted tubes, respectively. False Positives (FP) and False Negatives (FN) are defined accordingly.  After accumulating TP$_c$, FP$_c$, and FN$_c$ on\nall the clips with window size $k$ and class $c$,~we\\\\ define:$_{\\!}$ $\\text{VPQ}^k_{\\!}\\!=_{\\!}\\!\\frac{1}{N_{\\text{class}}}\\!\\sum_c\\!\\frac{\\sum_{(u,\\hat{u})\\in\\text{TP}_c}\\!\\text{IoU}(u,\\hat{u})}{|\\text{TP}_c|+\\frac{1}{2}|\\text{FP}_c|+\\frac{1}{2}|\\text{FN}_c|}$.$_{\\!}$ When$_{\\!}$ $k\\!=\\!1$,$_{\\!}$ VPQ$^1$}\n\\noindent \\red{is equivalent to PQ. For evaluation, $\\text{VPQ}^k$ is reported over $k\\!\\in\\!\\{0,5,10,15\\}$ and finally,  $\\text{VPQ}\\!=\\!\\frac{1}{4}\\sum_{k\\in\\{0,5,10,15\\}\\!}\\text{VPQ}^k$.}\n\\vspace{-5pt}", "cites": [6748], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily describes the VPQ metric, adapting the PQ metric from image panoptic segmentation to video. While it introduces the metric and its components, it lacks critical analysis of its strengths, weaknesses, or comparative advantages over other metrics. Additionally, it does not abstract or synthesize broader principles or patterns in video segmentation evaluation."}}
{"id": "00a35964-2ed1-45e6-bebf-18353ba63457", "title": "Results", "level": "subsubsection", "subsections": [], "parent_id": "eb6ba0ec-694d-457c-9e10-058f7925d0d3", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "VPS Performance Benchmarking"], ["subsubsection", "Results"]], "content": "\\label{sec:VPSeva}\n\\!Cityscapes-VPS\\!~$_{\\!}$ is$_{\\!}$ chosen$_{\\!}$ for$_{\\!}$ testing$_{\\!}$ VPS$_{\\!}$ methods.$_{\\!}$ As$_{\\!}$ shown$_{\\!}$ in$_{\\!}$ Table\\!~\\ref{table:vps}, ViP-DeepLab\\!~$_{\\!}$ is$_{\\!}$ the$_{\\!}$ top$_{\\!}$ one.\n\\begin{table}[t]\n\t\\centering  \\renewcommand\\thetable{13}\n\t\\small\n\\caption{{\\textbf{Quantitative VIS results on YouTube-VIS}~  the \\texttt{val} (\\S\\ref{sec:VISeva}) \\protect\\\\ in terms of {Precision@$K$}, {mAP}, Recall@$N$ and {IoU}.}}\n\\vspace{-8pt}\n\t\\resizebox{0.47\\textwidth}{!}{\n\t\t\\setlength\\tabcolsep{4.5pt}\n\t\t\\renewcommand\\arraystretch{1.0}\n\t\t\\begin{tabular}{|r||cccc|c|c|}\n\t\t\t\\hline\\thickhline\n\t\t\t\\rowcolor{mygray}\n\t\t\t Method & P@0.5$\\uparrow$ & P@0.75$\\uparrow$ & R@1$\\uparrow$ & R@10$\\uparrow$ &\\tabincell{c}{mAP$\\uparrow$\\\\0.5:0.95} & FPS$\\uparrow$ \\\\\n\t\t\t\\hline \\hline\n\t\t\tfIRN~ & 27.2 & 6.2 & 12.3 & 13.6 & 10.5 & 3 \\\\\n\t\t\t MaskTrack R-CNN~ &51.1 &32.6 &31.0 &35.5 &30.3 &20 \\\\\n             Sipmask~ &53.0 &33.3 &33.5 &38.9 &32.5 &24\\\\\n             STEm-Seg~ &55.8 &37.9 &34.4 &41.6 &34.6 & {9}\\\\\n             CrossVIS~ & 57.3 & 39.7 & 36.0 & 42.0 & 36.6 & 36 \\\\\n             SemiTrack~  & 61.1 & 39.8 & 36.9 & 44.5 & 38.3 & 10 \\\\\n             MaskProp~  &-  &\\textcolor{blue}{\\textbf{45.6}}  &-  &- &\\textcolor{blue}{\\textbf{42.5}}& $^\\dagger${1}\\\\\n             CompFeat~ &56.0 &38.6 &33.1 &40.3 &35.3  &$^\\dagger${17}\\\\\n             TraDeS~ &52.6 &32.8 &29.1  &36.6 &32.6&26\\\\\n             SG-Net~ &\\textcolor{green}{\\textbf{57.1}} &39.6 &\\textcolor{green}{\\textbf{35.9}} &\\textcolor{green}{\\textbf{43.0}} &36.3 &20\\\\\n             VisTR~  &\\textcolor{blue}{\\textbf{64.0}} &\\textcolor{green}{\\textbf{45.0}} &\\textcolor{blue}{\\textbf{38.3}} &\\textcolor{blue}{\\textbf{44.9}} &\\textcolor{green}{\\textbf{40.1}}&58\\\\\n             Propose-Reduce~ &\\textcolor{red}{\\textbf{71.6}} &\\textcolor{red}{\\textbf{51.8}} &\\textcolor{red}{\\textbf{46.3}} &\\textcolor{red}{\\textbf{56.0}} &\\textcolor{red}{\\textbf{47.6}} &{2}\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-14pt}\n\t\\label{table:YVISsota}\n\\end{table}\n\t\\vspace{-10pt}", "cites": [8838, 6767, 6760, 5798, 6761, 6757, 6771, 6756, 5799, 6748, 6750, 6752, 6749, 6746], "cite_extract_rate": 1.0, "origin_cites_number": 14, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a table comparing quantitative performance metrics of various video instance segmentation methods but offers minimal synthesis or explanation of the relationships between these approaches. While it highlights top-performing methods and includes some critical elements like FPS (speed) and performance across different thresholds, it lacks deeper analysis of strengths, weaknesses, or underlying trends in the cited works. The abstraction level is limited, as the section primarily focuses on presenting results rather than identifying broader patterns or theoretical principles."}}
{"id": "f3548146-51fc-4e43-87ec-53f4d1ee5115", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "4118f1c6-6409-444c-a181-ee89e3a8d6d4", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Performance Comparison"], ["subsection", "Summary"]], "content": "{From the results, we can draw several conclusions.\nThe most important of them is related to reproducibility. Across different video segmentation areas, many methods do not describe the setup for the experimentation or do not provide the source code for implementation. Some of them even do not release segmentation masks. Moreover,  different methods use various datasets and backbone models. These make fair comparison impossible and hurt reproducibility.}\n\\red{Another important fact discovered thanks to this study is the lack of information about execution time and memory use. Many methods particularly in the fields of AVOS, LVOS, and VPS, do not report execution time and almost no paper reports memory use. This void is due to the fact that many methods focus only on accuracy without any concern about running time efficiency or memory requirements.} However, in many application scenarios, such as mobile devices and self-driving cars,  computational power and memory are typically limited. As benchmark datasets and challenges serve as a main driven factor behind the fast evolution of segmentation techniques, we encourage organizers of future video segmentation datasets to give this kind of metrics its deserved importance in benchmarking.\nFinally, performance on some extensively studied video segmentation datasets, such as DAVIS$_{16}$~ in AVOS, DAVIS$_{17}$~ in SVOS, A2D Sentence~ in LVOS, have nearly reached saturation. Though some new datasets are proposed recently and claim huge space for performance improvement, {the dataset collectors just gather more challenging samples, without necessarily figuring out which exact challenges have and have not been solved.} \n\t\\vspace{-13pt}", "cites": [6764, 5637], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section analytically discusses broader trends in video segmentation research, such as issues of reproducibility and the lack of reporting execution time and memory usage. It synthesizes information from multiple papers and datasets to highlight a consistent problem across the field. While it identifies important gaps and limitations, it does not offer a fully novel framework or deep meta-level abstraction, but does generalize key concerns beyond individual studies."}}
{"id": "51f778cb-d198-4be0-b5eb-911f2c9fec29", "title": "Future Research Directions", "level": "section", "subsections": [], "parent_id": "a6c9d773-425e-48b4-9e0e-9613ad655a8e", "prefix_titles": [["title", "A Survey on Deep Learning Technique for\\\\ Video Segmentation"], ["section", "Future Research Directions"]], "content": "\\label{sec:6}\nBased on the reviewed research, we list several future research directions {that we believe should be pursued.}\n\\noindent$\\bullet$~\\textbf{Long-Term Video Segmentation}: Long-term video segmentation is much closer to practical applications, such as video editing. However, as the sequences in existing datasets often span several seconds, the performance of VOS models over long video sequences (\\eg, at the minute level) are still unexamined. Bringing VOS into the long-term setting will unlock new research lines, and put forward higher demand of the re-detection capability of VOS models.\n\\noindent$\\bullet$~\\textbf{Open World Video Segmentation}: Despite the obvious dynamic and open nature of the world, current VSS algorithms are typically developed in a closed-world paradigm, where all the object categories are known as a prior. These algorithms are often brittle once exposed to the realistic complexity of the open world, where they are unable to efficiently adapt and robustly generalize to unseen categories. For example, practical deployments of VSS systems in robotics, self-driving cars, and surveillance cannot afford to have complete knowledge on what classes to expect at inference time, while being trained in-house. This calls for smarter VSS systems, with a strong capability to identify unknown categories in their environments .\n{\\noindent$\\bullet$~\\textbf{Cooperation across Different Video Segmentation Sub-fields}: VOS and VSS face many common challenges, \\eg, object occlusion, deformation, and fast motion. Moreover, there are no precedents for modeling these tasks in a unified framework. Thus we call for closer collaboration across different video segmentation sub-fields.}\n\\noindent$\\bullet$~\\textbf{Annotation-Efficient Video Segmentation Solutions}: Though great advances have been achieved in various videos segmentation tasks, current top-leading algorithms are built on fully-supervised deep learning techniques, requiring a huge amount of annotated data. Though semi-supervised, weakly supervised and unsupervised alternatives were explored in some literature, annotation-efficient solutions receive far less attention and typically show weak performance, compared with the fully supervised ones. As the high temporal correlations in video data can provide additional cues for supervision, exploring existing annotation-efficient techniques in static semantic segmentation in the area of video segmentation is an appealing direction.\n\\noindent$\\bullet$~\\textbf{Adaptive Computation}: It is widely recognized that there exist high correlations among  video frames. Though such data redundancy and continuity are exploited to reduce the computation cost in VSS, almost all current video segmentation models are fixed feed-forward structures or work alternatively between heavy and light-weight modes. We expect more flexible segmentation model designs towards more efficient and adaptive computation~, which allows network architecture change on-the-fly -- selectively activating part of the network in an input-dependent fashion.\n\\noindent$\\bullet$~\\textbf{Neural Architecture Search}: Video segmentation models are typically built upon hand-designed architectures, which may be suboptimal for capturing the nature of video data and limit the best possible performance. Using neural architecture search techniques to automate the design of video segmentation networks is a promising direction.\n\t\\vspace{-8pt}", "cites": [709, 6780], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from two cited papers to highlight open challenges and emerging trends in video segmentation, particularly in open-world scenarios and adaptive computation. It critically identifies limitations of current methods, such as reliance on closed-world assumptions and fixed architectures, and abstracts these into broader research themes like annotation efficiency and neural architecture search."}}
