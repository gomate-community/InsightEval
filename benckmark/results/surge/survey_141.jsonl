{"id": "e3b1e733-5824-49ca-be1f-3ad93fd79761", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Introduction"]], "content": "\\customfootnotetext{1}{\\emph{denotes first authors with an equal contribution, in alphabetical order.}}\n\\customfootnotetext{2}{\\emph{denotes second authors with an equal contribution, in alphabetical order.}}\nSince the introduction of deep neural networks in effectively solving the ILSVRC~ image classification task~, deep learning has expanded its horizons in solving a large number of complex tasks. The ability of deep neural networks to act as good function approximators has enabled them to successfully solve tasks such as image classification~, object detection~, instance segmentation~, language modeling~, speech generation~, human pose estimation~, etc. Due to this widespread use of deep learning techniques, the efficiency and reliability of these techniques become equally important when they are deployed in the real world. Many of these applications are crucial, requiring special focus on the safety and security of such systems~.\nRecent studies have shown that deep learning models are susceptible to some carefully constructed small noise called \\textit{adversarial perturbations}, which when added to an input image, cause the network output to change drastically without making perceptible changes to the input image~. Such perturbations pose a severe threat to the real-world application of any neural network model, especially in scenarios of autonomous vehicles and facial verification systems. Studies have shown that if an attacker has access to the model which needs to be attacked (i.e., the target model) along with access to only a few images from the distribution of images using which the target model has been trained, then they might achieve very high fooling rates (see \\autoref{subsec:terminology}) on the entire distribution of images in the target dataset~.\nSeveral techniques have been proposed to generate adversarial examples. While some of these approaches~ focus on maximizing the loss function of the target model by changing the input in the opposite direction of its gradients, i.e., by using gradient ascent, others~ modify the input using a surrogate objective function that causes the target model to misclassify the modified image. For all of these techniques, the perturbations generated are different for different images, i.e., a separate optimization has to be performed for each image to generate an adversarially perturbed image. This kind of generation of adversarial perturbations is hence called \\textit{per-instance generation}.\n\\begin{figure}\n    \\includegraphics[width=\\linewidth]{figures/UAP_original.pdf}\n   \\caption{When the same perturbation (center) is added to clean images on the left, it causes misclassification of images generated on the right~. Note that the change on the clean images (left) after adding the perturbation are imperceptible.}\n\\label{fig:uap_main_illustration}\n\\end{figure}\nWhile per-instance adversarial perturbation varies for different samples in a dataset, there exist \\textit{image-agnostic perturbations} (see \\autoref{subsec:terminology}) called \\textit{universal adversarial perturbations (UAPs)} as introduced by Moosavi-Dezfooli \\etal~ that can fool state-of-the-art (SOTA) recognition models on most natural images with high probability and are quasi-imperceptible, i.e., not visible to the naked eye (see \\textbf{\\autoref{fig:uap_main_illustration}}). Since we need to compute only a single perturbation vector to fool all the images, they are much more efficient in terms of computation time and cost when compared to per-instance adversarial attacks. Generally, the $\\ell_p$ norm of the perturbation is kept small to make it quasi-imperceptible. Furthermore, UAPs generalize well across different architectures, exhibiting excellent transferability and fooling rates on models other than the target model. Since~, many methods have been introduced by researchers to generate UAPs, both \\textit{data-driven} and \\textit{data-independent} (see \\autoref{subsec:terminology}).~ introduced a method to generate UAPs using Universal Adversarial Networks (UANs), leveraging Generative Adversarial Networks (GANs).~ introduced a data-independent approach to generate UAPs by adulterating the features extracted at multiple different layers of the network. Their approach of crafting perturbations did not utilize any knowledge about the data distribution using which the target model has been trained.~ introduced a data-driven approach utilizing fooling and diversity loss along with a generative model to create UAPs.~ extended~ by introducing a two-stage process to create adversaries using class impressions without using any data. Several other works~ have also introduced methods to create adversarial attacks.\nUAPs being image-agnostic can misclassify any unseen image, making deep neural networks vulnerable to attacks. Defense mechanisms are hence required to prevent such attacks.~ introduced the Perturbation Rectifying Network (PRN) to defend against UAPs.~ proposed a method to increase the robustness of a model against UAPs by training with perturbed images using min-max optimization.~ extended the previous approach~ by introducing a shared training procedure for defense against universal adversarial perturbation.~ analyzed the robustness of neural network models against UAPs with varying degrees of shape and texture-based training. \n\\begin{figure}\n    \\includegraphics[width=\\linewidth]{figures/Vertical_timeline_with_type_cropped.png}\n    \\caption{The above timeline shows the year of publication of various attack and defense methods mentioned in this survey. These methods include UAP~, UAP with GM~, NAG~, SPM~, FFF~, PRN~, GD-UAP~, AAA~, UAT~, SAT~, and UAP via PD-UA~. (A) denotes an attacking method and (D) denotes a defense method.}\n    \\label{fig:my_label}\n\\end{figure}\nEven though most of the above work were related to image classification, usage of UAPs extends to both classification and regression tasks.~ showed the usage of UAPs in object recognition, image segmentation, and depth estimation.~ proposed a universal adversarial attack against image retrieval.~ also showed the existence of a universal (\\textit{token-agnostic}) perturbation vector that causes text to be misclassified with high probability.~ showed the existence of UAPs for audio classification and speech recognition systems.\nThis paper aims to (i) summarize the recent advances related to universal adversarial perturbations including the various attack and defense techniques, (ii) compare and analyze various methods proposed for generating UAPs, and (iii) cover the various tasks where the applicability of UAPs has been exhibited.\nThe rest of the paper is organized as follows - in \\autoref{sec:taxonomy}, we specify and define all the notations and terminologies used in the subsequent sections. In \\autoref{sec:existence_of_UAPs}, we briefly state the reasons for the existence of universal perturbations. In \\autoref{sec:attacks}, we try to cover all the different techniques of generating universal adversarial perturbations and provide a detailed comparison of these techniques. In \\autoref{sec:defenses}, we cover the defense techniques that are effective against the attacks introduced in \\autoref{sec:attacks}. Extension of universal perturbations to various tasks has been summarised in \\autoref{sec:applications_of_uaps}. \\autoref{sec:future_directions} summarizes the future directions in context to UAPs. Finally, we provide a conclusion of our survey in \\autoref{sec:conclusion}.", "cites": [38, 4719, 314, 4722, 305, 4725, 4721, 2143, 975, 8184, 4717, 97, 96, 836, 4720, 209, 4715, 4724, 906, 4718, 4723, 892, 520, 4716, 902, 895, 890, 4726, 7307], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction effectively synthesizes key concepts and cited papers to present a coherent narrative around the existence, generation, and defense against UAPs. It abstracts by distinguishing between data-driven and data-independent methods and highlights broader implications of UAPs across tasks and modalities. While it offers some comparisons and identifies research directions, deeper critiques of individual methods or their limitations are limited."}}
{"id": "5ebef26a-debd-480c-b54c-2f8aa980e267", "title": "Terminology", "level": "subsection", "subsections": [], "parent_id": "9d894fbf-626e-42c9-b3fe-176a2c4092e7", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Taxonomy"], ["subsection", "Terminology"]], "content": "\\label{subsec:terminology}\nFollowing is a list of terms along with their definitions:\n\\begin{itemize}\n    \\setlength\\itemsep{0.4em}\n    \\item \\textit{image-agnostic perturbation} or \\textit{universal adversarial perturbation}: Such a perturbation $\\delta$ which can be added to any image $x$ to make a neural network $f$ misclassify the perturbed image $(x+\\delta)$. \n    \\item \\textit{data-driven techniques}: Such techniques which utilize and require some images, $x \\in X$, for generating adversarial perturbations.\n    \\item \\textit{data-independent techniques}: Such techniques which do not consume any image, $x \\in X$, for generating adversarial perturbations.\n    \\item \\textit{target model}: Deep neural network under adversarial attack.\n    \\item \\textit{white box attacks}: Attacks in which the attacker has access to the underlying training policy of the target network model.\n    \\item \\textit{black box attacks}: Attacks in which the parameters and underlying architecture of the target network model is unknown to the attacker.\n    \\item \\textit{non-targeted adversarial attack}: The goal of a non-targeted attack is to slightly modify the source image in a way so that it is classified incorrectly by the target model, without special preference towards any particular output.\n    \\item \\textit{targeted adversarial attack}: The goal of a targeted attack is to slightly modify the source image in a way so that it is classified incorrectly into a specified target class by the target model.\n    \\item \\textit{saturation rate}: The proportion of pixels $p$ out of total pixels in the perturbation $\\delta$ which achieve the max-norm constraint ($\\ell_\\infty$) at the current iteration $t$ (used in ).\n    \\item \\textit{fooling rate}: The proportion of total perturbed images $(x+\\delta)$ in a dataset for which $f(x) \\neq f(x+\\delta)$ where $f$ is the target model.\n\\end{itemize}", "cites": [4719], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive list of terminology related to universal adversarial perturbations, with minimal synthesis or integration of ideas from the cited paper. It lacks critical evaluation or comparison of approaches, and the level of abstraction is limited to defining specific terms rather than identifying broader patterns or principles."}}
{"id": "1663eb12-73ed-42de-987a-a90ac2df5adb", "title": "Notations", "level": "subsection", "subsections": [], "parent_id": "9d894fbf-626e-42c9-b3fe-176a2c4092e7", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Taxonomy"], ["subsection", "Notations"]], "content": "These are the mathematical notations that are followed throughout the paper. We will stick to these notations unless it is explicitly specified.\n\\begin{itemize}\n    \\setlength\\itemsep{0.4em}\n    \\item $x$: Clean input to the target model, typically either a data sample or class impression~.\n    \\item $X$: Distribution of images in $\\mathbb{R}^d$ using which the target model under attack has been trained, i.e., the target distribution.\n    \\item $X_d$: Actual data points $x_{i} \\in X$ available to the attacker in case of data-driven techniques (see \\autoref{subsec:terminology}).\n    \\item $m$: Total number of data points in $X_d$.\n    \\item $f$: Target model under attack, which is a trained model with frozen parameters.\n    \\item $K$: Total number of layers in the target network $f$.\n    \\item $f(x)$: Model prediction for a given data sample $x$.\n    \\item $f^i$: $i^{th}$ layer of the target network $f$.\n    \\item $f^i_k$: $k^{th}$ activation in $i^{th}$ layer of the target model.\n    \\item $f^{ps/m}$: Output of the pre-softmax layer.\n    \\item $f^{s/m}$: Output of the softmax (probability) layer.\n    \\item $\\delta$: Additive universal adversarial perturbation (UAP).\n    \\item $\\Delta$: Distribution of perturbations $\\delta$ in $\\mathbb{R}^d$.\n    \\item $X_\\delta$: Dataset obtained by adding the perturbation $\\delta$ to all the data points in $X_d$.\n    \\item $\\xi$: Max-norm $(\\ell_p)$ constraint on the UAPs, i.e., maximum allowed strength of perturbation.\n    \\item $S_t$: Saturation rate (see \\autoref{subsec:terminology}) at the end of $t^{th}$ iteration.\n    \\item $F_t$: Fooling rate (see \\autoref{subsec:terminology}) at the end of $t^{th}$ iteration.\n    \\item $H$: The patience time interval of validation for verifying the convergence of the proposed optimization (used in ).\n\\end{itemize}", "cites": [4719, 4721], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a list of mathematical notations used throughout the survey without substantial synthesis or integration of ideas from the cited papers. It lacks critical evaluation or comparison of the approaches mentioned in the papers and does not abstract beyond individual definitions to present broader conceptual insights."}}
{"id": "5fb6e5bb-c797-4082-83ab-814c758aa1a1", "title": "Explaining the Existence of Universal Perturbations", "level": "section", "subsections": [], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Explaining the Existence of Universal Perturbations"]], "content": "\\label{sec:existence_of_UAPs}\nBefore diving into the various techniques of generating universal adversarial perturbations, it is essential to understand the reason behind the existence of such image-agnostic perturbations, which can be used to fool a target model $f$ on all the images in the target data distribution $X$.\nFor a target model $f$ to misclassify a sample data point $x \\in \\mathbb{R}^d$, it would be enough to add such a perturbation $\\delta \\in \\mathbb{R}^d$ to the point $x$ so that it just crosses the decision boundary nearest to it. To get the smallest possible $\\delta$, the vector $\\vec{\\delta}$ should be perpendicular to the nearest decision boundary. However, in the case of a universal adversarial perturbation, we want only \\textit{one} such $\\delta$ to be able to fool $f$ for \\textit{all} the data points available in $X$. This means that our objective while generating a universal perturbation is to find such a $\\delta$, which upon adding to all the data points $x$ in $X$, makes them cross the decision boundary nearest to them (see \\textbf{\\autoref{fig:uap_explanation}}).\n\\begin{figure}[h]\n\\centering\n    \\includegraphics[scale=0.5]{figures/uap_illust_algo.pdf}\n   \\caption{In this illustration, data points $x1$, $x2$ and $x3$ are super-imposed, and the classification regions $\\mathscr{R}_i$ (i.e., regions of constant estimated label) are shown in different colors. One of the possible ways to get a universal perturbation~ proceeds by aggregating sequentially the minimal perturbations sending the current perturbed points $x_i+\\delta$ outside of the corresponding classification region $\\mathscr{R}$.}\n\\label{fig:uap_explanation}\n\\end{figure}\nAt first, it might sound a challenging task to generate such a perturbation $\\delta$, which simultaneously fools all the images in $X$. However, by exploiting the geometric correlations between the different parts of the decision boundary of the target model, it has been shown that generating such perturbations is possible. One of the main reasons for the existence of universal perturbations is the existence of a low dimension subspace which captures this correlation between different parts of the decision boundary of the target model $f$ .", "cites": [975], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of why universal adversarial perturbations (UAPs) exist, drawing on geometric reasoning and the concept of low-dimensional subspaces, which are supported by the cited paper. It synthesizes the main idea from the referenced work but does not integrate multiple distinct papers to form a novel framework. While it offers some generalization by discussing the broader implications of decision boundary correlations, it lacks deeper critical evaluation or comparison of different theoretical perspectives."}}
{"id": "aee2862d-6c44-4609-ace5-23ca875ef55c", "title": "Attacks", "level": "section", "subsections": ["78ebaee9-e250-4a89-b978-42a649205d67", "646c0217-848f-4952-8913-d4dff64a68e5", "bf7a4ee5-546e-4219-8d37-40131ef343a0"], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"]], "content": "\\label{sec:attacks}\nUniversal adversarial perturbations as introduced in , are such \\textit{image-agnostic perturbations} $\\delta$ which when added to an image $x \\in X$, cause misclassification of the perturbed image $(x+\\delta)$ when given as input to a target network $f$. Essentially, the main objective of a universal perturbation $\\delta \\in \\mathbb{R}^d$ is to fool the target neural network model $f$ on almost all the input images belonging to the distribution $X$. That is, \n\\begin{equation}\n\\label{eq:1}\n    f(x+\\delta) \\neq f(x),\\ for\\ almost\\ all\\ x \\in X\n\\end{equation}\nIn the following sub-sections, we will be covering some of the data-driven and data-independent techniques of generating universal adversarial perturbations.", "cites": [975], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section introduces the concept of universal adversarial perturbations but primarily describes the definition and objective without integrating insights from other works or providing a comparative or critical analysis. There is limited abstraction or synthesis beyond the single cited paper, resulting in a low insight level."}}
{"id": "78ebaee9-e250-4a89-b978-42a649205d67", "title": "Data-Driven Techniques", "level": "subsection", "subsections": ["330e6c04-0671-4c00-b02d-05e3d3d08a2d", "e075e6cc-819b-4f78-b1ab-993cfac69a51", "7b79d853-8150-4356-943e-6fb9519843a5", "117ba9bc-d01c-4f24-aa18-fd050054c981"], "parent_id": "aee2862d-6c44-4609-ace5-23ca875ef55c", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Driven Techniques"]], "content": "As explained in \\autoref{subsec:terminology}, data-driven techniques require some images $x \\in X$ for the generation of universal perturbations. The actual number of images required for the generation of perturbations depends on the applied  approach. While some techniques~ require only a small fraction of images from the target distribution $X$, others~ require a relatively large fraction of images from the target distribution $X$.", "cites": [975, 4716], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by mentioning two papers and noting a difference in the number of images required for perturbation generation. However, it lacks deeper integration or a narrative that connects the ideas meaningfully. There is no critical analysis or evaluation of the approaches, and it fails to abstract broader principles or trends in data-driven universal adversarial perturbation techniques."}}
{"id": "330e6c04-0671-4c00-b02d-05e3d3d08a2d", "title": "Universal Adversarial Perturbations~\\cite{uap_paper", "level": "subsubsection", "subsections": [], "parent_id": "78ebaee9-e250-4a89-b978-42a649205d67", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Driven Techniques"], ["subsubsection", "Universal Adversarial Perturbations~\\cite{uap_paper"]], "content": "}\nAlong with satisfying \\autoref{eq:1}, the perturbations generated by this method should also follow the max-norm constraint, i.e., the strength of the perturbations $\\delta$ should not be greater than $\\xi$ so that the perturbations remain imperceptible to human eyes. Also, the perturbations should achieve at least a fixed amount of fooling rate on the available data points $X_d$, which can be specified in terms of the desired accuracy ($\\alpha$) on perturbed samples ($X_\\delta$). These equations can be summarised as\n\\begin{equation}\n\\label{eq:2}\n    \\| \\delta \\|_p \\leq \\xi\n\\end{equation}\n\\begin{equation}\n\\label{eq:3}\n    \\underset{x \\sim X_d}{\\mathbb{P}} \\left( f (x+\\delta) \\neq f (x) \\right) \\geq 1 - \\alpha.\n\\end{equation}\n\\setcounter{figure}{3}\n\\begin{figure*}[!b]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/uap_with_gm.pdf}\n  \\caption{Overview of the model used in . A random sample from a normal distribution is fed into a UAN . This outputs a perturbation , which is then scaled and added to an image. The new image is then clipped and and fed into the target model.}\n  \\label{fig:gn_model}\n\\end{figure*}\n\\begin{algorithm}[h]\n\\caption{Iterative Deep Fool Algorithm }\n\\begin{algorithmic}[1]\n\\State \\textbf{input:} Data points $X_d \\in X$, target model $f$, desired $\\ell_p$ norm of the perturbation $\\xi$, desired accuracy on perturbed samples $\\alpha$.\n\\State \\textbf{output:} UAP $\\delta$.\n\\State Initialize $\\delta \\gets 0^{d}$.\n\\While{$\\text{Err} (X_\\delta) \\leq 1-\\alpha$}\n\\For{each datapoint $x_i \\in X_d$}\n\\If{$f(x_i+\\delta) = f (x_i)$}\n\\State Compute the \\textit{minimal} perturbation that sends $x_i+\\delta$ to the decision boundary:\n\\begin{align*}\n\\quad \\quad \\Delta \\delta_i \\gets \\arg\\min_{r} \\| r \\|_2 \\text{ s.t. } f (x_i + \\delta + r) \\neq f (x_i).\n\\end{align*}\n\\State Update the perturbation: \n$$\\delta \\gets \\mathcal{P}_{p, \\xi} (\\delta+\\Delta \\delta_i).$$\n\\EndIf\n\\EndFor\n\\EndWhile\n\\end{algorithmic}\n\\label{alg:finding_universal_perturbations}\n\\end{algorithm}\nTo achieve the above constraints, the proposed \\textbf{\\autoref{alg:finding_universal_perturbations}} initializes $\\delta$ with \\textbf{$0^d$} and for each data point $x_i$ available in $X_d$, a smallest change $\\Delta \\delta_i$ is found which upon adding to $\\delta$ makes the target model $f$ misclassify the perturbed data point $(x_i+\\delta+\\Delta\\delta_i)$. The added perturbation ($\\delta + \\Delta \\delta_i)$ is then projected on an $\\ell_p$ ball of radius $\\xi$ using the operation $\\mathcal{P}_{p, \\xi} (\\delta+\\Delta \\delta_i)$ to get the updated $\\delta$. This is done to satisfy the max-norm constraint given by \\autoref{eq:2}.\nTo ensure constraint given by \\autoref{eq:3}, all the points in the available data set $X_d$ are iterated as above until the total error $\\text{Err} (X_\\delta)$ achieves the desired value as specified in terms of $\\alpha$. $\\text{Err} (X_\\delta)$ is calculated as \n\\begin{equation}\n\\label{eq:4}\n    \\text{Err}(X_\\delta) := \\frac{1}{m} \\sum_{i=1}^m 1_{f (x_i+\\delta) \\neq f(x_i)} \\geq 1-\\alpha\n\\end{equation}\nThe proposed algorithm given by \\textbf{\\autoref{alg:finding_universal_perturbations}}, finds one of the many possible $\\delta$ satisfying \\autoref{eq:2} and \\autoref{eq:3} and not necessarily the optimal one. Perturbations $\\delta$ generated by using the proposed algorithm \\textbf{\\autoref{alg:finding_universal_perturbations}} are illustrated in \\textbf{\\autoref{fig:uap_samples}} for various target models. We can identify some visual patterns in all of these perturbations. The perturbations for shallower networks such as VGG-16 and VGG-19~ have coarse patterns, while the perturbations for deeper networks such as GoogLeNet~ and ResNet-152~ have much finer patterns.\n\\setcounter{figure}{2}\n\\begin{figure}[h!]\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_samples/vgg_16.pdf}\n  \\caption{VGG-16}\n  \\label{fig:sfig1}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_samples/vgg_19.pdf}\n  \\caption{VGG-19}\n  \\label{fig:sfig2}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_samples/googlenet.pdf}\n  \\caption{GoogLeNet}\n  \\label{fig:sfig3}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_samples/resnet_iter2.pdf}\n  \\caption{ResNet-152}\n  \\label{fig:sfig4}\n\\end{subfigure}\n\\caption{Universal perturbations generated using \\autoref{alg:finding_universal_perturbations}~ for different target models.}\n\\label{fig:uap_samples}\n\\end{figure}", "cites": [975, 514, 97, 305], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a detailed description of the algorithm for generating Universal Adversarial Perturbations from Paper 1 but does little to synthesize or connect it with the other cited papers. It includes some visual results and basic observations (e.g., perturbation patterns varying with network depth), but lacks critical analysis or deeper abstraction to broader principles or trends."}}
{"id": "e075e6cc-819b-4f78-b1ab-993cfac69a51", "title": "Learning Universal Adversarial Perturbation with Generative Models~\\cite{Learning_UAPs_with_GM", "level": "subsubsection", "subsections": [], "parent_id": "78ebaee9-e250-4a89-b978-42a649205d67", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Driven Techniques"], ["subsubsection", "Learning Universal Adversarial Perturbation with Generative Models~\\cite{Learning_UAPs_with_GM"]], "content": "}\nIn this attack, a generative model, namely the Universal Adversarial Network (UAN, denoted by $\\mu$), is used to craft the universal perturbations. $\\mu$ is trained similarly as the generator network in a GAN~, and hence, after training $\\mu$ learns a whole distribution of perturbations rather than just a single perturbation as in .\nAn overview of the attack is illustrated in \\textbf{\\autoref{fig:gn_model}}. A random vector $z$ sampled from a normal distribution $\\mathcal{N}(0, 1)^{100}$ is given as input to $\\mu$, which outputs a perturbation $\\delta$ as output. This delta is then scaled by a factor $\\omega \\in (0, \\frac{\\xi}{||\\delta||_p}]$ where $\\xi$ is the maximum allowed strength of the perturbation and p = (2 or $\\infty$), to obtain a scaled perturbation $\\delta^{'}$. The adversarial image created by adding $\\delta^{'}$ to an image $x \\in X_{d}$ is clipped and finally fed into the target model $f$.\n\\setcounter{figure}{4}\n\\begin{figure*}[b]\n    \\includegraphics[width=\\textwidth]{figures/nag_figure.pdf}\n   \\caption{Overview of the approach proposed in NAG~ to model the distribution of generated perturbations for a given target model (Best viewed in colour).}\n\\label{fig:nag_fig}\n\\end{figure*}\nThe parameters of the UAN are then optimized using the following loss function\n\\begin{equation}\n\\label{eq:5}\n\\begin{aligned}\n    L_{\\text{nt}} = \\max\\{\\log{[f(\\delta^{'} + x)]_{c_0}} - \\max_{i{\\neq}c_0}\\log{[f(\\delta^{'} + x)]_{i}} - k\\} \\\\ + \\alpha\\cdot||\\delta^{'}||_p\n\\end{aligned}\n\\end{equation}\nfor a non-targeted attack where $c_0$ is the class label predicted by $f$, and\n\\begin{equation}\n\\label{eq:6}\n\\begin{aligned}\n    L_{\\text{t}} = \\max\\{\\max_{i{\\neq}c}\\log{[f(\\delta^{'} + x)]_{i}} - \\log{[f(\\delta^{'} + x)]_{c}} - k\\} \\\\ +  \\alpha\\cdot||\\delta^{'}||_p\n\\end{aligned}\n\\end{equation}\nfor a targeted attack where $c$ is the target class. It is important to note that the parameters of the target model are frozen and not optimized during the attack. \n\\begin{algorithm}[h]\n\\caption{Learning Universal Adversarial Perturbations with Generative Models~}\n\\begin{algorithmic}[1]\n\\State \\textbf{input:} Data points $X_d \\in X$, target model $f$, desired $\\ell_p$ norm of the perturbation $\\xi$. \n\\State \\textbf{output:} Trained UAN model $\\mu$ that has learned a distribution of UAPs $\\Delta$.\n\\While{max-iteration or convergence}\n\\State Sample a vector z from $\\mathcal{N}(0, 1)^{100}$.\n\\State $\\delta \\gets \\mu(z)$.\n\\State $\\delta^{'} \\gets \\omega\\cdot\\delta.$\n\\State get an image $x$ from $X_d$\n\\If{$\\argmax_{i}f(x+\\delta) = \\argmax_{i}f(x)$}\n    \\State For non-targeted attack, optimize \\autoref{eq:5}\n    \\State For targeted attack, optimize \\autoref{eq:6}\n\\EndIf\n\\EndWhile\n\\end{algorithmic}\n\\label{alg:finding_uap_using_gm}\n\\end{algorithm}\nDepending on the type of attack, \\autoref{eq:5} or \\autoref{eq:6} is optimized. The optimization is stopped once the required adversarial perturbation is found, and the confidence threshold condition is satisfied, which is given by \\autoref{eq:k_gan}.\n\\begin{equation}\n    \\label{eq:k_gan}\n    k > \\max_{i\\neq c_{0}}\\log[f(\\delta^{'} + x)]_{i} - \\log[f(\\delta^{'} + x)]_{c_{0}}\n\\end{equation}\nWhere $k$ is a hyperparameter. The complete approach can be summarized by \\textbf{\\autoref{alg:finding_uap_using_gm}}. The main advantage of this approach over~ is that it allows the UAN to learn a whole distribution of universal perturbations rather than just a single perturbation. This distribution is especially useful (i) to provide an insight into the working and susceptibility of the target model, (ii) accordingly prevent black-box attacks, (iii) ease transferability of the generated perturbations across networks and increase diversity between them, as well as, (iv) facilitate adversarial training (see \\autoref{sec:defenses}).", "cites": [975], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section primarily describes a generative model-based method for crafting universal adversarial perturbations, including its algorithm and loss functions. While it mentions that the UAN learns a distribution of perturbations instead of a single one, it does so in a descriptive manner without connecting it to broader trends or contrasting it meaningfully with other methods. The analysis is minimal, and the abstraction is limited to pointing out potential uses without deeper conceptual generalization."}}
{"id": "117ba9bc-d01c-4f24-aa18-fd050054c981", "title": "Stochastic Power Method~\\cite{asv_uap", "level": "subsubsection", "subsections": [], "parent_id": "78ebaee9-e250-4a89-b978-42a649205d67", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Driven Techniques"], ["subsubsection", "Stochastic Power Method~\\cite{asv_uap"]], "content": "}\nTo achieve the objective given by \\autoref{eq:1}, this approach proposes to maximize the difference between the activations of a particular layer for clean image $x$ and perturbed image $x+\\delta$. Mathematically,\n\\begin{equation}\\label{eq:7}\n\\|f^i(x + \\delta) - f^i(x) \\|_q\\to \\max, \\quad \\|\\delta\\|_p = \\xi\n\\end{equation}\nfor a small value vector $\\delta$ we have,\n$$f^i(x + \\delta) - f^i(x) \\approx J^i(x) \\delta$$ where \n$$J^i(x) = \\frac{\\partial f^i}{\\partial x} \\bigg\\rvert_{x}$$\nis the Jacobian matrix of $f^i$.\nFor the $q$ norm we can write,\n\\begin{equation}\\label{eq:8}\n\\|f^i(x + \\delta) - f^i(x) \\|_q \\approx \\| J^i(x) \\delta \\|_q\n\\end{equation}\nTo maximize the value on the L.H.S. of \\autoref{eq:8}, we need to maximize the R.H.S. of \\autoref{eq:8}.\nThus, the final optimization problem is reduced to \n\\begin{equation}\\label{eq:9}\n\\| J^i(x) \\delta \\|_q \\to \\max, \\quad \\|\\delta\\|_p = \\xi\n\\end{equation}\nand for the given set of data points $X_d$, \\autoref{eq:9} is modified to \n\\begin{equation}\\label{eq:10}\n\\sum_{x_j \\in X_d} \\| J^i(x_j) \\delta\\|_{q} \\to \\max, \\quad \\|\\delta\\|_p = \\xi\n\\end{equation}\n\\autoref{eq:10} can be solved by using the Power Method~. Let $X_d = \\lbrace x_1, x_2 \\hdots x_m \\rbrace$ be the subset of the training set chosen to create the perturbations and m be the total number of samples. We compute $J^i(x_j) \\in \\mathbb{R}^{m \\times n}$ for each $x_j \\in X_d$ and stack these matrices in the following manner:\n\\begin{equation}\\label{eq:12}\nJ^i(X_d) = \n\\begin{bmatrix}\nJ^i(x_1) \\\\\nJ^i(x_2) \\\\\n\\hdots \\\\\nJ^i(x_m)\n\\end{bmatrix}\n\\end{equation}\nWe then compute \\textit{matvec} functions of $J^i(X_d)$ and $(J^i(X_d))^T$ and run the Power Method for computing the desired $\\delta$. \\textbf{\\autoref{alg:uap_spm}} summarizes the proposed approach (refer to the actual paper for the detailed algorithm).\n\\begin{algorithm}[!ht]\n\\caption{Stochastic Power Method for generating UAPs~}\\label{alg:uap_spm}\n\\begin{algorithmic}[1]\n\\State \\textbf{input}: A batch of images $X_d = \\lbrace x_1, x_2, \\hdots x_m \\rbrace$, $f^i(x)$ - fixed hidden layer of the target model\n\\State \\textbf{output}: UAP $\\delta$\n\\For{$x_j \\in X_d$}\n\\State Compute the Jacobian of $J^i(x_j)$ and $(J^i(x_j))^T$  \n\\EndFor\n\\State Construct the $matvec$ functions of $J^i(X_d)$ and $(J^i(X_d))^T$ defined in \\autoref{eq:12}.\n\\State Run Power Method~ with desired $p$ and $q$.\n\\State \\textbf{return} $\\delta$\n\\end{algorithmic}\n \\label{alg:stochastic_pm}\n\\end{algorithm}", "cites": [4716, 917], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the Stochastic Power Method approach for generating Universal Adversarial Perturbations, drawing from Paper 1. It presents the mathematical formulation and algorithmic steps but does not critically evaluate the method or compare it with others. There is minimal abstraction or synthesis beyond reproducing the method's technical details."}}
{"id": "f75e2d69-8c45-4c2d-b718-3a3aa8f82e16", "title": "Fast Feature Fool~\\cite{fast_feature_fool", "level": "subsubsection", "subsections": [], "parent_id": "646c0217-848f-4952-8913-d4dff64a68e5", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Independent Techniques"], ["subsubsection", "Fast Feature Fool~\\cite{fast_feature_fool"]], "content": "}\nThe paper, for the first time, raises a reasonable concern for training data availability and thus proposes a novel data-independent approach to generate image-agnostic perturbations for a range of CNNs trained for object recognition, and shows its triple universality: (i) universality across multiple images from the target dataset over a given CNN, (ii) transferability across multiple networks trained on target dataset, and (iii) the surprising ability to fool CNNs trained on datasets different than the target dataset.\nDue to the unavailability of data, the training objective has to be different from the simple flipping of network output (as used in prior data-driven approaches), so they formulated an optimization problem to compute the perturbation $\\delta$ which misfires the features at individual layers of a CNN to impartially raise activations, eventually leading to the misclassification of the sample, thus fooling the CNN with high probability.\nLoss function used for the computation of $\\delta$ is given by,\n\\begin{equation}\n\\label{eq:fff_prod}\n  Loss = - \\log \\left( \\prod\\limits_{i=1}^K \\bar{f^i}(\\delta) \\right) \\quad \\text{s.t.} \\quad ||\\: \\delta\\: ||_{\\infty} < \\xi\n\\end{equation}\nwhere, $\\bar{f^i}(\\delta)$ is the mean activation at layer $i$ when the perturbation $\\delta$ is input to the CNN, and $K$ is the total number of layers in the CNN at which we maximize activations for the perturbation $\\delta$.\nThe proposed objective computes the product of mean activations at multiple layers in order, and, intuitively, the product is a more definite constraint to force activations at all\nlayers to increase simultaneously for the loss to reduce. Also, to avoid working with extreme values ($\\approx 0$), the logarithm of the product is used as the loss function. It is also worth noting that activations are considered after the non-linearity (typically ReLU). Therefore $\\bar{f^i}$ is non-negative, and it is empirically suggested to restrict optimization at convolution layers.\nFinally, convergence is understood when either the loss gets saturated or the fooling performance over a small held outset is maximized. The complete algorithm is summarized in \\textbf{\\autoref{algo:fff}}.\n\\begin{algorithm}[h]\n\\begin{algorithmic}[1]\n\\caption{UAP generation via FFF~}\n  \\State \\textbf{input:} Target model $f$, desired $\\ell_\\infty$ norm of the perturbation $\\xi$\n  \\State \\textbf{output:} UAP $\\delta$\n  \\State Initialize $\\delta$ randomly.\n  \\While{convergence}\n    \\State Optimize $\\delta$ to achieve higher $\\bar f^i(\\delta)$ as in \\autoref{eq:fff_prod}.  \n    \\State Clip $\\delta$ to satisfy imperceptibility constraint $\\xi$\n  \\EndWhile\n  \\State \\textbf{return} $\\delta$\n\\label{algo:fff}\n\\end{algorithmic}\n\\end{algorithm}\n\\begin{figure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/fff_uap_samples/caffenet.png}\n  \\caption{CaffeNet}\n  \\label{fig:sfig1}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/fff_uap_samples/vgg16.png}\n  \\caption{VGG-16}\n  \\label{fig:sfig2}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/fff_uap_samples/vgg19.png}\n  \\caption{VGG-19}\n  \\label{fig:sfig3}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/fff_uap_samples/googlenet.png}\n  \\caption{GoogLeNet}\n  \\label{fig:sfig4}\n\\end{subfigure}\n\\caption{Universal perturbations generated using Fast Feature Fool algorithm~ for different target models.}\n\\label{fig:fff_uap_samples}\n\\end{figure}", "cites": [975, 8184], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the Fast Feature Fool (FFF) method and its mathematical formulation, drawing from Paper 2. It integrates the technique into the broader context of data-independent UAP generation but lacks in-depth comparative or critical analysis with other methods. The synthesis is limited to explaining the method and its novelty, without broader abstraction or evaluation of its strengths and weaknesses."}}
{"id": "addf5a90-e538-4506-ada7-7ea15d4b07ff", "title": "GD-UAP~\\cite{gd_uap", "level": "subsubsection", "subsections": [], "parent_id": "646c0217-848f-4952-8913-d4dff64a68e5", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Independent Techniques"], ["subsubsection", "GD-UAP~\\cite{gd_uap"]], "content": "}\nThis approach is an extension of the previous data-independent approach~ for generating universal adversarial perturbations. Instead of optimizing the loss given by \\autoref{eq:fff_prod}, the proposed method tries to generate perturbations $\\delta$ that can produce \\textit{maximal spurious activations} at each layer of the target model $f$ using the following loss function,\n\\begin{equation}\n\\label{eq:gd_uap_loss_fn}\n    Loss = - \\log \\left( \\prod\\limits_{i=1}^K  \\Vert f^i(\\delta)\\Vert_2 \\right) \\quad \\text{s.t.} \\quad \\Vert\\:\\delta\\:\\Vert_{\\infty} < \\xi\n\\end{equation}\nObserve that the proposed loss function in \\autoref{eq:gd_uap_loss_fn} uses the $\\ell_2$ norm of activations of the intermediate layers instead of mean activations as used in . The authors have also shown the effectiveness of their approach when additional priors available about the target data, such as, (i) mean ($\\mu_d$) and standard deviation ($\\sigma_d$) of the target data $X_d$, or (ii) the actual data points in $X_d$. \n\\begin{algorithm}[htb!]\n\\begin{algorithmic}[1]\n  \\State \\textbf{input:} Target model $f$, data $g$. Note that $g=0$ for data-independent case, $g= d \\sim \\mathcal{N}(\\mu_d,\\sigma_d)$ for range prior case, and $g =X_d$  for training data samples case. \n  \\State \\textbf{output:} Image-agnostic adversarial perturbation $\\delta$\n  \\State Randomly initialize $\\delta_0 \\sim \\mathcal{U}[-\\xi,\\: \\xi]$ \\\\\n  $t=0$\\\\\n  $F_t=0$\n  \\While{$ F_t < \\text{min. of } \\{F_{t-H},F_{t-H+1}\\ldots F_{t-1}\\}$ }\n    \\State $t \\leftarrow t+1$\n    \\State Compute $f^{i}(g+\\delta)$ \n    \\State Compute loss = $- \\sum{\\log \\left( \\prod\\limits_{i=1}^K  \\Vert f^i(g+\\delta)\\Vert_2 \\right)}$ \n    \\State Update $\\delta_t$ through backprop.\n    \\State Compute the rate of saturation $S_t$ in the $\\delta_t$\n    \\If{$ S_t < \\theta  $}\n     \\State $\\delta_t \\leftarrow \\delta_t/2\\;$ \n    \\EndIf\n    \\State Compute $F_t$ of $\\delta_t$ on substitute dataset $D$\n  \\EndWhile\n  \\State $j \\leftarrow \\argmax\\ \\{F_{t-H},F_{t-H+1}\\ldots F_{t}\\}$\n  \\State \\textbf{return} $\\delta_{j}$\n  \\caption{GD-UAP  using different/no priors}\n\\label{algo:gd_uap}\n\\end{algorithmic}\n\\end{algorithm}\nInstead of scaling the generated perturbation $\\delta$ at regular intervals as proposed in , the authors have proposed adaptive rescaling of $\\delta$ based on its saturation rate (see \\autoref{subsec:terminology}). Rescaling of $\\delta$ by half is done only when the saturation rate $S_t$ (see \\autoref{sec:taxonomy}) is less than some constant $\\theta$. \nThe main advantage of this approach is that the authors have shown the effectiveness of their method in fooling neural networks over a wide variety of tasks such as object detection, semantic segmentation and depth estimation (see \\autoref{sec:applications_of_uaps}). The complete algorithm is summarized in \\textbf{\\autoref{algo:gd_uap}}.", "cites": [4719, 8184], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the GD-UAP method and its algorithm, including its loss function and adaptive rescaling mechanism. It makes minimal effort to synthesize or compare it with the cited papers, merely noting that it is an extension of previous approaches. There is no deeper analysis or abstraction to broader principles in universal adversarial perturbation generation."}}
{"id": "766929a3-6002-4698-af9d-e2c7dc2a1e47", "title": "Ask, Acquire and Attack: Data-free UAP Generation using Class Impressions~\\cite{aaa_uap_class_impression", "level": "subsubsection", "subsections": [], "parent_id": "646c0217-848f-4952-8913-d4dff64a68e5", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Independent Techniques"], ["subsubsection", "Ask, Acquire and Attack: Data-free UAP Generation using Class Impressions~\\cite{aaa_uap_class_impression"]], "content": "}\n\\begin{figure}[t]\n    \\includegraphics[width=\\linewidth]{figures/ask_acquire_attack_model.pdf}\n   \\caption{Overview of the proposed approach in . Stage-I, `Ask and Acquire', generates the class impressions to mimic the effect of actual data samples. Stage-II, `Attack', learns a neural network based generative model $G$ which crafts UAPs from random vectors $z$ sampled from a latent space.\n}\n\\label{fig:aaa_uap}\n\\end{figure}\nThe data-driven approaches create UAPs by utilising the underlying data distribution and optimizing the fooling objective, thereby achieving successful fooling rates. However, data-independent approaches have access to only the parameters and architecture of the target network model, leading to lower fooling rates.  `Ask, Acquire and Attack'~ is a two-stage process to craft UAPs, and tries to exploit the benefits of both data-driven and data-independent techniques (see \\textbf{\\autoref{fig:aaa_uap}}).\nIn the first stage, `Ask and Acquire,' the proposed methodology tries to imitate the real data samples by creating class impressions that are reconstructed from the target model's memory or trained parameters. The generation of impressions begin with a random noisy image sampled from a uniform distribution $\\mathscr{U}[0, 255]$ and updating it till it changes the predicted label category with high confidence. For this purpose maximization of the predicted confidence is done in \\autoref{eq:ask_acquire_loss}. Refer \\autoref{sec:taxonomy} for notations used. Different random initialization results in different class impressions.\n\\begin{equation}\n    \\label{eq:ask_acquire_loss}\n    CI_c = \\argmax_{x} f_c^{ps/m}(x)\n\\end{equation}\n\\begin{figure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/class_impressions/noise_image_vggf_1_499.jpg}\n  \\caption{Goldfish}\n  \\label{fig:sfig1}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/class_impressions/noise_image_vggf_190_499.jpg}\n  \\caption{Lakeland terrier}\n  \\label{fig:sfig2}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/class_impressions/noise_image_vggf_7_499.jpg}\n  \\caption{Cock}\n  \\label{fig:sfig3}\n\\end{subfigure}\n\\begin{subfigure}{.25\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/class_impressions/noise_image_vggf_78_499.jpg}\n  \\caption{Wolf spider}\n  \\label{fig:sfig4}\n\\end{subfigure}\n\\caption{Sample class impressions generated during the `Ask and Acquire' stage of  for the VGG-F target model.}\n\\label{fig:aaa_class_impressions}\n\\end{figure}\nFor the second stage, `Attack' a generator network $G$ is trained using the class impressions crafted in the previous stage, leveraging them as training data (see \\textbf{\\autoref{fig:aaa_uap}}). $G$ is trained similar to the generator of a GAN~. The loss functions used in this approach are inspired by~. The main focus of the generator network is to generate perturbations $\\delta$ which misclassify $x$ on addition. For this purpose, we try to minimize the confidence of a clean label on a perturbed sample by optimizing the fooling loss given by \\autoref{eq:aaa_fooling_loss}.\n\\begin{equation}\n    \\label{eq:aaa_fooling_loss}\n    L_f = -\\log(1 - f_c^{s/m}(x + \\delta))\n\\end{equation}\nTo ensure diversity among generated perturbations similar to , the pair-wise distance between the two embeddings $f(x + \\delta_i)$ and $f(x + \\delta_j)$ in a mini-batch is incorporated. Thus, a diversity loss given by \\autoref{eq:aaa_diversity_loss} is also proposed.\n\\begin{equation}\n    \\label{eq:aaa_diversity_loss}\n    L_d = -\\sum_{i,j = 1, i \\neq j}^K d(f(x + \\delta_i), f(x + \\delta_j))\n\\end{equation}\nTherefore total loss needed to be optimized is given by \\autoref{eq:aaa_total_loss}.\n\\begin{equation}\n    \\label{eq:aaa_total_loss}\n    Loss = L_f + \\lambda \\cdot L_d\n\\end{equation}\nThe proposed algorithm, as given by \\textbf{\\autoref{algo:aaa_algorithm}}, trains the generator model $G$ to create universal adversarial perturbations.\n\\begin{algorithm}[h]\n\\begin{algorithmic}[1]\n\\caption{Ask, Acquire and Attack: Data-free UAP Generation using Class Impressions~}\n  \\State \\textbf{input:} Target model $f$, generator network $G$, latent space $z$, random distribution $\\mu$, parameters learning rate $\\lambda$, max-perturbation constraint $\\xi$.\n  \\State \\textbf{output:} Distribution of UAPs $\\Delta$ learned by $G$.\n  \\State Create class impressions data $X_c$ for each class in the target model using \\autoref{eq:ask_acquire_loss}.\n  \\While{max iteration or convergence}\n  \\For{each datapoint $x$ in $X_c$}\n    \\State $\\delta \\gets G(z)$\n    \\State $x^{'} \\gets x + \\delta $\n    \\State Calculate loss as mentioned in $\\autoref{eq:aaa_total_loss}$.\n    \\State Update $G$ through backprop.\n  \\EndFor\n  \\EndWhile\n\\label{algo:aaa_algorithm}\n\\end{algorithmic}\n\\end{algorithm}", "cites": [4721], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the `Ask, Acquire and Attack' method in a procedural manner, focusing on the two-stage process and associated loss functions. It integrates the cited paper's content but lacks deeper connections to other data-independent or data-driven UAP generation methods. There is minimal critical evaluation or abstraction to broader trends or principles in UAP research."}}
{"id": "dcbb33c5-e08b-4e46-8873-bae59ce9c1bf", "title": "UAP Generation via PD-UA~\\cite{uap_prior_driven", "level": "subsubsection", "subsections": [], "parent_id": "646c0217-848f-4952-8913-d4dff64a68e5", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Data-Independent Techniques"], ["subsubsection", "UAP Generation via PD-UA~\\cite{uap_prior_driven"]], "content": "}\n\\begin{figure}[h]\n    \\includegraphics[width=\\linewidth]{figures/pd_ua_model.png}\n   \\caption{UAP generation technique proposed by . Note the texture bias image at the bottom left.}\n\\label{fig:pd_ua_model}\n\\end{figure}\nThe proposed methodology claims that the fooling of a target model is attributed to the predictive uncertainty of the model outputs. \\textit{Epistemic uncertainty} is associated with the parameters of a model that has been trained on a specific dataset (e.g., ImageNet~). In contrast, the \\textit{aleatoric uncertainty} is a data-independent task-dependent uncertainty which stays stable for various input data but differs for various tasks.\nProposed technique is summarised in \\textbf{\\autoref{fig:pd_ua_model}}. To approximate the \\textit{epistemic uncertainy}, the authors have proposed the use of Monte Carlo (MC) dropout~ for the approximation of \\textit{virtual epistemic uncertainty}~ using the following loss function for capturing the same,\n\\begin{equation}\n\\label{eq:pd_ua_epistemic}\n    U_e(\\delta) = \\sum_{i}^{K} \\sum_{k} p_e(\\Delta_{ik})\n\\end{equation}\n\\begin{equation}\n    p_e(\\Delta_{ik}) = \\frac{1}{T} \\sum_t^T[-\\log(\\|f_j^{i}(\\delta)\\|_2 \\cdot z_k^t)],\\\\\n    s.t.\\ \\|\\delta\\|_p < \\xi\n\\end{equation}\nwhere $\\Delta_{ik}$ is the $k^{th}$ neuron in the $i^{th}$ layer of the target model $f$, $z_k^t$ means the neuron $\\Delta_{ik}$ is dropped out through the $t^{th}$ feedforward network, $T$ is the total number of feedforward networks for MC dropout.\n\\begin{algorithm}[h]\n\\begin{algorithmic}[1]\n\\caption{UAP Generation via PD-UA~}\n  \\State \\textbf{input:} Target model $f$, parameters learning rate $\\lambda$, dropout probability $p$\n  \\State \\textbf{output:} Image-agnostic adversarial perturbation $\\delta$\n  \\State Initialize $\\delta$ with texture image.\n  \\While{max iteration or convergence}\n    \\State Compute $f^i(\\delta)$ at $i^{th}$ convolution layer. \n    \\State Approximate $z_j = 0$ via MC dropout.\n    \\State Compute loss function as in \\autoref{eq:pd_ua_loss}.\n    \\State Update $\\delta$ through backprop.\n  \\EndWhile\n  \\State \\textbf{return} $\\delta_{j}$\n\\label{algo:pd_ua}\n\\end{algorithmic}\n\\end{algorithm}\nA texture bias is introduced to take care of the \\textit{aleatoric uncertainty}, which helps maximize the activation of neurons in the target model $f$. The Maximum Activation Loss, as given in \\textbf{\\autoref{fig:pd_ua_model}}, which encourages the reproduction of texture details is given as,\n\\begin{equation}\n\\label{eq:pd_ua_aleatoric}\n    L_a = \\mathbb{E}[\\mathbb{G}_{ij}(\\delta) - \\mathbb{G}_{ij}(\\delta_0)],\\ \\mathbb{G}_{ij}(\\delta) = \\sum_k \\mathbb{F}_{ik}^l(\\delta)\\mathbb{F}_{jk}^l(\\delta)\n\\end{equation}\nwhere $\\mathbb{G}$ is the Gram matrix of the features extracted from certain layers of the target model $f$, $\\mathbb{F}_{ik}^l$ is the activation of the $i^{th}$ filter at position $k$ in the layer $l$, and $\\delta_0$ is the texture style image that is fixed during training, as shown in \\textbf{\\autoref{fig:pd_ua_model}}. The combined loss function can be written as, \n\\begin{equation}\n\\label{eq:pd_ua_loss}\n    Loss = U_e(\\delta) + \\rho \\times L_a\n\\end{equation}\n\\begin{table*}[t]\n\\centering\n\\caption{Summary of the  attributes of different attacking methods. The `Perturbation Norm' indicates the restricted $\\ell_p$-norm of the perturbations that the authors of the respective methods use to show their results. The fooling rates are mentioned from the respective papers with VGG-19~ as the target model.}\n\\label{tab:1}\n\\begin{tabular}{|l||c|c|c|c|}\n\\hline\n\\textbf{Method}               & \\textbf{Data-Driven/Independent} & \\textbf{Targeted/Non-Targeted} & \\textbf{Perturbation Norm} & \\textbf{Fooling Rate} \\\\ \\hline \\hline\n\\textit{UAP}~                  & Data-Driven                      & Non-Targeted                   & $\\ell_2, \\ell_\\infty$   & 77.8\\%     \\\\\n\\textit{UAP with GM}~         & Data-Driven                      & Non-Targeted/Targeted          & $\\ell_2, \\ell_\\infty$  & 84.6\\%      \\\\\n\\textit{NAG}~                  & Data-Driven                      & Non-Targeted                   & $\\ell_\\infty$    &   83.8\\%         \\\\\n\\textit{SPM}~                  & Data-Driven                      & Non-Targeted                   & $\\ell_\\infty$   & 60.0\\%            \\\\\n\\textit{FFF}~                  & Data-Independent                 & Non-Targeted                   & $\\ell_\\infty$      & 43.6\\%         \\\\\n\\textit{GD-UAP}~               & Data-Independent                 & Non-Targeted                   & $\\ell_\\infty$     & 40.9\\%          \\\\\n\\textit{Ask, Acquire, Attack}~ & Data-Independent                 & Non-Targeted                   & $\\ell_1$     & 72.8\\%               \\\\\n\\textit{UAP via PD-UA}~      & Data-Independent                 & Non-Targeted                   & $\\ell_\\infty$      & 48.9\\%          \\\\ \\hline\n\\end{tabular}\n\\end{table*}\nwhere $\\rho$ is a tradeoff factor between the two losses. Furthermore, the authors have used a Laplacian Pyramid Frequency Model to increase the low-frequency part of the perturbation during the update step of the optimization. The overall proposed technique is given in \\textbf{\\autoref{algo:pd_ua}}. A detailed specification of the approach can be found in the paper~.", "cites": [4719, 4721, 975, 4716, 8184, 514, 895], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a focused explanation of the PD-UA method for UAP generation, synthesizing the paper's use of epistemic and aleatoric uncertainty to guide the optimization. It includes relevant equations and algorithmic steps, showing some integration with the broader context of data-independent UAP generation techniques. However, the critical analysis is limitedfew limitations or trade-offs of the method are discussedand the abstraction to broader principles or patterns is minimal, focusing mainly on a single technique."}}
{"id": "bf7a4ee5-546e-4219-8d37-40131ef343a0", "title": "Comparison amongst Attacks", "level": "subsection", "subsections": [], "parent_id": "aee2862d-6c44-4609-ace5-23ca875ef55c", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Attacks"], ["subsection", "Comparison amongst Attacks"]], "content": "\\begin{figure}\n\\centering\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/uap.pdf}\n  \\caption{UAP\\;}\n  \\label{fig:sfig1}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/uap_w_gm.png}\n  \\caption{UAP with GM\\;}\n  \\label{fig:sfig2}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/nag.png}\n  \\caption{NAG\\;}\n  \\label{fig:sfig3}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/asv.png}\n  \\caption{SPM\\;}\n  \\label{fig:sfig4}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/fff.png}\n  \\caption{FFF\\;}\n  \\label{fig:sfig5}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/gd_uap.png}\n  \\caption{GD-UAP\\;}\n  \\label{fig:sfig6}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/aaa.jpg}\n  \\caption{AAA\\;}\n  \\label{fig:sfig7}\n\\end{subfigure}\n\\begin{subfigure}{.2\\textwidth}\n  \\centering\n  \\includegraphics[width=.8\\linewidth]{figures/uap_comparison/pd_ua.png}\n  \\caption{UAP via PD-UA\\;}\n  \\label{fig:sfig8}\n\\end{subfigure}\n\\caption{Universal adversarial perturbations generated by different techniques to fool the VGG-19 target model. Images are best viewed in color.}\n\\label{fig:aaa_class_impressions}\n\\end{figure}\nIn this sub-section, we summarize the strengths and weaknesses of the techniques covered in the previous subsections and compare them based on various parameters.\nSince their introduction in , the techniques to produce UAPs have seen an improvement both in terms of an increase in \\textit{fooling rates}, and a decrease in the number of samples in $X_d$ ($X_d = \\phi$ in case of data-independent techniques) required for generating the UAP.\nThe fooling rates in the case of data-independent techniques  are relatively low as compared to their data-driven counterparts~, which can be explained from the fact that the data-independent techniques do not have any prior knowledge about the distribution of the target data. \nThe amount of data using which the perturbations are optimized is an important factor in determining the time of convergence of an algorithm, as all the techniques involve iterating over all the available data points in $X_d$. While  requires a large amount of data in $X_d$ for perturbation generation,  has shown that by using only 64 images in $X_d$ fooling rates as high as 60\\% can be achieved. Since data-independent techniques do not consume any data points from the distribution $X$, these techniques converge much quicker than their data-driven counterparts but at the cost of lower fooling rates. \nSome of the techniques produce only a single universal adversarial perturbation as in , while others produce a whole distribution of universal adversarial perturbations~. Although a single universal adversarial perturbation is sufficiently capable of fooling a target model, learning a whole distribution of perturbations helps us to understand and analyze the behavior of these perturbations, and hence propose better defense techniques against them.\nAlthough the term \\textit{universal} in universal adversarial perturbation stands for the universality of the perturbation within the same data distribution $X$ using which the target model has been trained (i.e., the same perturbation can be used for all images in $X$ to fool the target network model), the perturbations produced by almost all the techniques introduced in the previous sections show excellent \\textit{transferability} in fooling neural networks other than the target model~. Additionally, it has also been shown that the same perturbation generated for a target model can be used to misclassify images from a data distribution thats different from the target data distribution. The decrease in the fooling rate on images not in the target data distribution is significant in the case of data-driven techniques as compared to data-independent techniques~.", "cites": [4719, 4721, 4716, 975, 8184], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic comparison of attack techniques for universal adversarial perturbations by synthesizing information from cited papers, particularly distinguishing between data-driven and data-free methods. It offers some critical insights into trade-offs between fooling rates and data dependency but lacks deeper evaluation or nuanced critique of individual approaches. Abstraction is limited to general observations about data independence and transferability, without broader conceptual frameworks."}}
{"id": "b4011128-8c20-4f0e-8987-b5e0b6bfb79f", "title": "Defenses", "level": "section", "subsections": ["f257ec54-93e9-4617-a768-7d029a632295", "4814e5bb-4a63-4eff-a0dc-860d3792567b", "f125ef0e-2cfd-4b75-b835-e4959e33e2e3"], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Defenses"]], "content": "\\label{sec:defenses}\nDeep neural networks being vulnerable to adversarial attacks, pose a serious threat to real-world applications such as autonomous cars, biometric identification, and surveillance systems, with security being a critical factor. The vulnerability of target models towards these adversarial attacks reflects the inability of the models to learn the fundamental visual concepts. The cross-model transferability of UAPs~ empowers attackers to generate universal perturbations rather than per-instance perturbations and also facilitates black-box attacks. Therefore, to create robust models and alleviate adversarial attacks, various defense methods were proposed, as discussed in this section.\nTo counter adversarial attacks, two ways of defenses are generally considered: one is to preprocess the image through various image processing algorithms at the inference time, and the other is by making the model more robust through adversarial training.", "cites": [4721, 975], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the importance of defending against adversarial perturbations and outlines two general defense strategies, but it does not synthesize or connect ideas from the cited papers in a meaningful way. It lacks critical evaluation of the methods or their limitations and offers minimal abstraction beyond the specific approaches mentioned."}}
{"id": "f257ec54-93e9-4617-a768-7d029a632295", "title": "Perturbation Rectifying Network~\\cite{uap_prn", "level": "subsection", "subsections": [], "parent_id": "b4011128-8c20-4f0e-8987-b5e0b6bfb79f", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Defenses"], ["subsection", "Perturbation Rectifying Network~\\cite{uap_prn"]], "content": "}\n\\begin{figure*}[t]\n    \\includegraphics[width=\\linewidth]{figures/PRN_1.png}\n   \\caption{The process proposed in PRN . During the training of PRN, the loss is calculated between the label predicted by passing the clean image and the rectified image through the target model. Also, note that the weights of the target model are frozen during training.}\n\\label{fig:PRN_model}\n\\end{figure*}\nThis paper  introduces a new network, namely, the \\textit{Perturbation Rectifying Network (PRN)}, which can be used as a separate network to preprocess the input image before passing it to the target model. The primary purpose of the PRN is to restore the perturbed image to the original image, i.e., to subtract the perturbation from the perturbed image. So in case a perturbation is present (checked via a binary classifier called the \\textit{Detector Network}), a rectified image is passed to the classifier network, else the original image is passed to the classifier network which classifies the input image. The whole process is described in \\textbf{\\autoref{fig:PRN_model}}. \nTo train the PRN, a large amount of diverse UAPs are required to perturb clean images. However, it is very time-consuming to generate perturbations using . So, the authors propose an algorithm to generate synthetic perturbations using a given set of universal adversarial perturbations. \nThe PRN is trained to optimize the following loss function using the set of synthetic perturbations, \n\\begin{equation}\n\\label{eq:prn_loss}\n     Loss = \\frac{1}{N} \\sum_{i=1}^N \\mathscr{L}(l_i, l_i^*)\n\\end{equation}\nwhere $\\mathscr{L}(\\cdot,\\cdot)$ is a loss function, $l_i$ and $l_i^*$ are the labels predicted by the target model for a clean image and for the perturbed image obtained after passing through PRN. \nAlso, note that the parameters of the target model which we wish to defend are frozen during the entire process. Hence, this method is compatible with defending a variety of neural network models against UAPs, and does not require the fine-tuning of the parameters of the target model.\nThe test time process is described in \\textbf{\\autoref{algo:prn}}.\n\\begin{algorithm}[t]\n\\begin{algorithmic}[1]\n\\caption{Method to compute correct confidence vector from a set of perturbed/non-perturbed images }\n  \\State \\textbf{input:} Target model $f$, trained PRN on target model, image dataset of perturbed/non-perturbed images ($X_\\delta$)\n  \\State \\textbf{output:} Correct softmax (probability) $f^{s/m}$ of perturbed/non-perturbed images\n  \\While{iterate over images in $X_\\delta$}\n    \\State Pass image($I$) through PRN and store the output as $R$. \n    \\State Subtract $I$ from $R$ and pass the result to $f(.)$.\n    \\State Pass the output of $f(.)$ to binary classifier $B$.\n    \\If{$B(f(R-I))==0$ (perturbation absent)}   \n     \\State $f^{s/m} \\leftarrow  f(I)\\;$\n    \\ElsIf{$B(f(R-I))==1$ (perturbation present)}    \n            \\State $f^{s/m} \\leftarrow  f(R)$\n    \\EndIf\n  \\EndWhile\n\\label{algo:prn}\n\\end{algorithmic}\n\\end{algorithm}", "cites": [4726, 975], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a clear description of the Perturbation Rectifying Network (PRN) and its training and testing procedures, primarily paraphrasing the methodology from the cited paper (doc_id: 4726). It includes a figure and algorithm from the paper but does not engage in meaningful synthesis with other works or offer critical evaluation of its strengths and weaknesses. The content is focused on specific implementation details without generalizing to broader trends or principles in UAP defense."}}
{"id": "4814e5bb-4a63-4eff-a0dc-860d3792567b", "title": "Universal Adversarial Training~\\cite{uat", "level": "subsection", "subsections": [], "parent_id": "b4011128-8c20-4f0e-8987-b5e0b6bfb79f", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Defenses"], ["subsection", "Universal Adversarial Training~\\cite{uat"]], "content": "}\n\\begin{figure*}[htb!]\n    \\includegraphics[width=\\linewidth]{figures/uap_shared_adver_training.png}\n   \\caption{This figure represents the process flow as mentioned in \\textbf{\\autoref{alg:shared_adv_pert}}. Perturbations created through the previous step are used to train a robust model. The combined process of universal adversarial perturbation generation and training is called Shared Adversarial Training.}\n\\label{fig:shared_adver_model}\n\\end{figure*}\nThe paper first proposes an efficient and straightforward optimization-based universal attack that can be generated using stochastic gradient descent methods, learning perturbations 13 times faster than the standard method~. Then, to defend against these attacks, it proposes two methods of universal adversarial training. The first one models the training process as a two-player min-max game where the minimization is over the target model parameters, and the maximization is over the universal adversarial perturbation. The second method further improves the defense efficiency by providing a `low-cost' algorithm with a slight decrease in robustness, but reducing the training time by half as compared to the previous one.\nDifferent from Moosavi-Dezfooli \\etal~, this paper proposed a stochastic gradient-based optimization for a $\\beta$-clipped loss function,\n\\begin{equation}\n    \\max_{\\delta}  \\mathscr{L}(w, \\delta) =  \\frac{1}{N}\\sum_{i=1}^{N} \\hat l(w, x_i + \\delta) \\text{, s.t. } \\| \\delta \\|_p \\leq \\xi \\label{eq:univ_prob}\n\\end{equation}\n\\begin{equation}\n\\hat l(w, x_i + \\delta) = \\text{min}\\{ l(w, x_i + \\delta) , \\, \\beta\\}\n\\end{equation}\nwhere $l(w, \\cdot)$ represents the loss used for training DNNs, $w$ represents the target model weights, $X=\\{x_i; i=1,\\ldots,N\\}$ represents training samples, $\\delta$ represents universal perturbation, and $\\beta$ is hyperparameter used to clip the unbounded cross-entropy loss.\nThe above optimization problem can be solved by a stochastic gradient method, where each iteration is based on a minibatch of samples instead of one instance. This accelerates computation on a GPU, and requires a simple gradient update instead of the complex DeepFool~ inner loop, resulting in fast convergence of the proposed method.\n\\textbf{Universal adversarial training} formulates the problem of training robust classifiers as a min-max optimization problem.\n\\begin{equation}\n    \\min_{w} \\max_{\\|\\delta\\|_{p} \\leq \\epsilon}  (w,\\delta) =  \\frac{1}{N}\\sum_{i=1}^{N} l(w, x_{i} + \\delta)\n\\label{eq:adv_univ}\n\\end{equation}\nPreviously, solving this optimization problem directly\nwas deemed computationally infeasible due to the substantial cost\nassociated with generating a universal adversarial perturbation~, but the authors show that unlike Madry \\etal~, updating the universal adversarial perturbation using a simple step is sufficient for building universally robust networks.\n\\begin{algorithm}\n    \\caption{Alternating stochastic gradient method for adversarial training against universal perturbation~}\n    \\label{alg:univ_adv_training}\n    \\begin{algorithmic}[1]\n        \\State \\textbf{input}: Training samples $X$, perturbation bound $\\epsilon$, learning rate $\\tau$, momentum $\\mu$\n        \\For{epoch $= 1 \\ldots N_{ep}$}\n        \\For{minibatch $B\\subset X$}\n        \\State Update $w$ with momentum stochastic gradient descent;\n        \\State \\qquad  $g_w \\gets  \\mu g_w  - \\mathbb{}_{x \\in B} [\\nabla_w \\, l(w, x + \\delta)]$\n        \\State \\qquad  $w \\gets w + \\tau g_w$ \n        \\State Update $\\delta$ with  stochastic gradient ascent;\n        \\State \\qquad $\\delta \\gets \\delta + \\epsilon \\, \\text{sign}(\\mathbb{}_{x \\in B} [\\nabla_{\\delta} \\, l(w, x + \\delta)])$ \n        \\State Project $\\delta$ to $\\ell_p$ ball.\n        \\EndFor\n        \\EndFor\n    \\end{algorithmic}\n\\end{algorithm}\nAs in \\textbf{\\autoref{alg:univ_adv_training}}, each iteration alternatively updates the target model weights $w$ using gradient descent, and then updates the universal adversarial perturbation $\\delta$ using gradient ascent, only once per step, and these updates accumulate for both $w$ and $\\delta$ through training.\n\\textbf{Low-cost Universal adversarial training:}\nAs UAPs are image-agnostic, so the results of the updating target model parameters and updating perturbation $\\delta$ should be invariant to their order of change in one iteration. Thus the proposed methodology calls for the simultaneous update for target model parameters and the universal adversarial perturbation in \\textbf{\\autoref{alg:univ_adv_training}}, which backpropagates only once per iteration and produces approximately universally robust models at almost no cost in comparison to natural training.", "cites": [917, 975, 4727, 4725, 906], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers (e.g., UAP generation, adversarial training, and the game-theoretic formulation) into a coherent explanation of universal adversarial training. It critically compares the efficiency of the proposed method with prior approaches like DeepFool and Madry's adversarial training, highlighting computational advantages and trade-offs in robustness. It identifies some broader patterns, such as the invariance of UAPs and the feasibility of simpler optimization for robust training, but stops short of meta-level abstraction."}}
{"id": "f125ef0e-2cfd-4b75-b835-e4959e33e2e3", "title": "Defending against Universal Perturbations with Shared Adversarial Training~\\cite{uap_shared_adversarial_training", "level": "subsection", "subsections": [], "parent_id": "b4011128-8c20-4f0e-8987-b5e0b6bfb79f", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Defenses"], ["subsection", "Defending against Universal Perturbations with Shared Adversarial Training~\\cite{uap_shared_adversarial_training"]], "content": "}\nThis paper introduces the idea of `shared adversarial training to increase the perceptibility of universal adversarial perturbations generated through an adversarially trained model, and handle the tradeoff between enhanced robustness against UAPs vs. reduced performance on clean data samples in a better fashion than prior work.\nThe authors define three relevant risks:\n\\begin{enumerate}\n    \\item \\textit{Expected Risk}: Expected loss of the model for the given data distribution,\n    \\begin{equation}\n        \\rho_{exp}(\\theta) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\, L(\\theta, x, y)\n    \\end{equation}\n    \\item \\textit{Adversarial Risk}: Expected adversarial loss dependent on specific samples,\n    \\begin{equation}\n        \\rho_{adv}(\\theta, \\mathcal{S}) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[\\sup\\limits_{\\xi(x) \\in \\mathcal{S}} L(\\theta, x + \\xi(x), y) \\right]\n    \\end{equation}\n    \\item \\textit{Universal Adversarial Risk}: Expected Adversarial Loss generalised over the entire data distribution,\n    \\begin{equation}\n        \\rho_{uni}(\\theta, \\mathcal{S}) = \\sup\\limits_{\\xi \\in \\mathcal{S}} \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[L(\\theta, x + \\xi, y) \\right]\n    \\end{equation}\n\\end{enumerate}\nAnd relate them as, \n\\begin{equation}\n    \\rho_{exp}(\\theta) \\leq \\rho_{uni}(\\theta, \\mathcal{S}) \\leq \\rho_{adv}(\\theta, \\mathcal{S}), \\;\\; \\forall \\theta \\;\\; \\forall \\mathcal{S} \\supset \\{\\mathbf{0}\\}\n\\end{equation}\nHere the set $\\mathcal{S}$ defines the space from which perturbations may\nbe chosen. The objective of adversarial training is to minimise $\\sigma \\cdot \\rho_{adv}(\\theta, \\mathcal{S}) + (1 - \\sigma) \\cdot \\rho_{exp}(\\theta)$. For this work, an adversary has been defined as a function that maps data points and a set of model parameters to find a strong perturbation that maximizes the fooling rate. A special kind of adversary called heap adversary is further introduced to compute perturbations against each of the heaps generated, as shown in \\textbf{\\autoref{fig:shared_adver_model}}. For further clarification regarding risks, refer~. The proposed algorithm given by \\textbf{\\autoref{alg:shared_adv_pert}} finds $\\delta$ through shared adversarial training.\n\\begin{algorithm}[!ht]\n\\caption{Method to generate shared adversarial perturbations~}\n\\label{alg:shared_adv_pert}\n\\begin{algorithmic}[1]\n\\State \\textbf{input}: Data points $d$, sharedness $s$\n\\State \\textbf{output}: $\\delta$ to be used for adversarial training\n\\State Initialize $delta$.\n\\For{ $d/s$ iterations}\n\\State Generate gradient $\\nabla$ by passing through target model\n\\State $\\nabla$ $\\leftarrow$ $sgn(\\nabla)*\\alpha_k$\n\\State Broadcast $\\nabla$ to size $s$\n\\State $\\delta$ $\\leftarrow$ $\\delta$ $+$ generated shared perturbation $\\nabla$\n\\State clip $\\delta$ to $[-\\xi, \\xi]$\n\\EndFor\n\\State Add shared perturbations $\\delta$ to $d/s$ heaps for adversarial training. \n\\State Iterate till convergence.\n\\State \\textbf{return} $\\delta$\n\\end{algorithmic}\n\\end{algorithm}\nThe benefits of shared adversarial training in this paper are proved to be extended to image segmentation tasks, apart from the usual image classification tasks. From the experiments, it is also noted that this technique leads to models that are particularly robust against universal adversarial perturbations.", "cites": [4720], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically introduces the concept of shared adversarial training and explains its relationship to different risk formulations. It synthesizes the key contributions of the cited paper but does not deeply compare or critique other defense strategies. The abstraction is moderate, as it generalizes the idea to tasks like image segmentation, though it remains focused on the specific framework proposed in the cited work."}}
{"id": "c1226f27-003d-47c8-b739-f5b718dce57a", "title": "Semantic Segmentation", "level": "subsection", "subsections": [], "parent_id": "e700dad3-aa12-463d-8bf7-2ff1bc54a9dd", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Extension of UAPs to Other Tasks"], ["subsection", "Semantic Segmentation"]], "content": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/segmentation/adversarial_target.jpg}\n    \\caption{Illustration of the effect of adding perturbation in generating a target segmentation map to hide pedestrians.}\n    \\label{fig:segmen_pedestrians_uap}\n\\end{figure*}\nSemantic segmentation is the task of assigning a class to every pixel in the image. Various applications rely on semantic segmentation, whether it is route navigation for autonomous cars, robot vision, or portrait mode of a smartphone. Before diving into how adversarial attacks for semantic segmentation are generated, it is important to exemplify the fooling in semantic segmentation. There exist adversarial perturbations that cause the model to output the same segmentation map for different arbitrary inputs. The attacker could also target removing a specific class, leaving the rest of the segmentation map unchanged~. As seen in \\textbf{\\autoref{fig:segmen_pedestrians_uap}}, adding the perturbation hides a specific class (pedestrian in this case) from the target segmentation map. This type of attack poses a severe threat to the real-world application of semantic segmentation in autonomous cars, robotics, or other computer vision-related tasks. Adversarial attacks for segmentation tasks are quite similar to the classification task. While the fooling rate for an image classification task is well defined, it is unclear for other tasks like segmentation. Therefore a task-independent `Generalized Fooling Rate (GFR)' is introduced here,\n\\begin{equation}\n    \\label{eq:GFR}\n    GFR(M) = \\frac{R - M(\\hat{y}_{\\delta}, \\hat{y})}{R}\n\\end{equation}\nHere $M$ is the metric for measuring the performance of a model for any given task, $\\hat{y}$ is the clean image output, $\\hat{y}_{\\delta}$ is the perturbed image output, and $R$ is the range of $M$.\n\\begin{figure}[b]\n\\centering\n\\noindent\\begin{minipage}{\\linewidth}\n  \\centering\n  \\begin{minipage}{.22\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/segmentation/pascal_small_fcn_alexnet_with_data_sat_diff_reg_0.png}\\\n    FCN-Alex\n  \\end{minipage}\n   \\begin{minipage}{.22\\textwidth}\n       \\centering\n    \\includegraphics[width=\\linewidth]{figures/segmentation/pascal_small_fcn8s_vgg16_with_data_sat_diff_reg_0.png}\\\n    {\\small FCN-8s-VGG}\n  \\end{minipage}\n  \\begin{minipage}{.22\\textwidth}\n   \\centering\n    \\includegraphics[width=\\linewidth]{figures/segmentation/pascal_small_vgg16_with_data_sat_diff_reg_0.png}\\\n    DL-VGG\n  \\end{minipage}\n  \\begin{minipage}{.22\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/segmentation/pascal_small_resnet_msc_with_data_sat_diff_reg_0.png}\\\n    DL-RN101\n  \\end{minipage}\n    \\vspace{0.002\\textwidth}\n\\end{minipage}\n\\vspace{.01cm}\n\\caption{UAPs crafted by GD-UAP (see \\autoref{algo:gd_uap}) for semantic segmentation with initial prior as 'data with less BG.'}\n\\label{fig:data-free-sample-perturbations-segmentation}\n\\end{figure}\n\\begin{figure}[t]\n\\centering\n\\noindent\\begin{minipage}{\\columnwidth}\n  \\centering\n  \\begin{minipage}{.20\\linewidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn_alexnet_im.png}\\\n  \\end{minipage}\n   \\begin{minipage}{.20\\textwidth}\n       \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn8s_vgg16_im.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.20\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_resnet_msc_im.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.20\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_vgg16_im.png}\\\n  \\end{minipage}\n  \\vspace{0.002\\textwidth}\n\\end{minipage}\n\\noindent\\begin{minipage}{\\columnwidth}\n  \\centering\n  \\begin{minipage}{.20\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn_alexnet_output.png}\\\n  \\end{minipage}\n   \\begin{minipage}{.20\\textwidth}\n       \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn8s_vgg16_output.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.20\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_resnet_msc_output.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.20\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_vgg16_output.png}\\\n  \\end{minipage}\n  \\vspace{0.004\\textwidth}\n\\end{minipage}\n\\noindent\\begin{minipage}{\\columnwidth}\n  \\centering\n  \\begin{minipage}{.20\\textwidth}\n  \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn_alexnet_pert.png}\\\n    FCN\\\\Alex\n  \\end{minipage}\n   \\begin{minipage}{.2\\textwidth}\n   \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_fcn8s_vgg16_pert.png}\\\n    FCN-8S\\\\VGG\n  \\end{minipage}\n  \\begin{minipage}{.2\\textwidth}\n      \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_resnet_msc_pert.png}\\\n    DL\\\\RN101\n  \\end{minipage}\n  \\begin{minipage}{.2\\textwidth}\n  \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation/2007_009413_vgg16_pert.png}\\\n    DL\\\\VGG\n  \\end{minipage}\n    \\vspace{0.002\\textwidth}\n\\end{minipage}\n\\vspace{0.1cm}\n\\caption{Semantic segmentation output with initial prior as 'data with less BG' for different models. The first row shows perturbed images, the second row shows segmentation output for clean images, and the third row shows segmentation output for perturbed images.}\n\\label{fig:multiple-nets-segmentation}\n\\end{figure}\n\\begin{figure*}[t]\n\\centering\n\\noindent\\begin{minipage}{\\textwidth}\n  \\centering\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_normal.png}\\\n  \\end{minipage}\n   \\begin{minipage}{.23\\textwidth}\n       \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_no_data.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_range.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_with_data.png}\\\n  \\end{minipage}\n  \\vspace{0.002\\textwidth}\n\\end{minipage}\n\\noindent\\begin{minipage}{\\textwidth}\n  \\centering\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_normal_output.png}\\\n  \\end{minipage}\n   \\begin{minipage}{.23\\textwidth}\n       \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_no_data_output.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_with_noise_output.png}\\\n  \\end{minipage}\n  \\begin{minipage}{.23\\textwidth}\n      \\centering\n    \\includegraphics[width=\\linewidth]{figures/depth/311_good_with_data_output.png}\\\n  \\end{minipage}\n  \\vspace{0.002\\textwidth}\n\\end{minipage}\n\\vspace{0.1cm}\n\\caption{The first row shows clean and perturbed images with various priors, and the second row shows the corresponding predicted depth maps from the KITTI~ dataset generated for monodepth-VGG.}\n\\label{fig:sample-depth-images-vgg}\n\\end{figure*}\nDefining the fooling rate in such a manner helps in effectively measuring the changes caused by the adversaries in the model's output. It also assists the attacker in determining the performance of the perturbation in terms of the damage caused to a model with respect to a metric.\nSemantic segmentation models are trained to perform pixel-level classification into $n$ categories, including the background. Performance is generally measured through mean Intersection over Union (IoU) computed between the predicted and ground truth segmentation map. Extension of UAPs~ directly for this task is non-trivial.~ showed that their algorithm for the generation of UAPs could be directly applied for this task without any changes. The authors reported their results on the PASCAL-VOC 2012~ dataset. Since this dataset had the background pixels percentage greater than half, the authors created a smaller training dataset with a lower background pixels percentage and named it as 'data with less BG'. The perturbation created with this data has a higher capability of fooling or an increased Generalized Fooling Rate for mean IoU (GFR(mIoU)).\nAs seen in \\textbf{\\autoref{fig:data-free-sample-perturbations-segmentation}}, perturbations learned for the task of semantic segmentation for different models look different across architectures, similar to object recognition. For the same input image, perturbations learned by different models produce very different outputs even when their clean image output looks similar, as can be noted from \\textbf{\\autoref{fig:multiple-nets-segmentation}}.\nMetzen \\etal proposed a novel approach to generate UAPs specifically for semantic segmentation. They introduced two methods, one to make the target model output a fixed segmentation map, and the other to remove one class from the target model output. They further show that on passing the generated UAPs (in case of a fixed target segmentation map) through the target model, the output looks similar to the target scene itself.\n~ proposed shared adversarial training for training robust models against universal adversarial perturbations for the semantic segmentation task. Their method showed a considerable increase in robustness for both targeted and non-targeted attacks for a little tradeoff in accuracy.", "cites": [4719, 975, 899, 4720], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of how UAPs can be applied to semantic segmentation, referencing relevant papers and introducing the Generalized Fooling Rate (GFR) as a generalized metric. While it connects ideas from multiple sources, the critical evaluation of these approaches is limited. The inclusion of GFR and the examples of attack effects offer some level of abstraction and synthesis, but the section could benefit from a deeper discussion of limitations and trade-offs among methods."}}
{"id": "e81aa347-fdd3-450f-b417-15454730b5df", "title": "Depth Estimation", "level": "subsection", "subsections": [], "parent_id": "e700dad3-aa12-463d-8bf7-2ff1bc54a9dd", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Extension of UAPs to Other Tasks"], ["subsection", "Depth Estimation"]], "content": "Most of the attacks discussed until now mainly focused on creating UAPs for classification tasks. Various recent works such as  showed an increase in the use of convolutional networks for computer-based regression tasks. Depth estimation is the task of obtaining a depth map of an RGB image. For monocular-depth estimation, it can also be framed as a \\textit{pixel-level continuous regression} problem. Algorithms to calculate the depth map do not rely on hand-crafted features and instead use deep neural networks. Adversarial attacks on these models may cause the network to regress the depth per pixel erroneously. Targeted attacks on these networks result in the depth of a specific class to be inaccurate (see \\textbf{\\autoref{fig:sample-depth-images-vgg}}).~ provided an algorithm for crafting UAPs for CNNs performing regression. Similar to semantic segmentation, the fooling rate for depth estimation is reported through Generalized Fooling Rate (see\\autoref{eq:GFR}). The authors showed that the fooling performance of the perturbation also varies with the metric used for analysis, hence reported fooling through $GFR(\\delta)$, $GFR(Abs.Rel.Err.)$ and $GFR(RMSE)$. \nUniversal adversarial perturbations generally target non-robust features of an image for fooling.~ conjectured that \\textit{\"the attacks are made possible not by perturbing salient pixels containing important depth cues, but by mostly perturbing non-salient pixels.\"}. Based on this conjecture, they introduced a training regime for creating a robust model, proposing to mask out the non-salient pixels using a saliency map\\footnote{\\textit{Saliency Map: a set of a small number of pixels from which a CNN can estimate depth accurately.}} since they are the most vulnerable to adversarial attacks.", "cites": [4719, 4728, 4729, 2324], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates concepts from multiple papers to discuss how UAPs can be extended to depth estimation, showing reasonable synthesis. It connects the idea of perturbing non-salient pixels with robust model training, but the critical analysis is limited, with only one conjecture being mentioned without a broader evaluation. The abstraction level is moderate, as it generalizes the vulnerability of regression tasks and identifies a principle about non-salient pixel importance."}}
{"id": "7072b6a0-0384-43f5-95c8-576ac7b3e3db", "title": "Image Retrieval", "level": "subsection", "subsections": [], "parent_id": "e700dad3-aa12-463d-8bf7-2ff1bc54a9dd", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Extension of UAPs to Other Tasks"], ["subsection", "Image Retrieval"]], "content": "\\begin{figure}[h]\n    \\includegraphics[width=\\linewidth]{figures/img_retrieval.pdf}\n   \\caption{The UAP when added to multiple images cause most of the images to shift in the feature space without preserving original neighborhood relationships. The top is the perturbation and dots represent the features of images~ (Best viewed in color).}\n\\label{fig:img_retrieval}\n\\end{figure}\nImage retrieval is a well-established research topic in computer vision , which aims to find relevant images from a dataset given a query image. Li \\etal was the first to propose a universal adversarial attack against image retrieval systems (see \\textbf{\\autoref{fig:img_retrieval}}). Concretely, image retrieval attacks are to make the retrieval system return irrelevant images to the query at the top of the ranking list. Extensive efforts have been made to improve search accuracy, but very little work has been done around the vulnerability of state-of-the-art image retrieval systems. Although retrieval systems use CNNs as the feature extractor, it is still challenging to apply existing UAPs generation techniques in image retrieval directly because of several reasons, namely (i) different dataset label formats, (ii) different goals, (iii) different sizes of model input, and (iv) different model output and optimization methods, as explained in .\nA novel universal adversarial perturbation attack method for image retrieval is proposed in~, tackling the above challenges. A neural network generates a UAP, which, when used to slightly alter the query along with randomized resizing of images and UAPs, breaks the neighborhood relationships among image feature points, causing degradation in the corresponding ranking metric. Image retrieval can be viewed as a ranking problem, from which perspective the relationship between query and references plays an important role. Thus, these relationships are utilized to improve attack performance further. Such attacks showed its efficacy against the real-world visual\nsearch engines like Google Images, revealing threats to such image-retrieval systems.", "cites": [2143], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 3.0, \"abstraction\": 3.5},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section synthesizes the work of Li \\etal by framing the problem of UAPs in image retrieval and explaining the unique challenges compared to image classification. It provides a moderate level of analysis by highlighting the differences in datasets, goals, and model structures. While it introduces a novel attack method, it does not compare multiple "}}
{"id": "814545c0-8c97-415d-b553-56d51ab94a6e", "title": "Text Classification", "level": "subsection", "subsections": [], "parent_id": "e700dad3-aa12-463d-8bf7-2ff1bc54a9dd", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Extension of UAPs to Other Tasks"], ["subsection", "Text Classification"]], "content": "Before talking about perturbations in text classification, we first need to define what fooling a text classification system means. A perturbation in the text classification system means replacing or adding a few words with their synonymous words (see \\autoref{tab:2}).~ describes a token-agnostic perturbation, which when applied to each token of the text (sentence), can misclassify the text. \n\\begin{table}[h]\n\\centering\n\\begin{tabular}{|p{0.75\\linewidth}|c|}\n\\hline\nText (top: original, bottom: adversarial) & Prediction \\\\\n\\hline\ni walk \\textbf{and} dana runs . & 1 \\\\\ni walk \\textbf{,} dana runs . & 0 \\\\\n\\hline\n\\end{tabular}\n  \\caption{Adversarial sample generated on CoLA evaluation data whose annotations are labeled according to grammatical correctness of sentences.}\n  \\label{tab:2}\n\\end{table}\n describes a method to add a UAP to the embedding vector of the tokens of the sentence. These perturbations are generated so that the actual meaning of the sentence does not change. A norm in the n-dimensional embedding space is defined for this purpose, and then a method similar to~ is used to generate the perturbation. There are possibilities of such attacks in NLP systems such as language translation and sentiment classification too.", "cites": [4715, 975], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of universal adversarial perturbations (UAPs) in text classification, primarily paraphrasing the cited papers. It mentions the token-agnostic nature of UAPs and describes a method involving embedding vectors but does not synthesize or connect ideas across sources in a meaningful way. There is minimal critical analysis or abstraction beyond the specific papers."}}
{"id": "31f7caec-4670-49eb-83f7-aef9f95891fd", "title": "Future Directions", "level": "section", "subsections": [], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Future Directions"]], "content": "\\label{sec:future_directions}\nThe performance of universal adversarial perturbations has increased not only in terms of increased fooling rates and lesser or no data requirement but also in terms of convergence time required to craft such perturbations. Exploration and creation of such UAPs using different novel methods not only lay the path for better defense mechanisms but also helps us to have a better understanding of the behavior of the decision boundary of target models.  has shown that models produced by shape-biased training (trained on object shapes) are as vulnerable to UAPs as those by texture-biased (trained on image textures) ones. However, those produced by both training biases are better in performance than other model architectures. The same paper shows that untargeted UAPs are more likely to tilt the decision of the classifier towards specific class labels.\nWhile the extensions covered in \\autoref{sec:applications_of_uaps} show the applicability and effectiveness of universal adversarial perturbations in a wide variety of tasks, actual applicability of UAPs to fool modern AI systems is still far-fetched. This is mainly because the fooling rates achieved with UAPs are still very less than those achieved by per-instance perturbation generation methods~. Nevertheless, due to the minimum computation required during deployment (only the addition of UAP to the input image), UAPs are more practical for attacking any system in realtime. Hence, an increase in the fooling rates of UAPs to levels comparable to those of per-instance perturbations would further increase their applicability in the real world. This would, therefore, make way for better defense mechanisms for attacks against deep neural networks.", "cites": [906, 890], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section attempts to synthesize information from the cited papers to discuss the practicality and potential of UAPs, noting their lower fooling rates compared to per-instance methods and their computational advantages. It provides a basic level of critical analysis by pointing out limitations in fooling rates and implications for real-world use. However, the synthesis is limited in scope and depth, and the abstraction remains grounded in specific observations without forming higher-level principles or trends."}}
{"id": "8f8951b6-78ea-44cf-83f9-5fb951a91fbb", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "a7dff76d-8261-469c-ac7c-85a9e78b868b", "prefix_titles": [["title", "Universal Adversarial Perturbations: A Survey"], ["section", "Conclusion"]], "content": "This paper presented the first comprehensive survey in the direction of universal adversarial perturbations in deep learning. Deep neural networks are found to be vulnerable to adversarial attacks regardless of their high performance and accuracy. Since , many papers have introduced various data-driven and data-independent algorithms to generate universal adversarial perturbations. While being quasi-imperceptible, these perturbations are transferable across multiple networks. Data-independent approaches enable attackers to fabricate white-box attacks and can pose a severe threat to security-critical applications when applied in the real world. In this paper, we surveyed various pre-eminent attacks and defense techniques for both non-targeted and targeted attacks and discussed the reason for their existence. We also discussed the application of these UAPs for various tasks such as object recognition, semantic segmentation, depth estimation, image retrieval, and text classification.\nAfter the review, we conclude that the universal adversarial perturbations pose a significant risk to the application of deep neural networks in the physical world. The current performance of defense techniques for creating robust models is competent, but there is no elixir as such. However, owing to the active research in this field, it is hoped that various attacks and defense techniques to create robust models through deep learning will show up in the future.\n\\subsection*{Acknowledgments}\nWe thank the invaluable inputs and suggestions by Dakshit Agrawal, Aarush Gupta, and other members of Vision and Language Group, IIT Roorkee, that were integral for the successful completion of this paper. We also thank the Institute Computer Centre (ICC), IIT Roorkee, for providing GPU workstations required for performing various experiments involved in the paper.\n\\label{sec:conclusion}\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egpaper_final}\n}\n\\end{document}", "cites": [975], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited literature on universal adversarial perturbations and integrates them into a coherent overview of the problem and its implications. It provides a general abstraction by discussing the broader risks UAPs pose to real-world deep learning applications. However, it lacks in-depth critical analysis, such as evaluating the strengths and weaknesses of specific methods or identifying unresolved issues in the field."}}
