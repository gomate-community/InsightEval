{"id": "d277d9a2-375a-4a41-9ddb-db3a89d81595", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "697307fb-5058-4940-99fb-a281f51f4034", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Introduction"]], "content": "\\label{introduction}\n\tNowadays, deep neural networks (DNNs) have shown their great power in addressing masses of challenging problems in various areas, such as computer vision , audio , and natural language processing (NLP) . Due to their tremendous success, DNN-based systems are widely deployed in the physical world, including many security-critical areas . However, a series of studies  have found that crafted inputs by adding imperceptible perturbations could easily fool DNNs. These modified inputs are so-called adversarial examples, which bring potential security threats to DNN-based systems even in the black-box scenario where the target system is not available to attackers. For example, Figure \\ref{isnatncefigure} shows an adversarial attack on the physical sentiment analysis system named ParallelDots\\footnote{\\url{https://www.paralleldots.com}}. In this case, we cannot obtain any knowledge of the system architecture, model parameters, and training data. However, it fails to distinguish the adversarial example correctly and output erroneous results. In fighting against the threats of adversarial examples, researchers have conducted numerous works on attacks and defenses, leading to a dramatic increase in both theory and application techniques, varying from images to texts. Here, we focus on the adversarial examples in the text domain rather than the well-investigated image domain. \n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\subfigure[original input]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{instance_a.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\subfigure[adversarial text]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{instance_b.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\centering\n\t\t\\caption{Instance of an adversarial attack on the popular text analysis system, ParallelDots. ParallelDots provides a series of APIs for various NLP tasks (\\eg{}, sentiment analysis) that have achieved state-of-the-art (SOTA) performance. We employ a popular adversarial technique based on the genetic algorithm  to craft adversarial texts and evade the ParallelDots. We can find that the text is predicted as negative in high confidence when the words \\emph{inspire} and \\emph{wonderful} in the original input are simply replaced by \\emph{touch} and \\emph{good}, respectively.}\n\t\t\\label{isnatncefigure}\n\t\\end{figure}\n\tIn NLP, DNNs are widely employed in many fundamental tasks (\\eg{}, text classification, natural language inference, and machine translation). Unfortunately, these DNN-based systems suffer obvious performance degradation in facing adversarial examples. Papernot \\etal{}  first found that attackers could generate adversarial examples by adding imperceptible noises into texts, which would induce classifiers to produce incorrect results. Then, an arms race starts in the text domain battleground, resulting in the exposure of studies in this emerging field. Most of the adversarial attacks in texts focus on specific NLP tasks , which will bring potential security concerns to our users. For instance, in the real world, when booking food online, users tend to search for nearby recommended restaurants in mobile apps and read reviews of their products. The service providers  will give suggestions according to the posted comments via various techniques like sentiment analysis . However, these DNN-based text analyzers could be easily fooled by adversarial examples. Attackers can interfere with product ratings by posting adversarial texts. More seriously, attackers can maliciously propagate disinformation via adversarial texts to reap profits and cause profit losses to consumers. Thus, effective defense methods need to be devised, and robust models should be developed for the community.\n\tFor defense, countermeasures have been proposed to enhance the robustness of DNN-based text analyzers. Nevertheless, they are obviously not prepared for the emerging threats of adversarial examples, so that continuous efforts should be taken further. Figure \\ref{statisticsjpg} shows us the publications of adversarial examples in recent years, and it reveals that numerous studies are developing various adversarial techniques which pose challenges to defense. At present, adversarial texts detection  and model enhancement  are two mainstream ideas in fighting against the threats of adversarial texts, but both of them exhibit obvious weakness. For instance, adversarial text detection is only suitable for certain adversarial attacks. Model enhancement like adversarial training suffers the shortcoming in distinguishing adversarial texts generated by unknown adversarial techniques. In summary, tackling unknown adversarial techniques, generalized to different languages, and effective to a wide range of NLP tasks are the three obstacles for the existing defense methods. To bridge this striking gap, it is urgent to inspire researchers to invest in the study of adversarial attacks and defenses in the text domain. Thus, a comprehensive survey is needed to present the preliminary knowledge and introduce the challenges of this field.\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\subfigure[publications in all areas]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{all_areas_color.png}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\subfigure[publications in texts]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{text_color.png}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\centering\n\t\t\\caption{Publications of adversarial examples. Figure \\ref{statisticsjpg}(a) shows the number of publications in the field of adversarial example, which is collected by Carlini, covering a wide range such as image, audio, text, \\etc{}. Figure \\ref{statisticsjpg}(b) represents the number of publications in the adversarial text domain.}\n\t\t\\label{statisticsjpg}\n\t\\end{figure}\n\tIn adversarial attacks and defenses, several surveys focus on the image domain , but few in texts . Here, we introduce these three surveys in texts and list the differences between them. \n\t\\begin{itemize}\n\t\t\\item In 03/2019, Belinkov \\etal{}  mainly focused on the interpretability of machine learning in NLP. They only review some attacks to understanding these models' failure, but their work lacks surveying the defense methods against adversarial attacks.\n\t\t\\item In 03/2020, Xu \\etal{}  systematically reviewed cutting-edge algorithms in the field of images, graphics, and texts. For adversarial attacks in texts, they only describe some methods according to different NLP tasks, but they do not analyze which kind of attack is suitable for the task, nor do they compare the similarities and differences between these methods. Meanwhile, the authors also do not pay attention to the defense in the text domain.\n\t\t\\item In 04/2020, Zhang \\etal{}  mainly compared attack methods in the image domain and described how adversarial attacks were implemented in texts. They divide adversarial attacks into black-box and white-box attacks, just like in the image domain. However, this classification method does not reflect how to generate adversarial examples in NLP. Due to the difference between texts and images, adversarial examples can be classified as char-level, word-level, sentence-level, and multi-level attacks according to the perturbation units in texts. Besides, the specially designed defense method (\\ie{}, spelling-check) in NLP is not introduced in their \\textit{defense} section.\n\t\t\\item In addition, all of them lack some important guidelines such as the difference between Chinese-based and English-based adversarial examples, interpretability of adversarial examples, and combination with other interesting works (\\eg{}, adding adversarial perturbations into deepfake texts to fool deepfake detectors ).\n\t\\end{itemize}\n\tIn this paper, we review the studies of adversarial examples in the text domain with the goal to build robust DNN-based text analyzers by understanding the generation of adversarial texts, the weakness and strengths of existing defense methods, and the adversarial techniques for different NLP tasks. The advances of our work are summarized as follows.  \n\t\\begin{itemize}\n\t\t\\item We review not only adversarial attacks and defenses in the text domain, but also interpretation, imperceptibility, and certification works. Our systematic and comprehensive review helps newcomers to understand this research filed.\n\t\t\\item The prior three surveys only focus on works related to English-based models, and neither of them reviews the efforts of evaluating the robustness of Chinese-based models. We bridge this gap and analyze the differences of adversarial examples between English-based and Chinese-based models.\n\t\t\\item We classify the adversarial texts into \\textit{char-level}, \\textit{word-level}, \\textit{sentence-level}, and \\textit{multi-level} according to the perturbation units in generating adversarial texts. Additionally, we focus on the adversarial attacks for the different NLP tasks. We hope this could inspire future researchers to understand the generation of adversarial texts and further develop general and effective defense methods for these NLP tasks.\n\t\t\\item We combine adversarial examples with model analysis methods to study the adversarial attacks and defenses. We review related analysis methods to explore NLP models' behaviors, contributing to proving the rationality of adversarial attack and defense methods.\n\t\\end{itemize}\n\tThe rest of this paper is organized as follows. We first give the preliminary knowledge of adversarial examples in Section \\ref{background}. Section \\ref{adversarialattacksintext} reviews the adversarial attacks for text classification. Attacks on other NLP tasks are presented in Section \\ref{adversarialexamplesonothertasks}. We introduce the defense methods in Section \\ref{defense}. Section \\ref{Chinesebasedmodels} shows related works on Chinese-based models, and we analyze the differences from English-based ones. Finally, Section \\ref{discussion} discusses our findings and the challenges from the reviewed works, which can shed new light on the following research direction. Section \\ref{conclusion} gives a conclusion about our comprehensive survey.", "cites": [5718, 205, 892, 3863, 313, 5829, 5828, 314, 8957, 5826, 4722, 209, 5832, 5827, 5830, 893, 2465, 2401, 3859, 5833, 5834, 3699, 4235, 4078, 5831], "cite_extract_rate": 0.6756756756756757, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited works to form a coherent narrative on adversarial attacks in the text domain, particularly highlighting differences between prior surveys and current gaps. It provides a critical analysis of the shortcomings in existing surveys and identifies key limitations in attack and defense methods. While it generalizes some patterns (e.g., classification by perturbation units), it stops short of offering deeper meta-level insights or overarching principles."}}
{"id": "af01e25a-8888-4896-a952-a40b9d1d8076", "title": "Interpretation of Adversarial Examples", "level": "subsection", "subsections": [], "parent_id": "7637a2bd-4cd6-4630-89e2-0afc850edf5f", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Interpretation of Adversarial Examples"]], "content": "Answering why adversarial examples exist can help us devise more effective and practical defense methods. In recent years, researchers have been continuously exploring this question since they observed the adversarial examples in 2014. However, the existence of adversarial examples is still an open question to the community. Here, we briefly introduce the recent efforts in exploring this question.\n\t\\begin{enumerate}[leftmargin=*]\n\t\t\\item \\textbf{Model Linear Hypothesis.} Goodfellow \\etal{} proposed the linearity hypothesis and claimed that the existence of adversarial examples was the linear behavior of DNNs in high-dimensional space. The classifier is not sensitive to adversarial perturbations added to each input dimension, but it will misbehave when the perturbations are applied to all dimensions. Other studies like  also support the linear hypothesis contributing to the vulnerability of DNNs. However, Sabour \\etal{}  doubled this point and demonstrated that the linearity hypothesis did not apply to their work because of the internal representation of adversarial examples in the DNN.\n\t\t\\item \\textbf{Data Distribution Influence.} Shafahi \\etal{}  claimed that DNN models were vulnerable to adversarial examples due to the data distribution. For a dataset, if adjacent pixels in images are highly correlated, the model is relatively robust against adversarial examples based on these images. While the pixels spread out and have less correlation, the vulnerability to adversarial examples will increase. However, this could explain the existence of adversarial examples in images well, but it is not applicable to texts.\n\t\t\\item \\textbf{Input Features.} Ilyas \\etal{}  demonstrated that adversarial examples were not bugs, but features. The features can be classified as robust and fragile in prediction. Both of them could be used for prediction, but adversarial examples are generated when the perturbations are added to the fragile features. Thus, the adversarial examples widely exist in image, text, or other domains.\n\t\\end{enumerate}\n\tHowever, the reason for the existence of adversarial examples is still not clear, though continuous efforts have been paid in recent years. Recently, the DNN models have achieved tremendous success in many challenging tasks, but it is still a black-box to us, which draws continuous efforts to open it. Experience simply tells us that the deeper and wider network is the master key for improving performance. Thus, it is difficult to understand why our DNN models are susceptible to adversarial examples. Understanding the working mechanism of DNNs will be a key step in identifying the existence of adversarial examples. We hope that our survey could inspire future researchers to investigate this open question, which will promote the usage of DNNs in safety-critical areas.", "cites": [967, 969, 5836, 892, 5837, 5835], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several hypotheses on the existence of adversarial examples by connecting ideas from multiple papers, such as the linearity hypothesis, data distribution influence, and feature-based explanations. It includes some critical evaluation, like noting that the linearity hypothesis may not fully apply due to internal DNN representations. However, the abstraction remains moderate, as it identifies general patterns but does not offer a meta-level framework or novel theoretical synthesis."}}
{"id": "e34d2918-2c0c-4ca5-986f-0cac9b8993dd", "title": "Transferability of Adversarial Examples", "level": "subsection", "subsections": [], "parent_id": "7637a2bd-4cd6-4630-89e2-0afc850edf5f", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Transferability of Adversarial Examples"]], "content": "Szegedy \\etal{} first found that adversarial examples generated from a neural network could also make another network misbehave by different datasets, reflecting their transferability. Therefore, attackers can train a substitute model and utilize the transferability of adversarial examples for the attack when they have no access and query restriction to target models. Recently, studies show that different types of adversarial attacks have different transferability . For instance, adversarial examples generated from one-step gradient-based methods are more transferable than iterative methods , but their attack abilities are the opposite. Hence, the generation of adversarial examples with high transferability is not only the premise to carry out black-box attacks, but also a metric to evaluate generalized attacks.", "cites": [923, 314, 5838], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key findings from multiple cited papers to explain the concept and implications of adversarial example transferability. It also provides critical insights by highlighting the trade-off between transferability and attack effectiveness across different attack types. The abstraction is strong as it connects transferability to broader application scenarios like black-box attacks, suggesting its strategic importance in adversarial robustness."}}
{"id": "dc001980-e008-4359-983f-bed84c0054fa", "title": "Taxonomy of Adversarial Attacks", "level": "subsubsection", "subsections": [], "parent_id": "517dcaa0-072e-462f-aa02-a4b70f3d4127", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Taxonomy of Adversarial Examples"], ["subsubsection", "Taxonomy of Adversarial Attacks"]], "content": "\\label{typesofadversarialattack}\n\tAdversarial attacks can be conducted in both white-box and black-box scenarios. In the white-box scenario, adversaries have full access to target models. They can generate perfect adversarial examples by leveraging target models' knowledge, including model architectures, parameters, and training data. In the black-box scenario, adversaries can not obtain any knowledge of the target models. They utilize the transferability  of adversarial examples or repeated queries for optimization to perform a black-box attack. \n\tAccording to the desire of adversaries, adversarial attacks can be divided into targeted and non-targeted attacks. In the targeted attack, the generated adversarial example $\\emph{x'}$ is purposefully classified into a specified class $t$, which is the adversary's target. This process mainly relies on increasing the confidence score of class $t$. In the non-targeted attack, the adversary only aims at fooling the model rather than expect the desired output. The result $\\emph{y'}$ can be any class except for $\\emph{y}$. Contrary to the targeted attack, the non-targeted attack operates via reducing the confidence score of the correct class $y$.\n\tIn the text domain, adversarial attacks can be classified as char-level, word-level, sentence-level, and multi-level (shown in Figure \\ref{AEintexts}) according to the perturbation units in generating adversarial examples. Char-level attacks indicate that adversaries modify several characters in words to generate adversarial examples that can fool the detectors. Specifically, the modifications are mostly misspellings, and the common operations include insertion, swap, deletion, and flip. Word-level attacks involve various word perturbations. Attackers generate adversarial examples by inserting, replacing, or deleting certain words in various manners. Sentence-level attacks usually insert a sentence into a text or rewrite the sentence while maintaining its meanings. Multi-level attacks incorporate more than one of the three perturbation attacks to achieve the imperceptible and high success rate attack.\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\setlength{\\belowcaptionskip}{-0.3cm}\n\t\t\\includegraphics[width=\\linewidth]{AEintext.pdf}\n\t\t\\caption{Classification of adversarial texts based on the perturbations units.} \\label{AEintexts}\n\t\\end{figure}", "cites": [892], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and structured overview of adversarial attacks, categorizing them by attack scenario (white-box vs. black-box), intent (targeted vs. non-targeted), and perturbation level (char, word, sentence, multi-level). It references one paper to support its explanation of adversarial examples but does not synthesize multiple sources or compare different approaches. There is minimal critical evaluation or identification of broader trends."}}
{"id": "01a6946c-26d2-4715-84cd-3d93e2d6a9b9", "title": "Taxonomy of Defenses against Adversarial Attacks", "level": "subsubsection", "subsections": [], "parent_id": "517dcaa0-072e-462f-aa02-a4b70f3d4127", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Taxonomy of Adversarial Examples"], ["subsubsection", "Taxonomy of Defenses against Adversarial Attacks"]], "content": "\\label{typesofadversarialdefense}\n\tIn defending against adversarial attacks, the goal is to build a robust DNN model in tackling various known and unknown adversarial techniques well . In the existing studies, the mainstream defense strategies could be divided into adversarial example detection and model enhancement. \n\tAdversarial example detection indicates to directly distinguish adversarial examples from the legitimate inputs based on the observed subtle differences. Model enhancement involves parameters update or architecture modification, such as adversarial training and adding additional layers. In texts, spelling check and adversarial training are two major ways for defending against adversarial attacks. The spelling check is a special detection method in NLP, while adversarial training is a general approach employed in image, text, audio, \\etc{}", "cites": [7305], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of defense strategies against adversarial attacks in text-based DNNs, mentioning detection and enhancement methods. It integrates the cited paper by referencing the difficulty of evaluating defenses but does not elaborate on how the paper informs or supports the taxonomy. There is limited critical analysis or abstraction beyond the surface-level categorization."}}
{"id": "5a33b7a9-ff14-42ad-a78b-cab87026c4d5", "title": "Metrics on Imperceptibility in texts", "level": "subsection", "subsections": [], "parent_id": "7637a2bd-4cd6-4630-89e2-0afc850edf5f", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Metrics on Imperceptibility in texts"]], "content": "\\label{metric}\n\tAdversarial examples are crafted by adding imperceptible perturbations into legitimate inputs to incur erroneous output labels . In the image domain, various metrics are adopted to measure the imperceptibility of adversarial examples. $L_{p}$ norm is the most commonly used method defined as\n\t\\begin{equation} \\label{triangle}\n\t\\Vert\\triangle c \\Vert_{p}=\\sqrt[p]{\\sum_{i=1}^{n} |c'_{i}-c_{i}|^{p}} \n\t\\end{equation}\n\twhere $\\triangle c$ represents the perturbations. $c'_{i}$ and $c_{i}$ are the $i$-th factors in $n$-dimensional vectors $\\vec{c'}$ and $\\vec{c}$, respectively. Formula \\eqref{triangle} represents a series of distances, where $p$ could be 0 , 2 , $\\infty$ , and so on. Specially, when $p$ is equal to zero, $\\Vert\\triangle c \\Vert_{0}$=$\\sum bool(c_{i}\\ne 0)$. $bool$ is a logical function with 0 or 1. \n\tHowever, it is impossible to borrow the ``imperceptible'' measurements in images to texts. In comparison with the pixel-level modification in images, perturbations for generating adversarial texts operate the characters, words, or sentences, which are visible to humans due to the introduced grammatical and spelling errors. A successful attack needs to maintain the semantic meaning of the generated adversarial texts the same as the original ones and be imperceptible to humans. Therefore, imperceptible adversarial examples (\\eg{}, Figure \\ref{isnatncefigure}) in the text domain should satisfy the following basic requirements. (1) No obvious errors could be easily observed by human eyes. (2) The crafted adversarial texts should convey the same semantic meaning as the original ones. (3) The model output on the adversarial text and the legitimate input should be different, which means an erroneous output occurred. Thus, the majority of metrics adopted in images can not be directly applied in measuring texts due to the symbolic  representations of perturbations in texts rather than the number representations like the pixel. Next, we detail the metrics (\\eg{}, Euclidean distance, Edit distance, Cosine similarity, and Jaccard Similarity Coefficient) employed for measuring the imperceptibility of adversarial texts. \n\t\\textbf{Euclidean Distance}. The original Euclidean distance is the beeline from one point to another in Euclidean space. As the mapping of text to this space, it acts as a metric to calculate the similarity between two objects, which are represented as vectors. Thus, given two word vectors $\\vec{m}=(m_1,\\ldots, m_i, \\ldots, m_k)$ and $\\vec{n}=(n_1,\\ldots, n_i, \\ldots, n_k)$, the Euclidean distance $ED$ of these two vectors is defined as\n\t\\begin{equation} \\label{EuclideanDistance}\n\tED\\!=\\!\\sqrt{(m_1\\!-\\!n_1)^2\\!+\\!\\cdots\\!+\\!(m_i\\!-\\!n_i)^2+\\!\\cdots\\!+\\!(m_k\\!-\\!n_k)^2}\n\t\\end{equation}\n\twhere $m_{i}$ and $n_{i}$ are the $i$-th factors in the $k$-dimensional vectors, respectively. The lower the distance is, the more similar they are. \n\t\\textbf{Edit Distance}. Edit distance refers to the number of editing operations required to convert one string to another, and Levenshtein distance  is a widely used edit distance. For two strings $a$ and $b$, the Levenshtein distance $lev$ is calculated by \n\t\\begin{eqnarray} \\label{editDistance}\n\tlev(i,j)=\\left\\{\n\t\\begin{array}{ll}\n\t\\max (i,j),\\quad\\quad \\min (i,j)=0 \\\\\n\t\\min\\left\\{ \\begin{array}{ll}\n\tlev(i-1,j)+1 \\\\\n\tlev(i,j-1)+1 \\quad otherwise. \\\\\n\tlev(i-1,j-1)+1_{a_i\\neq b_i} \n\t\\end{array}\n\t\\right.\n\t\\end{array} \n\t\\right.\n\t\\end{eqnarray}\n\twhere $lev(i,j)$ is the distance between the first $i$ characters in $a$ and the first $j$ characters in $b$. The lower it is, the more similar the two strings are.\n\t\\textbf{Cosine Similarity}. Cosine similarity refers to the similarity between two vectors by measuring the cosine of the angle between them. For two given word vectors $\\vec{m}$ and $\\vec{n}$, the cosine similarity $CD$ is calculated by \n\t\\begin{equation} \\label{cosinesimilarity}\n\tCD = \\frac{\\vec{m} \\cdot \\vec{n}}{\\Vert m \\Vert \\cdot \\Vert n \\Vert} = \\frac{\\sum\\limits_{i=1}^k m_i \\times n_i}{\\sqrt{\\sum\\limits_{i=1}^k (m_i)^2} \\times \\sqrt{\\sum\\limits_{i=1}^k (n_i)^2}}   \n\t\\end{equation}\n\tCompared with Euclidean distance, the cosine similarity pays more attention to the difference between the directions of two vectors. The more consistent their directions are, the more similar they are. \n\t\\textbf{Jaccard Similarity Coefficient}. The Jaccard similarity coefficient is used to compare the similarity between a limited sample set. For two given sets A and B, their Jaccard similarity coefficient $J(A, B)$ is calculated by\n\t\\begin{equation} \\label{JaccardSimilarity}\n\tJ\\left(A, B\\right) = |A \\cap B| / |A \\cup B| \n\t\\end{equation}\n\twhere $0 \\leq J(A,B) \\leq 1$. The closer the value of $J(A,B)$ is to 1, the more similar they are. In texts, intersection $A \\cap B$ refers to similar words in the samples, and union $A \\cup B$ is all words without duplication.\n\tThese aforementioned metrics are widely applied in tackling various machine learning tasks. Euclidean distance and cosine distance accept vectors for calculation, while the Jaccard similarity coefficient and edit distance directly operate on the raw texts without any transformations into vectors. Particularly, Michel \\etal{} proposed a natural criterion for adversarial texts on sequence-to-sequence models. This work focuses on evaluating the semantic equivalence between adversarial examples and the original ones. Experimental results show that strict constraints are useful for keeping meaning-preserving, but the performance compared with the aforementioned metrics needs further research efforts.", "cites": [975, 967, 890, 314, 5833, 3103, 8958, 894, 892, 902, 906, 914, 5829], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts on imperceptibility metrics from the broader adversarial example literature and applies them to the text domain, establishing a coherent framework. It provides a critical analysis by pointing out the limitations of image-based metrics in text and highlights the need for domain-specific approaches. While it introduces patterns (e.g., distinction between vector-based and raw-text metrics), it stops short of forming a novel or overarching theoretical framework."}}
{"id": "31cd2141-07aa-4e48-843f-12381de7f937", "title": "Datasets in Texts", "level": "subsection", "subsections": [], "parent_id": "7637a2bd-4cd6-4630-89e2-0afc850edf5f", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Preliminaries"], ["subsection", "Datasets in Texts"]], "content": "\\label{datasets}\n\tWe survey the top-tier conferences and journals in artificial intelligence (AI) and NLP (\\eg{}, ICLR, ACL, AAAI, EMNLP, IJCAI, NAACL, COLING, TACL, TKDE, and JMLR) to collect the employed databases in texts. Table \\ref{datasetsinthetable} shows the details of the widely-adopted datasets employed in the studies of adversarial attacks.\n\t\\begin{table*}[t]\n\t\t\\scriptsize\n\t\t\\centering\n\t\t\\caption{Twelve popular text datasets employed in the studies of adversarial attacks. The second column shows the name of each data with the source download link. The following three columns give a brief description, size, and application in which NLP task. NLI is short for natural language inference, NMT is short for neural machine translation, and QA is short for question and answer.}\n\t\t\\label{datasetsinthetable}\n\t\t\\begin{adjustbox}{width=\\linewidth,center}\n\t\t\t\\begin{tabular}{|l|l|l|l|l|}\n\t\t\t\t\\hline \n\t\t\t\t\\multirow{3}{*}{Task} & \\multirow{3}{*}{Name} & \\multirow{3}{*}{Description} & \\multirow{3}{*}{Size}  &  \\multirow{3}{*}{Application} \\\\\n\t\t\t\t& & & &   \\tabularnewline\n\t\t\t\t& & & &  \\tabularnewline\n\t\t\t\t\\hline \n\t\t\t\t\\hline\n\t\t\t\t\\multirow{8}{*}{classification} & AG's news\\footnotemark[2] & News from over 2,000 sources & 144K &   \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& DBPedia\\footnotemark[2] & Structured content from Wikimedia projects & 45K &   \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& Amazon\\footnotemark[2] & Product reviews on Amazon & 2 million &   \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& Yahoo\\footnotemark[2] & Yahoo! Answers Comprehensive Questions & 1.4 million &  \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& Yelp\\footnotemark[2] & User reviews of merchants & 140K &  \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& IMDB\\footnotemark[2] & polarized movie reviews & 50K & \\tabincell{l}{ \\\\ } \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& MR\\footnotemark[3] & movie-review data & 10K &  \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& SST\\footnotemark[4] & standard sentiment dataset from Stanford & 240K &  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\tQA & SQuAD\\footnotemark[5] & dataset for question answering and reading comprehension from Wikipedia & 100K &  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{NLI}& SNLI\\footnotemark[6] & human-written English sentence pairs  & 570K &  \\tabularnewline\n\t\t\t\t\\cline{2-5}\n\t\t\t\t& MultiNLI\\footnotemark[7] & crowd-sourced collection of sentence pairs & 433K &  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\tNMT & WMT14\\footnotemark[8] & parallel texts (\\eg{}, German/English) for translation models & -- &  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\end{table*}\n\tBeyond investigating the popular text databases employed in adversarial attacks, we also explore how many datasets are adopted in the recent works and which tasks they apply. Figure \\ref{howmanydatasets} presents the proportion in the corresponding NLP task. We can observe that more than half of the databases focus on text classification, which is a critical NLP task.\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\setlength{\\belowcaptionskip}{-0.3cm}  \n\t\t\\includegraphics[width=\\linewidth]{pie_Chart.png}\n\t\t\\caption{Statistics of datasets used in the research of adversarial attacks. Totally, there are 52 different datasets employed in related works. All of them can be found in Table \\ref{tabpaper_information}.} \\label{howmanydatasets}\n\t\\end{figure}\n\t\\footnotetext[2]{\\url{https://course.fast.ai/datasets}}\n\t\\footnotetext[3]{\\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/}}\n\t\\footnotetext[4]{\\url{https://github.com/stanfordnlp/sentiment-treebank}}", "cites": [4846, 5848, 5849, 5843, 5847, 5829, 9133, 5840, 5844, 3103, 5845, 5842, 5851, 1632, 5830, 5850, 2565, 5839, 5841, 5833, 8012, 5846, 4235, 7586, 8959], "cite_extract_rate": 0.78125, "origin_cites_number": 32, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive overview of text datasets used in adversarial research, listing them in a table and briefly mentioning their applications. It lacks synthesis of ideas across the cited papers and does not engage in critical evaluation or identify broader trends or principles. The content offers minimal analytical depth, focusing more on factual aggregation than on insightful discussion."}}
{"id": "9d7f9e26-b7ed-40f5-92b8-f3630a83eebd", "title": "Char-level Attacks", "level": "subsection", "subsections": [], "parent_id": "744ede91-1e59-47e5-9ed0-4ceac7757786", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Char-level Attacks"]], "content": "Char-level attacks indicate that adversaries modify several characters in words to generate adversarial examples that can fool the detectors. Generally, the modifications are often misspellings, and the operations include insertion, swapping, deletion, and flipping. Although this kind of attack can achieve a high success rate, misspellings can be easily detected. Next, we introduce some representative char-level attacks. \n\tGao \\etal{} proposed a char-level attack, called DeepWordBug, to generate adversarial examples in the black-box scenario, which followed a two-step pipeline. The first stage quantifies the importance of words and determines which one to change. The calculation process for the first stage is shown in \n\t\\begin{equation} \\label{firststage}\n\t\\begin{split}\n\tCS(x_i)=&[F(x_1,\\ldots,x_{i-1},x_i)-F(x_1,x_2,\\ldots,x_{i-1})]+\\\\&\\lambda[F(x_i,x_{i+1},\\ldots,x_n)-F(x_{i+1},\\ldots,x_n)]\n\t\\end{split}\n\t\\end{equation}\n\twhere $CS(x_i)$ represents the importance score of $i$-th word in $(x_{1},\\ldots,x_{n})$, evaluated by the function $F$. $\\lambda$ is a hyper-parameter. The second stage adds imperceptible perturbations to the selected words through swapping, flipping, deletion, and insertion. Meanwhile, edit distance is used to preserve the readability of generated adversarial examples.  \n\tGil \\etal{} derived a new method DISTFLIP based on HotFlip . The authors distill the knowledge of the procedure in HotFlip for training their model. Through the trained model, the authors generate adversarial examples to conduct a black-box attack. This method performs better than HotFlip on a toxicity classifier, and its run-time in generating adversarial examples is ten times faster than HotFlip. However, the capability to distill the knowledge of any white-box attacks is not clear.", "cites": [5853, 5833, 5852], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of char-level attacks with a focus on describing individual methods (DeepWordBug, DISTFLIP) and their components. It integrates some information by mentioning common operations and goals, but lacks deeper connections or a unified framework. There is minimal critical evaluation, such as questioning the generalization of DISTFLIP's knowledge distillation, and no clear abstraction of broader principles or patterns in the text adversarial domain."}}
{"id": "8fce6467-8e06-4ccb-9a18-af7004b0280c", "title": "Gradient-based attacks", "level": "subsubsection", "subsections": [], "parent_id": "b4a6e968-9839-41d5-a721-50141a19865b", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Word-level Attacks"], ["subsubsection", "Gradient-based attacks"]], "content": "Studies on adversarial examples in the image domain are more active than those in texts. Inspired by the fast gradient sign method (FGSM)  in the image domain, attackers generate adversarial examples by calculating the gradient of text vectors in a model.\n\tAs far as we know, Papernot \\etal{} first studied the problem of adversarial examples in texts and contributed to producing adversarial input sequences. The authors leverage computational graph unfolding to evaluate the forward derivative  (\\ie{}, the modelâ€™s Jacobian $J(x)$), which is related to the search of modified words. In detail, they utilize FGSM to guide the perturbations, and it can be represented as \n\t\\begin{eqnarray}\n\tsign(J(x)[i,\\arg\\max_{0,1}(p_j)])\n\t\\end{eqnarray} \n\twhere $i$ is the $i$-th word in an input sequence, and $p_j$ indicates the probability of belonging to category $j$. If the value of $\\arg\\max_{0,1}(p_j)$ changes, the modification is effective. However, the words in input sequences are iteratively selected for substitution. Hence, there may exist grammatical errors in the generated adversarial examples. \n\tDifferent from Papernot \\etal{} , Samanta \\etal{} employed FGSM to evaluate the important or salient words, which deeply affected the results of classification when they were removed. Three modification strategies (\\textit{i.e.}, insertion, replacement, and deletion) are introduced to craft top $k$ words with the highest importance, where $k$ is a threshold. Except for the deletion strategy, both insertion and replacement on top $k$ words require an additional dictionary for operation. Thus, the authors establish a pool of candidates for each word in the experiments, including synonyms, typos, and type-specific keywords. However, the establishment of the candidate pool suffers huge consumption, and there may be no candidate pool for some top $k$ words in the actual inputs.   \n\tUnlike the above methods, Sato \\etal{} proposed iAdv-Text by adding perturbations in the embedding space. iAdv-Text formulate this as an optimization problem, which jointly minimizes objection function $\\mathcal{J}_{iAdvT}(D,W)$ on the entire training dataset $D$ with parameters $\\emph{W}$. The optimization procedure is shown in \n\t\\begin{equation} \\label{eq5}\n\t\\begin{split}\n\t\\mathcal{J}_{iAdvT}(D,W) = &\\frac{1}{|D|}\\mathop{\\arg\\min}_{W}\\{\\sum_{(\\hat{X},\\hat{Y})\\in D}\\ell(\\hat{X},\\hat{Y},W)+\\\\&\\lambda\\sum_{(\\hat{X},\\hat{Y})\\in D}\\alpha_{iAdvT}\\}\n\t\\end{split}\n\t\\end{equation}\n\twhere $\\hat{X}$ and $\\hat{Y}$ represent the inputs and labels, respectively. $\\lambda$ is a hyper-parameter to balance the two loss functions. $\\ell(\\hat{X},\\hat{Y},W)$ is the loss function of individual training sample $(\\hat{X},\\hat{Y})$ in $D$. $\\alpha_{iAdvT}$ is a maximization process to find the worst case weights of the direction vectors calculated by\n\t\\begin{equation} \\label{functionofiAdvT}\n\t\\alpha_{iAdvT} = \\frac{\\epsilon g}{\\Vert g \\Vert_2}, g = \\nabla_{\\alpha}\\ell(\\vec{w} + \\sum_{k=1}^{|V|}a_kd_k, \\hat{Y}, W)\n\t\\end{equation}\n\twhere $\\sum_{k=1}^{|V|}a_kd_k$ is the perturbation generated from each input on its word embedding vector $\\vec{w}$, $\\epsilon$ is a hyper-parameter to control adversarial perturbations, $a_{k}$ is the $k$-th factor of a $|V|$-dimensional word embedding vector $\\alpha$, $d_{k}$ is the $k$-th factor of a $|V|$-dimensional direction vector $\\vec{d}$, which is a mapping from one word to another in embedding space. iAdv-Text restricts the direction of perturbations with cosine similarity for finding a substitution, which is in a pre-defined vocabulary rather than an unknown word. \n\tBehjati \\etal{}  designed a universal perturbations added to any input. They optimize the gradient of loss function $loss$ to construct a word vocabulary $V$ containing words that apply to all data. The optimization is shown in \n\t\\begin{eqnarray}\\label{Behjatietal}\n\tw'_i=\\arg\\min_{w'_i \\in V} cos(emb(w'_i),(emb(w_i)+\\alpha r_i))\n\t\\end{eqnarray}\n\twhere $r_i=\\nabla_{emb(w_i)}loss(l,f(x'))$. $emb(w_i)$ is the corresponding embedding of word $w_i$, $l$ represents the label. If it is a targeted attack, $l$ is the target label, and the learning rate $\\alpha$ is negative. Otherwise, $l$ is the ground truth label, and $\\alpha$ is positive. The words in $V$ will insert into texts to generate adversarial examples. However, the insertion occurs at the beginning of the input sequence leading to grammatical errors and breaking the imperceptibility rule.", "cites": [5841, 5842, 5830, 892, 894], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates several gradient-based adversarial attack methods for text, highlighting commonalities and differences (e.g., substitution strategies, use of embedding spaces). It includes some critical evaluation, such as pointing out the grammatical errors and inefficiencies in candidate pool construction. However, the synthesis remains limited to methodological descriptions without a deeper unifying framework, and abstraction is constrained to general observations about techniques rather than identifying broader theoretical patterns."}}
{"id": "cb07270b-e207-4c05-8ba5-bcabec467a6d", "title": "Importance-based attacks", "level": "subsubsection", "subsections": [], "parent_id": "b4a6e968-9839-41d5-a721-50141a19865b", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Word-level Attacks"], ["subsubsection", "Importance-based attacks"]], "content": "By analyzing the existing methods of generating adversarial texts, researchers noticed that the importance of each word in determining the final predictions is vastly different. Based on this initial idea, researchers have launched successful attacks by computing the importance of words and modifying these high valuable words. Importance-based attacks usually follow a two-step pipeline.\n\t\\begin{enumerate}[leftmargin=*]\n\t\t\\item Calculating the importance of words by querying the target model multiple times.\n\t\t\\item Modifying these important words via insertion, deletion, or replacement. \n\t\\end{enumerate}\n\tRen \\etal{}  designed a synonym replacement method PWWS working on the word-level. The authors construct a synonym set $\\mathbb{L}_i$ for each word in the inputs. To search for suitable substitutions, an optimization process in formula \\eqref{synonymset} is conducted by maximizing the word saliency of these words.\n\t\\begin{equation} \\label{synonymset}\n\t\\begin{split}\n\tR(w_i,\\mathbb{L}_{i})=\\arg\\max{P(y_{true}|x)-P(y_{true}|x_{i}')}\n\t\\end{split}\n\t\\end{equation}\n\twhere $R(w_i,\\mathbb{L}_{i})$ substitutes the best candidate synonym $w_{i}^{*}$ of $i$-th word in the text $x$. $x_{i}'$ is obtained by replacing the $i$-th word in $x$ with each candidate. $P(y|x)$ is the classification probability of $x$. After that, the final process determines the order of replacement in $x$. \n\tHsieh \\etal{}  thought that the changes in words with the highest or lowest attention scores could substantially undermine self-attentive models' predictions. Hence, they exploit the attention scores as a potential source of vulnerability and modify target words with the highest or lowest scores. A random word in the vocabulary is greedily selected to replace the target word until the attack succeeded. Although the constraint on the embedding distance is imposed to keep semantically similar, the vocabulary construction is not clear. Similarly, Yang \\etal{}  greedily searched for the weak spot of the input sentence by replacing a word with the padding. If the probability changes much after modification, the word will be replaced with a randomly selected word in the vocabulary. Due to the lack of constructions, this attack sometimes changes the semantics of the original sentence.  \n\tJin \\etal{}  presented Textfooler, a black-box attack to fool the bidirectional encoder representations from transformer model (BERT) on text classification. They first identify the important words for the target model and then prioritize to replace them with synonyms until the prediction is altered. The word importance $I_{w_i}$ is calculated as \n\t\\begin{eqnarray}\n\tI_{w_i}=\\left\\{\n\t\\begin{array}{ll}\n\tF_Y(X)-F_Y(X_{w_i}),if F(X)=F(X_{w_i})=Y \\\\\n\tF_Y(X)-F_Y(X_{w_i})+F_{\\hat{Y}}(X_{w_i})-F_{\\hat{Y}}(X), \\\\\n\tif F(X)=Y,F(X_{w_i})=\\hat{Y}, and \\quad Y\\neq \\hat{Y}\n\t\\end{array} \n\t\\right.\n\t\\end{eqnarray}\n\twhere $w_i$ is the $i$-th word in $X$. $F_Y(\\cdot)$ represents the prediction score for the $Y$ label.   \n\tConsidering the limitations (\\textit{i.e.,} out-of-context and unnaturally complex token replacements) of synonym substitution methods, Garg \\etal{}  used contextual perturbations from a BERT masked language model to generate adversarial examples. They search for important words similar to Jin \\etal{} . Then, words from the pre-trained BERT masked language model are used to replace the important words in the inputs or inserted to adjacent positions.", "cites": [5849, 8959], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of importance-based attacks by explaining a common two-step pipeline and illustrating how different papers implement it. It integrates multiple works (Ren, Hsieh, Yang, Jin, Garg) and identifies some limitations, such as unnatural replacements and unclear vocabulary construction. However, the synthesis is constrained to a methodological explanation rather than a broader conceptual framework, and the critique remains at a surface level without deeper evaluation of trade-offs or implications."}}
{"id": "5e766cfb-bd34-4e06-a371-319b57a74027", "title": "Other attacks", "level": "subsubsection", "subsections": [], "parent_id": "b4a6e968-9839-41d5-a721-50141a19865b", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Word-level Attacks"], ["subsubsection", "Other attacks"]], "content": "Apart from gradient-based and importance-based attacks, researchers also propose other ways to create adversarial examples such as the genetic algorithm and particle swarm optimization-based search algorithm.\n\tAlzantot \\etal{}  proposed a continuous optimization method at the word-level using the genetic algorithm (GA) . GA initializes the generation by nearest neighbor replacement. The optimization is computed as\n\t\\begin{equation} \\label{nearestneighborreplacement}\n\tx_{adv}=\\mathcal{P}^{g-1}_{\\arg\\max f(\\mathcal{P}^{g-1})_{target}} \n\t\\end{equation}\n\twhere $\\mathcal{P}^{g-1}$ represents the $(g-1)$-th generation, $f$ is the target model, $x_{adv}$ means the best individuals in this generation that can fool $f$ to produce incorrect predictions. If the samples in $x_{adv}$ do not satisfy the requirements, they are selected as the next generation to repeat the previous optimization process, where $\\mathcal{P}^{g}={x_{adv}}$. Different from DeepWordBug , GA utilizes Euclidean distance to maintain the semantics. \n\tZang \\etal{}  proposed a novel attack model, which incorporated the sememe-based word substitution method and particle swarm optimization-based search algorithm. For each word in a given text, the authors use HowNet to find its sememe and then add the same labeled words to the word list. After that, a particle swarm optimization algorithm is applied to search for adversarial examples in a discrete search space composed of all the word lists. This work solves the lack of search space reduction methods and inefficient optimization algorithms, significantly improving the success rate of adversarial attacks.", "cites": [5833, 2565, 5829], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates two different attack methodologies (GA and particle swarm optimization) and connects them to broader issues in word-level adversarial attacks, such as search space reduction and optimization efficiency. It provides some critical insight by highlighting how these methods address existing limitations, but the comparison and deeper critique are limited. The abstraction remains somewhat constrained to methodological descriptions rather than revealing higher-level principles."}}
{"id": "b35348ef-663e-45ed-a6ed-04b953a814d0", "title": "Sentence-level Attacks", "level": "subsection", "subsections": [], "parent_id": "744ede91-1e59-47e5-9ed0-4ceac7757786", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Sentence-level Attacks"]], "content": "Compared with char-level and word-level attacks, the sentence-level attack is more flexible. The modified sentence can be inserted at the beginning, middle, or end of the text when the semantics and grammar are correct. To some extent, the sentence-level attack can be seen as a special kind of word-level attack manipulated by adding some ordered words. This kind of attack usually appears in other NLP tasks like natural language inference (NLI) , neural machine translation (NMT) , reading comprehension (RC) , and question answering (QA) . In the text classification, this kind of attack is much less than others. \n\tIyyer \\etal{}  designed syntactically controlled paraphrase networks (SCPNS) for generating adversarial examples by grammar conversion, which relied on the encoder-decoder architecture of SCPNS. Given a sequence and a corresponding target syntax structure, the authors encode them by a bidirectional LSTM and decode them by LSTM. The decoder is augmented with soft attention over encoded states  and the copy mechanism . They then modify the inputs to the decoder to incorporate the target syntax structure for the generation. The syntactically adversarial sentences can not only fool pre-trained models but also improve the robustness of them to syntactic variation. However, the measurement of paraphrase quality and grammaticality requires much human effort. The structure of the sentence has changed, although the semantic difference is small.", "cites": [5854, 2470, 1632, 168, 4235, 7186, 5847], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of sentence-level attacks, highlighting their flexibility and application in various NLP tasks. It synthesizes the key idea from Iyyer et al.'s work on syntactically controlled paraphrase networks, linking it to the broader context of adversarial generation. The section also includes some critical evaluation by noting the limitations in paraphrase quality assessment and the semantic changes in generated attacks. However, the analysis remains somewhat surface-level and does not offer a deep comparative or meta-level framework."}}
{"id": "c7760775-486e-4856-bb89-ff5980a44113", "title": "Multi-level Attacks", "level": "subsection", "subsections": [], "parent_id": "744ede91-1e59-47e5-9ed0-4ceac7757786", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Attacks for classification in Texts"], ["subsection", "Multi-level Attacks"]], "content": "Multi-level attacks incorporate at least two of the three adversarial attacks to create more imperceptible and high success rate adversarial examples. Therefore, unlike a single method, the multi-level attack calculation is more expensive and more complicated.\n\tLiang \\etal{} utilized the FGSM to determine what, where, and how to insert, remove, and modify. They use the natural language watermarking technique  to ensure generated adversarial examples compromise their utilities. In the white-box scenario, they define hot training phrases and hot sample phrases by computing the cost gradients of inputs. The former sheds light on what to insert, and the latter implies where to insert, remove, and modify. In the black-box scenario, the hot training phrases and hot sample phrases are obtained through the fuzzing technique. When an input is fed to the target model, they use isometric whitespace to substitute the origin word each time. The difference between the results before and after modification is the deviation of each word. The larger it is, the more significant the corresponding word is to the classification. Hence, hot training phrases are the most frequent words in the set of inputs, which consist of the largest deviation words for each training sample. Hot sample phrases are the words with the largest deviation for every test sample.\n\tLike one pixel attack in the image domain, a similar method named HotFlip was proposed by Ebrahimi \\etal{}. HotFlip is a white-box attack in texts, which relies on an atomic flip operation to swap one character with another by gradient computation. Compared with DeepWordBug , the adversarial examples from HotFlip are more imperceptible due to the fewer modifications. The flip operation is represented by \n\t\\begin{equation} \\label{eq10}\n\t\\begin{split}\n\t\\vec{v}_{ijb} = &(\\vec{0},\\ldots;(\\vec{0},\\ldots(0,0,\\ldots,0,-1,0,\\ldots,1,0)_j,\\\\&\\ldots,\\vec{0})_i;\\vec{0},\\ldots)\n\t\\end{split}\n\t\\end{equation}\n\tThe formula \\eqref{eq10} means that the $j$-th character of $i$-th word in an example is changed from $a$ to $b$, which are both characters at $\\emph{a}$-th and $\\emph{b}$-th places in the alphabet. -1 and 1 are the corresponding positions for $a$ and $b$, respectively. The alteration from directional derivative along this vector is calculated to find the biggest growth in the loss $\\emph{J}(x, y)$. The procedure of calculation is shown in \n\t\\begin{equation} \\label{biggestincrease}\n\t\\max\\nabla_{x}J(x, y)^T\\cdot\\vec{v}_{ijb} = \\mathop{\\max}_{ijb}\\frac{\\partial J^{(b)}}{\\partial x_{ij}} - \\frac{\\partial J^{(a)}}{\\partial x_{ij}}\n\t\\end{equation}\n\twhere $x_{ij}$ is a one-hot vector, which denotes the $\\emph{j}$-th character of $\\emph{i}$-th word, $y$ refers to the corresponding label vector, $T$ is a transpose function. Apart from character-level attack, HotFlip could also be used on word-level by different modifications. Although HotFlip performs well, only a few successful adversarial examples are generated with one or two flips under strict constraints, thus it is not suitable for a large-scale experiment.\n\tLi \\etal{} proposed an attack framework TextBugger for generating adversarial examples, which could mislead the deep learning-based text understanding system in both black-box and white-box settings. Similar to DeepWordBug , TextBugger also searches for important words to modify. In the white-box scenario, Jacobian matrix $J$ is used to calculate the importance of each word.\n\t\\begin{equation}\n\tC_{x_i} = J_{F(i,y)} = \\frac{\\partial F_y(x)}{\\partial x_i}\n\t\\end{equation}\n\twhere $F_y(\\cdot)$ represents the confidence value of class $y$, $C_{x_i}$ is the important score of $i$-th word in $x$. Then, similar modification strategies like DeepWordBug are used to generate both character-level and word-level adversarial examples. In the black-box scenario, the authors segment documents into sequences, and then they query the target model to filter out sentences with different predicted labels from the original ones. The odd sequences are sorted in an inverse order according to their confidence scores calculated by the removal operation as\n\t\\begin{equation} \\label{removingmethod}\n\t\\begin{split}\n\tC_{x_i} = &F_y\\left(x_1,\\ldots,x_{i-1},x_i,x_{i+1},\\ldots,x_n\\right) \\\\& - F_y\\left(x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n\\right)\n\t\\end{split}\n\t\\end{equation}\n\tThe final modification process is the same as that in the white-box setting.\n\tCompared with Deep-fool  and TextBugger , Vijayaraghavan \\etal{}  applied reinforcement learning to generate adversarial examples in a black-box setting, following an encoder-decoder framework. They extract character and word information from inputs encoded to produce hidden representations of words. Then, an attention mechanism is applied to the decoder for identifying the most relevant text units that highly affect the predictions. During the decoding step, the perturbation vectors are added to those units, and the creations are optimized using target model predictions.", "cites": [5833, 3103, 5844, 5846, 914], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section demonstrates strong synthesis by integrating methods from multiple papers (e.g., FGSM, HotFlip, TextBugger, and reinforcement learning) and connecting their shared goal of generating adversarial examples with varying levels of abstraction. It includes some critical analysis, particularly in evaluating the limitations of HotFlip and contrasting techniques like DeepWordBug and TextBugger. However, the abstraction remains moderate, as the section primarily focuses on specific methods rather than offering higher-level theoretical or conceptual insights."}}
{"id": "f683ab1e-0a81-471f-a050-f908dd0f2567", "title": "Attack on Reading Comprehension Systems", "level": "subsection", "subsections": [], "parent_id": "60350839-4653-460f-8134-0be4be772e64", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Techniques for Other NLP Tasks"], ["subsection", "Attack on Reading Comprehension Systems"]], "content": "\\label{RCS}\n\tThe reading comprehension task means that the machine answers the query after reading a given context and the corresponding query. As a constraint, the answer to the query must be a paragraph (\\ie{}, several consecutive words) that can be found in the original context. Attackers usually modify the content through sentence-level attacks and induce the system to produce a different answer to the query.\n\tTo explore whether reading comprehension systems are vulnerable to adversarial examples, Jia \\etal{} inserted sentence-level adversarial perturbations into paragraphs to test the systems without changing the answers or misleading humans. They extract nouns and adjectives in the question and replace them with antonyms. Meanwhile, named entities and numbers are changed by the nearest word in GloVe embedding space. The modified question is transformed into a declarative sentence as the adversarial example, which is then concatenated to the end of the original paragraph. This process is called ADDSENT by the authors. Another way ADDANY randomly chooses words of the sentences to craft. Compared with ADDSENT, ADDANY does not consider the grammaticality of sentences, and it needs to query the model several times. This work's core idea is to draw the models' attention to the generated sequences rather than original sequences to produce incorrect answers. \n\tCurrently, there is no good defense method to resist this kind of attack. The analysis of the coherence of contextual semantics may be helpful for detection.", "cites": [4235], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of adversarial attacks on reading comprehension systems, primarily summarizing the method of Jia et al. It mentions two attack variations (ADDSENT and ADDANY) but does not deeply compare or synthesize insights from multiple works. There is limited critical evaluation or abstraction to broader principles, making it more descriptive than analytical."}}
{"id": "4ba8a06d-f447-476f-bfe0-ec4eccddab8a", "title": "Attack on Natural Language Inference Models", "level": "subsection", "subsections": [], "parent_id": "60350839-4653-460f-8134-0be4be772e64", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Techniques for Other NLP Tasks"], ["subsection", "Attack on Natural Language Inference Models"]], "content": "\\label{NLI}\n\tNatural language inference is mainly to judge the semantic relationship between two sentences (\\ie{}, premise and hypothesis) or two words. To some extent, it can be regarded as a classification task to ensure that the model can focus on semantic understanding.\n\tLi \\etal{}  designed a word-level attack to affect the model inference of entity relationship. The method follows the same two-step pipeline of importance-based attacks, and the importance of each word $I_{w_i}$ is calculated like \n\t\\begin{eqnarray} \\label{nNaturaLanguageInf}\n\tI_{w_i}=o_y(S)-o_y(S_{\\backslash w_i})\n\t\\end{eqnarray} \n\twhere $o_y(S)$ denotes the logit output by the target model for correct label $y$, $S_{\\backslash w_i}$ represents the rest $S$ except for the word $w_i$. Unlike synonyms or similar words substitution in the embedding space , a BERT is used for the word replacement, ensuring the semantic similarity and grammar-correct of generated adversarial examples. Considering the word segmentation of a text, the authors divide the substitution into two parts. If the important words are single after segmentation, they are iteratively replaced by the candidates calculated via BERT. Otherwise, the phrase containing an important word is iteratively replaced by the candidates, which are phrases either. \n\tMinervini \\etal{} cast the generation of adversarial examples as an optimization problem and proposed a novel multi-level attack. The authors maximize the proposed inconsistency loss $J_{I}$ to search for substitution sets $S$ (\\textit{i.e.}, adversarial examples) by using a language model as \n\t\\begin{equation}\n\t\\begin{split}\n\t\\mathop{maximize}\\limits_{S} J_{I}(S) = &\\left[p(S;body)-p(S;head)\\right]_{+}, \\\\&s.t. \\log p_{L}(S)\\leq\\tau\n\t\\end{split}\n\t\\end{equation}\n\twhere $[x]_{+}=\\max(0,x)$. $p_{L}(S)$ refers to the probability of the sentences in $S$.\n\t\\begin{itemize}\n\t\t\\item $\\tau$: a threshold on the perplexity of generated sequences\n\t\t\\item ${{X_{1},\\ldots,X_{n}}}$: the set of universally quantified variables in a rule to sequences in S\n\t\t\\item $S= \\lbrace{X_{1}\\to s_{1},\\ldots,X_{n}\\to s_{n}}\\rbrace$: a mapping from $\\lbrace{X_{1},\\ldots,X_{n}}\\rbrace$\n\t\t\\item $p(S; body)$ and $p(S; head)$: probability of the given rule, after replacing $X_{i}$ with the corresponding sentence $S_{i}$\n\t\t\\item $body$ and $head$: represent the premise and the conclusion of the NLI rules\n\t\\end{itemize}\n\tHowever, the generated adversarial examples may keep different semantics from the original because of ignoring the semantic changes between modifications and original words.", "cites": [5851, 5843, 5829], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers, particularly focusing on the methods used to attack NLI models. It connects Li et al.'s word-level attack with Minervini et al.'s multi-level optimization approach, offering a coherent narrative. While it does include some critical evaluation (e.g., noting the potential for semantic change), the critique is limited and does not fully compare or contrast the approaches. The section identifies a general trend in adversarial text generation but does not abstract to a meta-level framework or principle."}}
{"id": "859d59d9-5e21-4fb3-a3a7-1fd9c29ea03a", "title": "Attack on Machine Translation Models", "level": "subsection", "subsections": [], "parent_id": "60350839-4653-460f-8134-0be4be772e64", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Techniques for Other NLP Tasks"], ["subsection", "Attack on Machine Translation Models"]], "content": "\\label{NMT}\n\tMachine translation means that the machine can automatically map one language to another. Attackers slightly modify the content of an input language, resulting in the failure to obtain the expected translation result.\n\tBelinkov \\etal{} conducted a char-level black-box attack to explore the vulnerability of three different neural machine translation models . The authors devise adversarial examples depending on natural and synthetic language errors, including typos, misspellings, etc. Although these generations can easily fool three different models, they are also visible due to grammatical problems. Apart from the black-box attack, Ebrahimi \\etal{} and Cheng \\etal{}  generated adversarial examples with gradient optimization in the white-box settings. Compared with Belinkov \\etal{}, Ebrahimi \\etal{} have demonstrated that char-level adversarial examples in black-box attacks are much weaker than white-box ones in most cases, and the generations in the word-level from Cheng \\etal{}  are more fluent and better. \n\tZou \\etal{}  generated adversarial examples in the word-level via a new paradigm based on reinforcement learning. They construct a generator based on the generative adversarial networks (GANs)  to create adversarial examples as follows. The environment (\\textit{i.e.,} discriminator and victim NMT model) receives the tokens of current sentences into the agent to select suitable candidates for replacing the target tokens. The candidates of each token are collected through the victim NMT model within Euclidean distance. After modification, the discriminator receives these modified sentences and returns the agent a surviving feedback signal. The process is repeated until the termination signal is received. Unlike the previous works, this work balances semantic approximation and attack effects through self-supervision, and both have achieved good results.", "cites": [5854, 7217, 1003, 4846, 5857, 5856, 5855, 5845, 1096], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple adversarial methods for attacking machine translation models, integrating works on black-box, white-box, and reinforcement learning-based approaches. It provides comparative insights between methods (e.g., Belinkov vs. Ebrahimi, Cheng, and Zou), noting differences in effectiveness and fluency. However, while it identifies strengths and weaknesses, it does not offer a deeper, nuanced critique or a meta-level abstraction of broader trends in adversarial attacks across NLP tasks."}}
{"id": "f0afc92f-597d-4c92-8fd2-8578028934b7", "title": "Attack on Question and Answer Systems", "level": "subsection", "subsections": [], "parent_id": "60350839-4653-460f-8134-0be4be772e64", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Techniques for Other NLP Tasks"], ["subsection", "Attack on Question and Answer Systems"]], "content": "Inspired by the model's sensitivity to semantically similar questions, Gan \\etal{}  generated diverse paraphrased questions in the sentence-level, guiding target models to produce different answers. They obtain all n-grams (up to 6-grams) from the source questions via a language model, and the stopwords in them would be removed. Then, they search the paraphrase database  for paraphrases of the remaining. In constrain, the equivalence score between the substitutions and n-grams needs to be greater than 0.25. After paraphrase generation, a pre-trained model  is applied to filter the generated questions with a score greater than 0.95. Compared with adversarial attacks on reading comprehension systems, this work is to modify the question rather than declarative content like Figure \\ref{mrcandqaaaa}. Tan \\etal{}  also guided the question and answer model to produce incorrect answers by perturbing the words in the questions. However, the generations are likely to have grammatical errors and easy to be detected.\n\tTo deal with the non-differentiable and discrete attributes of texts, Wang \\etal{}  proposed a tree-based autoencoder to transfer the discrete text into a continuous representation space for creating adversarial perturbations. They firstly select the adversarial seed (\\textit{i.e.,} the input sentence) transferred into a continuous embedding. Next, the optimization similar to C\\&W attack  is conducted upon the embedding to search for perturbations. Finally, the modified embedding is decoded back to adversarial examples with semantic similarity. In the targeted attack for question and answer models, the declarative sentence obtained by reconstructing the target answer and question is treated as an adversarial seed. The generated adversarial examples are inserted into the end of the paragraph like Jia \\etal{}.  \n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\subfigure[attack on RC]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{QAandRC_a.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\subfigure[attack on QA]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{QAandRC_b.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\centering\n\t\t\\caption{Adversarial attacks on the machine reading comprehension and question and answer systems. (a) is from Jia \\etal{}, and (b) is the instance in Gan \\etal{} .}\n\t\t\\label{mrcandqaaaa}\n\t\\end{figure}", "cites": [5840, 890, 8012, 4235], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of adversarial techniques targeting question and answer systems by summarizing the methods used in the cited papers. It makes minimal effort to synthesize ideas across works or highlight broader patterns, and there is little critical evaluation of the approaches or their limitations. The narrative is largely centered on individual contributions without deeper analysis or abstraction."}}
{"id": "b1d84ee1-0f72-4dee-9d20-df6037c76035", "title": "Summary of Adversarial Attacks", "level": "subsection", "subsections": [], "parent_id": "60350839-4653-460f-8134-0be4be772e64", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial Techniques for Other NLP Tasks"], ["subsection", "Summary of Adversarial Attacks"]], "content": "\\begin{table*}[!t]\n\t\t\\centering\n\t\t\\caption{Summary of existing adversarial attacks. We mainly show the category, time, work, targeted/non-targeted, black/white, model, data, task, gradient related or not, and project url. For each of them, the papers are sorted by category and time. T/N is short for the targeted/non-targeted attack, W/B is short for the white/black-box attack, C is short for text classification, RC is short for reading comprehension, and TS is short for text summarization.} \n\t\t\\label{tabpaper_information}\n\t\t\\begin{adjustbox}{width=\\linewidth,center}\n\t\t\t\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{3}{*}{Category} & \\multirow{3}{*}{Time} & \\multirow{3}{*}{Work} & \\multirow{3}{*}{\\tabincell{c}{Targeted/ \\\\ Non-targeted}} & \\multirow{3}{*}{\\tabincell{c}{White/ \\\\ Black}} & \\multirow{3}{*}{Model}  & \\multirow{3}{*}{Data} & \\multirow{3}{*}{task} & \\multirow{3}{*}{Gradient} & \\multirow{3}{*}{Project URL} \\\\\n\t\t\t\t& & & & & & & & & \\tabularnewline \n\t\t\t\t& & & & & & & & & \\tabularnewline\n\t\t\t\t\\hline \n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{\\tabincell{c}{char}} & 2018.1.26 & Gao & N & B & LSTM & \\tabincell{c}{Enron Spam Dataset \\\\ IMDB} & C & N & \\url{https://github.com/QData/deepWordBug} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.4.30 & Belinkov & N & B & \\tabincell{c}{char-CNN \\\\ Nematus  \\\\ char2char } & \\tabincell{c}{WCPC , RWSE\\footnotemark[9] \\\\ MERLIN , MAE } & NMT & N & \\url{https://github.com/ybisk/charNMT-noise}\t\\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.8.20 & Ebrahimi & T & binary & char-CNN & TED  & NMT & Y & \\url{https://github.com/jebivid/adversarial-nmt} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.6.2 & Gil & N & B & GRU & Toxic Comment\\footnotemark[10] & C & N & \\url{https://github.com/orgoro/white-2-black}\n\t\t\t\t\\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.4.7 & Wang & binary & W & BERT & \\tabincell{c}{THUCNews\\footnotemark[11] \\\\ Wechat Finance Dataset} & C & Y & â€” \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{21}{*}{\\tabincell{c}{word}} & 2016.11.2 & Papernot & binary\t& W & LSTM & IMDB & C & Y & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2017.4.9 & Samanta & N & W & CNN & IMDB,twitter & C & Y\t& â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.7.13 & Sato & N & W & FFNN, LSTM & \\tabincell{c}{IMDB,RCV1 \\\\ Elec\\footnotemark[12],MR \\\\ Dbpedia} & C & Y & \\url{https://github.com/aonotas/interpretable-adv} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.7.15 & Mudrakarta & T & W & \\tabincell{c}{LSTM, NP \\\\ QANet}\t& \\tabincell{c}{VQA 1.0  , SQuAD \\\\ WikiTableQuestions} & RC,QA & Y & \\url{ https://github.com/pramodkaushik/acl18\\_results} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.7.15 & Glockner & N & B & \\tabincell{c}{bi-LSTM,ESIM \\\\ DAM} & SNLI & NLI & N & \\url{https://github.com/BIU-NLP/Breaking\\_NLI} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.10.31 & Alzantot & T & B & LSTM,RNN & IMDB,SNLI & C & N & \\url{https://github.com/nesl/nlp\\_adversarial\\_examples} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.5.12 & Behjati & binary & W & LSTM & AGâ€™s news,SST & C & Y & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7.28 & Ren & N & B & char-CNN, LSTM & \\tabincell{c}{AG's news,IMDB \\\\ Yahoo! Answers} & C & N & \\url{https://github.com/JHL-HUST/PWWS/} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7.28 & Hsieh & binary & binary & \\tabincell{c}{LSTM,BERT \\\\ Transformer} & Yelp,MultiNLI,WMT15\\footnotemark[2] & C,NMT & N & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7.28 & Cheng & T & W & Transformer & LDC corpus, WMT14 & NMT & Y & â€”  \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7.28 & Zhang & T & binary & bi-LSTM,BiDAF & IMDB,SNLI\t& C,NLI\t& Y\t& \\url{https://github.com/LC-John/Metropolis-Hastings-Attacker} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.2.7\t& Jin\t& N\t& B\t& CNN,LSTM,BERT\t& \\tabincell{c}{AGâ€™s news,IMDB,Fake\\footnotemark[13] \\\\ Yelp,MR,SNLI,MultiNLI} & C,NLI & N & \t\\url{https://github.com/jind11/TextFooler} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.2.7 & Cheng & binary & W & seq2seq & \\tabincell{c}{DUC2003, DUC2004 \\\\ Gigaword, WMT15} & NMT,TS & Y & \\url{https://github.com/cmhcbb/Seq2Sick} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.3 & Yang & N & B & CNN,LSTM & IMDB,Yahoo! Answers & C & N\t& â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.7.5 & Zang\t& T\t& B\t& LSTM, BERT\t& IMDB,SST,SNLI\t& C,NLI\t& N\t& \\url{https://github.com/thunlp/SememePSO-Attack} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.7.5\t& Zou & N & W & \\tabincell{c}{RNN-Search  \\\\ Transformer} & WMT14 & NMT\t& N & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.7.5 & Tan & N & B & \\tabincell{c}{BERT, BiDAF \\\\ Transformer \\\\ Seq2Seq } & SQuAD, WMT14 & QA, NMT & N &\t\\url{https://github.com/salesforce/morpheus} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.7.5 & Zheng & binary & binary & parser & English Penn Treebank & C & Y & \t\\url{https://github.com/zjiehang/DPAttack} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.9.7 & Garg\t& N\t& B\t& CNN,LSTM,BERT\t& \\tabincell{c}{Amazon,Yelp,IMDB \\\\ MR,MPQA\\footnotemark[14] \\\\ SUBJ,TREC\\footnotemark[15] }\t& C & N\t& â€”  \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.9.7 & Li & N & B & BERT & \\tabincell{c}{Yelp, IMDB, AGâ€™s news \\\\ SNLI, MultiNLI} & C,NLI & N & \\url{https://github.com/LinyangLee/BERT-Attack} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2021.2.9 & Maheshwary & N & B & \\tabincell{c}{CNN,LSTM,BERT \\\\ ESIM,InferSent} & \\tabincell{c}{AG's news, MR, Yelp \\\\ Yahoo Answers, IMDB \\\\ SNLI, MultiNLI} & C,NLI & N & â€” \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{7}{*}{\\tabincell{c}{sentence}} & 2017.9.7 & Jia & T & B & \\tabincell{c}{Match-LSTM  \\\\ BiDAF } & SQuAD & RC & N & \\url{https://github.com/robinjia/adversarial-squad} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.4.30 & Zhao & N & B & LSTM,Google Translate & SNLI & NLI,NMT & N & \\url{https://github.com/zhengliz/natural-adversary} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.7.15 & Ribeiro & N & B & \\tabincell{c}{Visual7W \\\\ fastText} & Visual7W data,MR,IMDB & QA,C & N &\t\\url{https://github.com/marcotcr/sears}  \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.11.16 & Iyyer & N & B & LSTM & SST,SICK\t& C\t& N\t& \\url{https://github.com/miyyer/scpn} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.11.16 & Wang & T & B & BSAE & SQuAD & RC & N & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7 & Wallace & T & W & RNN,IR & Quizbowl questions & QA & N & \\url{https://github.com/Eric-Wallace/trickme-interface/} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.7.28 & Gan & T & B & \\tabincell{c}{BERT , DrQA  \\\\ BiDAF } & SQuAD & QA & N & â€” \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{11}{*}{\\tabincell{c}{multi}} & 2018.7.13\t& Liang & T & binary & char-CNN & \\tabincell{c}{Dbpedia, MR, MPQA \\\\ Customer review\\footnotemark[16]} & C & Y & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.7.15 & Ebrahimi & N & W & char-CNN,LSTM  & AGâ€™s news & C & Y & \\url{https://github.com/AnyiRao/WordAdver} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.10.31 & Minervini\t& N\t& W\t& \\tabincell{c}{DAM , ESIM  \\\\ bi-LSTM} & SNLI, MultiNLI &\tNLI\t& Y\t& \\url{https://github.com/uclnlp/adversarial-nli} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.10.31 & Blohm & N & binary & CNN,LSTM & MovieQA\\footnotemark[17] & QA & N & \\url{https://github.com/DigitalPhonetics/reading-comprehension} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2018.10.31 & Niu & N & B & VHRED,RL & \\tabincell{c}{Dialogue Corpus \\\\ CoCoA} & Dialogue & N & \\url{https://github.com/WolfNiu/AdversarialDialogue} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.2.24\t& Li & N & binary & CNN,LSTM & IMDB,MR & C & Y & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.6.2 & Zhang & N & B & \\tabincell{c}{BOW,LSTM,BERT \\\\ ESIM.DecAtt \\\\ DIIN} & \\tabincell{c}{Quora Question Pairs \\\\ Wikipedia\\footnotemark[18]} & C & N & \\url{https://g.co/dataset/paws} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.9.20\t& Vijayaraghavan & N & B & CNN & AGâ€™s news, IMDB & C & N & â€” \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2019.11.4\t& Wallace & T & W & \\tabincell{c}{Bi-LSTM,ESIM \\\\ DAM,BiDAF} & SST,SNLI,SQuAD & C,NLI,RC & Y & \\url{https://github.com/Eric-Wallace/universal-triggers} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.9.7 & Wang & T & W & \\tabincell{c}{BERT, Transformer \\\\ BiDAF} & Yelp, SQuAD & C,QA & Y & \\url{https://github.com/AI-secure/T3} \\tabularnewline\n\t\t\t\t\\cline{2-10}\n\t\t\t\t& 2020.12.29 & Li & N & B & BERT & \\tabincell{c}{Sogou,IflyTek \\\\ Weibo,Law34\\footnotemark[19]} & C & N & â€”  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\end{table*}\n\tAdversarial attack methods have developed rapidly in recent years. Across the four categories (\\ie{}, char, word, sentence, and multi-level), high-quality adversarial texts are becoming more difficult to detect by the human eyes. On the other hand, the diversified legitimate and generated texts also promote the development of adversarial attacks and defenses. Therefore, there is still much room for improvement in generating adversarial examples such as transferability and deployment in real-world. \n\tTo demonstrate the generation methods of adversarial texts and their corresponding attributes in detail, we build Table \\ref{tabpaper_information} and Table \\ref{instance}. Through the two tables, we summarize and analyze the promising trend of the generation method.\n\t\\textbf{Possible problems with white-box attacks.} Table \\ref{tabpaper_information} summarizes the existing adversarial attacks in texts. We observe that the majority of white-box attacks in Table \\ref{tabpaper_information} employ the optimization of gradients for the attack. Gradient-based methods are widely used in the image domain with many variants , which can also be applied to texts. However, there are some shortcomings in using the gradients, such as vanishing and exploding gradient problems  and limitations of the access to target models. In addition, gradient masking  could incur the gradients useless in some cases, leading to failure in gradient-based methods. \n\t\\footnotetext[9]{\\url{https://www.informatik.tu-darmstadt.de/ukp/research\\_6/data/index.en.jsp}}\n\t\\footnotetext[10]{\\url{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/}}\n\t\\footnotetext[11]{\\url{https://github.com/thunlp/THUCTC}}\n\t\\textbf{Black-box attacks in the real world.} Table \\ref{tabpaper_information} also shows that the focus of research is on more realistic black-box attacks at present. It is basically difficult for adversaries to obtain target models' full knowledge in the physical world, so they do not know what dataset and model the defenders use. In this case, a black-box attack that can effectively deceive the system deployed in the physical world is expected. However, existing black-box attacks do not satisfy the requirements. We create 1,000 adversarial examples via various black-box attacks  to evaluate their performance on a physical classification system ParallelDots, but only about 9\\% of the samples can successfully fool the system. Although these adversarial examples are not specially designed for ParallelDots, the results also reflect the insufficient transferability of adversarial examples in real applications. On the other hand, if a model's output is the hard label rather than a score (\\textit{e.g.,} the output of Figure \\ref{isnatncefigure}(a) is 1 rather than 64.30\\%), the black-box attacks such as importance-based ones need to be improved and adapted to new situations.\n\t\\footnotetext[12]{\\url{http://riejohnson.com/cnn_data.html}}\n\t\\footnotetext[13]{\\url{https://www.kaggle.com/c/fake-news/data}}\n\t\\footnotetext[14]{\\url{http://mpqa.cs.pitt.edu/}}\n\t\\footnotetext[15]{\\url{https://cogcomp.seas.upenn.edu/Data/QA/QC/}}\n\t\\textbf{Evaluation by human eyes.} Furthermore, outstanding adversarial texts not only achieve a high success rate to fool DNNs, but also need to have good readability, semantic similarity, and imperceptibility. Hence, we can also evaluate generated adversarial examples through instances (in Table \\ref{instance}). Modifications on texts are generally divided into char-level, word-level, sentence-level, and multi-level. The char-level operates on the characters, and others modify words or sentences. In Table \\ref{instance}, the word-level adversarial examples seem more imperceptible than the char-level ones, although people are robust against misspellings . Nevertheless, some char-level methods also perform very well such as HotFlip . Generally, the more operations there are, the easier it is to be perceived. The more imperceptible the perturbations are, the better the readability and semantic similarity would be.\n\t\\begin{table*}[t]\n\t\t\\centering\n\t\t\\caption{Instances of some adversarial attacks. The content in parentheses is the modified word or sentence. After modification, the output of the model changes from one category to another. NNR is short for nearest neighbor replacement, ST is short for synonym substitution, GC is short for Grammar conversion.}\n\t\t\\label{instance}\n\t\t\\begin{tabular}{|p{1cm}<{\\centering}|p{1.7cm}<{\\centering}|p{12.5cm}|p{1.1cm}<{\\centering}|}\n\t\t\t\\hline\n\t\t\tCategory & Work & \\multicolumn{1}{c|}{Instance} & Operation \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{1}{*}{char} & Gao & \\tabincell{l}{This film has a special place (\\textbf{plcae}) in my heart (\\textbf{herat}). \\quad  \\textbf{positive $\\rightarrow$ negative}} & swap \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{3}{*}{word} & Alzantot & \\tabincell{l}{A runner (\\textbf{racer}) wants to head for the finish line. \\quad \\textbf{86\\%Entailment$\\rightarrow$43\\%Contradiction}} & NNR \\\\ \n\t\t\t\\cline{2-4}\n\t\t\t& Ren & \\tabincell{l}{seoul allies calm on nuclear (\\textbf{atomic}) shock. south koreaâ€™s key allies play down a shock admission \\\\ its scientists experimented to enrich uranium. \\quad \\textbf{74.25\\%Sci/Tech$\\rightarrow$86.66\\%World}} & ST \\\\\n\t\t\t\\cline{2-4}\n\t\t\t& Garg & \\tabincell{l}{Our server was great (\\textbf{enough}) and we had perfect service (\\textbf{but}). \\quad  \\textbf{positive $\\rightarrow$ negative}} & insert \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{1}{*}{sentence} & Iyyer & \\tabincell{l}{there is no pleasure in watching a child suffer. (\\textbf{in watching the child suffer, there is no pleasure.}) \\\\ \\textbf{negative $\\rightarrow$ positive}} & GC\\\\\n\t\t\t\\hline\n\t\t\t\\multirow{1}{*}{multi} & Liang & \\tabincell{l}{The Old Harbor Reservation Parkways are three \\sout{historic} roads in the Old Harbor area of Boston.\\\\ (\\textbf{Some exhibitions of Navy aircrafts were held here.}) They are part of the Boston parkway system \\\\ designed by Frederick Law Olmsted. They include all of William J. Day Boulevard running from \\\\ Castle (\\textbf{Cast1e}) Island to Kosciuszko Circle along Pleasure Bay and the Old Harbor shore. The part \\\\ of Columbia Road from its northeastern end at Farragut Road west to Pacuska Circle (formerly  \\\\ called Preble Circle). \\textbf{87.3\\%Building$\\rightarrow$95.7\\%Means of Transportation}} & \\tabincell{c}{Insert \\\\ delete \\\\ flip} \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table*}", "cites": [8419, 1171, 5865, 5848, 5855, 5870, 9133, 5844, 2470, 790, 8385, 168, 4036, 1632, 38, 5833, 8012, 5853, 4235, 2058, 7586, 912, 1113, 5868, 673, 303, 5866, 5854, 5860, 1348, 5850, 7796, 5839, 5862, 5861, 5846, 5869, 919, 5849, 5843, 1901, 8418, 5845, 5857, 1139, 5867, 439, 5842, 5851, 5830, 2565, 1173, 5856, 862, 8959, 4846, 5847, 5859, 5829, 5840, 3103, 923, 1096, 7, 5858, 5863, 5841, 5864], "cite_extract_rate": 0.723404255319149, "origin_cites_number": 94, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive summary of adversarial attack methods, organizing them by category and listing paper details in a table. While it provides structured information, there is minimal synthesis of ideas or integration across works. The analysis is limited, offering no evaluation of method strengths/weaknesses, and no abstraction to broader patterns or principles."}}
{"id": "0bb687e6-9e9d-44aa-8ea1-968b7a310601", "title": "Detecting Misspellings and Unknown Words", "level": "subsection", "subsections": [], "parent_id": "1804625a-81a9-4a52-8749-90a4382611c3", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Detecting Misspellings and Unknown Words"]], "content": "For defense, it is natural to consider whether it can be directly identified based on the difference between adversarial examples and legitimate texts. In the char-level attacks, the majority of modified words are misspellings due to the operations. Similarly, these words may also be unknown in some word-level attacks (\\eg{}, word substitution attack). It naturally comes up with an idea to detect adversarial examples by checking misspellings and unknown words. Unknown words are low-frequency or unseen words in the pre-trained model's vocabulary. Misspellings can also be treated as unknown words, which are out of the vocabulary.\n\tPruthi \\etal{}  built a word recognition model in front of the downstream classifier to distinguish adversarial examples in the char-level. The recognition model treats misspellings as unknown words. In tackling these words, three feedback mechanisms are applied to deal with them. (1) The model passes it through, regardless of whether it is adversarial. (2) Neutral words like `a' or `the' are used for replacement. (3) The word recognition model is retrained with a larger and less-specialized corpus. This work outperforms the general spelling check and adversarial training. Besides, Li \\etal{} applied a context-aware spelling check service to detect misspellings. However, experimental results show that the detection is effective on char-level modifications and partly useful on word-level attacks. The spelling check method is also not suitable for adversarial examples based on other languages like Chinese .\n\tFurthermore, Zhou \\etal{}  introduced three components (\\ie{}, discriminator, estimator, and recovery) to the model, which was used to discriminate both char-level and word-level perturbations. The components are trained with the original corpus in the training phase. When new text is fed to the detector, the discriminator classifies each token representation as to the perturbation or not. If a token is marked as adversarial, the estimator generates an approximate embedding vector to replace the token representation for recovery. The detector highly relies on the original corpus, leading to the failure of adversarial examples from other corpora. Meanwhile, the basis for classifying whether a token is perturbed based on neighboring tokens is not clear.", "cites": [4078, 3103, 5871], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 2.7}, "insight_level": "medium", "analysis": "The section provides an analytical overview of methods for detecting adversarial text through misspellings and unknown words, synthesizing ideas from three distinct papers while highlighting their relative strengths and limitations. It compares the effectiveness of different approaches (e.g., feedback mechanisms, context-aware spelling check, and the DISP framework) and notes the partial utility and language-specific shortcomings of these methods. However, it lacks deeper abstraction or a novel framework, and its critical analysis is somewhat surface-level."}}
{"id": "063d028b-08cd-40f0-9602-c426e8ef4b67", "title": "Model Enhancement", "level": "subsection", "subsections": ["bec8e5d9-ca56-4850-8db6-a3501aae190e", "f6552377-06ba-48b5-91a6-4f62f8128dab", "9219db05-661b-4254-be49-5062c0611b8c"], "parent_id": "1804625a-81a9-4a52-8749-90a4382611c3", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Model Enhancement"]], "content": "The direct detection of inputs will fail when faced with some word-level and sentence-level attacks without unknown words. Therefore, researchers defend against adversarial attacks by ensuring the security of models. Mainstream model enhancement methods include changing the model architecture or updating the model parameters, such as adversarial training , certified robustness training , and functional enhancement .", "cites": [5872, 943, 5494], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions three model enhancement methods and cites three papers but fails to synthesize their contributions or establish a coherent narrative. It lacks critical evaluation of the approaches and does not abstract broader principles or trends from the cited works."}}
{"id": "bec8e5d9-ca56-4850-8db6-a3501aae190e", "title": "Adversarial Training", "level": "subsubsection", "subsections": [], "parent_id": "063d028b-08cd-40f0-9602-c426e8ef4b67", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Model Enhancement"], ["subsubsection", "Adversarial Training"]], "content": "\\label{Adversarialrainin}\n\tIn texts, adversarial training and its variants are widely applied to defend against adversarial examples. Researchers mix adversarial examples with the original ones for retraining, improving the models' tolerance to adversarial examples. \n\tWang \\etal{}  applied data augmentation to create diverse samples for adversarial training. Compared with the AddSent-trained model , abundant data and semantic-relations can overcome model overstability and increase their robustness. Similarly, Wang \\etal{}  and Wang \\etal{}  extracted the synonyms for the training data and replaced the original words to construct a larger dataset for retraining. Nevertheless, they are specially designed for the synonym substitution attack and may fail to detect other kinds of adversarial attacks like the char-level ones.\n\tThe aforementioned methods  are traditional adversarial training  that directly constructs the training dataset with all the adversarial examples and legitimate ones. Yet, researchers have demonstrated that traditional adversarial training is weak against iterative attacks and proposed iterative training methods in the image domain . Inspired by the iterative methods in images, Liu \\etal{}  and Liu \\etal{}  created new adversarial examples at each epoch and added them for training. Through the iterative optimization on loss functions, the retrained models are more robust against adversarial examples. \n\tUnlike the above methods, Xu \\etal{}  proposed a novel adversarial training approach LexicalAT based on GANs . LexicalAT has two components, generator and classifier. The generator creates adversarial examples with the designed replacement actions, and the classifier returns feedback of this action by calculating the absolute difference in probability between the adversarial examples and the corresponding original texts. Then, the generator maximizes the expectation of the feedback by policy gradient, and the classifier minimizes the loss function until convergence. LexicalAT combines a knowledge base and adversarial learning and improves the robustness of sentiment classification models to a certain degree. Dinan \\etal{}  conducted a similar work to construct robust models. Differently, they use crowderworkers instead of the generator in Xu \\etal{} , so their work needs much human effort.\n\tIn adversarial training, data diversity is the key factor in determining the robustness of models, which relies on heuristic approximations to the worst-case perturbations. As a result, adversarial training is vulnerable to unknown attacks.", "cites": [5873, 8013, 7217, 1003, 917, 4235, 892, 5874, 5839], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes adversarial training approaches across multiple papers, connecting traditional and iterative methods, as well as the use of GANs and human annotators. It provides a coherent narrative and highlights limitations, such as vulnerability to unknown attacks. While it offers some level of critical analysis and identifies general patterns (e.g., the importance of data diversity), it does not deeply critique the methods or provide a novel, overarching theoretical framework."}}
{"id": "f6552377-06ba-48b5-91a6-4f62f8128dab", "title": "Functional Improvement", "level": "subsubsection", "subsections": [], "parent_id": "063d028b-08cd-40f0-9602-c426e8ef4b67", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Model Enhancement"], ["subsubsection", "Functional Improvement"]], "content": "DNN contains many built-in functions as well as functions that can be added externally. Researchers utilize the specially designed functions to reduce differences in the representation of adversarial examples and legitimate samples in the model, thus eliminating the impact of adversarial perturbations on the model. Here, we introduce existing works in this aspect.\n\tJones \\etal{}  constructed a robust encoding function to map inputs to a smaller and discrete encoding space. They construct a vocabulary containing the most frequent words in texts. Through clustering words in the dictionary, the words in a cluster share the same encoding. These encodings are the models' training data, \\ie{}, $f_{\\alpha}=g(\\alpha (x))$. $f_{\\alpha}$ is the classifier, and $g(\\cdot)$ receives the outputs from the encoding function $\\alpha$. For an adversarial example, the perturbations and the corresponding original words will be in the same cluster (stability), and the non-perturbations are not affected (fidelity). However, the performance is restricted by the size of the vocabulary. Besides, the trade-off between stability and fidelity also needs more detailed analysis.\n\tLi \\etal{} incorporated the external knowledge to the multi-head attention  for enhancing the robustness of NLI systems. The model can search for external knowledge when conducting NLP tasks, helping the model to explore beyond the data distribution of specific tasks. Experimental results show a significant improvement in defending against adversarial examples when the knowledge is added to the cross-encoder in their models. Although the method does not need extra parameters and is suitable for any model with attention units, the quality and size of external knowledge limit the performance of the method.", "cites": [5872, 38, 8960], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of two methods aimed at improving DNN robustness in text through functional enhancements. It integrates and explains the key mechanisms from the cited papers, connecting them to the broader goal of reducing adversarial impact. The section also identifies limitations such as vocabulary size constraints and reliance on external knowledge, indicating some critical evaluation. However, it lacks deeper abstraction or a novel framework that would unify these methods into broader principles."}}
{"id": "9219db05-661b-4254-be49-5062c0611b8c", "title": "Certification", "level": "subsubsection", "subsections": [], "parent_id": "063d028b-08cd-40f0-9602-c426e8ef4b67", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Model Enhancement"], ["subsubsection", "Certification"]], "content": "The detection of unknown words and adversarial training partially mitigate the threats of adversarial examples, but they are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations . Hence, researchers have proposed certification robustness training to search for a boundary. In some conditions, a model is guaranteed to be robust against an attack, \\ie{}, it can not cross the boundary, no matter how adversaries create adversarial examples. \n\tJia \\etal{} presented certified robustness training by optimizing the interval bound propagation (IBP) upper bound , which could limit the loss of worst-case perturbations and compress the living space of adversarial examples. This method is provably robust to the attacks with word substitution on IMDB and SNLI. Similarly, Huang \\etal{}  also applied IBP for certified robustness training. However, IBP only works to certify DNNs with continuous inputs, so it is not applicable to other models, such as the char-level one . Differently, Ko \\etal{}  proposed a gradient-based approach to find the minimum distortion of neural networks or lower bounds for robustness quantification. It is suitable for various models and has no restrictions like IBP, but it is inefficient and poses a computational challenge for Transformer verification due to the self-attention mechanism.\n\tConsidering that the current certification methods only deal with naive DNNs, Shi \\etal{}  proposed a novel method to certify the robustness of more complex Transformers. A Transformer model is decomposed into a number of sub-layers. Each sub-layer contains multiple positions, and each position consists of multiple neurons. The global lower and upper bounds of each neuron (\\textit{w.r.t.}, the input within the perturbation space) are calculated to efficiently obtain a safety guarantee by reducing the distance between bounds. Compared with IBP-based methods , the certified robustness bounds in this work are much tighter, and they also identify word importance as the same as importance-based methods.\n\tYe \\etal{}  designed a structure-free certified defense method that can guarantee the robustness of any pre-trained model. They construct a smoothed classifier $g^{RS}$ by introducing random substitutions from a synonym set, where $R$ represents the perturbed words and $S$ refers to the corresponding substitutions. The certification of the newly constructed model is defined as\n\t\\begin{eqnarray}\n\t\\Delta x \\overset{def}{=} \\min_{X'\\in S_X}g^{RS}(X',y)-\\max_{X'\\in S_X}g^{RS}(X',c)  >0\n\t\\end{eqnarray}\n\twhere $X'$ represents the modified sentences in synonym set $S_X$. $c$ is any label except the true label $y$. If the lower bound is larger than the upper bound (\\textit{i.e.,} $\\Delta x$), the smoothed classifier is certified robust.\n\tHowever, this kind of method is largely affected by models, testing data, and optimization methods. It is not general, as the detection of unknown words and adversarial training.\n\t\\begin{table*}[t]\n\t\t\\scriptsize\n\t\t\\centering\n\t\t\\caption{Summary information of defense methods against adversarial examples. We mainly show the category, time, work, model, data, attack, and project url.}\n\t\t\\label{defenseinformation}\n\t\t\\begin{adjustbox}{width=\\linewidth,center}\n\t\t\t\\begin{tabular}{|c|c|c|c|c|c|c|}\n\t\t\t\t\\hline \n\t\t\t\t\\multirow{3}{*}{Category} & \\multirow{3}{*}{Time} & \\multirow{3}{*}{Work} & \\multirow{3}{*}{Model}  &  \\multirow{3}{*}{Attack} & \\multirow{3}{*}{NLP Task} & \\multirow{3}{*}{Project URL} \\\\\n\t\t\t\t& & & & & &  \\tabularnewline\n\t\t\t\t& & & & & &  \\tabularnewline\n\t\t\t\t\\hline \n\t\t\t\t\\hline\n\t\t\t\t\\multirow{3}{*}{Detection} & 2019.2.24 & Li & Microsoft Azure & char-level & C & â€” \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.7.28 & Pruthi & bi-LSTM, BERT & word, char-level & C & \\url{https://github.com/danishpruthi/adversarial-misspellings} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.11.4 & Zhou & BERT & word,char-level & C & \\url{https://github.com/joey1993/bert-defender} \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{7}{*}{\\tabincell{c}{Adversarial \\\\ training}} & 2018.11.16 & Wang & BSAE & sentence-level & RC & â€” \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.9.15 & Wang & CNN,LSTM & word-level & C & â€” \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.11.4 & Xu & CNN,LSTM,BERT & word-level & C & \\url{https://github.com/lancopku/LexicalAT} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.11.4 & Dinan & BERT & char,word-level & dialogue & â€”  \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.2.7 & Liu & \\tabincell{c}{QANet, BERT \\\\ ERNIE2.0 } & sentence-level & RC & â€”  \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.2.7 & Liu & char-CNN. LSTM & char, word-level & C & â€”  \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.8.28 & Wang & CNN,LSTM & word-level & C & \\url{https://github.com/Raibows/RSE-Adversarial-Defense} \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{\\tabincell{c}{Functional \\\\ Improvement}} & 2019.8.31 & Li & \\tabincell{c}{DAM , BERT \\\\ Transformer} & word-level & NLI &  â€”  \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.7.5 & Jones & BERT & char-level & C,NLI & â€”  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{Certification} & 2019.6.9 & Ko & LSTM & char,word-level & C & \\url{https://github.com/ZhaoyangLyu/POPQORN} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.7.28 & Huang & CNN & char,word-level & C & \\url{https://github.com/deepmind/interval-bound-propagation/tree/master/examples/language/} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2019.11.4 & Jia & BOW,CNN,LSTM & word-level & C,NLI & \\url{https://github.com/robinjia/certified-word-sub} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.4.30 & Shi & Transformer &  word-level &  C  & \\url{https://github.com/shizhouxing/Robustness-Verification-for-Transformers} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.7.5 & Ye & CNN, BERT & word-level & C & \\url{https://github.com/lushleaf/Structure-free-certified-NLP} \\tabularnewline\n\t\t\t\t\\cline{2-7}\n\t\t\t\t& 2020.8.12 & Li & \\tabincell{c}{TextCNN  \\\\ bi-LSTM} & char,word-level & C & â€”  \\tabularnewline\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\end{table*}", "cites": [5871, 2476, 5872, 3103, 5873, 5878, 5877, 1096, 1901, 8960, 5875, 5874, 5839, 1159, 5876, 4078, 1619, 5494], "cite_extract_rate": 0.782608695652174, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple certification methods, explaining their mechanisms and distinguishing factors (e.g., IBP, gradient-based, and structure-free approaches). It provides a critical comparison of their strengths and limitations, such as applicability to certain models or computational inefficiency. The abstraction is moderate, offering general observations like the trade-offs between model complexity and certification feasibility, but not fully extracting meta-level principles."}}
{"id": "c8ca8542-4edc-4b76-a92d-61d84ef18eb0", "title": "Theoretical Analysis", "level": "subsection", "subsections": [], "parent_id": "1804625a-81a9-4a52-8749-90a4382611c3", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Defenses Against Adversarial Attacks in Texts"], ["subsection", "Theoretical Analysis"]], "content": "The aforementioned methods shown in Table \\ref{tabpaper_information} and Table \\ref{defenseinformation} are actual ways for adversarial attacks and defenses, but none of them explain theoretically why NLP models give different predictions. However, analyzing and explaining models' abnormal behavior is the fundamental way to carry out or solve adversarial attacks, which is lacking at present. \n\tAt present, the related model analysis works in NLP take legitimate data as inputs and observe the behavior of DNNs. According to the objects, we divide analysis methods  into two categories: external input and model's internal structure. These works have confirmed the theoretical correctness of some existing methods. They also help us have a better understanding of DNNs and then propose stronger attacks and defenses. \n\t\\textbf{External input.} Studies have demonstrated that the changes of external inputs (\\eg{}, input composition  or representation ) will affect the outputs of models. For example, Arras \\etal{}  extended the layer-wise relevance propagation (LRP) method to LSTM, producing reliable explanations of which words were responsible for attributing sentiment in individual texts. Gupta \\etal{}  proposed layer wise semantic accumulation (LISA) method to explain how to build semantics for a recurrent neural network (RNN) and how the saliency patterns act in the decision. During these findings, the authors analyze the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction. The two works prove the theoretical correctness of these importance-based attacks, such as DeepWordBug , PWWS , and Textfooler .\n\t\\textbf{Internal structure.} Exploring the performance of the internal units of the model is a more effective analysis method. Aubakirova \\etal{}  presented activation clustering to track the maximally activated neurons. Similarly, Dalvi \\etal{}  studied individual neurons capturing certain properties that are deemed important for the task. Through this way, they can increase model transparency and uncover the importance of the individual parameters, helping understand the inner workings of DNNs. Researchers have realized the defense methods by operating the neurons in the image domain . Whether it is feasible in texts is worth exploring. \n\tJacovi \\etal{}  presented an analysis into the inner workings of CNNs for processing text. They have demonstrated that the filters capture semantic classes of ngrams, and max-pooling separates the ngrams related to the final classification from the others. By inserting several ngrams, the filters will produce the results beyond extraction, leading to misclassification. Wallace \\etal{}  applied HotFlip  to the AllenNLP for interpreting models' weaknesses. However, they simply analyze the realization of different models and do not go deep into the network's internal behavior, contributing to the implementation of defense methods.\n\tIndeed, researchers can combine adversarial examples with existing model analysis methods to explore and analyze models' behavior, such as which layer of the model changes the prediction and differences in propagation path (\\ie{}, composed of activated neurons) between adversarial examples and legitimate inputs. The combination can inspire us to come up with more effective ways to eliminate the vulnerability of models.", "cites": [2910, 5881, 5833, 5879, 5883, 5880, 205, 5882], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to provide a structured view of theoretical analysis approaches in NLP, distinguishing between external input and internal structure analysis. While it identifies some gaps (e.g., lack of theoretical explanations for adversarial prediction differences), the critical evaluation is limited in depth. It offers moderate abstraction by linking analysis techniques to model robustness and suggesting future directions for defense."}}
{"id": "a67ace58-d936-4133-a08b-2b60867592d1", "title": "Attack", "level": "subsection", "subsections": [], "parent_id": "1246daa9-3531-4f92-b8a9-9822a164a00e", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Adversarial examples in Chinese-based models"], ["subsection", "Attack"]], "content": "Adversarial attacks in Chinese-based models are different from those in English due to the text attributes. First, Chinese texts need segmentations before feeding to the models. Second, each token after segmentation is a signal character, word, or phrase. The operations, such as swapping in the char-level and simple substitution of a phrase in the word-level, are not suitable for better generations in Chinese. To deal with these challenges, researchers investigate and design new ways to generate adversarial Chinese texts. \n\tWang \\etal{}  proposed a Chinese char-level attack against BERT, which could map the discrete text into a high-dimensional embedding space. Due to the mapping capability of BERT, attackers can search the high-dimensional embedding space for modification. The perturbed embeddings are mapped back to the characters with the closest semantics. In the targeted attack scenario, the optimization of the objective function $g(\\cdot)$ following C\\&W attack  is defined as \n\t\\begin{eqnarray}\n\tg(x')=\\max \\left[\\max\\left\\{f(x')_i:i\\neq t\\right\\}-f(x')_t,-\\kappa\\right]\n\t\\end{eqnarray}\n\twhere $f(x')_i$ is the $i$-th element in a logit vector from BERT model $f$. $\\kappa$ encourages the optimization to find perturbed character $x'$ classified as class $t$ with high probability. \n\tIn the non-targeted attack scenario, $g(\\cdot)$ is slightly different from the targeted attack scenario.\n\t\\begin{eqnarray}\n\tg(x')=\\max \\left[f(x')_t-\\max\\left\\{f(x')_i:i\\neq t\\right\\},-\\kappa\\right]\n\t\\end{eqnarray}\n\tHere, $t$ is the original class of the input. However, the embedding of $x'$ is closest to the original character $x$, but their semantics may be different, and sometimes $x'$ is unnatural to Chinese readers. \n\tLi \\etal{}  followed the importance-based methods  to quantify the importance of each segmentation replaced by the pieces in pre-constructed vocabulary. The pieces are similar to the segmentation, which can be a signal character, word, or phrase. The generations in this work are more natural and semantically similar than Wang \\etal{} . We treat the phrases in pieces as special sentences that are shorter than normal, so that the attack can be seen as a multi-level one. \n\tAdversarial examples in Chinese are shown in Figure \\ref{chinesebasedsamples}. When the Chinese text is slightly modified, the prediction of the model is converted from one to another. However, the meaning of translations changes more obviously. If we feed the Chinese text and its translation into Chinese-based and English-based classifiers respectively, the consistency of two models' outputs is worth exploring as a basis for judging adversarial examples.\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\subfigure[Wang et al.]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{AEinChinese_a.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\subfigure[Li et al.]{\n\t\t\t\\begin{minipage}[t]{0.5\\linewidth}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=\\linewidth]{AEinChinese_b.pdf}\n\t\t\t\\end{minipage}\n\t\t}\n\t\t\\centering\n\t\t\\caption{Adversarial examples in Chinese. (a) is Wang \\etal{} , and (b) is the instance in Li \\etal{} .}\n\t\t\\label{chinesebasedsamples}\n\t\\end{figure}", "cites": [886, 5859, 5843, 5870], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key points from the cited papers, explaining how adversarial attacks in Chinese differ from English and highlighting different approaches (e.g., Wang et al. vs. Li et al.). It provides some critical analysis by pointing out limitations such as unnatural semantics in generated adversarial examples. However, the abstraction is limited, and the analysis remains focused on methodological distinctions rather than broader theoretical or practical implications."}}
{"id": "897a908d-1634-4984-b8c2-781e7254006a", "title": "Generation of Adversarial Examples", "level": "subsection", "subsections": [], "parent_id": "c668fad8-3b0a-4b93-8d38-3007046ffb10", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Discussions"], ["subsection", "Generation of Adversarial Examples"]], "content": "We have reviewed over 40 published or pre-printed papers on the topic of adversarial example generation. In reviewing these attacks, we have some interesting findings, including challenges, which may shed new light on designing more powerful attacks.\n\t\\textbf{Limitation of char-level attacks.} Compared with word-level and sentence-level attacks, the char-level attacks are more obvious to human eyes and easier to be detected by some spelling-check tools. Besides, it is difficult to generate an outstanding sample by only modifying one or two characters. In most cases, people have to increase the number of modified characters to generate adversarial examples, resulting in reduced imperceptibility and readability. \n\t\\textbf{The failure of transferability in reality.} Currently, the majority of studies on adversarial texts are about theoretical models and rarely related to practical applications. We have used the adversarial examples presented in recent works to attack ParallelDots like Figure \\ref{isnatncefigure}, but most of the adversarial examples are ineffective and can be correctly classified. Only a few samples successfully fool this system, which means that the transferability of these adversarial examples is bad. For the physical NLP systems, we can not obtain any knowledge of them, and the query may be limited sometimes. Hence, transferability is the main choice for attacking these physical applications, which is the key factor for practical attacks. \n\t\\textbf{Lacking general methods.} There are no well-performed adversarial perturbations in texts that can fool any DNN-based model (so-called universal adversarial perturbations). Although Wallace \\etal{} find input-agnostic sequences that can trigger specific classifications to generate universal adversarial examples, these sequences impact the readability of inputs, and the generated samples are offensive in nature. \n\t\\textbf{Lacking better evaluation methods.} Most of the studies evaluate their performances of adversarial attacks by using success rate or accuracy. Only a few works, employ speed, scale, and efficiency into consideration, although they only list the attacks' time. Whether there is a relationship among the scale of the dataset, time consumed, and success rate of adversarial attacks is still unknown. If there exists such a relationship, the trade-off of these three aspects may be a research point in future work, like the related study of speed in adversarial examples. Besides, the experimental results on different datasets are various when the attack method is the same. Whether the type and amount of data may affect adversarial attacks is worth pondering.", "cites": [5841, 5833, 3103, 5842, 5830, 5846, 7586, 5829], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers to identify overarching issues in adversarial text generation, such as the limitations of character-level attacks, poor transferability, lack of general methods, and evaluation shortcomings. It critically evaluates the practicality and effectiveness of existing techniques, highlighting key gaps and their implications for real-world applications. While it provides strong analytical depth, its abstraction could be slightly enhanced by formalizing the identified patterns as broader principles or frameworks."}}
{"id": "f49d96fb-676f-4a07-8aa2-02c0071c73c6", "title": "Defense Methods Against Adversarial Attacks", "level": "subsection", "subsections": [], "parent_id": "c668fad8-3b0a-4b93-8d38-3007046ffb10", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Discussions"], ["subsection", "Defense Methods Against Adversarial Attacks"]], "content": "We have reviewed nearly 20 published or pre-printed papers on the topic of defense methods against adversarial attacks. In reviewing these methods, we have some interesting findings, including challenges, which may shed new light on designing more robust models.\n\t\\textbf{Application of adversarial examples.} In order to ensure the safety of the model, we can employ adversarial samples to expose its vulnerabilities for further improvements actively. For example, Blohm \\etal{}  generated adversarial examples to discover the limitations of their machine reading comprehension model. In different scenarios, their model is robust against meaning-preserving lexical substitutions but fails in importance-based attacks. Fortunately, some other attributions (\\eg{}, answer by elimination via ranking plausibility) can be added to improve the model's performance. Cheng \\etal{}  proposed a projected gradient method to verify the robustness of seq2seq models. They find that seq2seq models are more robust to adversarial attacks than CNN-based classifiers. Through the various adversarial examples, we can know which features, functions, or models can better resist these attacks and guide us where to start and how to improve.\n\t\\textbf{Lacking beachmarks.} Various methods have been proposed to study adversarial attacks and defenses in texts, but there is no benchmark. Researchers use different datasets (in Section \\ref{datasets}) in their works, making it difficult to compare these methods' advantages and disadvantages. Meanwhile, it also affects the selection of metrics. There is no exact statement about which metric measure is better in a situation and why it is more useful than others. Some comparisons have been made in Textbugger  with several metrics. The best one in this work may be only suitable for it, but ineffective in other works. \n\t\\textbf{Generalization abilities of detectors.} Tackling the unknown adversarial attacks is one of the main challenges for defense. In the past four years, researchers have been working towards this goal to design a general method. However, none of the existing works meet this need. We think that future work can focus more on designing a general defense to a single NLP task and then extending it to other NLP tasks. \n\t\\textbf{A Platform for research.} In terms of a quick start in this aspect, it is necessary to establish an open-source toolbox (\\eg{}, AdvBox and cleverhans in the image domain) for the research on adversarial texts. The toolboxes in the image domain integrate existing representative methods of generating adversarial images. People can easily do some further studies by them, which reduce time consumption for repetition and promote the development in this field. Compared with those in the image domain, the visual analytics framework proposed by Laughlin \\etal{} lacks diverse attack and defense methods. TextAttack  contains some representative attacks, including char-level, word-level, and sentence-level. If more attack and defense methods can be incorporated into it, the toolbox will become more powerful.", "cites": [973, 5884, 3103, 5885, 893], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by synthesizing information from multiple cited works to highlight trends such as the use of adversarial examples for model improvement and the lack of standardized benchmarks. It critically identifies limitations like poor generalization of detectors and the need for better toolboxes, but the analysis remains somewhat high-level without deeper comparative or novel framework-level insights."}}
{"id": "99244c31-822a-4beb-a33c-8d2843150f76", "title": "Further Work", "level": "subsection", "subsections": [], "parent_id": "c668fad8-3b0a-4b93-8d38-3007046ffb10", "prefix_titles": [["title", "Towards a Robust Deep Neural Network in Texts: A Survey"], ["section", "Discussions"], ["subsection", "Further Work"]], "content": "In future work, studies on adversarial examples can start from the following aspects: As an attacker, it is worth designing universal perturbations as they work in the image domain . Any text with universal perturbations can induce a model to produce the incorrect output. Moreover, more wonderful universal perturbations can fool multi-model or any model on any text. On the other hand, enhancing the transferability is meaningful in more practical black-box attacks, and the combination of optimization-based and transferability-based methods is another viable way like the work in . On the contrary, defenders prefer to revamp this vulnerability in DNNs completely, but it is no less difficult than redesigning a network. Both of them are long and arduous tasks with the common efforts of many people. At the moment, defenders can draw on methods from the image area to text for improving the robustness of DNNs, \\textit{e.g.}, adversarial training, adding extra layer, optimizing cross-entropy function, or weakening the transferability of adversarial examples.\n\tAlongside, the combination of deepfake and adversarial examples (also called AdvDeepfakes) is a worthy research direction. Deepfake  refers to a technique to naturally synthesize human imperceptible fake images and editing images via artificial intelligence (AI), especially through GANs. In response to this emerging challenge, researchers have constructed various deepfake detectors , but they fail to detect AdvDeepfakes  where attackers add adversarial perturbations to deepfake images. Inspired by these works, whether the fake text detectors  are robust against AdvDeepfakes needs further exploration. On the other hand, it also encourages researchers to build more robust detectors.", "cites": [5886, 5887, 5159, 975, 5156, 5888, 5890, 5889, 1603, 8957, 917, 5834, 5157, 5831], "cite_extract_rate": 0.8235294117647058, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes insights from multiple papers on adversarial techniques and defense strategies, particularly in the context of deepfake detection. It connects ideas such as universal perturbations, transferability, and the vulnerability of deepfake detectors to adversarial examples. While it provides some critical commentary on the challenges and limitations (e.g., robustness of detectors, difficulty in revamping DNNs), it could offer a more in-depth comparative or evaluative analysis of the cited methods. It abstracts the discussion to broader themes like black-box attacks and the intersection of deepfake and adversarial techniques, but the framework remains at a mid-level rather than offering a novel, meta-level perspective."}}
