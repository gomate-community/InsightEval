{"id": "14c01c59-0f70-4828-a9b8-da25837c5d0a", "title": "Non-IID data and Learning Strategies", "level": "paragraph", "subsections": ["7ac2156c-0d87-4916-9e6f-fe025def95b2", "f2e1016b-2b82-4959-a237-4510a21a11ff", "96cdd854-8b6f-419f-8b8f-5e76b50aebf6"], "parent_id": "c27d000d-4a26-473a-98db-6bd9e8f79632", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning"], ["paragraph", "Problem formulation"], ["paragraph", "Non-IID data and Learning Strategies"]], "content": "Typical centralized supervised learning algorithms have the IID assumption, i.e., the training and test data is independently identically distributed. In decentralized settings like federated learning, non-IID poses a challenge because the different data distribution result in significant skewness across devices or locations. Non-IID data among devices/locations encompass many different forms. There can be skewed distribution of features (probability $\\mathcal{P}(x)$), labels (probability $\\mathcal{P}(y)$), or the relationship between features and labels (e.g., varying $\\mathcal{P}(y|x)$ or $\\mathcal{P}(x|y)$) among devices/locations. Previous reviews categorized this this as horizontal, vertical and hybrid data partitions in Federated Learning. In this review, we focus on skewed distribution of labels, i.e.,  $\\mathcal{P}_{P_i}(y) \\not= \\mathcal{P}_{P_j}(y)$ for different data partitions $P_i$ and $P_j$. \nPrevious study has shown DNN models with batch normalization suffer from Non-IID data , the accuracy of FL reduces significantly by up to 55\\% for neural networks trained for highly skewed non-IID data , where each clinet device trains only on a single class of data. \nCommon techiniques to deal with non-IID include:\n\\begin{itemize}\n    \\item data augmentation: create a common dataset which can be shared globally, the dataset can come from a publicly available proxy data source , or perhaps a distillation of the raw data following .\n    \\item schedule client participation during training : FedFMC, FedCD, cluster similar devices / multi-center/ hirarchical clustering of local updates, FedPD, adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions  \n    \\item greater number of models, but more communication cost:\n    \\item ensemble: similar to  scheduling \n    \\item regularization on the server, e.g. FedAwS, server imposes a geo- metric regularizer after each round to encourage classes to be spreadout in the embedding space\n    \\item personalized FL/ continual local training, based on MAML\n\\end{itemize}", "cites": [7721, 605, 8110], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the concept of non-IID data in FL by integrating definitions and challenges from multiple papers, but the integration is surface-level. It lists common approaches to address non-IID data with minimal analysis of their relative merits or limitations. The section lacks critical evaluation and abstraction into broader principles or frameworks."}}
{"id": "7ac2156c-0d87-4916-9e6f-fe025def95b2", "title": "Optimization", "level": "paragraph", "subsections": [], "parent_id": "14c01c59-0f70-4828-a9b8-da25837c5d0a", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning"], ["paragraph", "Problem formulation"], ["paragraph", "Non-IID data and Learning Strategies"], ["paragraph", "Optimization"]], "content": "While a variety of studies have made assumptions for the per-client optimization functions in the IID setting, we review basic convergence results for $H$-smooth convex functions under the assumption that the variance of the stochastic gradients is bounded by $\\sigma ^2$. Given the following notations in a standard FL setting: $N$ is the total number of clients, $M$ is the number of participated clients per round, $T$ is the total number of communication rounds, $K$ is the local SGD steps per round. Federated averaging can conducted in either of the following two settings: one is to keep $x$ fixed in local updates during each round and compute a total of $KM$ gradients at the current $x$, in order to run accelerated minibatch SGD, the convergence rate is then upper bounded by $O(\\frac{H}{T^2}+\\frac{\\mu}{\\sqrt{TKM}})$. The other is to ignore all but 1 of the M active clients, which allows sequential SGD to run for $KT$ steps, this approach has an upper bound of $O(\\frac{H}{(TK)^2}+\\frac{\\mu}{\\sqrt{TK}})$. As in the non-IID settings, key assumptions are given for inter-client gradient, local functions on each client and other participation constraints. A detailed discussion of different convergence rates for non-IID setting can be found in .", "cites": [591], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides some synthesis by integrating standard FL notations and convergence results into a discussion of optimization for federated NLP, though it primarily references a single paper. It offers basic analytical insight by contrasting two optimization settings and their convergence bounds but lacks deeper comparison or critique of these methods. The abstraction is moderate, as it generalizes the optimization problem and presents a high-level view of different approaches, but it does not elevate to a meta-level discussion of broader principles in FL optimization."}}
{"id": "278f731f-ee0f-4876-9af0-2685b9d83f98", "title": "Language modeling", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Language modeling"]], "content": "A language model (LM) refers to a model that provides probabilities of word sequences through an unsupervised distribution estimation. As an essential component for NLP systems,  LM is utilised in a variety of NLP tasks, i.e., machine translation, text classification, relation extraction, question-answering, etc. In FL, most LMs are deployed on a virtual mobile keyboard, i.e., the Google Keyboard (Gboard). Thereby, recent literature are mostly produced by authors from Google, LLC. Recent works on language modelling in Federated NLP mainly target on solving a word-level LM problem in mobile industry. That is mobile keyboard suggestion, which is a well representative of federated NLP applications. To improve mobile keyboard suggestions, federated NLP models aim to be more reliable and resilient. Existing models offer quality improvements in typing or even expression (e.g., emoji) domains, such as next-world predictions, emoji predictions, query suggestions, etc. \nConsidering the characteristics of mobile devices, a decentralized computation approach is constrained by computation resource and low-latency requirement. A mobile device has limited RAM and CPU budgets, while we expect keyboards to provide a quick and visible response of an input event within 20 milliseconds. Thereby, the model deployed in client sides should perform fast inference. \nMost works  consider variants of LSTMs  as the client model. Given the limited computation budget on each device, we expect the parameter space of a neural language model to be as small as possible without degrading model performance. CIFG  is a promising candidate to diminish LM’s complexity and inference-time latency. It employs a single gate to harness both the input and recurrent cell self-connections. In such a way, the amount of parameters is downsized by 25 $\\%$ .  leverages CIFG for next word predictions, and simplifies the CIFG model by removing peephole connections. To further optimise the model in size and training time, they ties input embedding and CIFG output projection matrices.  applies a pretrained CIFG network as an emoji prediction model. In particular, the pretraining process involves all layers, excluding the output projection layer, using a language learning task. To enhance performance, the authors enable embedding sharing between inputs and outputs. The pretraining of LM exhibits fast convergence for the emoji model.  employs a character-level RNN , targeting on out-of-vocabulary(OOV) learning tasks, under FL settings. Specifically, they use CIFG with peephole connections and a projection layer. The projection layer diminishes the dimension of output and accelerates the training. They use multi-layer LSTMs to enhance the representation power of the model, which learns the probability of word occurrence. GRU  is another simpler variant of the LSTM.  leverage GRU as the neural language model for mobile keyboard next-word predictions. Similar to CIFG, it reduces the model complexity on parameter spaces without hurting the model performance. To downsize the amount of trainable parameters, they also apply tied embedding in the embedding layer and output layer by share of the weights and biases.\n proposes another LM for keyboard query suggestions to reduce the burden of training LSTMs. Specifically, they train a LSTM model on the server for generating suggestion candidates, while merely federated training a triggering model, that decides the occurrence of the candidates. The triggering model uses logistic regression to infer the probability of a user click, significantly lessening the computation budgets in comparison of RNN models.  also states the direct use of RNN is not the proper means to decode due to its large parameters size, which further causes slow inference. Hereby, they propose to leverage a n-gram LM that derived from a federated RNN for decoding. In particular, they overcome large memory footprints problem and enhance model performance by introducing an approximation algorithm based on SampleApprox . It approximates RNN models into n-gram models. Still, they use CIFG and group-LSTM (GLSTM)  for approximation. While, GPT2  is one of the state-of-the-art transformer-based LMs with 1.5 billion parameters. Considering its performance on centralized language modeling tasks,  uses GPT2 as LM. They propose a dimensionality reduction algorithm to downsize the dimension of GPT2 word embedding to desired values (100 and 300).\nFor federated optimization, existing federated optimization algorithms differ in client model aggregation on the server-side. In federated language modeling, most existing works  use FedAvg as the federated optimization algorithm. Another optimization strategy, called FedAtt, has also shown its feasibility and validity in language models .  \nIn FedAvg, gradients, that computed locally over a large population of clients, are aggregated by the server to build a novel global model. Every client is trained by locally stored data and computes the average gradient with the current global model via one or more steps of SGD. Then, it communicates model updates with the server. The server performs the weighted aggregation of the client updates to build a novel global model. Client updates are immediately abandoned on the sever once the accumulation is completed.  trains the global model from scratch in the server, using FedAvg. Specifically, the initial global model has either been randomly initialized or pretrained on proxy data. However, it increases the federated training rounds on clients. Thereby, it leads to a high communication and computation costs in FL. They also use SGD as the server-sided optimizer for training. They found Adam and AdaGrad provide no beneficial improvement on convergence.  introduces a novel federated training approach, called central pre-training with federated fine-tuning. To address the drawback in , the server pretrains a model with centralized and public data as the global model at the initial time. Each clients then obtains the pretrained weights as the initial weights, and later trained on local data in a federated fashion. But the improvement is limited to large network, i.e., GPT2. They also propose a pretrained word embedding layer for federated training, which only enhance accuracy for the large word embedding network (i.e., GPT2). Whereas, with the combination of pretraining models, it harms the performance. They leverage Adam as the optimizer for training.  uses momentum and adaptive L2-norm clipping on each client’s gradient in FedAvg, leading to a faster convergence. The authors argue momentum and adaptive clipping performed on gradients improves the robustness of model convergence and performance.  also uses clipping for regularization in FedAvg by setting the upper bound of user updating to constrain each client contribution (i.e., clipping). In addition,  founds using momentum with Nesterov accelerated gradients significantly outperforms using SGD as server optimizer, in terms of convergence rate and model performance.  applies Nesterov momentum as both the local and the server optimizer. \n first introduces the attention mechanism into federated aggregation of client models. This optimization algorithm is referred as Attentive Federated Aggregation (FedAtt). It is a layer-wise soft attention mechanism applied on the trained parameters of the NN model. Intuitively, the federated optimization algorithm learns to optimize the global model by providing a good generalization on each client model for a quick local adaptation. Hereby, it reduces local training rounds and saves the computation budgets, further accelerating the learning process. The generalization in FedAtt is decided by the similarity between each client and the server, and the relative importance of each client. For a good generalization, they minimise the weighted summed distance of each client model and the global model on parameters spaces. They introduce attentive weights as the weights of the client models. Particularly, the attentive weight of each client model is a non-parametric attention score derived from each layer of NN.  Differ from pre-trained FedAvg, FedAtt finds a well-generalized global model on each federated training round by iteratively updating parameters. Consequently, it further lessens the federated communication budgets. For local training, the client-sided optimizer is momentum. While, for global parameters updates, they uses SGD.\nThe existing works on federated language modeling mainly contribute on optimizing model aggregation process, but not focusing on privacy preserving approach. Adding privacy preserving techniques into federated optimization process is seen as a bonus, rather than an essential means of privacy guarantees. In Federated LMs, the commonly used privacy preserving technique is differential privacy (DP) . A DP algorithm is expected to characterize the underlying probability distribution without compromising personally identifiable data. In general, it injects calibrated noise into the aggregated data while not affecting the outcomes. Most DP approaches are used for user-level privacy guarantees. In FL, we define user-level DP as a privacy guarantees, to preserve the trained models with or without the presence of any one client's data. DP usually serves on the client sides before model aggregation .  integrates a randomized mechanism in FedAtt optimization by introducing a white noise with the mean of 0 and the standard deviation $\\sigma$. They also introduce a magnitude coefficient $\\beta \\in (0,1]$ to govern the effect of the randomization in FL. However, the level of its DP guarantees is unrevealed. Hereby, it fails to show the trade-off between data utility and privacy protection for its privacy-preserving countermeasure implementation.  incorporates the Gaussian mechanism in FedAvg to cope with the user-based heterogeneity of data in language models. In particular, it perform DP guarantees by adding Gaussian noise with a noise multiplier of 1, after clipping. They argue a high level of DP guarantees exhibits a notable reduction in unintended memorization, caused by heterogeneity of training data.", "cites": [6182, 581, 6179, 589, 8986, 586, 584, 6180, 6181, 243], "cite_extract_rate": 0.6470588235294118, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on federated language modeling, particularly in the context of mobile keyboards, connecting different techniques like CIFG, GRU, and FedAtt to highlight trends in model optimization and training strategies. It includes critical analysis, such as the limitations of pretraining in federated settings and the trade-offs between model size and performance. While it identifies patterns in approaches (e.g., using lighter recurrent models for client-side deployment), the abstraction remains somewhat constrained to the NLP-FL application domain without broader conceptual generalization."}}
{"id": "0024dc32-f679-43ce-8118-4349ff2f1b69", "title": "Classification", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Classification"]], "content": "Text Classification is procedure of identifying the pre-defined Text Classification is the procedure of identifying the pre-defined category for varied-length of text . It can be extended to many NLP applications including sentiment analysis, question answering and topic labeling .\nTraditional text classification tasks can be deconstructed into four steps: text preprocessing, dimension reduction, classification and evaluation. \nThough the deep learning models have achieved state-of-the-art results in text classification , uploading or sharing text data to improve model performance is not always feasible due to different privacy requirements of clients. For example, financial institutions that wish to train a chatbot for their clients cannot be allowed to upload all text data from the client-side to their central server due to strict privacy protection statements. Then applying the federated learning paradigm is an approach to solve the dilemma due to its advances in privacy preservation and collaborative training.  In which, the central server can train a powerful model collaboratively with different local labeled data at client devices without uploading the raw data considering increasing privacy concerns in public. \nHowever, there are several challenges for applying federated learning to text classification tasks in NLP. One is to design proper aggregating algorithms to handle the gradients or weights uploaded by different client models. Traditional federated learning can be considered as a special paradigm of distributed learning, thus aggregating algorithms, such as FedAvg , FedAtt  has been proposed to generalize the model on the central server. Considering the unevenly distributed data at different client devices and different amounts of data at the different local datasets. \n has attempted the text classification using the standard FedAvg algorithm to update the model parameter with local trained models. It uses different local datasets to pre-train the word embeddings, and then concatenate all word embeddings. After filtering the widths and feature maps from the concatenated word embeddings, the max-over-time pooling was used to aggregate the features, thus getting vectors with the same length. Finally, they use softmax activation on the fully connected layer, it will translate the vectors to the final sentence classification results (categories).\nLater, scientists from the Machine learning area brought in new approaches of uploading and aggregating, for example,  using Knowledge distillation .  however use fine-tuning instead of FedAvg to update parameters.   average the logits outputs from the last layer of the model instead of directly take the average of model parameters. It then uses knowledge distillation to learn the knowledge from the client devices instead of traditional. \nIn addition, model compression has been introduced to federated text classification tasks due to the dilemma of computation restraints on the client-side. They attempted to reduce the model size on the client-side to enable the real application of federated learning. The computation restriction on the client devices limits the application of traditional FL. For example, 4-layer BERT or 6-layer BERT is still too large for mobile devices such as smartphones. The scholars then focus to perform the model compression while still following the federated learning paradigm. The knowledge distillation then has been applied to transfer local model information while keeping the model size small at the local devices in . It utilises knowledge distillation instead of model parameter fusion to update parameters. The soft-label predictions on a public distillation dataset are sent to the central model to be distilled. Thus, the central model can learn the local knowledge on client devices through distilling the logits of different client models without sharing or uploading the local model parameters and gradients.\nTo ensure the privacy preservation of FL while keeping the communication, the encryption of data is one of the top priority considerations in applying federated learning in NLP. Encryption on communication between edge-device and central server is a standard approach in federated learning to preserve privacy for end-users on edge devices.  adds encryption on client-central server communication using differential privacy. It used the approach  proposed the attack-adaptive aggregation which prevent the attack at the central server aggregation module. \nTo overcome the communication dilemma of FL, one-shot or few-shot federated learning was proposed to allow the central server can successfully train the central model with only one or a few rounds of communication under poor communication scenarios. However, the shared data restriction of federated learning is still left to be solved. Considering the trend of higher restriction of data sharing and uploading, it will be harder to get a sufficient size of data shared to both central servers and client servers. In this way, the knowledge distillation cannot be used to solve the model compression problem in federated learning.\n reduced the communication of previous federated learning by utilising the soft labels dataset distillation mentioned in   and . It thus successfully extend the soft-labeling methods to two new techniques: soft-reset and random masking, and then successfully using the dataset distillation  to realise the one-round communication federated learning for text classification tasks. Each client in   distils their local dataset to a much smaller synthetic one, and then only uploads the small-sized synthetic dataset to the server. Thus, no gradients or weights is transmitting from the client model to the central server model. The distilled dataset can be as small as one data sample per category, in this way the communication in federated learning can be reduced to as low as one round.", "cites": [8111, 6184, 6183, 6186, 593, 8112, 6180, 681, 6185, 8679, 8110], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to explain the application of federated learning in text classification, integrating concepts like FedAvg, knowledge distillation, and model compression. It shows some critical analysis by pointing out limitations of traditional aggregation methods and the potential issues with knowledge distillation under strict data-sharing restrictions. While it identifies broader patterns, such as the trade-off between communication efficiency and model performance, the abstraction remains at a moderate level without a deeper theoretical or conceptual unification of the field."}}
{"id": "00c98230-d384-49ec-93eb-45c4de0cc1bf", "title": "Speech Recognition", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Speech Recognition"]], "content": "Speech recognition is the task of recognising speech within audio and converting it into text. Voice assistants such as Amazon Alexa or Apple Siri use on-device processing to detect wake-up words (e.g. \"Hey Siri\"), which is a typical usage for speech recognition on smartphones. Only when the wake-up words are detected, further processing like information retrieval or question answering will be running on the cloud. Methods for speech recognition include dynamic time wraping , Hidden Markov Models  and modern end-to-end deep neural models . More recently, wav2vec  masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned, this method demonstrates the feasibility of speech recognition with limited amounts of labeled data.\nOn device wake-up word detectors face two main challenges: First, it should run with minimal memory footprint and computational cost. Second, the wake word detector should behave consistently in any usage setting, and show robustness to background noise.  performed neural network architecture evaluation and exploration for running keyword spotting on resource-constrained microcontrollers, they showed that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy.  investigated the use of federated learning on\ncrowdsourced speech data to learn a resource-constrained wake word detector. They showed that a revisited Federated Averaging algorithm with per-coordinate averaging based on Adam in place of standard global averaging allows the training to reach a target stopping criterion of 95\\% recall per 5 FAH within 100 communication rounds on their crowdsourced dataset for an associated upstream communication costs per client of 8MB.\nThey also open sourced the Hey Snips wake word dataset \\footnote{http:// research.snips.ai/datasets/keyword-spotting}.  proposed a decentralized feature extraction approach in federated learning to address privacy-preservation issues for speech\nrecognition, which is built upon a quantum convolutional neural network (QCNN) composed of a quantum circuit encoder for feature extraction, and a recurrent neural network (RNN) based end-to-end acoustic model (AM). The proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks.  introduced a framework for speech recognition by which the degree of non-IID-ness can be varied, consequently illustrating a trade-off between model quality and the computational cost of federated training. They also showed that hyperparameter optimization and appropriate use of variational noise are sufficient to compensate for the quality impact of non-IID distributions, while decreasing the cost.", "cites": [6187, 6188, 6189, 864], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on federated learning in speech recognition, highlighting key contributions such as algorithm adaptation, privacy preservation, and handling non-IID data. It provides critical analysis by discussing the trade-offs between model quality and training cost, and by mentioning the use of hyperparameter optimization as a solution. The section abstracts some broader patterns, particularly in the context of privacy and decentralization, though a more meta-level discussion could further elevate its insight quality."}}
{"id": "85286e7c-e4de-4ae2-9287-1bbbc64ce2e9", "title": "Sequence Tagging", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Sequence Tagging"]], "content": "Sequence tagging, e.g. POS tagging, Named Entity Recognition, plays an important role in both natural language understanding and information extraction. Statistical models like Hidden Markov Model and Conditional Random Fields were heavily used, modern approaches rely on deep representations from Recurrent Neural Net, Convolution Neural Net or transformer like architectures. A few recent works focus on biomedical Named Entity Recognition in the federated setting.  pretrained and fine tuned BERT models for NER tasks in a federated manner using clinical texts, their results suggested that conducting pretraining and fine tuning in a federated manner using\ndata from different silos resulted in reduced performance compared with training on centralized\ndata. This loss of performance  is mainly due\nto separation of data as ”federated communication\nloss” .  Given the limit of data access, the experiments were conducted with clinical notes from a single healthcare system to simulate different silos.  proposed a FedNER method for medical NER, they decomposed the medical NER model in each platform into\na shared module and a private module. The private\nmodule was updated in each platform using the local\ndata to model the platform-specific characteristics.\nThe shared module was used to capture the shareable\nknowledge among different platforms, and was updated in a server based on the aggregated gradients from multiple platforms.  The private module consists of two top layers in our medical NER model, i.e, Bi-LSTM and CRF, which aim to learn\nplatform-specific context representations and label\ndecoding strategies.  The private module\nwas only trained with local data and exchange neither its parameters nor gradients. The shared module consisted\nof the other bottom layers in our NER model, such\nas the word-level CNN and all types of embedding.\nDifferent from the private module, the shared one\nmainly aims to capture the semantic information in\ntexts.  introduced a privacy preserving medical relation extraction model\nbased on federated learning, they leveraged a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters.", "cites": [6191, 6190], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two key papers on federated NER in healthcare, integrating their approaches into a coherent narrative about shared and private modules. It provides some critical insight by identifying the 'federated communication loss' and how data separation affects performance. However, abstraction is limited, as it primarily discusses architectural distinctions without broader theoretical generalization."}}
{"id": "77d10f6e-5c2e-4b44-820b-082594c48663", "title": "Recommendation", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Recommendation"]], "content": "Recommendation systems are heavily data-driven. Typical recommendation models use collaborative filtering methods , in which past user item interactions are sufficient to detect similar users and/or similar items and make predictions based on these estimated proximities. Collaborative filtering algorithms can be further divided into two sub-categories that are generally called memory based and model based approaches. Memory based approaches directly works with values of recorded interactions, assuming no model, and are essentially based on nearest neighbours search (for example, find the closest users from a user of interest and suggest the most popular items among these neighbours). Model based approaches assume an underlying “generative” model that explains the user-item interactions and try to discover it in order to make new predictions. Unlike collaborative methods that only rely on the user-item interactions, content based approaches  use additional information about users and/or items. If we consider the example of a movies recommendation system, this additional information can be, for example, the age, the sex, the job or any other personal information for users as well as the category, the main actors, the duration or other characteristics for the movies (items).\nGiven different partitions of users and items, federated recommendation models can be horizontal, vertical or transfered. In horizontal federated recommendation systems, items are shared but users belong to different parties. A typical work is Federated Collaborative Filter (FCF)  proposed to use a central server to keep the item latent factor matrix, while the user latent factors are stored locally on each device. In the training time, the server distributes the item latent factor to each party, the participants update their user latent factor by local rating matrix data and send the item latent factor updates back to the server for aggregation. To avoid the inter trust problem,  introduced a fully decentralized setting where participants have full access to the item latent factor and communicate with each other to update the model. Moreover, meta learning has been used for personalized federated recommendation.  designed a meta learner to learn generalized model parameters for each participant, then each participant's recommendation is regarded as a personalized task where a support set is used to generate the recommendation model and the gradient is computed on a query set.  introduced another fedrated meta learning algorithm for recommendation, in which separate support and query sets are not necessary. Their approach performs relatively well within less amount of training episodes. Besides,  proposed DiFacto, which is a distributed factorization method and addressed the efficiency problem when it comes to large scale user item matrices. In comparison, vertical federated systems have been designed for feature distributed learning problem where participants hold different feature sets.  proposed an\nasynchronous stochastic gradient descent algorithm. Each party could use an\narbitrary model to map its local features to a local prediction. Then local predictions from different parties are aggregated into a final output using linear and\nnonlinear transformations. The training procedure of each party is allowed to be\nat various iterations up to a bounded delay. This approach does not share any\nraw data and local models. Therefore, it has fewer privacy risks. Besides, for a\nhigher level of privacy, it can easily incorporate the DP technique. Similar to\nhorizontal FedRec, there are also works that further utilize cryptography techniques.\n presented a secure gradient-tree boosting algorithm. This algorithm\nadopts HE methods to provide lossless performance as well as preserving privacy.\nAnd  proposed a secure linear regression algorithm. MPC protocols are\ndesigned using garbled circuits to obtain a highly scalable solution.\nParties of vertical FedRec could also be two recommenders with different item\nsets. For instance, a movie RecSys and a book RecSys have a large user overlapping\nbut different items to recommend. It is assumed that users share a similar\ntaste in movies with books. With FedRec, the two parties want to train better\nrecommendation algorithms together in a secure and privacy-preserving way.\n proposed a secure, distributed item-based CF method. It jointly improves\nthe effect of several RecSys, which offer different subsets of items to the same\nunderlying population of users. Both the predicted ratings of items and their\npredicted rankings could be computed without compromising privacy nor predictions’ accuracy. We refer readers to  for more detailed discussion for federated recommendation systems.", "cites": [6192, 3695, 667], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches to federated recommendation systems, connecting horizontal, vertical, and transfer-based methods. It integrates insights from different papers to present a structured overview of how federated learning can be applied to recommendation. While it offers some critical evaluation, such as noting privacy advantages and efficiency improvements, it lacks deeper comparative analysis or identification of broader principles."}}
{"id": "c6eff2f8-bbf6-4e76-a0c8-634a3c246976", "title": "Health Text Mining", "level": "subsection", "subsections": [], "parent_id": "b803ddde-840f-45b4-bc4f-97790f62c33f", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Federated learning in NLP"], ["subsection", "Health Text Mining"]], "content": "Federated learning has emerged as an important framework for health text mining, due to the privacy concern among different hospitals and medical organizations. Besides, most health data exhibits systemic bias towards some specific groups or patterns, e.g. hospitals, diseases and communities. Again, this non-IID issue raises big challenges when applying federated learning into heath text mining tasks. There have been some tasks that were studied in federated learning setting in healthcare, including patient similarity learning , patient representation learning and phenotyping , predictive or classification modeling , biomedical named entity recognition. \nSpecifically,  designed a two-stage federated approach for medical record classification. In the first stage, they pre-trained a patient representation model by training an neural network to predict ICD and CPT codes from the text of the notes.  \nIn the second stage, a phenotyping machine learning model was trained in a federated manner using clinical notes that are distributed across multiple sites for the target phenotype. In this stage, the notes mapped to fixed-length representations from stage one are used as input features and whether the patient has a certain disease is used as a label with one of the three classes: presence, absence or questionable.  proposed a simple federated architecture for early detection of Type-2 diabetes. After comparing the proposed\nfederated learning model against the centralised approach, they showed that the federated learning model ensures significant privacy over\ncentralised learning model whereas compromising accuracy for a subtle extend.  To cope with the imbalanced and non-IID distribution inherent in user's monitoring data,  designed a generative convolutional autoencoder (GCAE), which aims to achieve accurate and personalized health monitoring by refining the model with a generated class-balanced dataset from user's personal data. It is noticed that GCAE is lightweight to transfer between the cloud and edges, which is useful to reduce the communication cost of federated learning in FedHome.  described a federated approach on a brain age prediction model on structural MRI scans distributed across multiple sites with diverse amounts of data and subject (age) distributions. In these heterogeneous environments, a Semi-Synchronous protocol provides faster convergence.", "cites": [6196, 6195, 6194, 6193, 6197], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of various federated learning applications in health text mining, listing different tasks and models. It shows minimal synthesis by grouping related tasks, but lacks deeper integration of ideas. There is little critical evaluation of the cited papers or identification of overarching patterns or principles, resulting in a primarily descriptive and low-level insight."}}
{"id": "3236fefb-3aa8-4c29-9657-8f37a548cb71", "title": "Privacy Evaluation", "level": "paragraph", "subsections": [], "parent_id": "b583c30f-772f-4c61-bb64-a930edb18d03", "prefix_titles": [["title", "Federated Learning Meets Natural Language Processing: A Survey"], ["section", "Benchmarks"], ["subsection", "Evaluation Aspects"], ["paragraph", "Model Evaluation"], ["paragraph", "Privacy Evaluation"]], "content": "The goal of privacy metrics is to measure the degree of privacy enjoyed by users in a system and the amount of protection offered by privacy-enhancing technologies.  discussed a selection of over eighty\nprivacy metrics and introduce categorizations based on the aspect of privacy they measure, their required\ninputs, and the type of data that needs protection. In general, privacy metrics can be classified with four common characteristics: adversary models, data sources, inputs and output meatures.", "cites": [6198], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions a survey on privacy metrics and introduces a classification based on four characteristics, but it lacks deeper synthesis with the cited paper and does not connect ideas across other works. There is no critical evaluation of the metrics or the approaches discussed, and the abstraction remains limited to a surface-level categorization without broader conceptual insights."}}
