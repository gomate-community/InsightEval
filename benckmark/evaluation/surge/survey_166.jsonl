{"id": "f8106a5d-a884-4b76-9c38-01bc71e57496", "title": "Introduction", "level": "section", "subsections": ["bb9b2ccb-5c5d-4d6f-b51b-c72a2d0a7cc6", "ef46ff4f-9e81-42ea-b646-cd3dfde7f58a", "b036b76b-2611-4beb-af9f-efea2b99ec73"], "parent_id": "c3821299-617c-4d6a-adb1-1d6fca364f6f", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Introduction"]], "content": "\\label{sec:Intro}\nDeep learning is a frontier for artificial intelligence, aiming to be closer to its primary goal\\ZJH{��}artificial intelligence.\nDeep learning has seen great success in a wide variety of applications, such as natural language processing, speech recognition, medical applications, computer vision, and intelligent\ntransportation system~.\nThe great success of deep learning is due to the larger models ~.\nThe scale of these models has included hundreds of millions of parameters.\nThese hundreds of millions of parameters allow the model to have more degrees of freedom enough to awe-inspiring description capability.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.80\\linewidth]{fig2.pdf}\n    \\caption{The development cycle of model.\n    }\n    \\label{fig1}\n\\end{figure}\nHowever, the large number of parameters requires a massive amount of training data with labels~.\nImproving model performance by data annotation has two crucial challenges.\nOn the one hand, the data growth rate is far behind the growth rate of model parameters, \\ZJH{which} has primarily hindered the further development of the model.\nOn the other hand, the emergence of new tasks has far exceeded the speed of data updates, and annotating all samples is \\p{labor-intensive and time-consuming.}\nTo tackle this challenge, many researchers build new datasets by generating samples, thereby speeding up model iteration and reducing the cost of data\nannotation~.\nBesides, \\p{a group of researchers devise pre-trained models and transfer learning to solve this challenge~, such as Transformers~, BERT~ and GPT~.}\nThese works have achieved incredible results.\n\\ZJH{Unfortunately}, the generated data is only used as base data to initialize the model.\nTo obtain a high-precision usable model, \\p{labeling and updating specific data is often necessary.}\nSo various work based on weak supervision has been proposed~.\nA great many researchers have proposed using few-shot to push the model to learn from fewer samples~.", "cites": [5831, 7, 1445, 113, 38], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the general context of deep learning and its data requirements, mentioning some cited papers (e.g., BERT, GPT, Transformers) without deeply connecting their ideas or evaluating their contributions. It outlines challenges and briefly introduces solutions but lacks a critical or comparative discussion, and minimal abstraction is achieved beyond surface-level observations."}}
{"id": "bb9b2ccb-5c5d-4d6f-b51b-c72a2d0a7cc6", "title": "Significance of \\REO{Human-in-the-loop", "level": "subsection", "subsections": [], "parent_id": "f8106a5d-a884-4b76-9c38-01bc71e57496", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Introduction"], ["subsection", "Significance of \\REO{Human-in-the-loop"]], "content": "}\nIntegrated a priori knowledge in the learning framework is an effective means to deal with sparse data, as the learner does not need to induce the knowledge from the data~.\n\\p{More recently, an ever-growing number of researchers have made efforts to incorporate pre-training knowledge into their learning framework}~.\nAs special agents, humans have rich prior knowledge.\n\\REO{If the developer \\ZJH{can} encourage the machine to engage with learning human wisdom and knowledge, it would help deal with sparse data, especially in medical fields like clinical diagnosis and lack of training data~.\nFurthermore, recent advances in cognitive science and human-machine interaction have suggested that human-related elements, such as emotional state and practical capability, impact human teaching performance and machine learning results on different tasks.}\n\\p{A multitude of researchers have proposed utilizing a conception called} ``\\REO{human-in-the-loop}\" to tackle the above challenges, mainly addressing these issues by incorporating human knowledge into the modeling process~. Human-in-the-loop conception is an extensive area of research that covers the intersection of computer science, cognitive science, and psychology.\nAs illustrated in Fig.~\\ref{fig2}, \\REO{human-in-the-loop} (namely ``\\REO{human-in-the-loop}\" and ``machine learning\") is an active research topic in machine learning, and there has been a rich publication in the past ten years.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.88\\linewidth]{fig1.pdf}\n    \\caption{The increasing research interest in the \\REO{human-in-the-loop}, obtained through Google scholar search with key-words: ``\\REO{human-in-the-loop}\"  and ``machine learning\".\n    }\n    \\label{fig2}\n\\end{figure}\nAs shown in Fig.~\\ref{fig1}, a conventional machine-learning algorithm typically consists of three parts~.\nThe first is data preprocessing, the second is data modeling, and the last is the developer modifying the existing process to improve performance.\nGenerally speaking, the performance and results of machine learning models are unpredictable, which leads to a large degree of uncertainty in which part of the machine-human interaction is capable of bringing the best learning effect.\nDifferent researchers focus on manual intervention in distinguishable parts.\nIn this paper, we investigate existing studies on \\REO{human-in-the-loop} technology via various implementations of \\REO{human-in-the-loop} from different practical perspectives\\ZJH{(e.g. Data Processing, Model Training and Inference, and System construction and Application).}\nIt is crucial to explore how interaction type interplays with other components of a \\REO{human-in-the-loop} pipeline to affect the intelligent systems' learning outcomes.\nIn addition, more research focuses on the design of independent systems to help complete the improvement of the model.\nIn this paper, we first discuss the work of improving model performance from data processing.\nNext, we discuss the work of improving model performance through interventional model training.\nFinally, we discuss the configuration of the system independent ``\\REO{human-in-the-loop}\".\n\\REO{\nWe \\ZJH{explore} the following questions around human-in-the-loop for machine learning:\n\\begin{itemize}\n\\setlength{\\itemsep}{0pt}\n\\setlength{\\parsep}{0pt}\n\\setlength{\\parskip}{0pt}\n\\item What is the human-in-the-loop for machine learning challenges, and what are the possible solutions to advance this research?\n\\item From a data perspective, what is the research status of human-in-the-loop for machine learning, research challenges, and what are the future directions?\n\\item From the perspective of model training, what is the research status of human-in-the-loop for machine learning, research challenges, and what are the future directions?\n\\item From an application perspective, what is the research status of human-in-the-loop for machine learning, research challenges, and what are the future directions?\n\\end{itemize}\n}", "cites": [3186], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a general analytical overview of the significance of human-in-the-loop in machine learning, particularly from a data and system perspective. While it references a cited paper to support the idea of using prior knowledge in learning, it does not deeply synthesize or connect multiple sources to form a comprehensive narrative. There is limited critical evaluation of the cited work or broader trends, and some abstraction is attempted (e.g., identifying interaction types and research questions), but it remains at a surface level without deeper conceptual unification."}}
{"id": "c713e1de-29be-4c63-a338-b41f4be348a1", "title": "Data Processing", "level": "section", "subsections": ["1acc640c-7cdb-43f8-8108-433e658a9cab", "dc913bc7-62f7-4629-b402-fd7ea29ba285", "09906928-8633-4dbc-b917-fca48be75a1f"], "parent_id": "c3821299-617c-4d6a-adb1-1d6fca364f6f", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Data Processing"]], "content": "\\label{sec:DP}\nAt present, deep learning has played an irreplaceable role in many fields~.\nThe great success of deep learning is due to larger-scale models, which include hundreds of millions of parameters ~.\nSuch a large amount of parameters empower the model with enough degrees of freedom to obtain awe-inspiring description capability. A massive amount of training data with labels are required to deal with a considerable number of parameters~.\nHowever, making annotations requiring much labor is likely to lag behind the growth in model capacity, and available datasets are quickly becoming outdated in size and density~.\n\\p{So the methods of utilizing unlabeled data to improve the model capability have increasingly gained much attention} ~.\nThe most significant difficulty is based on the fact that unlabeled data usually include incorrect samples, such as disturbing images, defective statements, and violations of constraints. Suppose these inaccurate samples are exactly sampled as the key one, the errors brought about will be fatal~.\nTo tackle this challenge, numerous researchers focus on exploring the way to generate a more rich sample space~, trying to develop a universal model such as Transformers~, BERT~ or GPT~ so that the model can learn features more effectively.\nBased on these successful methods, researchers then consider the further step: \\p{adopting little data to obtain more satisfactory results. So these models are employed for more tasks by fine-tuning and performing incredible results} ~.\nAlthough these methods still need to annotate a lot of data, which brings unnecessary trouble, we still noticed that interference in the model performance only some critical samples in the new datasets.\nHere goes to a critical issue that needs to be solved urgently,\n\\emph{How do we find out the key samples, and can we annotate key samples more easily?}\n\\begin{table*}[t]\n\\centering\n\\caption{A overview of representative works in data processing.\nDP: data preprocessing; DA: data Annotation; IL: iterative labeling;\nCV: Computer Vision;\nNLP: Natural Language Processing;\nSP: Speech Processing.\n}\n\\label{DPtable1}\n\\scalebox{0.78}{\\begin{tabular}{@{}p{82px}|p{18px}<{\\centering}p{18px}<{\\centering}p{18px}<{\\centering}|p{26px}<{\\centering}|p{12px}<{\\centering}p{12px}<{\\centering}p{12px}<{\\centering}p{18px}|p{128px}|c@{}}\n\\toprule\n\\multicolumn{1}{c|}{\\multirow{2}{*}{Work}} & \\multicolumn{3}{c}{Data Processing} & \\multicolumn{1}{|c|}{\\multirow{2}{*}{Year}} & \\multicolumn{4}{c}{Area} & \\multicolumn{1}{|c}{\\multirow{2}{*}{Task}}  & \\multicolumn{1}{|c}{\\multirow{2}{*}{\\REO{Quantitative results}}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){6-9}\n\\multicolumn{1}{c|}{}                      & DP      & DA     & IL     & \\multicolumn{1}{c|}{}                      & CV  & NLP  & SP  & Other & \\multicolumn{1}{c|}{}  & \\multicolumn{1}{c}{}                         \\\\ \\cmidrule(r){1-1} \\cmidrule(lr){1-11}\nYu~\\etal~                                        &         &        & \\checkmark      & 2015                                      & \\checkmark   &      &     &       & Scene and Object Categories     &\\REO{$0.82->0.88(mAP)$}              \\\\\nHe~\\etal~                                        &         &        & \\checkmark      & 2016                                      &     & \\checkmark    &     &       & CCG Parser                 &\\REO{$84.2\\%->85.9\\%(F_1)$}                   \\\\\nSelf~\\etal~                                      & \\checkmark       &        &        & 2016                                      &    &     &    & \\checkmark     & Data Analysis             &\\REO{$--$}                    \\\\\nZhuang~\\etal~                                   & \\checkmark       &        &        & 2017                                      &     &      &     & \\checkmark     & Knowledge Bases Integration        &\\REO{$84.3\\%->86.6\\%(F_1)$}           \\\\\nLi~\\etal~                                        & \\checkmark       &        &        & 2017                                      &     &      &     & \\checkmark     & Data Integration                  &\\REO{$--$}            \\\\\nKIM~\\etal~                                        &         & \\checkmark      &        & 2018                                      &     &      & \\checkmark   &       & Finding Sound Events           &\\REO{$--$}               \\\\\nDoan~\\etal~                                      & \\checkmark      &        &        & 2018                                      &     &      &     & \\checkmark     & Data Analysis         &\\REO{$--$}                         \\\\\nDong~\\etal~                                       & \\checkmark       &        &        & 2018                                      &     & \\checkmark    &     & \\checkmark     & \\small{Data Fusion;DOM extraction}  &\\REO{$--$}    \\\\\nGentile~\\etal~                                   & \\checkmark       & \\checkmark      &        & 2019                                      &     & \\checkmark    &     &       & Dictionary Expansion        &\\REO{$41.17\\% \\uparrow$}                  \\\\\nZhang~\\etal~                                    &         & \\checkmark      & \\checkmark      & 2019                                      &     & \\checkmark    &     &       & Entity Extraction          &\\REO{$0.8376->0.8644(F_1)$}                    \\\\\nLaure~\\etal~                                      & \\checkmark       &        &        & 2019                                      & \\checkmark   & \\checkmark    & \\checkmark   & \\checkmark     & Data Preparation      &\\REO{$--$}                        \\\\\nGurajada~\\etal~                                   & \\checkmark       &        &        & 2019                                      &     & \\checkmark    &     &       & Entity Resolution                  &\\REO{$--$}           \\\\\nLou~\\etal~                                        & \\checkmark       &        &        & 2019                                      &     & \\checkmark    &     &       & \\small{Knowledge Graph Programming}     &\\REO{$98.78\\% (top-10~hit)$}               \\\\\nLiu~\\etal~                                        &         & \\checkmark      & \\checkmark      & 2019                                      & \\checkmark   &      &     &       & Person Re-Identification        &\\REO{$45.55\\%->71.52\\%(mAP)$}              \\\\\nWallace~\\etal~                                    &         & \\checkmark      &        & 2019                                      &     & \\checkmark    &     &       & Question Answering                  &\\REO{$--$}          \\\\\nFan~\\etal~                                        &         &        & \\checkmark      & 2019                                      & \\checkmark   &      &     & \\checkmark     & Network Anomaly Detection       &\\REO{$84.71\\%(FPR)$}              \\\\\nKrokos~\\etal~                                     &         &        & \\checkmark      & 2019                                      & \\checkmark   &      &     & \\checkmark     & Knowledge Discovery             &\\REO{$67.7\\%->88.2\\%(Acc)$}              \\\\\nKlie~\\etal~                                       &         & \\checkmark      &        & 2020                                      &     & \\checkmark    &     &       & Entity Linking                       &\\REO{$35\\% \\uparrow ( annotation~speed)$}         \\\\\nChai~\\etal~                                       & \\checkmark       &        &        & 2020                                      &     &      &     & \\checkmark     & Outlier Detection               &\\REO{$88\\%->94\\%(recall)$}              \\\\\nButler~\\etal~                                     &         & \\checkmark      &        & 2020                                      & \\checkmark   &      &     &       & Facial Expressions                 &\\REO{$--$}            \\\\\nRistoski~\\etal~                                   & \\checkmark       &        &        & 2020                                      &     & \\checkmark    &     &       & Relation Extraction                  &\\REO{$0.02->0.0321(F_1)$}         \\\\\nQian~\\etal~                                       & \\checkmark       &        &        & 2020                                      &     & \\checkmark    &     &       & Entity Name Understanding           &\\REO{$--$}           \\\\\nLe~\\etal~                                         &         & \\checkmark      & \\checkmark      & 2020                                      & \\checkmark   &      &     &       & \\small{Self-Annotation For Video Object}  &\\REO{$34.1\\%->56.6\\%(mIoU)$} \\\\\nBartolo~\\etal~                                    &         & \\checkmark      &        & 2020                                      &     & \\checkmark    &     &       & Reading Comprehension            &\\REO{$39.1\\%(F_1)$}             \\\\\nCutler~\\etal~                                     & \\checkmark       &        &        & 2021                                      &     & \\checkmark    &     &       & Entity Recognition                   &\\REO{$--$}         \\\\\nMeng~\\etal~                                       &         & \\checkmark      &        & 2021                                      & \\checkmark   &      &     &       & \\small{3D Point Cloud Object Detection}        &\\REO{$--$}       \\\\\nZhang~\\etal~                                      &         & \\checkmark      &        & 2021                                      & \\checkmark   &      &     &       & \\small{Screentone and Manga Processing}        &\\REO{$--$}       \\\\\nAdhikari~\\etal~                                   &         & \\checkmark      &        & 2021                                      & \\checkmark   &      &     &       & Object Detection                        &\\REO{$--$}      \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}\nThe intuitive idea to solve this concern with a specific method is in three steps:\nSelect a few samples which models can not recognize.\n(1) Use the particular approach to annotate selected samples.\n(2) Push the model to learn features from the latest annotated samples.\n(3) This idea allows the model to make the most of the data information at the least cost.\nMultiple researchers try to apply \\REO{human-in-the-loop}-based methods to optimize models from the perspective of data. According to surveys, scientists spend about 80\\% of their time on data processing compared to model building~.\nWe investigated the data processing methods based on \\REO{human-in-the-loop} and established a pipeline as shown in Fig.~\\ref{Dp1}. We reviewed the representative works in data processing and showed the classification result as shown in Table~\\ref{DPtable1}.\nThis section explores the strengths and deficiencies of data processing with \\REO{human-in-the-loop} by demonstrating data preprocessing, data annotation, and iterative labeling.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{Dp1.pdf}\n    \\caption{A \\REO{human-in-the-loop} data processing pipeline.\n    }\n    \\label{Dp1}\n\\end{figure}", "cites": [5520, 7040, 2470, 7, 113, 4190], "cite_extract_rate": 0.13333333333333333, "origin_cites_number": 45, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of data processing in human-in-the-loop machine learning, citing several works but primarily listing them without deep synthesis or critical evaluation. It mentions some common challenges, such as annotation costs and noisy unlabeled data, but fails to connect the cited works into a coherent narrative or identify broader patterns or principles. The focus remains descriptive rather than analytical."}}
{"id": "dc913bc7-62f7-4629-b402-fd7ea29ba285", "title": "Data Annotation", "level": "subsection", "subsections": [], "parent_id": "c713e1de-29be-4c63-a338-b41f4be348a1", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Data Processing"], ["subsection", "Data Annotation"]], "content": "\\label{subsec:Data Annotation}\nFor new tasks, annotating data is a complex but crucial task to realize artificial intelligence. \\p{A considerable number of researchers have proposed employing a \\REO{human-in-the-loop}-based method for fast and precise (compared to fragile labeling) operations, especially in NLP and CV domains.}\nIn NLP tasks, data annotation is bifurcated into two categories. One is the annotation of specific task datasets, such as entity extraction~, entity linking~, and the other is more abstract tasks, such as Q\\&A tasks~ and reading comprehension tasks~.\nThe entity processing task is critical in NLP, and its success or failure directly affects the performance of NLP~.\nCurrently, there are two main methods for entity extraction, one is to formulate regular expressions for automatic extraction, and the other is the entity mentioned in the manual tagged document.\nHowever, neither of these two strategies can extract entities efficiently and accurately. Zhang~\\etal~ devised a \\REO{human-in-the-loop}-based entity extraction method to obtain the best return on investment in a limited time.\nWith the deepening of research, we need to handle a growing base of tasks. The emergence of new schemes is beyond our expectations.\nRegular expressions can help handle common data, but there is no expected magic for new data never seen before. To mitigate this issue, a few studies proposed approaches to solve the cross-domain problem in entity links.\nThey find the entities mentioned in the text and filter and discriminate them according to entities sorting information. This method is especially suitable for semantic disambiguation tasks~.\nHow to cope with more complicated tasks is also a focus of research now. Researchers attempt to integrate human experience and knowledge to endow machines with more intelligence.\nMore specifically, to what extent do neural network models understand the natural language, and can they be further enhanced? To explain this problem and explore more interpretability of the neural network model.\nWallace~\\etal~ developed an open application system that contains an interactive interface to talk with the machine, thereby generating more Q\\&A language materials to collect more research data and help the researcher explain the model predictions.\nBartolo~\\etal~ tried three different sets of annotation methods in the reading comprehension task to build a gradually more robust model in the annotation cycle. Significantly, they created a challenging dataset by collecting 36,000 samples.\nHowever, with the enhancement of the cyclic model, the performance gradually deteriorates. In contrast, the more robust model can still learn from the data intensively collected by the weaker model in the loop~.\nAs for CV, \\REO{human-in-the-loop} specifically explored how to exploit weak labeling to provide feedback at present. Besides, it also analyzed how to provide users with a unified intervention experience.\nIt is involved in numerous tasks, such as person re-identification, face recognition, 3D point cloud object detection, and object detection.\nWhile considerable current pedestrian re-identification (Re-ID) methods can achieve superior results under the training of a large amount of labeled data, these models cannot produce an exceptional performance as in the experiment when deployed in a natural environment. Moreover, so much data is new in a natural environment because these data have not been in the training set. The trickier part is that new data will constantly accumulate over time, which can cause the model to fail to work. To tackle this problem, Liu~\\etal~ proposed a human-in-cycle model based on reinforcement learning, which released the limitation of pre-labeling and upgraded the model through continuously collected data. The goal is to minimize human annotation work meanwhile maximizing the performance of Re-ID.\nIn addition to directly using reinforcement learning for dynamic learning, researchers also pay attention to expanding and refining data on a new task.\nFacial expression recognition is an exciting task in CV, which is of great help to sentiment analysis and behavior analysis tasks. Traditional facial expression recognition can only deal with the seven simplest facial expressions (\\ie happiness, sadness, fear, anger, disgust, surprise, and contempt).\nIn real life, it is additionally important to deal with more micro-expressions. More specifically, it is an interesting task that builds more refined micro expression processing datasets based on existing expression identification. Butler~\\etal~ exploited a micro-expression recognition method based on the \\REO{human-in-the-loop} system. This method provides a flexible interface for manual proofreading of automatically processed tags, thereby ensuring the accuracy and usability of the extended dataset.\nIn addition to directly constructing new datasets, it is also of great significance to explore existing datasets, especially for tasks that are difficult to label, such as target detection tasks, the labeling workload is enormous.\nTo reduce the labor and time cost of annotation of the bounding box of video objects, Le~\\etal~ applied an efficient and straightforward interactive self-annotation framework based on cyclic self-supervised learning. The entire framework consists of automatic model learning and interactive processes.\n\\p{The automatic learning process makes the model learn faster and speeds up the interaction process.} In the interactive recursive annotation, the detector receives feedback from the human annotator to process the human loop annotation scene. Moreover, to save labeling time, they proposed a new level correction module, which strengthens the utility of neighbor frames by CNN by reducing the distance of annotated frames at each time step.\nBased on the framework of Le~\\etal, Adhikari~\\etal~ modified the framework to be completed in one stage, and the most significant work of humans in it has become to correct errors instead of performing full annotations, which further improved the user experience.\nUsing the above two approaches is ineffective for more intricate image tasks, such as 3D point cloud labeling tasks. Because of the limited effect of employing only one stage for labeling, Meng~\\etal~ designed a multi-stage \\REO{human-in-the-loop} labeling method based on predecessors.\nHowever, the previous work only started from the perspective of data annotation. \\p{It ignored integrating human experience and knowledge into the model to the greatest extent to incorporate human knowledge and intelligence effectively.}\nZhang~\\etal~ considered specific talents and skills of humans in painting, and these skills cannot be fully quantified as rules and knowledge.\nIf the model can learn painting skills, it would undoubtedly help \\REO{human-in-the-loop} application takes a significant step forward.\nThey present a data-driven framework for generating comics from digital illustrations.\nTo further create high-quality comics, these three components are humanely annotated by the artist.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.88\\linewidth]{Dp3.pdf}\n    \\caption{The \\REO{human-in-the-loop} framework based on reinforcement learning.\n    }\n    \\label{Dp3}\n\\end{figure}", "cites": [2470], "cite_extract_rate": 0.07692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several human-in-the-loop approaches for data annotation in NLP and CV, connecting them to common themes such as efficiency, accuracy, and handling new data. It provides some critical analysis by pointing out the limitations of prior methods and the need to integrate human knowledge more effectively. However, it lacks deeper abstraction or a novel overarching framework, instead focusing on summarizing and slightly connecting individual methods."}}
{"id": "09906928-8633-4dbc-b917-fca48be75a1f", "title": "Iterative Labeling", "level": "subsection", "subsections": [], "parent_id": "c713e1de-29be-4c63-a338-b41f4be348a1", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Data Processing"], ["subsection", "Iterative Labeling"]], "content": "At present, there is still a high degree of coupling between deep learning tasks and data processing, and the performance of deep learning largely depends on the quality of the data.\nA large amount of high-quality labeled data is needed for a new task to obtain better performance.\nHowever, labeling large-scale data requires a lot of labor and takes a long time, while many iterations of tasks cannot afford such a cost and wait such a long time.\nUnlike weak annotate and automatically annotate, \\REO{human-in-the-loop}-based methods emphasize finding the essential samples that play a decisive factor in new sample data.\nUnlike the data annotation mentioned above in \\ref{subsec:Data Annotation}, data iterative labeling pays more additional attention to user experience, not just directly allowing users to perform data annotation.\nFrom annotation to iterative labeling, the goal has been changed in the following two aspects: to focus on adding knowledge and experience to the learning system. The other is to focus on the interaction with users.\nYu~\\etal~ utilized a partially automated labeling scheme for annotation, which free up human labor by using deep learning of \\REO{human-in-the-loop}.\nThis constitutes the basic prototype of simple iterative annotation.\nRecently, with the proliferation of reinforcement learning, Liu~\\etal~ developed a representative \\REO{human-in-the-loop} system based on reinforcement learning, which applied reinforcement learning to carry out iterative labeling. A typical framework of \\REO{human-in-the-loop} for reinforcement learning is shown in Fig.~\\ref{Dp3}. This novel attempt extends the practical usability of \\REO{human-in-the-loop} to the field of reinforcement learning for the first time, which brings a valuable contribution to the \\REO{human-in-the-loop} community.\nIn addition to implementing the simple manual intervention, they take person Re-Identification as a research task and explore how to minimize human annotation work while optimizing the performance of Re-ID. Fan~\\etal~ set out to solve the data challenge in the scheme of network anomaly detection to allow users to intervene in data labeling rather than implement simple user labeling.\nThey introduce a new intelligent labeling method, and the method combines active learning and visual interaction to detect network abnormalities through the iterative labeling process of users.\nThe difference is that they began to pay attention to the connection between the algorithm and the visual interface, and the algorithm and the optical interface are tightly integrated.", "cites": [4190], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of iterative labeling by connecting it to the broader theme of human-in-the-loop in machine learning and referencing specific works. It highlights the shift from traditional annotation to iterative labeling with a focus on user interaction and knowledge integration. However, the critical evaluation is limited, and while it identifies a general pattern (integration of algorithms with visual interfaces), it does not offer deeper abstraction or a meta-level framework."}}
{"id": "0bc33d32-8398-479f-bb98-03783869fc4f", "title": "Model Training and Inference", "level": "section", "subsections": ["05a26887-62e0-4187-b1e5-7314820a54e0", "bb05bff6-913b-4822-8d89-8f589c2ff249"], "parent_id": "c3821299-617c-4d6a-adb1-1d6fca364f6f", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"]], "content": "\\label{sec:MTI}\nIn many fields of Artificial Intelligence, such as Natural Language Process (NLP) and Computer Vision (CV), there are a variety of approaches that leverage human intelligence to train and infer experimental results.\nFor both NLP and CV, related research spans deep learning~ techniques and human-machine hybrid methods.\nThese heuristic methods have taken the diverse quality of human creativity into account to achieve high-quality results.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.88\\linewidth]{fig_xlw_1.pdf}\n    \\caption{The model training and inferencing workflow of \\REO{human-in-the-loop} in Natural Language Processing. The human participants provide various feedback in the stage of model training and inferencing according to specific tasks to boost the performance of NLP models.\n    }\n    \\label{fig_xlw_1}\n\\end{figure}", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a very general and descriptive overview of human-in-the-loop approaches in model training and inference for NLP and CV. It mentions deep learning and human-machine hybrid methods but does not meaningfully synthesize the content from the cited paper or connect it to broader themes. There is no critical evaluation or abstraction beyond the specific papers, limiting its insight quality."}}
{"id": "a9265be7-cb5e-466f-b0a1-73be44da2992", "title": "Syntactic and Semantic Parsing", "level": "subsubsection", "subsections": [], "parent_id": "05a26887-62e0-4187-b1e5-7314820a54e0", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Natural Language Process"], ["subsubsection", "Syntactic and Semantic Parsing"]], "content": "Besides text classification, \\REO{human-in-the-loop} approaches for syntactic and semantic parsing are also promising. Syntactic parsing is a process to obtain the valid syntactic structure of input sentences. The goal of \\ZJH{semantic parsing} is to map natural language to formal domain-specific semantic representations. A \\REO{human-in-the-loop} parsing method~ is proposed to improve the parsing accuracy of CCG parsing by employing non-expert to answer simple what-questions generated from the parser's output. These answers are treated as soft constraints when re-training the model. This work is the first attempt at introducing \\REO{human-in-the-loop} for syntactic parsing. However, most parsing technologies still face several challenges: (1) the purpose or expression of users can be ambiguous or vague under some circumstances, posing obstacles for them to get the ground truth in one shot, (2) in the real-world scenario, the performance of state-of-the-art parsers are generally not high enough, and (3) since the mainstream neural network-based models are known as ``black-box\" \\ZJH{that} indicates the lack of explainability, it is difficult for end-users to verify the parsing results independently.\nCurrently, Yao~ \\etal~ \\ZJH{propose} allowing the semantic parsers system to ask end-users clarification questions and produce an If-Then program simultaneously. Although recent works successfully verified the effectiveness of interactive semantic parsing in practice, they are generally restricted to a specific type of formal language.\n\\ZJH{Furthermore, }Yao~ \\etal~ develop a model-based interactive semantic parsing (MISP) as the general principle for interactive semantic parsing.\n\\begin{table*}\n\\centering\n\\caption{A brief overview of representative works in \\REO{human-in-the-loop} NLP. Each row represents one work. Works are sorted by task types (TC: Text Classification. SSP: Syntactic and Semantic Parsing. TS: Text Summarization. QA: Question Answering. SA: Sentiment Analysis). Each column corresponds to a dimension from the two subsections (task, motivation).}\n\\label{tab_xlw_1}\n\\scalebox{0.78}{\\begin{tabular}{l|ccccc|p{18px}<{\\centering}p{18px}<{\\centering}p{18px}<{\\centering}|c}\n\\toprule\n\\multicolumn{1}{c|}{}                                & \\multicolumn{5}{c|}{{\\color[HTML]{000000} \\textbf{Task}}}                                                    & \\multicolumn{3}{c|}{{\\color[HTML]{000000} \\textbf{Motivation}}}                                         \\\\ \\cmidrule(l){2-9}\n\\multicolumn{1}{c|}{\\multirow{-2}{*}{\\textbf{Work}}} & \\multicolumn{1}{l}{TC} & \\multicolumn{1}{l}{SSP}  & TS & \\multicolumn{1}{l}{QA} & \\multicolumn{1}{l|}{SA} & \\multicolumn{1}{l}{Performance} & \\multicolumn{1}{l}{Interpretability} & \\multicolumn{1}{l}{Usability} & \\multicolumn{1}{|c}{\\multirow{-2}{*}{\\textbf{\\REO{Quantitative results}}}} \\\\ \\midrule\n\\textit{Arous et al. (2021)}      & \\checkmark                                               &    &    &                        &                         & \\checkmark                               & \\checkmark                                    &                         &\\REO{$0.840->0.960(Acc)$}       \\\\\n\\textit{Karmakharm et al. (2019)} & \\checkmark                                              &    &    &                        &                         & \\checkmark                               &                                      & \\checkmark                          &\\REO{$--$}      \\\\\n\\textit{Yao et al. (2019)}        &                        & \\checkmark                           &    &                        &                         & \\checkmark                               & \\checkmark                                    & \\checkmark                        &\\REO{$0.640->0.968(Acc)$}        \\\\\n\\textit{Yao ZiYu et al. (2019)}   &                        & \\checkmark                           &    &                        &                         & \\checkmark                               & \\checkmark                                    &                           &\\REO{$0.615->0.729(Acc)$}       \\\\\n\\textit{Ziegler et al. (2019)}                         &                         &    & \\checkmark  &                        &                         & \\checkmark                               &                                      &                             &\\REO{$--$}     \\\\\n\\textit{Stiennon et al. (2020)}                         &                         &    & \\checkmark  &                        &                         & \\checkmark                               &                                      &                           &\\REO{$--$}       \\\\\n\\textit{Hancock et al. (2019)}                           &                         &    &    & \\checkmark                      &                         & \\checkmark                               &                                      &                          &\\REO{$0.447->0.463(Acc)$}        \\\\\n\\textit{Wallace et al. (2019)}                           &                         &    &    & \\checkmark                      &                         & \\checkmark                               &                                      & \\checkmark                           &\\REO{$0.4->0.6(Acc)$}     \\\\\n\\textit{Liu et al. (2021)}                               &                         &    &    &                        & \\checkmark                       &                                 & \\checkmark                                    & \\textbf{}                &\\REO{$0.71->0.82(Precision)$}        \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}", "cites": [6914, 3568, 2470, 7459], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers, particularly connecting interactive parsing and human clarification for syntactic and semantic tasks. It highlights general challenges like ambiguity and model explainability, which ties the works together analytically. However, it lacks deeper comparative analysis and broader abstraction, limiting the depth of insight."}}
{"id": "2bb7822c-1dff-4ac2-aefa-7da501de0b83", "title": "Text Summarization", "level": "subsubsection", "subsections": [], "parent_id": "05a26887-62e0-4187-b1e5-7314820a54e0", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Natural Language Process"], ["subsubsection", "Text Summarization"]], "content": "Besides applying a \\REO{human-in-the-loop} framework to topic modeling, researchers also use \\ZJH{it} to generate new texts. Text Summarization (TS) generates a shorter version of a given sentence/text while preserving its meaning~.\nIn recent years, there have been some significant breakthroughs in this field. For instance, Ziegler~\\etal~ fine-tune pre-trained language models with reinforcement learning by exploiting a reward model trained from human preferences. Then the model is used to generate summaries over Reddit TL, DR, and CNN/DM datasets. However, one limitation of their framework is that there are low agreement rates between labelers and researchers.\n\\ZJH{Stiennon~\\etal~ propose to gather a dataset composed of human preferences between pairs of summaries as the first step}. Then the prediction of the human-preferred summary is generated by a reward model (RM) trained via supervised learning. Lastly, the score produced by the RM is maximized as much as possible by a policy trained via reinforcement learning (RL). Their method ensures a relatively higher labeler-researcher agreement through the above steps and successfully separates the policy and value networks.", "cites": [3568], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of text summarization within human-in-the-loop ML by discussing the methods used in the cited papers, particularly the use of reward models and reinforcement learning. It identifies a limitation in one approach (low agreement rates) and outlines the steps of another to highlight its improvements. While it integrates the cited works to some extent, the analysis remains somewhat surface-level without deeper comparison or synthesis into a broader framework."}}
{"id": "36b32e26-d09b-4b31-8768-2034295d9ade", "title": "Question Answering", "level": "subsubsection", "subsections": [], "parent_id": "05a26887-62e0-4187-b1e5-7314820a54e0", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Natural Language Process"], ["subsubsection", "Question Answering"]], "content": "Recently, various \\REO{human-in-the-loop} related frameworks have been designed to apply dialogue and Question Answering (QA). The purpose of this task is to allow chatbots/agents to have a conversation with users. These \\REO{human-in-the-loop} dialogue intelligent systems can be bifurcated into two main categories: online feedback loop and offline feedback loop~. For the online feedback loop, human feedback is utilized to update the model continuously. Compared with traditional approaches that mismatch the training set and online use case for dialogue systems, researchers have demonstrated that the application of online reinforcement learning can improve the model with human feedback. For instance, a lifetime learning framework is proposed by Hancock~\\etal~. The self-feeding mechanism in this framework enables the chatbot to generate new examples when the conversation with users goes well, and these new examples are exploited to re-train itself continuously. For the offline feedback loop, a large set of human feedback needs to be collected as a training set, then this training set is used to update the model. \\ZJH{For instance, Wallace~\\etal~ employ ``trivia enthusiasts\" to creatively generate adversarial examples that can confuse their QA system.} These examples are finally implemented for negative training. Since some of the end-user feedback can be misleading, offline methods \\ZJH{is} more appropriate for improving the robustness of the model.", "cites": [7696, 2470, 7459], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by distinguishing between online and offline feedback loops in human-in-the-loop QA systems, and it connects these ideas to broader goals of model robustness and automation. It provides some critical analysis by noting the limitations of traditional approaches and the potential for misleading feedback. However, it lacks deeper evaluative critique and higher-level abstraction that would elevate it to a high insight level."}}
{"id": "1fcac0d6-3842-4339-8769-4386da80082c", "title": "Sentiment Analysis", "level": "subsubsection", "subsections": [], "parent_id": "05a26887-62e0-4187-b1e5-7314820a54e0", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Natural Language Process"], ["subsubsection", "Sentiment Analysis"]], "content": "Sentiment Analysis (SA) is one of the attractive research branches of Opinion Mining (OM). \\ZJH{The research scope of SA is the computational study of individuals' opinions and attitudes toward entities mentioned in a text}. The entities generally refer to individuals or events. Recently numerous neural network-based approaches have been widely utilized and demonstrated their effectiveness in solving sentiment analysis tasks~. Most deep learning-based methods for SA use accuracy and F1-score as evaluation metrics. Since these metrics can only evaluate the predictive performance, they lack the mechanisms to explain when and why the sentiment models give false predictions in run-time~. Liu~\\etal~ \\ZJH{introduce} an explainable \\REO{human-in-the-loop} SA framework for sentiment analysis \\ZJH{task}. The execution of their framework \\ZJH{is} segmented into three steps: First, the \\REO{human-in-the-loop} SA model analyzes local feature contributions. This goal is achieved by executing a data perturbation process. Next, local features are aggregated to calculate the explainable global-level features and humans participate in this loop to assess the relevance of the top-ranked global features to the ground truth and report the errors they find in this process. Finally, the system calculates an erroneous score based on global-level and local-level sentimental features for each instance. Scores higher than a specific threshold are indicated as wrong predictions.\n\\REO{", "cites": [8126], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of an explainable human-in-the-loop sentiment analysis framework by Liu~\\etal, synthesizing the three-step process described in the cited paper. While it integrates the framework's components and connects them to broader issues like model explainability, it does not compare this method with others or critically assess its limitations. Abstraction is limited to explaining the general structure of the approach without identifying broader patterns in the field."}}
{"id": "f630fa70-d906-44e9-b9c6-9a6e1c817458", "title": "Summarization for Human-in-the-Loop in NLP", "level": "subsubsection", "subsections": [], "parent_id": "05a26887-62e0-4187-b1e5-7314820a54e0", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Natural Language Process"], ["subsubsection", "Summarization for Human-in-the-Loop in NLP"]], "content": "}\nA brief overview of representative works in \\REO{human-in-the-loop} NLP is shown in Table~\\ref{tab_xlw_1}.\n\\ZJH{For most of the surveyed papers above, their original purpose is to apply \\REO{human-in-the-loop} techniques to various NLP tasks for better performance. }\nThe effectiveness of the approaches proposed by these surveyed papers is evaluated via multiple metrics. The experimental results in the documents we \\ZJH{investigate} show that a relatively small set of human feedback can dramatically boost model performance. For instance, the \\REO{human-in-the-loop} technique improves the classification accuracy for text classification~. Similar situations occur in dialogue and question answering where the QA systems have higher ranking metric hits . Besides, \\REO{human-in-the-loop} techniques have also enhanced the model's robustness and generalization . In addition to improving model performance, some studies have demonstrated that \\REO{human-in-the-loop} methods enable models to be more interpretable and usable in solving NLP problems. For instance, Arous~\\etal~ incorporate human rationales into an attention-based Bayesian framework reasonably while weighing worker reliability, thus providing a more human-understandable interpretation of classification results and enhancing the model performance at the same time. Liu~\\etal~ chose uni-grams as the explainable feature for LIME~; thus the proposed system allows the end-users to understand better the overall contribution of each word to the final sentiment classification made by the model. Wallace~\\etal~ invite ``trivia enthusiasts\" to creatively generate specific adversarial questions that confuse the intelligent question answering system. These questions can be treated as probes further to explore the inherent characteristics of the underlying model behaviors.", "cites": [2470, 7507, 7459], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by highlighting common themes like performance improvement and interpretability, but it lacks deeper connections or a novel framework. It describes examples of human-in-the-loop applications in NLP but does not critically evaluate their strengths or weaknesses. The abstraction is minimal, focusing on specific applications rather than broader principles."}}
{"id": "fe5110b6-2880-4de0-a93e-d5954cc62b1c", "title": "Object Detection", "level": "subsubsection", "subsections": [], "parent_id": "bb05bff6-913b-4822-8d89-8f589c2ff249", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Computer Vision"], ["subsubsection", "Object Detection"]], "content": "Object detection, as one of the most fundamental and challenging problems in computer vision~, has received significant attention in recent years~. The goal of object detection is to detect instances of visual objects of a specific class (such as individuals, vehicles, or other creatures) in digital images.\nYao~\\etal~ point out that iterations between queries may be expensive \\ZJH{and} time-consuming, making it unrealistic for executing interaction with end-users. They present an interactive object detection architecture to employ individuals to correct a few annotations proposed by a detector for the un-annotated image with the maximum predicted annotation cost. However, it is still difficult to detect some occluded objects, tiny objects, and blurred objects \\ZJH{by} these approaches. Madono~\\etal~ put forward an efficient \\REO{human-in-the-loop} object detection framework composed of bi-directional deep SORT~ and annotation-free segment identification (AFSID). Humans' role in this architecture is to verify the object candidates that bi-directional deep SORT can not detect automatically. Then train the model over the supplementary objects annotated by individuals.", "cites": [8429], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two human-in-the-loop approaches for object detection but does not effectively synthesize the information to form a broader narrative. It lacks critical evaluation of the methods and only briefly mentions limitations (e.g., difficulty in detecting occluded objects) without in-depth analysis. The abstraction is minimal, as it does not generalize to broader principles or identify systematic patterns across the cited works."}}
{"id": "a0e60286-0dfc-49e1-92ff-d6a28e028c97", "title": "Image Restoration", "level": "subsubsection", "subsections": [], "parent_id": "bb05bff6-913b-4822-8d89-8f589c2ff249", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Computer Vision"], ["subsubsection", "Image Restoration"]], "content": "Image restoration (IR) aims to recover the preliminary version of damaged images~. The image inpainting frameworks proposed by previous studies can be bifurcated as exemplar-based~ approaches and deep learning-based~ methods. Although profound learning-based works are the mainstream and show decent results, neural network-based approaches constantly suffer from over-fitting when only a relatively small training set is available on a large dataset. Besides, \\ZJH{in real-world application,} the restored images are often filled with unknown artifacts like uneven texture or monotone color due to the missing crucial semantic information in severely corrupted areas. Weber~\\etal~ propose an interactive machine learning system for image restoration based on Deep Image Prior (DIP)~.\n\\ZJH{Their proposed \\REO{human-in-the-loop} framework embeds human knowledge into the training process by the following steps. }\nInitially, the images from the dataset are sent to the automated DIP for preliminary restoration.\n\\ZJH{Secondly, the operators actively refine the images via a pre-designed user interface.\nThirdly, the refined images are sent back to the input of DIP again for further polishing. Finally, the whole loop continues until the restoration reaches the user's expectation.} In the field of Electron Microscopy, one drawback of automation is that it generally ignores the expertise of the microscopy user that comes with manual analysis. To alleviate such a challenging problem, Roels~\\etal~ propose a hybrid \\REO{human-in-the-loop} system that incorporates expert microscopy knowledge with the power of large-scale parallel computing to enhance the Electron Microscopy image quality by exploiting image restoration algorithms.", "cites": [524, 1251], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two cited papers to present a coherent narrative on how human-in-the-loop systems address limitations in automated image restoration. It connects the idea of Deep Image Prior with a hybrid system in Electron Microscopy, suggesting a synthesis of concepts. While it provides some critical points (e.g., overfitting in small datasets and artifacts in real-world applications), it stops short of a deeper evaluative analysis. The abstraction is moderate as it identifies patterns in the use of human knowledge to enhance restoration quality but does not elevate the discussion to broader principles or frameworks."}}
{"id": "3e8dfa4a-b197-4de6-be91-4cb2c84f81ec", "title": "Image Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "bb05bff6-913b-4822-8d89-8f589c2ff249", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Computer Vision"], ["subsubsection", "Image Segmentation"]], "content": "\\ZJH{Image segmentation (i.e. semantic segmentation) is a crucial step in most image studies. Image segmentation (IS) aims at assigning a class label to each pixel in the image~.} This field has recently become explosive popularity because it plays a crucial role in a wide range of computer vision applications~. However, few works explore how to effectively expose failures of ��top-performing �� semantic segmentation models and rectify the models by utilizing such counter-examples reasonably. Wang~\\etal~ \\ZJH{present} a two-step hybrid system with human efforts for troubleshooting pixel-level image labeling models. The hybrid system first automatically picks up un-labeled images from a large pool. These selected unlabelled images are used to compose an unlabeled set, which is the most informative in exposing the weaknesses of the target model. To reduce the number of false positives, individuals filter the unlabeled set to obtain a smaller straight set. In the second step, they fine-tune and re-train the target model to study from the counter-examples contained in the refining set without ignoring previously seen examples. Data annotation is always complicated and expensive in the medical image process domain~. Ravanbakhsh~\\etal~ introduce a training protocol based on combining the conditional Generative Adversarial Network (cGAN) and human workers interactively. For complex cases, human experts are responsible for annotating them. These newly annotated images are used to continue the training and inference procedure.\n\\begin{table*}[t]\n\\centering\n\\caption{A brief overview of representative works in \\REO{human-in-the-loop} CV. Each row represents one work. Works are sorted by their task types (OD: Object Detection. IR: Image Restoration. IS: Image Segmentation. IE: Image Enhancement. VOS: Video Object Segmentation). Each column corresponds to a dimension from the two subsections (task, motivation).}\\label{tab_xlw_2}\n\\scalebox{0.78}{\\begin{tabular}{l|ccccc|ccc|c}\n\\toprule\n\\multicolumn{1}{c|}{}                                & \\multicolumn{5}{c|}{{\\color[HTML]{000000} \\textbf{Task}}}                           & \\multicolumn{3}{c|}{{\\color[HTML]{000000} \\textbf{Motivation}}}                                         \\\\ \\cmidrule(l){2-9}\n\\multicolumn{1}{c|}{\\multirow{-2}{*}{\\textbf{Work}}} & \\multicolumn{1}{l}{OD} & \\multicolumn{1}{l}{IR} & IS & IE & \\multicolumn{1}{l|}{VOS} & \\multicolumn{1}{l}{Performance} & \\multicolumn{1}{l}{Interpretability} & \\multicolumn{1}{l}{Usability} &\\multicolumn{1}{|c}{\\multirow{-2}{*}{\\textbf{\\REO{Quantitative results}}}}\\\\ \\midrule\n\\textit{Yao et al. (2012)}~        & \\checkmark                      &                        &    &    &                          & \\checkmark                               &                                      &                             &\\REO{$--$}   \\\\\n\\textit{Madono et al. (2020)}~      & \\checkmark                      &                        &    &    &                          & \\checkmark                               &                                      &                             &\\REO{$--$}   \\\\\n\\textit{Roels et al. (2019)}~       &                        & \\checkmark                      &    &    &                          & \\checkmark                               &                             \\checkmark         &                             &\\REO{$--$}  \\\\\n\\textit{Weber et al. (2020)}~       &                        & \\checkmark                      &    &    &                          & \\checkmark                               &                                      &                            &\\REO{$0.2816->0.2227(DSSIM)$}    \\\\\n\\textit{Wang et al. (2020)}~        &                        &                        & \\checkmark  &    &                          & \\checkmark                               &                                      &                           &\\REO{$0.1365->0.4233(mIoU)$}    \\\\\n\\textit{Ravanbakhsh et al. (2020)}~ &                        &                        & \\checkmark  &    &                          & \\checkmark                               &                                      &                             &\\REO{$0.645->0.846(Acc)$}  \\\\\n\\textit{Murata et al. (2019)}~      &                        &                        &    & \\checkmark  &                          & \\checkmark                               &                                      &                              &\\REO{$--$}   \\\\\n\\textit{Fischer et al. (2020)}~     &                        &                        &    & \\checkmark  &                          & \\checkmark                               &                                      &                            &\\REO{$--$}    \\\\\n\\textit{Benard et al. (2017)}~      &                        &                        &    &    & \\checkmark                        & \\checkmark                               &                                      & \\checkmark                           &\\REO{$0.504->0.822(IoU)$}   \\\\\n\\textit{Oh et al. (2019)}~          &                        &                        &    &    & \\checkmark                        & \\checkmark                               &                                      & \\checkmark\n\\textbf{}                     &\\REO{$0.555->0.691(AUC)$}  \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table*}", "cites": [7501, 6743, 9067, 8490], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of selected works on human-in-the-loop image segmentation, with some mention of the goals and methods of the cited studies. However, it lacks deeper synthesis of ideas across papers, meaningful comparison of their approaches, and broader abstraction to general principles or frameworks. The table lists quantitative results but does not contextualize or analyze them in a way that contributes to a novel or insightful narrative."}}
{"id": "e624e03b-afdf-4176-a586-2d2a19973fa4", "title": "Video Object Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "bb05bff6-913b-4822-8d89-8f589c2ff249", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Computer Vision"], ["subsubsection", "Video Object Segmentation"]], "content": "The goal of \\ZJH{Video Object Segmentation} (VOS) is to segment a particular object instance in the entire video sequence of the object mask on a manual or automatic first frame~. This research area has become popular in the computer vision community~. Since videos have intrinsic characteristics such as motion blur, bad composition, occlusion, etc., it is harder for fully automatic approaches to segment more complex sequences accurately.\nEmploying user \\ZJH{to} input for each frame is unrealistic due to its expensive costs and time consumption. Thus, the \\REO{human-in-the-loop} framework is adopted for solving such problems. Benard~\\etal~ introduce a novel \\ZJH{interactive video object segmentation} method based on~. The core idea of their \\REO{human-in-the-loop} framework is to utilize the current segmentation mask as an additional input. A practical framework for interactive segmentation scenario is designed by Oh~\\etal~, named Interaction-and-Propagation Networks (IPN). The IPN is composed of two modules and the critical architecture of these two modules is deep convolutional neural networks. The primary operations of these two modules are interaction and propagation, respectively. Individuals are allowed to interact with the proposed model several times; meanwhile, the feedback is provided in scribbles on multiple frames during this interactive procedure.\n\\REO{", "cites": [6743, 9067, 3806, 6685, 3799], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of human-in-the-loop approaches for video object segmentation, mentioning several papers and their core ideas. However, it lacks deeper synthesis by not connecting these methods into a broader framework or trend. There is minimal critical analysis or abstraction, as the discussion remains largely focused on summarizing individual techniques without evaluating their strengths, weaknesses, or general principles."}}
{"id": "ab00964e-2af9-4d7d-9926-15d0a0afc828", "title": "Summarization for Human-in-the-Loop in CV", "level": "subsubsection", "subsections": [], "parent_id": "bb05bff6-913b-4822-8d89-8f589c2ff249", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Model Training and Inference"], ["subsection", "Computer Vision"], ["subsubsection", "Summarization for Human-in-the-Loop in CV"]], "content": "}\nA brief overview of representative studies in \\REO{human-in-the-loop} CV is displayed in Table~\\ref{tab_xlw_2}. It can be observed from Table~\\ref{tab_xlw_2} that the motivation of all the surveyed \\REO{human-in-the-loop} works for computer vision is to boost the model performance. From the experiment results of all these surveyed papers, although the evaluation criteria are different, the system that incorporates the \\REO{human-in-the-loop} method performs better than without combining it. Taking Madono~\\etal~ as an example, they conduct experiments for pedestrian detection and the results have proven at least two advantages for this task: For one thing, the proposed approach boosts the recall rate by \\ZJH{11$\\%$} at most over deep SORT.\nFor \\ZJH{the other}, the amount of unlabeled samples that need manual annotation is decreased by  \\ZJH{67$\\%$} at most compared with bi-directional deep SORT without AFSID, which dramatically improves the overall model performance. Associating the contents in Table~\\ref{tab_xlw_1}, this phenomenon in CV is similar to NLP, which demonstrates that the core motivation of almost all \\REO{human-in-the-loop} studies in both CV and NLP serves the purpose of boosting model performance. We also notice that in Table~\\ref{tab_xlw_2}, only one work~ tries to bring interpretability for the model. Roels~\\etal~ have validated the potential enhancements \\ZJH{that} DenoisEM can provide in 3D EM image interpretation by denoising SBF-SEM image data of an Arabidopsis thaliana root tip. Besides, \\REO{human-in-the-loop} conception can also improve the usability of CV models. For instance, Madono~\\etal~ have proven that their framework is advantageous/practical in scenarios where obtaining annotations is a costly affair. Oh~\\etal~ validate the usefulness and robustness of their Interaction-and-Propagation Networks with real interactive cutout use-cases.\n\\REO{Hudec~\\etal~ propose a new classification of aggregate functions in terms of mixed behavior by the variability of ordinal sums of associative and disjunctive functions, which play an important role for model fusing the knowledge of domain experts.}", "cites": [6743], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several key works in human-in-the-loop computer vision, connecting them to a common theme of improving model performance. It also draws parallels between CV and NLP, indicating abstraction at a moderate level. While it provides some critical observations (e.g., only one work focuses on interpretability), the analysis remains largely descriptive with limited in-depth critique or novel synthesis of concepts."}}
{"id": "39a2317d-bf74-4faa-aa4a-0f045d1d4d78", "title": "Code Production Tools", "level": "subsection", "subsections": [], "parent_id": "98ec5e48-5dcd-43db-a93d-1d3429991e20", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "System construction and Application"], ["subsection", "Code Production Tools"]], "content": "}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.88\\linewidth]{fig_syx3.pdf}\n    \\caption{With the help of \\REO{human-in-the-loop} system, human can collaborate a project with computer.}\n    \\label{fig_syx_3}\n\\end{figure}\n\\REO{\nThe programming of programs and the training of models are \\p{essential subtasks} of artificial intelligence, and this two tasks are highly coupled with humans. However, with the establishment of the open-source community, \\p{various} codes and model resources have been disclosed, \\p{making simple program writing and model training a repetitive work and thus leading to an automated basis for the model.} The \\p{fundamental} difficulty is \\p{making} integration tools learn to splice existing components, which requires integration tools to learn human integration experience. Under the guidance of this idea, there have been some tools for automation. With the help of these \\p{practical tools}, developers can collaborate on existing projects, rather than writing projects from scratch (Figure~\\ref{fig_syx_3}).}\n\\REO{\nSoftware testing was first addressed. MacHiry~\\etal~ \\p{designed} a test generation system called Dynodroid for fuzzing unmodified Android applications.\nDynodroid decomposed an application into a set of event-driven programs.\nDynodroid \\p{employed} the Android framework to generate a series of events and automatically executed these generated events to interact with the surrounding environment. It collected interactive feedback to program functions. The most worth mentioning is that \\p{Dynodroid would use manual intervention mode when necessary, and also tried to exploit input feedback to adjust the frame to generate new output dynamically.}\nThrough the above approach, Dynodroid implemented fuzz testing of unmodified Android applications in a human-machine \\p{hybrid manner}.\nYan~\\etal~ \\p{put forward} a human\\& tool-centric vulnerability analysis system that leveraged humans (with varying levels of expertise) to perform well-defined subtasks. \\p{The vulnerability analysis system had a larger scale of available programs and higher labor utilization than previous work.}\nBesides, in the practical application situation, the coders pay attention to the test of the code and want to see the application that can make the program correction.\nBohme~\\etal~ proposed the first \\REO{human-in-the-loop} semi-automatic program repair framework called LEARN2FIX.\nLEARN2FIX repaired bugs by negotiating with users to observe errors, trained an automatic error oracle through the bug samples marked in the negotiation and finally realized the automatic repair of the program. LEARN2FIX learned a sufficiently accurate automatic oracle with \\p{relatively}  low labeling effort (e.g. 20 queries).\nProductization tools of \\REO{human-in-the-loop} systems in software engineering bring great convenience to programmers. In the future, the application of \\REO{human-in-the-loop} in this field will expand from debugging and software testing to most coding programs. In particular, with the introduction of pre-trained models such as BERT~ and GPT~, human-machine collaborative programming is becoming a new focus.}\n\\REO{\nWith the development of machine learning, the training of deep models has become another \\p{prevalent} task.\nWith the continuous exploration of model training, more researchers focus on constructing semi-automatic or even fully automatic training tools.\n\\p{The researchers make efforts to incorporate human knowledge during various stages of model training} (Fig.~\\ref{fig_syx_4}).\nTo solve the problem that the underlying structure of the model is unknown in the model design process, Salam~\\etal~ \\p{applied} a semi-automated \\REO{human-in-the-loop} attribute design framework to \\p{assist human analysts in converting} original attributes into classifications of valid derived properties of the question. The framework first provided a human analyst with k buckets containing promising original attribute selections based on a random walk-based heuristic. \\p{It then iteratively allowed human analysts to deal with attributes involving attributes based on a scalable and efficient greedy heuristic algorithm.} The top-l derived attribute finally got a designed model frame, and the designer completed the detailed design interactively.\nTo address the model training process, the ultimate goal of an ML system becomes the problem of reducing the time it takes to get a deployable model from scratch.\nMA~\\etal~ \\p{exploited} a human-machine collaboration system called Helix which optimized execution across iterations by appropriately reusing or recomputing intermediate results. It is worth pointing out that Helix \\p{contained} a workflow management module as well as a visualization module to interact with people, thus enhancing the usability of the tool.\nTo solve the problem that transparency and operability are ignored in the process of model optimization,\nRenner~\\etal focused on both transparency (how the system is explained to humans) and operability (how users provide feedback or coach the system) in interactive machine learning. It combined novel topic visualization techniques with a human-centered interactive topic modeling system. \\p{Furthermore}, it revealed how users understand and interact with machine learning models and guided the further development of \\REO{human-in-the-loop} systems.\n}\n\\REO{", "cites": [7], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes several human-in-the-loop tools in the context of code production and model training but lacks deeper synthesis or comparison across them. It provides a factual summary of their functionalities and applications without evaluating their strengths, weaknesses, or broader implications. The abstraction level is limited, as it does not generalize to identify overarching principles or frameworks."}}
{"id": "e20324a8-495a-4e12-aeeb-c1378ddd7f15", "title": "The Challenges and Discussion", "level": "subsection", "subsections": [], "parent_id": "6ae88cf4-25d1-4051-bd16-86454c3279be", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Discussion and Future Directions"], ["subsection", "The Challenges and Discussion"]], "content": "}\n\\noindent\n\\textbf{How to add human experience and knowledge to computer vision tasks? }\nBy reviewing the previous works, we find that most \\REO{human-in-the-loop} \\ZJH{researches only focus} on natural language processing. Analyzing the reasons, \\p{it is not} easy to directly allow people to interact with images effectively( except for direct labeling ), and add human experience and knowledge to the model throughout the cycle.\nWith the development of multi-modal technology, \\p{utilizing} multi-modality for image representation \\ZJH{can} be an effective way~.\n\\REO{It is particularly worth pointing out that Holzinger~\\etal~ \\p{adopted} Graph Neural Networks as a method of choice, enabling information fusion for multi-modal capability, \\ZJH{which is a milestone.}}\n\\p{Besides, applying inverse reinforcement learning also seems to be a feasible and practical solution~.}\n\\noindent\n\\textbf{How does the model learn human knowledge and experience from a higher dimension? }\nThe goal of \\REO{human-in-the-loop} is to connect humans to the model loop \\p{in a specific way}, so that the machine can learn human knowledge and experience during the loop. Most current methods achieve this goal through human data annotation \\ZJH{which is only the most basic realization process.} \\p{As the saying goes, it is better to teach a man fishing than to give him fish. Researchers are supposed to take how to help agents acquire this knowledge effectively into account~.}\nLanguage is an experience accumulated in the human learning process. \\p{Researchers currently focus on employing human intervention in dialogue to enable machines to learn human knowledge and intelligence from dialogue procedures iteratively~.}\nIn addition, numerous reasoning tasks contain higher-dimensional knowledge. By integrating humans into the reasoning loop, the machines can also learn more about human experience~.\nImage quality evaluation and design tasks are a higher level of human activity. Although human aesthetics and design inspiration can constitute the theory\\ZJH{; however,} more inspiration and aesthetics still come from human experience~. \\ZJH{\\p{If we find a productive way to allow the model to learn more expert experience, the model's improvement can be dramatic.}}\n\\noindent\n\\textbf{How to select key samples? }\nThe critical technology for the \\REO{human-in-the-loop} is obtaining essential samples and labeling them with human intervention. At present, researchers mainly exploit confidence-based methods to obtain critical samples. This method plays an irreplaceable role in classification tasks~.\nHowever, for other tasks \\ZJH{(e.g. semantic segmentation, regression, and target detection tasks)}, \\p{confidence is the evaluation that is not so noticeable and thus cannot reflect the improvement of the system.}\nActive learning aims to train an accurate prediction model with the least cost by marking the examples that provide the most information. There are multiple mature and worthy reference methods in selecting criteria, and perhaps researchers can obtain inspiration from these methods~.\n\\noindent\n\\textbf{How to construct an evaluation benchmark? }\nTo develop the entire community, \\ZJH{providing an effective test benchmark is important.} At present, there is no uniform standard for \\REO{human-in-the-loop} research benchmarks. To better explore this research topic, it is essential to study how to develop evaluation methods and benchmarks for \\REO{human-in-the-loop} systems. Moreover, the formation of a unified benchmark is also conducive to the further refinement of research~. The current \\REO{human-in-the-loop}-based research is a more influential direction for exploring ways that are more conducive to \\REO{human-in-the-loop}. \\ZJH{In addition to} creating the standards for these interaction methods, restricting and theorization are also particularly important.\n\\noindent\n\\REO{\n\\textbf{How to implement a general multitasking framework by \\REO{human-in-the-loop}? }}\n\\ZJH{The real-world task is complex and in its current form, so it is not easy to completely solve it with one characterization~.}\nWith the emergence of a unified large-scale pre-training model~, we have seen the hope of achieving a universal model through \\REO{human-in-the-loop} fine-tuning. In particular, the current machine learning models are not as intelligent as humans, so it may be the next direction to consider using a suitable way to introduce human knowledge into large models.\n\\REO{", "cites": [7040, 7302, 7079], "cite_extract_rate": 0.21428571428571427, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section attempts to synthesize ideas from the cited papers by connecting human-in-the-loop mechanisms with dialogue learning and multi-modality. However, the integration is somewhat limited and lacks a novel framework. It shows moderate critical analysis by pointing out limitations of current methods (e.g., reliance on basic annotation), but deeper evaluation or contrast of approaches is missing. The abstraction is reasonable, identifying broader issues such as general multitasking frameworks and benchmark development."}}
{"id": "0ce9c5fd-10c7-44f4-99e0-1fef7b5f0e28", "title": "Future Directions in Real-world Applications", "level": "subsubsection", "subsections": [], "parent_id": "7ec5a39b-c94b-4757-89a9-7964da300e12", "prefix_titles": [["title", "A Survey of Human-in-the-loop for Machine Learning"], ["section", "Discussion and Future Directions"], ["subsection", "Future Directions"], ["subsubsection", "Future Directions in Real-world Applications"]], "content": "}\n\\begin{itemize}\n\\setlength{\\itemsep}{0pt}\n\\setlength{\\parsep}{0pt}\n\\setlength{\\parskip}{0pt}\n\\item\n\\REO{It is fundamental to choose an appropriate artificial intervention time, especially the tasks with solid demand for reliability and safety~.}\n\\item\n\\REO{For a system with human-computer interaction, users' expectations of experience usually take precedence over performance~.}\n\\item\n\\REO{Modeling the sensor signal and solving the unified coding of abstract and concrete information is an essential problem in the process of human-computer interaction~.}\n\\item\n\\REO{Human intervention remains on superficial judgments (such as acceptance/rejection or direction), and exploring more complex feedback is also a critical issue in \\ZJH{human-in-the-loop} applications~.}\n\\item\n\\REO{Human-in-the-loop-based systems should have high robustness and generalization ability to domain changes, disturbances, and ``out-of-range'' samples~.}\n\\end{itemize}", "cites": [7057], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section outlines several high-level challenges in human-in-the-loop machine learning for real-world applications, but it lacks a deep synthesis of the cited paper (e.g., 'Domain Generalization: A Survey') and does not clearly connect ideas across multiple works. It does not provide critical evaluations or comparisons of the approaches discussed, and while it attempts to generalize around concepts like robustness and feedback complexity, the abstraction remains limited and superficial."}}
