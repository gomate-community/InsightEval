{"id": "b5fdf9c3-044e-45b3-8bb1-0b7d691c74d6", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "3313ad50-7887-4970-bb3f-a8d5311e5f61", "prefix_titles": [["title", "Generate FAIR Literature Surveys with Scholarly Knowledge Graphs"], ["section", "Introduction"]], "content": "When conducting scientific research, reviewing the existing literature is an essential activity~. \nFamiliarity with the state-of-the-art is required to effectively contribute to advancing it and do relevant research. Mainly because published scholarly knowledge is unstructured~, it is currently very tedious to review existing literature. Relevant literature has to be found among hundreds and increasingly thousands of PDF articles. This activity is supported by library catalogs and online search engines, such as Scopus or Google Scholar~. Because the search is keyword based, typically large numbers of articles are returned by search engines. Researchers have to manually identify the relevant papers. Having identified the relevant papers, the relevant pieces of information need to be extracted in order to obtain an overview of the literature. Overall, these are manual and time consuming steps. We argue that a key issue is that the scholarly knowledge communicated in the literature does not meet the FAIR Data Principles~. While PDF articles can be found and accessed (assuming Open Access or an institutional subscription), the scholarly literature is insufficiently interoperable and reusable, especially for machines. For units more granular than the PDF article, such as a specific result, findability and accessibility score low even for humans.\nWe present a methodology and its implementation integrated into the Open Research Knowledge Graph (ORKG)~ that can be used to generate and publish literature surveys in form of machine actionable, comparable descriptions of research contributions. Machine actionability of research contributions relates to the ability of machines to access and interpret the contribution data.  The benefits for researchers of such an infrastructure are (at least) two-fold. Firstly, it supports researchers in creating state-of-the-art overviews for specific research problems efficiently. Secondly, it supports researchers in publishing literature surveys that adhere to the FAIR principles, thus contributing substantially to reuse of state-of-the-art overviews and therein contained information, for both humans and machines. \nLiterature reviews are articles that focus on analysing existing literature. Among other things, reviews can be used to gain understanding about a research problem or to identify further research directions~. Reviews can be used by authors to quickly obtain an overview of either emerging or mature research topics~. Review papers are important for research fields to develop. When review papers are lacking, the development of a research field is weakened~. Compiling literature review papers is a complicated task~ and is often more time consuming than performing original research~.  The structure of such articles often consists of tables that compare published research contributions. Although in the literature the terms ``literature review'' and ``literature survey'' are sometimes used interchangeably, we make the following distinction. We refer to the tables in review articles as \\textit{literature surveys}. Together with a (textual) analysis and explanation, they form the \\textit{literature review}. The state-of-the-art (SoTA) analysis is a special kind of literature review with the objective of comparing the latest and most relevant papers in a specific domain. \nWe implement the presented methodology in the ORKG. The ORKG is a scholarly infrastructure designed to acquire, publish and process \\emph{structured} scholarly knowledge published in the literature~. ORKG is part of a larger research agenda aiming at machine actionable scholarly knowledge that understands the ability to more efficiently compare literature as a key feature. \nWe tackle the following research questions: \n\\begin{enumerate}\n    \\item How to generate literature surveys using scholarly knowledge graphs?\n    \\item How to ensure that published literature surveys comply with the FAIR principles?\n     \\item How to effectively specify and visualize literature surveys in a user interface?\n\\end{enumerate}\nIn support of the first research question, we present a methodology that describes the steps required to generate literature surveys. In support of the second research question, we describe how the FAIRness of the published literature review is ensured. Finally, in support of the third research question, we demonstrate how the methodology is implemented within the ORKG. \nThe paper is structured as follows. Section~\\ref{section:motivating-example} motivates the work. Section \\ref{section:related-literature} reviews related work. Section~\\ref{section:system-design} presents the system design, the underlying methodology and its implementation. Section~\\ref{section:data-collection} explains how the knowledge graph is populated with data. Section~\\ref{section:evaluation} presents the evaluation of the system, specifically system FAIRness and performance. Finally, Section~\\ref{section:discussion} discusses the presented and future work.", "cites": [6548, 6549], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes the cited papers by framing them within the context of scholarly communication and the need for structured, machine-actionable knowledge. It connects the ORKG's purpose with broader challenges in literature review. However, the critical analysis is limited to general statements about the limitations of document-based communication, without deeper evaluation of the cited papers. The section abstracts some concepts, such as the FAIR Data Principles and the need for structured knowledge, but the insight remains focused on the survey's own goals rather than broader trends in scholarly knowledge organization."}}
{"id": "32d3f690-7c93-4310-b607-6553d1915112", "title": "Related work", "level": "section", "subsections": [], "parent_id": "3313ad50-7887-4970-bb3f-a8d5311e5f61", "prefix_titles": [["title", "Generate FAIR Literature Surveys with Scholarly Knowledge Graphs"], ["section", "Related work"]], "content": "\\label{section:related-literature}\nThe task of comparing research contributions can be reviewed in light of the more general task of comparing resources (or entities) in a knowledge graph. While this is a well-known task in multiple domains (for instance in e-commerce systems~), not much work has focused on comparison in knowledge graphs, specifically. One of the few works with this focus is by \\citeauthor{Petrova2017}~ who created a framework for comparing entities in RDF graphs using SPARQL queries. In order to compare contributions, they first have to be found. Finding is an information retrieval problem. As a well-known technique, TF-IDF~ can be used for this task. More sophisticated techniques can be used to determine the structural similarity between graphs (e.g., ) and matching semantically similar predicates. This relates to dataset interlinking~ or more generally ontology alignment~. For property alignment, techniques of interest include edit distance (e.g., Jaro-Winkler~ or Levenshtein~) and vector distance. \\citeauthor{Gromann2019}~ found that fastText~ performs best for ontology alignment. \nIn light of the FAIR Data Principles~, scholarly data should be Findable, Accessible, Interoperable and Reusable both for humans and machines. Due to the publication format, literature survey tables published in scholarly articles weakly adhere to the FAIR guidelines, particularly so for machines. Scholarly data should be considered first-class objects~, including data used to create literature surveys. \\citeauthor{Rodriguez-Iglesias2016}~ describe the difficulties of making data FAIR within the plant sciences. They argue that it is more complicated than reformatting data. On the other hand they suggest that most FAIR principles can be implemented relatively easily by using off-the-shelf technologies. \\citeauthor{Boeckhout2018}~ argue that the FAIR principles alone are not sufficient to lead to responsible data sharing. More applied principles are needed to ensure better scholarly data. This claim is supported by the findings of \\citeauthor{Mons2017}~ who suggest that there are very diverse interpretations of the guidelines. In their work, they try to clarify what is FAIR and what is not. \nAn efficient literature comparison relies on scholarly knowledge being represented in a structured way. There is substantial related work on representing scholarly knowledge in structured form~. Building on the work of numerous philosophers of science, \\citeauthor{Hars2001}~ proposed a comprehensive scientific knowledge model that includes concepts such as theory, methodology and statement. More recently, ontologies were engineered to describe different aspects of the scholarly communication process. Semantic Publishing and Referencing (SPAR)\\footnote{http://purl.org/spar/\\{cito,c4o,fabio,biro,pro,pso,pwo,doco,deo\\}} is a collection of ontologies that can be used to describe scholarly publishing and referencing of documents~. \\citeauthor{IniestaCorcho:SePublica2014}~ reviewed the state-of-the-art ontologies to describe scholarly articles. \\citeauthor{Sateli2015SemanticRO}~ use some of these scholarly ontologies to add semantic representations of scholarly articles to the Linked Open Data cloud. A literature survey comparing scholarly ontologies is available via the ORKG.\\footnote{\\url{https://www.orkg.org/orkg/comparison/R8342}} Most of these ontologies are designed to capture metadata about and structure of scholarly articles, not the content communicated in articles. \nAnother literature survey is created to compare approaches for semantically representing scholarly communication.\\footnote{\\url{https://www.orkg.org/orkg/comparison/R8364}}\nAn initial attempt for semantifying review articles was done in~. The work comprises a relatively rigid ontology for describing contributions (mainly centered around research problems, approaches, implementations and evaluations) and a prototypical implementation using Semantic MediaWiki. We relax this constraint, since we are not limited by a rigid ontology schema but rather allow arbitrary domain-specific semantic structures for research contributions. The work by \\citeauthor{DBLP:conf/ercimdl/VahdatiFALV19}~ focuses on semantic article representations for generating literature overviews. Their method is to use crowdsourcing to generate the overviews. \\citeauthor{Kohl2018}~ present CADIMA, a system that supports systematic literature reviews. The tool supports the formal process of performing a literature review but does, for example, not publish data in machine actionable form for reuse.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{img/methodology.pdf}\n    \\caption{Research contribution comparison methodology.}\n    \\label{fig:workflow-comparison}\n\\end{figure}", "cites": [8369], "cite_extract_rate": 0.04, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several papers by connecting the tasks of entity comparison in knowledge graphs, semantic representation of scholarly articles, and FAIR data principles, forming a coherent narrative on the limitations of existing approaches for generating literature surveys. It includes some critical analysis, such as noting that CADIMA does not publish data in machine-actionable form. However, the synthesis and abstraction remain somewhat surface-level, with limited discussion of deeper patterns or a novel theoretical framework."}}
{"id": "fbebdd62-b2b1-408f-9c69-5d4f7f13cfce", "title": "Align contribution descriptions", "level": "subsection", "subsections": [], "parent_id": "a1917066-53fd-432d-9c65-ea4825cda6c7", "prefix_titles": [["title", "Generate FAIR Literature Surveys with Scholarly Knowledge Graphs"], ["section", "System design"], ["subsection", "Align contribution descriptions"]], "content": "As described in the first step, comparisons are built using shared or similar properties of contributions. In case the same property has been used between contributions, these properties are grouped and form one \\textit{comparison row}. However, often different properties are used to describe the same concept. This occurs for various reasons. The most obvious reason is when two different ontologies are used to describe the same property. For example, for describing the population of a city, DBpedia uses \\textit{dbo:populationTotal} while WikiData uses \\textit{WikiData:population} (actually the property identifier is P1082; for the purpose here we use the label). When comparing contributions, these properties should be considered as equivalent. Especially for community-created knowledge graphs, differently identified properties likely exist that are, in fact, equivalent.\nTo overcome this problem, we use pre-trained fastText~ word embeddings to determine the similarity of properties. If the similarity is higher than a predetermined threshold $\\tau$, the properties are considered equivalent and are grouped. This happens when the similarity threshold $\\tau \\geq 0.9$ (also empirically determined). In the end, each group of properties will be visualized as one row in the comparison table. The result of this step is a list of statements for each contribution, where similar properties are grouped. Based on this similarity matrix $\\gamma$ is generated \n\\begin{equation}\n    \\gamma_{p_{i}} = \\left [ cos(\\overrightarrow{p_i}, \\overrightarrow{p_j}) \\right ]\n    \\label{eq:similarity_matrix}\n\\end{equation}\nwith $cos(.)$ as the cosine similarity of vector embeddings for property pairs $(p_i, p_j) \\in \\mathcal{P}$, whereby $\\mathcal{P}$ is the set of all contributions.\nFurthermore, we create a mask matrix $\\Phi$ that selects properties of contributions $c_i \\in \\mathcal {C}$, whereby $\\mathcal{C}$ is the set of contributions to be compared. Formally,\n\\begin{equation}\n    \\Phi_{i,j} = \\begin{cases}\n1 & \\text{ if } p_{j} \\in c_i \\\\ \n0 & \\text{ otherwise }  \n\\end{cases}\n    \\label{eq:mask_matrix}\n\\end{equation}\nNext, for each selected property $p$ we create the matrix $\\varphi$ that slices $\\Phi$ to include only similar properties. Formally,\n\\begin{equation}\n\\varphi_{i,j} =(\\Phi_{i,j})_{\\substack{c_i \\in \\mathcal{C}\\\\ p_j \\in sim(p) }}\n\\label{eq:slice_mask}\n\\end{equation}\nwhere $sim(p)$ is the set of properties with similarity values $\\gamma[p] \\geq \\tau$ with property $p$. Finally, $\\varphi$ is used to efficiently compute the common set of properties~. This process is displayed in Algorithm~\\ref{alg:alining-properties}.\n\\begin{algorithm}\n\\caption{Align contribution descriptions}\\label{alg:alining-properties}\n\\begin{algorithmic}[1]\n\\Procedure{AlignProperties}{properties, threshold}\n\\ForEach {property $p_1 \\in properties$}\n    \\ForEach {property $p_2 \\in properties$}\n        \\State $similarity \\gets$ cos(Embb($p_1$), Embb($p_2$))\n        \\If {$similarity > threshold$}\n        \\State $similarProps \\gets similarProps \\cup \\{ p_1, p_2 \\}$\n        \\EndIf\n    \\EndFor\n\\EndFor\n\\Return $similarProps$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}", "cites": [8369, 6548], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the use of word embeddings from fastText and the concept of property alignment in knowledge graphs, integrating these into a methodology for grouping similar contribution properties. It provides a formal framework with equations and an algorithm to express the alignment process, showing abstraction beyond individual papers. However, the critical analysis is limited, as it does not evaluate the limitations of fastText or compare it with other methods for property alignment."}}
{"id": "252651d1-6cd2-4bc9-abba-721f972b6d1e", "title": "Technical details", "level": "subsection", "subsections": [], "parent_id": "a1917066-53fd-432d-9c65-ea4825cda6c7", "prefix_titles": [["title", "Generate FAIR Literature Surveys with Scholarly Knowledge Graphs"], ["section", "System design"], ["subsection", "Technical details"]], "content": "The user interface of the comparison feature is seamlessly integrated with the ORKG front end, which is written in JavaScript and is publicly available\\footnote{\\url{https://gitlab.com/TIBHannover/orkg/orkg-frontend}}. The back end of the comparison feature is a service separate from the ORKG back end written in Python and also available Open Source\\footnote{\\url{https://gitlab.com/TIBHannover/orkg/orkg-similarity}}. The comparison back end is responsible for step two and three of the comparison methodology. The input in step two is the set of contribution IDs. The API selects the related statements and aligns the properties and returns the data needed to visualize the comparison. This data includes the list of papers, list of all properties and the values per property. \n\\begin{table*}[t]\n\\caption{List of imported survey tables in the ORKG. The paper and table reference can be used to identify the original table. }\n\\label{table:imported-review-tables}\n\\begin{adjustbox}{scale=.90}\n\\begin{tabular}{l|l|l|r|l|l}\n\\toprule\n\\textbf{Paper reference} & \\textbf{Table reference} & \\textbf{Research problem} & \\textbf{Papers} & \\textbf{ORKG representation} & \\textbf{Information loss} \n\\\\ \\midrule\n\\citeauthor{Bikakis2016}~ & \nTable 1 & Generic visualizations & \n11 &\n\\url{https://orkg.org/orkg/c/pdLJDk} & \nNo \n\\\\ \n\\citeauthor{Bikakis2016}~ & \nTable 2 & Graph visualizations & \n21 &\n\\url{https://orkg.org/orkg/c/Rx476Z} & \nNo\n\\\\ \n\\citeauthor{Diefenbach2018a}~ & \nTable 2 & \nQuestion answering evaluations & \n33 &\n\\url{https://orkg.org/orkg/c/gaVisD} & \nNo\n\\\\ \n\\citeauthor{Diefenbach2018a}~ & \nTable 3,4,5,6 & \nQuestion answering systems & \n26 &\n\\url{https://orkg.org/orkg/c/IuEWl2} & \nNo\n\\\\ \n\\citeauthor{Hussain2017a}~ & \nTable 4 &\nAuthor name disambiguation &\n5 &\n\\url{https://orkg.org/orkg/c/vDxKdr} & \nNo\n\\\\\n\\citeauthor{Hussain2017a}~ & \nTable 5 &\nAuthor name disambiguation &\n6 &\n\\url{https://orkg.org/orkg/c/XXg8Wg} & \nNo\n\\\\ \n\\citeauthor{Hussain2017a}~ & \nTable 6 &\nAuthor name disambiguation &\n9 &\n\\url{https://orkg.org/orkg/c/9rOwPV} & \nNo\n\\\\ \n\\citeauthor{Hussain2017a}~ & \nTable 7 &\nAuthor name disambiguation &\n6 &\n\\url{https://orkg.org/orkg/c/mB7kIK} & \nNo\n\\\\ \n\\citeauthor{Naidu2018}~ & \nTable 4 &\nText summarization &\n52 &\n\\url{https://orkg.org/orkg/c/OUqYB9} & \nNo\n\\\\ \n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\end{table*}", "cites": [8253], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the technical architecture of the ORKG system and provides a list of imported survey tables without synthesizing broader insights from the cited papers. There is no critical evaluation or comparison of the approaches presented in the cited works, nor any abstraction to identify general patterns or principles. The focus remains on system components and data representation."}}
