{"id": "edde07f6-ba19-4e5f-9251-7cbe2f890158", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "86a0553c-8edc-49a7-8382-330c5ec7b9b7", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nThere has been a tremendous surge in the development and application of machine learning models in recent years due to their flexibility and capability to represent trends in complex systems .\nThe parameters of a machine learning model can often be calibrated, with sufficient data, to give high fidelity representations of the underlying process \n.\nIt is now feasible to construct deep learning models over datasets of tens of thousands to millions of data points with modern computational resources  . In many scientific applications, however, there may not be large amount\\REVISION{s} of data available for training. Unlike data from internet or text searches, computational and physical experiments are typically extremely expensive. Moreover, even if ample data exists, the machine learning model may yield behaviors that are inconsistent with what is expected physically when queried in an extrapolatory regime.\nTo aid and improve the process of building machine learning models for scientific applications, it is desirable to have a framework that allows the incorporation of physical principles and other a priori information to supplement the limited data and regularize the behavior of the model.  Such a framework is often referred to as ``physics-constrained'' machine learning within the scientific computing community .\n provide a taxonomy for theory-guided data science, \nwith the goal of incorporating scientific\nconsistency in the learning of generalizable models.  The information used to constrain models can be simple, such as known range or positivity constraints, shape constraints, or monotonicity constraints that the machine learning model must satisfy.  The constraints can also be more complex; for example, they can encode knowledge of the underlying data-generating process in the form of a partial differential equation. \nSeveral recent conferences highlight the interest in ``physics-informed'' machine learning~. \nMuch of the existing research in physics-informed machine learning has focused on incorporating constraints in neural networks , often through the use of objective/loss functions which penalize constraint violation . Other works have focused on incorporating prior knowledge using Bayesian inference that expresses the data-generating process as dependent on a set of parameters, the initial distribution of which is determined by the available information, e.g., functional constraints . Unlike deterministic learning approaches, the predictions made using approximations trained with Bayesian inference are accompanied with probabilistic estimates of uncertainty/error.\nWithin the Bayesian regression framework, Gaussian processes (GPs) are popular for constructing ``surrogates'' or ``emulators'' of data sources that are very expensive to query.  The use of GPs in a \nregression framework to predict a set of function values is called Gaussian Process Regression (GPR). \nAn accurate GPR can often be constructed using only a relatively small number of training data (e.g. tens to hundreds), which consists of pairs of input \nparameters and corresponding response values.  Once constructed, the GPR can be thought of as a machine-learned metamodel and used to provide \nfast, cheap function evaluations for the purposes of prediction, sensitivity \nanalysis, uncertainty quantification, calibration, and optimization. \nGP regression models are constructed with data obtained from computational simulation~ or field data; in geostatistics, the process of applying Gaussian processes to field data has been used for decades and is frequently referred to as \\emph{kriging} . \nIn this survey we focus on the use of constrained GPRs\nthat honor or incorporate a wide variety of physical constraints~.\nSpecifically,\nwe focus on the following topics, after a short review of Gaussian process regression in Section \\ref{gpr}. \nSection \\ref{overview_of_constraints} presents an overview and a classification of constraints according to how the constraint is enforced during the construction of a GP. \nSection \\ref{sec:bound_constraints} discusses bound constraints, in which the GP prediction may be required to be positive, for example, or the prediction may be required to fall between upper and lower bounds.\nSection \\ref{sec:monotonicity_constraints} discusses monotonicity and related convexity constraints.\nConstraints may also be more \ntightly integrated with the underlying physics:  the GP can be constrained to satisfy \nlinear operator constraints which represent physical laws expressed as partial different equations (PDE). \nThis is discussed in Section~\\ref{sec:pde_constraints}.  \nSection \\ref{sec:boundary_constraints} discusses intrinsic boundary condition constraints.\nWe review several different approaches for enforcing each of these constraint types. \nFinally, Section \\ref{sec:computation_considerations} is a compendium of computational details for implementing the constraints of Sections \\ref{sec:bound_constraints} -- \\ref{sec:boundary_constraints}, together with a summary of computational strategies for improving GPR and brief commentary about the challenges of applying these strategies for the constrained GPs considered here.\nThe taxonomy we present is formulated to enable practitioners to easily query this overview for information on the specific constraint(s) they may be interested in. \nFor approaches that enforce different constraints but have significant overlap in methodology, references are made between sections to the prerequisite subsection where the technical basis of an approach is first discussed in detail. This is done, for example, when discussing spline-based approaches which are used for both bound constraints in Section \\ref{sec:bound_constraints} and monotonicity constraints in Section \\ref{sec:monotonicity_constraints}.\nNot all physical constraints can be neatly divided into the categories that we focus on in Sections \\ref{sec:bound_constraints} -- \\ref{sec:boundary_constraints}. For example, with a view toward computer vision,  considered GPR for pose estimation under rigid (constant angle and length) and non-rigid (constant length) constraints between points. They proved that linear equality constraints of the form $A\\mathbf{y} = \\mathbf{b}$, if satisfied by all the data vectors $\\mathbf{y}$, are satisfied by the posterior mean predictor of a GP. Then, at the cost of squaring the input dimension, they translated quadratic length constraints into such linear constraints for pairwise products of the input variables. \nIn another example,  applied GPR to predict the behavior of hyperelastic materials, in which the stress-stretch constitutive relation naturally exhibits rotational invariance. The rotational invariance was enforced by deriving a finite expansion of the Cauchy stress tensor in powers of the Finger tensor that satisfies the rotational invariance by virtue of its structure, and GPR was performed for the coefficients of the expansion.  \nWe mention these examples to illustrate that physical constraints are varied, and in some cases the method to enforce them can depend highly on the specific nature of the constraint.\nEven within the selected categories represented by Sections \\ref{sec:bound_constraints} -- \\ref{sec:boundary_constraints}, the literature on constrained Gaussian processes is extensive and expanding rapidly. Consequently, we cannot provide a complete survey of every instance of constrained GPR.  \nRather, we strive to discuss main areas of research within the field. The goal is to aid readers in selecting methods appropriate for their applications and enable further exploration of the literature. We present selected implementation details and numerical examples, giving references to the original works for further details. Many of the authors of these works have developed codebases and released them publicly. Finally, we remark that we have adopted consistent notation (established in Section \\ref{gpr}) for GPR that does not always follow the notation of the original works exactly.", "cites": [8602, 7624, 2924, 2925, 8601, 7626, 8603, 7625, 2926], "cite_extract_rate": 0.25, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works to present a coherent narrative about the role of constraints in GPR for scientific applications. It abstracts common themes, such as how constraints can be simple or complex, and highlights broader implications like the need for consistent notation and the integration of physical laws into the model. While it provides some critical commentary on the limitations of a complete survey, deeper evaluative analysis of the cited methods is not the primary focus."}}
{"id": "b2219c1a-2c01-43ce-b260-76e3cc26abb8", "title": "Discrete Constraints using Truncated Gaussian Distributions", "level": "subsection", "subsections": [], "parent_id": "6b851013-5efe-40f6-a2a4-48c879d41f38", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Bound Constraints"], ["subsection", "Discrete Constraints using Truncated Gaussian Distributions"]], "content": "\\label{sec:daveiga}\nBy noting that a Gaussian process \\eqref{eq:multivar_norm} is always trained and evaluated at a finite set of points $X$, global constraints over \\REVISION{continuous domain $\\Omega$ (such as an interval in one dimension)} can be approximated by constraints at a finite set of $N_c$ auxiliary or ``virtual'' points $\\mathbf{x}_i, ..., \\mathbf{x}_{N_c} \\in \\REVISION{\\Omega}$. This approach, introduced by , requires constructing an unconstrained GP and then, over the virtual points, transforming this GP to \\REVISION{the} \\emph{truncated} multivariate \\REVISION{normal density $\\mathcal{TN}(\\textbf{z}; \\bm{\\mu},\\Sigma,\\textbf{a},\\textbf{b})$ as a postprocessing step. The truncated multivariate normal is defined and discussed in detail in Section \\ref{sec:mvn}.}\nMore specifically,  construct an approximation which is conditioned on a truncated multivariate Gaussian distribution at the auxiliary points.\nWe point out how this approach \\REVISION{affects} the mean posterior predictions of the GP.\nThe unconstrained mean predictor is conditioned on the data $(X, \\mathbf{y})$: \n\\begin{equation}\\label{eq:unconstrained_mean}\n\\mathbb{E}\\left[ f(\\bold{x}^*) \\ \\big| \\ f(X) = \\bold{y} \\right].\n\\end{equation}\nThis setup is augmented by a fixed, finite set of discrete points $\\{\\mathbf{x}_i\\}_{i=1}^{N_c}$, and the predictor \\eqref{eq:unconstrained_mean} is replaced by the predictor \n\\begin{equation}\\label{eq:constrained_mean}\n\\mathbb{E}\\left[ f(\\bold{x}^*) \\ \\big| \\ f(X) = \\bold{y} \\text{ and } a \\le f(\\mathbf{x}_i) \\le b \\text{ for all $i = 1, 2, ... N_c$}\\right].\n\\end{equation}\nAs $[f(\\bold{x}_1), ..., f(\\bold{x}_{N_c})]^\\top$ is normally distributed in the unconstrained case \\eqref{eq:unconstrained_mean}, in the constrained case \\eqref{eq:constrained_mean} it is distributed according to the truncated multivariate normal. \nIn a few special cases, the mean and covariance of the truncated \\REVISION{normal} can be derived analytically.  In one dimension, the mean at a \nsingle prediction point, $z_i$, is the unconstrained mean plus a factor which incorporates \nthe change in the probability mass of the Gaussian distribution to reflect the truncation: \n\\begin{equation}\n\\mathbb{E} \\left({z_i} \\mid  a\\leq z_i\\leq b \\right) = \\mu + \\sigma\\frac{\\phi(\\alpha)-\\phi(\\beta)}{\\Phi(\\beta)-\\Phi(\\alpha)}\n\\end{equation}\nwhere $\\alpha = \\frac{a - \\mu}{\\sigma}$, $\\beta = \\frac{b-\\mu}{\\sigma}$, and $\\phi$ and $\\Phi$ are the probability density function and cumulative density function of a univariate standard normal distribution, respectively. In general, sampling and computing the moments of \\REVISION{$\\mathcal{TN}(\\textbf{z}; \\bm{\\mu},\\Sigma,\\textbf{a},\\textbf{b})$} is computationally demanding.  estimate moments empirically using an expensive rejection sampling procedure, based on a modified Gibbs sampler, to generate samples that honor the truncation bounds. We discuss the computational challenge of estimating the moments further in Section \\ref{sec:mvn}. \nIn contrast to the warping approach (Section~\\ref{sec:transform_output}) or the spline approach (Section~\\ref{sec:splines}) which maintain a global enforcement of the constraints, the bounds in \\eqref{eq:constrained_mean} can depend on the location: $a_i \\le f(\\mathbf{x}_i) \\le b_i$, representing different bounds in different regions of $I$ (see Section 4 of  for an example).\nA downside of using the approach described here is that it is unclear how many virtual points $\\bold{x}_i$ are needed to approximately constrain the GP globally with a prespecified level of confidence; some studies with increasing $N_c$ are presented by . However, if the number of points can be chosen adequately, this approach can be used to enforce not only bound constraints but also monotonicity and convexity constraints~; see Section \\ref{sec:monotonicity_constraints} for more details. These types of constraints can also include linear transformations of a Gaussian process~. \n\\REVISION{", "cites": [2927], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the approach of using truncated Gaussian distributions for discrete constraint enforcement, connecting it to the broader context of constrained GP regression. It critically discusses the computational challenges and limitations of the method, such as the difficulty in determining the number of virtual points needed for global constraints. While it provides some abstraction by highlighting how this method can be extended to other constraint types (e.g., monotonicity), the analysis remains grounded in specific technical aspects rather than offering a meta-level overview."}}
{"id": "8e0d497b-ef1e-4371-8df3-71cb0e9092b9", "title": "Constrained maximum likelihood optimization to enforce nonnegativity constraints", "level": "subsection", "subsections": [], "parent_id": "6b851013-5efe-40f6-a2a4-48c879d41f38", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Bound Constraints"], ["subsection", "Constrained maximum likelihood optimization to enforce nonnegativity constraints"]], "content": "\\label{sec:nonneg}\nAnother option for handling bound constraints is to constrain the optimization of the log-marginal-likelihood \\eqref{eq:log_like}, so that hyperparameters are chosen to enforce bounds.  \n introduced this approach to enforce nonnegativity constraints\nup to a small probability $0 < \\epsilon \\ll 1$ of violation \nat a finite set of constraint points $\\{\\mathbf{x}_i\\}_{i=1}^{N_c}$,\n\\begin{equation}\n\\label{eq:nonneg}\nP\\big( (f(\\bold{x}^*_i) \\ | \\ \\bold{y},X,\\bold{x}^*_i,\\bm{\\theta}) < 0 \\big) \\leq \\epsilon, \\quad i = 1, 2, ..., N_c. \n\\end{equation}\nFor a Gaussian likelihood, the unconstrained posterior $f^*$ follows a Gaussian distribution \\eqref{eq:gpreg}, and the probabilistic constraint \\eqref{eq:nonneg} can be written in terms of the posterior mean $\\widehat{m}(\\bold{x})$ and posterior standard deviation $s(\\bold{x}) = \\sqrt{\\hat{v}(\\bold{x})}$ given by \\eqref{eq:post_mean_var}, and probit function $\\Phi^{-1}$ (see Section \\ref{sec:warping_fn}): \n\\begin{equation}\n\\label{eq:nonneg2}\n\\widehat{m}(\\bold{x}^*_i) + \\Phi^{-1}(\\epsilon)s(\\bold{x}^*_i) \\geq 0,  \\quad i = 1, 2, ..., N_c. \n\\end{equation}\n chose $\\epsilon = 2.3\\%$ so that $\\Phi^{-1}(\\epsilon)=-2$, i.e., the mean minus two standard deviations \nis nonnegative. With the condition that $f(\\bold{x}_j)$ be within $\\nu > 0$ of the observations $y_j$, $j = 1, ..., N$, the maximization of the log-marginal-likelihood then becomes\n\\begin{align}\n\\begin{split}\n\\label{eq:nnlik}\n\\text{Seek }& \\quad \\bm{\\theta}^* = \\underset{\\bm{\\theta}}{\\text{argmax}} \\log(p(\\bold{y}|X,\\bm{\\theta}))  \\\\\n\\textrm{subject to }& \\quad  0  \\leq \\widehat{m}(\\bold{x}_i)-2s(\\bold{x}_i), \\quad  i=1, ..., N_c \\\\  \n\\textrm{and }& \\quad 0  \\leq \\nu - | y_j - f(\\bold{x}_j)|,  \\quad j=1, ..., N.  \n\\end{split}\n\\end{align}\n solve the constrained optimization problem \\eqref{eq:nnlik} with $\\nu = 0.03$ using a nonlinear interior point solver, demonstrating that nonnegativity is enforced with high probability and also that posterior variance is significantly reduced.  \nWhile this tends to be more expensive than a usual unconstrained optimization of the marginal log-marginal-likelihood, the effect on the posterior \\eqref{eq:gpreg} is to change the hyperparameters, while preserving the Gaussian form, so that more expensive inference methods such as MCMC are not required. In principle, two-sided bounds and other types of constraints can be treated in this fashion, although  consider nonnegative constraints in their numerical examples. \n}", "cites": [2928], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key aspects of the cited paper by explaining how it enforces nonnegativity through constrained maximum likelihood optimization. It abstracts the method to mention the potential for two-sided bounds and other constraints. However, the critical analysis is limited, as it does not evaluate limitations or contrast this approach with alternatives in depth."}}
{"id": "fb32446a-d668-496a-9bc8-a088f5766b41", "title": "Splines", "level": "subsection", "subsections": ["59799be4-bba8-40c8-abc3-bc8dcbb00a5b", "c1c4f0fe-ff10-4a1c-8ef9-52bff2ae54d3", "81f6205f-e4dc-4e6a-bfba-712f55411f53"], "parent_id": "6b851013-5efe-40f6-a2a4-48c879d41f38", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Bound Constraints"], ["subsection", "Splines"]], "content": "\\label{sec:splines}\n present a constrained Gaussian process formulation involving splines, where they place a multivariate Gaussian prior on a class of spline functions. The constraints are incorporated through constraints on the coefficients of the spline functions. \nTo avoid the difficulty of enforcing a bound constraint $a \\le f(\\bold{x}) \\le b$ globally on \\REVISION{a continuous domain $\\Omega$} for all predictions, the approach\\REVISION{es} in Section\\REVISION{s} \\ref{sec:daveiga} \\REVISION{and \\ref{sec:nonneg}} enforced constraints only at a finite set \\REVISION{of} points. \nIn contrast, the approach taken by  is to instead consider a spline interpolant whose finite set of knot values are governed by a GP. This reduces the infinite-dimensional GP to a finite-dimensional one, for which the distributions of the knot values (i.e., the coefficients of the spline expansion) must be inferred. By using a set of piecewise linear splines that form a partition of unity, this approach guarantees that the set of all values between neighboring knots are bounded between the values of the knots. Thus if the knot values satisfy prescribed bound or monotonicity constraints, then so must all values in between them; that is, the global constraints are satisfied if the finite-dimensional constraints are. The problem then reduces to sampling the knot values from a truncated multivariate normal.", "cites": [8604], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section integrates the concept of using splines to enforce bound constraints within Gaussian process regression and contrasts it with previous point-wise constraint methods. It abstracts the approach by explaining how the use of a partition of unity ensures global constraint satisfaction through finite-dimensional inference. While it provides a clear explanation of the method, it offers limited direct comparison or critique of the cited paper's specific limitations."}}
{"id": "59799be4-bba8-40c8-abc3-bc8dcbb00a5b", "title": "GPR for spline coefficients", "level": "subsubsection", "subsections": [], "parent_id": "fb32446a-d668-496a-9bc8-a088f5766b41", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Bound Constraints"], ["subsection", "Splines"], ["subsubsection", "GPR for spline coefficients"]], "content": "We first discuss the spline formulation in one input dimension, and without loss of generality assume that the process being modeled is restricted to the domain [0,1]. Let $h(x)$ be the standard tent function, i.e., the piecewise linear spline function defined by\n\\begin{equation}\nh(x) = \\text{max}(1-|x|,0)\n\\end{equation}\nand define the locations of the knots to be $x_i = i/M$ for $i=0,1,...M$, with $M+1$ total spline functions. Then for any set of spline basis coefficients $\\xi_i$, the function representation is given by\n\\begin{equation}\\label{eq:spline_expansion}\nf(x) = \\sum_{i=0}^M \\xi_i h(M(x-x_i)) = \\sum_{i=0}^M \\xi_i h_i(x).\n\\end{equation}\nThis function representation gives a $C^0$ piecewise linear interpolant of the point values $(x_i, \\xi_i)$ for all $i=0,1,...,M$.\nThe crux of the spline approach to GPR lies in the following argument. Suppose we are given a set of $N$ data points at unique locations $(x_j,y_j)$. Define the matrix $A$ such that\n\\begin{equation}\nA_{ij} = h_i(x_j).\n\\end{equation}\nThen any set of spline coefficients $\\bm{\\xi}$ that satisfy the equation \n\\begin{equation}\\label{eq:spline_system}\nA\\bm{\\xi} = \\bold{y}\n\\end{equation}\nwill interpolate the data exactly. Clearly solutions to this system of equations will exist only if the rank of $A$ is greater than $N$, which requires that any given spline basis spans no more than two data points. Intuitively, this is because a linear function is only guaranteed to interpolate two points locally. Supposing that we make $M$ large enough to satisfy this condition, we can find multiple solutions to the system \\eqref{eq:spline_system}.\nWe now assume the knot values $\\bm{\\xi}$ to be governed by a Gaussian process with covariance function $K$. Because a linear function of a GP is also a GP, the values of $\\bm{\\xi}$ and $\\mathbf{y}$ are governed jointly  by a GP prior in the form \n\\begin{equation}\n\\begin{bmatrix}\n\\mathbf{y} \\\\\n\\bm{\\xi}\n\\end{bmatrix}\n\\sim \\mathcal{N}\\left(\n\\begin{bmatrix}\n\\mathbf{0}\\\\\n\\mathbf{0}\n\\end{bmatrix},\n\\begin{bmatrix}\nAKA^\\top & KA^\\top\\\\\nAK & K\\\\\n\\end{bmatrix}\n\\right)\n\\end{equation}\nwhere each entry of the covariance matrix is understood to be a matrix. Upon observation of the data $\\mathbf{y}$, the conditional distribution of the knot values subject to $\\mathbf{y}=A\\bm{\\xi}$ is given by\n\\begin{equation}\np(\\bm{\\xi} \\ \\big| \\ \\mathbf{y}=A\\bm{\\xi}) = \\mathcal{N}\\Big(\\bm{\\xi}; KA^\\top (AKA^\\top)^{-1} \\mathbf{y}, K - KA^\\top(AKA^\\top)^{-1}AK\\Big)\n\\end{equation}\nThis formula is similar to that proposed by , in which a GP is interpolated to a regular grid design to take advantage of fast linear algebra. In this case, we are now interested in evaluating the distribution further conditioned on the inequality constraints given by\n\\REVISION{\\begin{equation}\\label{eq:spline_TN_C}\np(\\bm{\\xi} \\ \\big| \\ \\mathbf{y}=A\\bm{\\xi}, \\mathbf{a} \\leq \\bm{\\xi} \\leq \\mathbf{b}) = \\mathcal{TN}\\Big(\\bm{\\xi}; KA^\\top (AKA^\\top)^{-1} \\mathbf{y}, K - KA^\\top(AKA^\\top)^{-1}AK ,\\mathbf{a}, \\mathbf{b} \\Big)\n\\end{equation}\nwhere \nthe truncated normal \\REVISION{density} $\\mathcal{TN}(\\bm{\\mu}, \\Sigma, \\mathbf{a}, \\mathbf{b})$ is defined and discussed in Section \\ref{sec:mvn}.} We illustrate bound constrained GPs using this approach in Figure \\ref{fig:fn12}. We discuss monotonicity constraints using this approach in Section \\ref{sec:splines_monotonic} and constrained MLE estimation of the hyperparameters in Section \\ref{sec:mle}. Several constraint types can be combined in this approach, in which case $\\mathcal{C}$ in \\eqref{eq:spline_TN_C} is a convex set defined by a set of linear inequalities in $\\bm{\\xi}$ . \n\\begin{figure}[htpb!]\n\\includegraphics[width=0.46\\linewidth]{figs/Figure_1_cropped.pdf}\n\\includegraphics[width=0.45\\linewidth]{figs/Figure_2_cropped.pdf}\n\\caption{\n\\emph{Left}: Comparison of a bound constrained GP with lower bound $a = -20$ and upper bound $b = 20$ versus an unconstrained GP. \n\\emph{Right}: Comparison of a positivity constrained GP (lower bound $a = 0$) versus an unconstrained GP.\nThe data and hyperparameters are from .\nThe dotted lines are $\\mu \\pm 1\\sigma$ prediction intervals.\n}\\label{fig:fn12}\n\\end{figure}", "cites": [7627, 8604], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section analytically explains the GPR for spline coefficients approach, integrating concepts from the cited papers by connecting the idea of using a GP on a regular grid (Paper 1) with the incorporation of inequality constraints (Paper 2). However, synthesis remains limited to a single methodological thread, and the critical evaluation of the cited works is minimal. Some abstraction is achieved by framing the approach in terms of general constraints, but it does not rise to the level of identifying overarching principles."}}
{"id": "c1c4f0fe-ff10-4a1c-8ef9-52bff2ae54d3", "title": "Sampling", "level": "subsubsection", "subsections": [], "parent_id": "fb32446a-d668-496a-9bc8-a088f5766b41", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Bound Constraints"], ["subsection", "Splines"], ["subsubsection", "Sampling"]], "content": "\\label{sec:spline_sampling}\nJust as for the discrete constraint method discussed in Section \\ref{sec:daveiga}, sampling from the truncated normal distribution for the spline coefficients $\\bm{\\xi}$ introduces a new computational challenge into the GPR framework. While we discuss this in more detail and for several dimensions in Section \\ref{sec:mvn}, we give a cursory discussion of this following . We consider one dimension and the one-sided constraint $f(x) \\ge b$ on $[0,1]$.\nThe original method of  was to use a rejection sampling approach by sampling from the untruncated \ndistribution with a mean shifted to the mode (or maximum \\emph{a posteriori} point) of the true posterior. That is, one first solves the problem\n\\begin{equation}\\label{eq:posterior_mode}\n\\bm{\\xi}^* = \\underset{\\bm{\\xi}}{\\text{argmin}}(\\bm{\\xi}-\\bm{\\mu})^\\top\\Sigma^{-1}(\\bm{\\xi}-\\bm{\\mu})\n\\end{equation}\nsubject to the bound constraints $\\bm{\\xi} \\geq \\mathbf{b}$,\nwhere $\\bm{\\mu} = KA^\\top (AKA^\\top)^{-1} \\mathbf{y}$ and $\\Sigma = K-KA^\\top(AKA^\\top)^{-1}AK$. This is a convex quadratic program (assuming the covariance matrix is not too ill-conditioned) and may be solved efficiently.\nOne then draws samples from $\\mathcal{N}(\\bm{\\xi}^*,\\Sigma)$ and accepts or rejects the samples based on an inequality condition, described in more detail in .  \nThis is a simple approach, but it does not perform well at larger scale. The probability of rejecting any sample increases exponentially with the number of splines $M$. Furthermore, imprecision in the mode evaluation from the optimization process can lead to a deterioration of acceptance (for example, if the computed mode only satisfies monotonicity constraints to within some solver tolerance). \nOther approaches to sampling from the multivariate normal rely on Markov chain Monte Carlo methods, and are discussed in Section \\ref{sec:mvn}.", "cites": [8604], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical discussion of the sampling method for constrained Gaussian process regression with splines, focusing on the limitations of rejection sampling and its scalability issues. It synthesizes the method described in the cited paper and connects it to broader considerations in constraint handling. While it evaluates the method critically and highlights its shortcomings, the abstraction remains somewhat limited to the specific context of bound constraints and one-dimensional cases."}}
{"id": "47d03984-771a-4f08-895f-be7081965518", "title": "Monotonicity Constraints", "level": "section", "subsections": ["0cb9a716-84ec-439d-8c95-906a1685fe29", "d1712d3f-2a33-40bd-9970-111e15bce37d", "a89a65e0-a175-468f-b05f-9de36c2b9479", "919a9a81-117f-4d5c-a5e0-4924bb0b524a"], "parent_id": "86a0553c-8edc-49a7-8382-330c5ec7b9b7", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Monotonicity Constraints"]], "content": "\\label{sec:monotonicity_constraints}\nMonotonicity constraints are an important class of ``shape constraints'' which are frequently required in a variety of applications. For example,  applied monotonicity-constrained GPR for the output of the Los Alamos National Laboratory ``Lady Godiva'' nuclear reactor, which is known to be monotonic with respect to the density and radius of the spherical uranium core.  considered monotonic Bayesian modeling of medical dose-response curves, as did  for predicting sales from various prices of consumer goods. \nRoughly speaking, given a method to enforce bound constraints, monotonicity constraints can be enforced by utilizing this method to enforce \n$\\bold{f}' \\ge \\bold{0}$ on the derivative of the Gaussian process in a ``co-kriging'' setup for the joint GP \n$[\\bold{f};\\bold{f}']$. Indeed, many of the works reviewed in Section \\ref{sec:bound_constraints} considered both bound, monotonicity, and convexity constraints under the general heading of ``linear inequality constraints'' . As a result, some of the methods below are based on techniques reviewed in Section \\ref{sec:bound_constraints}, and we frequently refer to that section.", "cites": [8604], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces monotonicity constraints and provides a few application examples, but primarily serves as a descriptive overview. It mentions a connection to bound constraints and points to techniques from another section, but lacks in-depth synthesis, critical evaluation, or abstraction of broader principles from the cited works."}}
{"id": "a89a65e0-a175-468f-b05f-9de36c2b9479", "title": "Monotonic splines", "level": "subsection", "subsections": [], "parent_id": "47d03984-771a-4f08-895f-be7081965518", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Monotonicity Constraints"], ["subsection", "Monotonic splines"]], "content": "\\label{sec:splines_monotonic}\nThe spline formulation, presented in Section \\ref{sec:splines} to globally enforce a bound constraint of the form $f \\ge a$  may be extended easily to enforce monotonicity constraints or other linear inequalities. For example, if $C$ is a first-order (backward or forward) finite difference matrix relating neighboring spline values, then monotonicity is enforced globally by sampling values of the knots $\\bm{\\xi}$ subject to the constraint\n\\begin{equation}\nC\\bm{\\xi} \\geq \\mathbf{0};\n\\end{equation}\nsee  or .\nThis inequality is also used in the rejection sampler of Section \\ref{sec:spline_sampling} as a constraint to identify the MAP estimate to increase the sampling efficiency.\nBound and monotonicity constraints can be enforced simultaneously by requiring both $\\bm{\\xi} \\geq \\mathbf{b}$ and $C\\bm{\\xi} \\geq \\mathbf{0}$ in the sampling, though the acceptance ratio drops substantially with combined constraints.", "cites": [8604], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how monotonic splines can be used to enforce monotonicity constraints in Gaussian process regression. It references one paper but does so in a limited manner, primarily to illustrate the use of constraints. There is minimal synthesis, abstraction, or critical evaluation beyond stating the method and its effect on sampling efficiency."}}
{"id": "919a9a81-117f-4d5c-a5e0-4924bb0b524a", "title": "Convexity", "level": "subsection", "subsections": [], "parent_id": "47d03984-771a-4f08-895f-be7081965518", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Monotonicity Constraints"], ["subsection", "Convexity"]], "content": "\\label{sec:convexity}\nThe sections above illustrated how a method for bound constraints can be used with first derivatives of a Gaussian process $f$ to enforce \n\\REVISION{$\\partial f/ \\partial{x_{i}} \\ge 0$} and thereby monotonicity for the GP $f$, either globally as in Section \\ref{sec:splines_monotonic} or at a finite set of virtual points as in Sections \\ref{constrained_likelihood_with_derivative_information} and \\ref{daveiga_monotonic}. Similar nonnegativity constraints can be applied to higher derivatives of $f$ as well. In one dimension, this can be used to enforce convexity via the constraint\n\\begin{equation}\\label{eq:one_dimension_convexity}\n\\frac{\\partial^2 f}{\\partial x^2} \\ge 0,\n\\end{equation}\ntreating the left-hand side as a GP with covariance kernel \n\\begin{equation}\n\\frac{\\partial^4 k}{\\partial x^2 \\partial {x'}^2}(x,x').\n\\end{equation}\nAlthough monotonicity can be enforced in arbitrary dimensions, convexity presents a challenge in dimensions greater than one, since it cannot be expressed as a simple linear inequality involving the derivatives of $f$ as in \\eqref{eq:one_dimension_convexity}.\nAs  point out, enforcing convexity in higher dimensions requires that \\eqref{eq:one_dimension_convexity} be replaced by the condition that the Hessian of $f$ be positive semidefinite. Sylvester's criterion yields the equivalent condition that each leading principal minor determinant of the Hessian be positive. Such inequality constraints involve \\emph{polynomials} in partial derivatives of $f$. As polynomial functions of GPs are no longer GPs, the bound constraint methods in Section \\ref{sec:bound_constraints} no longer apply. \nWhile higher dimensional convexity constraints are outside the scope of this survey, several references we have mentioned discuss the implementation of convexity constrained Gaussian processes in greater detail. \n discuss how convexity in one dimension of the form \\eqref{eq:one_dimension_convexity} can be enforced at virtual points\nusing the (partially) truncated multinormal, in a way analogous to Section \\ref{daveiga_monotonic},\nwhile convexity in two dimensions can be enforced using the elliptically truncated multinormal distribution.\n and  point out that for the spline basis considered in Section \\ref{sec:splines}, convexity in one dimension amounts to requiring that the successive differences of the values at the spline knots are increasing in magnitude, i.e. \n\\begin{equation}\n\\xi_{k+1}-\\xi_{k} \\geq \\xi_{k}-\\xi_{k-1} \\text{ for all } k.\n\\end{equation}\nThis is equivalent to requiring that the second-order finite differences be positive. This can also easily be applied in higher dimensions to guarantee that the second partial derivatives are positive globally, although this does not imply convexity.", "cites": [8604], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the approach to convexity from the context of monotonicity and connects it to the cited work on inequality constraints. It abstracts the key principle that convexity in higher dimensions requires positive semidefiniteness of the Hessian, which leads to a discussion of the limitations of existing methods. However, it does not deeply compare or critique the different approaches, nor does it fully integrate multiple papers into a novel framework, limiting its critical depth."}}
{"id": "1196fbad-4be9-4538-a3fa-a23a66d55c86", "title": "Block Covariance Kernel", "level": "subsection", "subsections": [], "parent_id": "fbf12205-4c0f-46ca-a510-8409ad72bca8", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Differential Equation Constraints"], ["subsection", "Block Covariance Kernel"]], "content": "\\label{pde_constraints}\n\\REVISION{The work of  introduced a joint GPR approach using a four-block covariance kernel allowing observations of both the solution $u$ and the forcing $f$ to be utilized. The principle behind this approach} is that if $u(\\mathbf{x})$ is a GP with mean function $m(\\mathbf{x})$ and covariance kernel\n$k(\\mathbf{x},\\mathbf{x'})$, \n\\begin{equation}\n\\label{eq:gp_for_u}\nu \\sim \\mathcal{GP}(m(\\mathbf{x}),k(\\mathbf{x},\\mathbf{x'}))\n\\end{equation}\nand if $m(\\cdot)$ and $k(\\cdot, \\mathbf{x'})$ belong to the domain of $\\mathcal{L}$, then\n$\\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}} k(\\mathbf{x},\\mathbf{x'})$ defines a valid covariance kernel for a \nGP with mean function $\\mathcal{L}_{\\mathbf{x}} m(\\mathbf{x})$. \nThis Gaussian process is denoted $\\mathcal{L}u$:\n\\begin{equation}\n\\label{eq:gp_for_Lu}\n\\mathcal{L}u \\sim \\mathcal{GP}(\\mathcal{L}_{\\mathbf{x}} m(\\mathbf{x}), \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}} k(\\mathbf{x},\\mathbf{x'})).\n\\end{equation}\nNote from \\eqref{eq:lin_diff_op} that the operator $\\mathcal{L}$ takes as input a function of a single variable $\\mathbf{x}$. When applying \n$\\mathcal{L}$ to a function of two variables such as $k(\\mathbf{x}, \\mathbf{x'})$, we use a subscript as in \\eqref{eq:gp_for_Lu} to denote the application of $\\mathcal{L}$ in the indicated variable, i.e., considering the input to $\\mathcal{L}$ as a function of the indicated variable only. Note that a special case of this, for $\\mathcal{L} = \\partial/\\partial x_{d_i}$, appeared in Section \\ref{constrained_likelihood_with_derivative_information}. The same formula \\eqref{eq:gp_for_Lu} was utilized in earlier works on GPR with differential equation constraints by  and .  showed a similar result in addressing the problem of identification of basis elements in the finite-dimensional approximation of PDE solutions.  The latter work presents a Bayesian formulation of a numerical homogenization approach in which the optimal bases are shown to be polyharmonic splines and the optimal solution in the case of a white noise source term is a Gaussian field.\nThe notation ``$\\mathcal{L}u$'' for the GP \\eqref{eq:gp_for_Lu} is suggested by noting that if one could apply $\\mathcal{L}$ to the samples of the GP $u$, then the mean\nof the resulting stochastic process $\\mathcal{L}[u]$ would indeed be given by \n\\begin{align}\n\\text{mean}\\left(\\mathcal{L}[u](\\mathbf{x})\\right) &=\n\\mathbb{E} \\left[ \\mathcal{L} [u](\\mathbf{x}) \\right] = \\mathcal{L} \\mathbb{E}\\left[ u(\\mathbf{x}) \\right] = \\mathcal{L} m(\\mathbf{x})\n\\end{align}\nand the covariance by\n\\begin{align}\n\\begin{split}\n\\label{eq:cov_f_f}\n\\text{cov}\\left( \\mathcal{L}[u](\\mathbf{x}) , \\mathcal{L}[u](\\mathbf{x'}) \\right) &= \n\\mathbb{E} \\left[ \\mathcal{L}_{\\mathbf{x}} [u(\\mathbf{x})] \\mathcal{L}_{\\mathbf{x'}} [u (\\mathbf{x'})] \\right] =\n\\mathbb{E} \\left[ \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}} \\left[ u(\\mathbf{x}) u(\\mathbf{x'}) \\right] \\right] \\\\\n\\qquad &=  \\mathcal{L}_{\\mathbf{x}}  \\mathbb{E} \\left[ \\mathcal{L}_{\\mathbf{x'}} \\left[ u(\\mathbf{x}) u(\\mathbf{x'}) \\right] \\right] \n= \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}} \\mathbb{E} \\left[ u(\\mathbf{x}) u(\\mathbf{x'}) \\right] \\\\\n&= \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}  \\left[ \\text{cov}\\left( u(\\mathbf{x}), u(\\mathbf{x'}) \\right) \\right] \n= \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}  k(\\mathbf{x}, \\mathbf{x'}).\n\\end{split}\n\\end{align}\nThis justification is formal, as in general the samples of the process $\\mathcal{L}u$ defined by \\eqref{eq:gp_for_Lu} cannot be identified as $\\mathcal{L}$ applied to the samples of $u$ ; a rigorous interpretation involves the posterior predictions and reproducing kernel Hilbert spaces of the processes $u$ and $\\mathcal{L}u$ . \nIf scattered measurements\n$\\mathbf{y}_f$ \non the source term $f$ in \\eqref{eq:pde_constraint} are available at domain points $X_f$,\nthen this can be used to train and obtain predictions for $\\mathcal{L}u$ from the GP \\eqref{eq:gp_for_Lu}  in the standard way. If, in addition, measurements $\\mathbf{y}_u$\nof $u$ are available at domain points $X_u$\na GP co-kriging procedure can be used. In this setting physics knowledge of the form \\eqref{eq:pde_constraint} enters via the data $(X_f, \\mathbf{y}_f)$ and can be used to improve prediction accuracy and reduce variance of the GPR of $u$. \nThe co-kriging procedure requires forming the joint Gaussian process $[u;f]$.\nSimilarly to the derivative case considered in Section \\ref{constrained_likelihood_with_derivative_information}, the covariance matrix of the resulting GP is a four block matrix assembled from\nthe covariance matrix of the GP \\eqref{eq:gp_for_u} for the solution $u$, the covariance of the GP \\eqref{eq:gp_for_Lu} for the forcing function, and the cross terms.\nGiven the covariance kernel $k(\\mathbf{x}, \\mathbf{x}')$ for $u$, the covariance kernel of this joint GP \nis\n\\begin{equation}\\label{eq:joint_covariance}\nk\n\\left(\n\\begin{bmatrix} \n\\mathbf{x}_1 \\\\ \n\\mathbf{x}_2 \n\\end{bmatrix}\n,\n\\begin{bmatrix} \n\\mathbf{x}'_1 \\\\ \n\\mathbf{x}'_2 \n\\end{bmatrix}\n\\right)\n=\n\\begin{bmatrix}\n\\phantom{\\mathcal{L}_{\\mathbf{x}}} k(\\mathbf{x}_1,\\mathbf{x}'_1) & \n\\phantom{\\mathcal{L}_{\\mathbf{x}}}\\mathcal{L}_{\\mathbf{x}'}  k(\\mathbf{x}_1,\\mathbf{x}'_2)\\\\ \n\\mathcal{L}_{\\mathbf{x}}   k (\\mathbf{x}_2,\\mathbf{x}'_1) \n& \\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}} k(\\mathbf{x}_2,\\mathbf{x}'_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nK_{11} & \nK_{12} \\\\ \nK_{21} &\nK_{22} \n\\end{bmatrix}.\n\\end{equation}\nThe covariance between $u(\\mathbf{x})$ and $f(\\mathbf{x'})$ is given by $\\mathcal{L}_{\\mathbf{x}'}  k(\\mathbf{x}_1,\\mathbf{x}'_2)$ in the upper right block of the kernel and can be justified by a calculation similar to \\eqref{eq:cov_f_f}; see . Similarly the covariance between $u(\\mathbf{x}')$ and $f(\\mathbf{x})$ is represented by the bottom left block $\\mathcal{L}_{\\mathbf{x}} k(\\mathbf{x}_2,\\mathbf{x}'_1)$ of the kernel. \nIn this notation, the joint Gaussian process for $[u; f]$\nis then\n\\begin{equation}\\label{eq:joint_GP_uf}\n\\begin{bmatrix}\nu({X}_1) \\\\ \nf({X}_2)\n\\end{bmatrix}\n\\sim \\mathcal{GP}\\left( \n\\begin{bmatrix}\n\\phantom{\\mathcal{L}}\nm({X}_1) \\\\\n\\mathcal{L}\nm({X}_2)\n\\end{bmatrix},\n\\begin{bmatrix}\nK_{11}(X_1,X_1) & \nK_{12}(X_1,X_2) \\\\ \nK_{21}(X_2,X_1) &\nK_{22}(X_2,X_2)\n\\end{bmatrix}\n\\right),\n\\end{equation}\nwhere $K_{12}(X_1,X_2) = \\left[K_{21}(X_2,X_1)\\right]^\\top$.\nGiven data $({X}_u,\\mathbf{y}_u)$ and $({X}_f,\\mathbf{y}_f)$, the GP kernel hyperparameters \nmay be trained by assembling the four-block covariance matrix in \\eqref{eq:joint_GP_uf} with \n${X}_1 = {X}_u$, ${X}_2 = {X}_f$,\n\\begin{equation}\\label{eq:covariance_over_data}\nK_{\\text{data}}=\n\\begin{bmatrix}\nK_{11}({X}_u,{X}_u) & \nK_{12}({X}_u,{X}_f) \\\\ \nK_{21}({X}_f,{X}_u) &\nK_{22}({X}_f,{X}_f)\n\\end{bmatrix}\n\\end{equation}\nand minimizing the negative log-marginal-likelihood\n\\begin{multline}\n\\label{NLML}\n-\\log{p(\\bold{y}_u,\\bold{y}_f|X_u,X_f,\\bm{\\theta)}} = \n\\frac{1}{2} \\left(\n\\mathbf{y}-\\mathbf{m}\n\\right)^{\\top}\nK_{\\text{data}}^{-1}\n\\left(\n\\mathbf{y}-\\mathbf{m}\n\\right)\n + \\frac{1}{2}\\log |{K}_{\\text{data}}| + \\frac{N}{2} \\log (2\\pi), \\\\\n\\text{with } \n\\mathbf{y} \n= \n\\left[\n\\begin{array}{c}\n\\mathbf{y}_u \\\\ \n\\mathbf{y}_f\n\\end{array} \n\\right]\n\\text{ and }\n\\mathbf{m} \n=\n\\left[\n\\begin{array}{c}\n\\phantom{\\mathcal{L}} m({X}_u) \\\\ \n\\mathcal{L} m({X}_f)\n\\end{array} \n\\right].\n\\end{multline} \nIn the presence of noise on measurements of $u$ and $f$, a standard approach analogous to the Gaussian likelihood \\eqref{eq:gauss_like} is to introduce two noise hyperparameters $\\sigma_{u}$ and $\\sigma_{f}$ and replace the four-block covariance\nmatrix \\eqref{eq:covariance_over_data} by \n\\begin{equation}\n\\begin{bmatrix*}[l]\nK_{11}({X}_u,{X}_u)          + \\sigma_{u}^2 {I}_{N_u}& \nK_{12}({X}_u,{X}_f)\\\\ \nK_{21}({X}_f,{X}_u)&\nK_{22}({X}_f,{X}_f)          + \\sigma_{f}^2 {I}_{N_f}\n\\end{bmatrix*}\n\\end{equation}\nThe inclusion of the additional terms depending on $\\sigma_{u}^2$ and $\\sigma_{f}^2$ correspond to an assumption of uncorrelated \nwhite noise on the measurements ${Y}_u$ and ${Y}_f$, i.e., \n\\begin{equation}\n{Y}_u = u({X}_u) + \\bm{\\epsilon}_u, \\quad\n{Y}_f = f({X}_f) + \\bm{\\epsilon}_f,\n\\end{equation}\nwith $\\bm{\\epsilon}_u \\sim \\mathcal{N}(\\mathbf{0},\\sigma_{u}^2 {I}_{N_u})$ and independently $\\bm{\\epsilon}_f \\sim \\mathcal{N}(\\mathbf{0},\\sigma_{f}^2 {I}_{N_u})$, given $N_u$ data points for $u$ and $N_f$ data points for $f$. \nThe implementation of the constrained Gaussian process kernel \\eqref{eq:joint_covariance} for constraints of the form\n\\eqref{eq:pde_constraint} raises several computational problems.\nThe first is the computation of $\\mathcal{L}_{\\mathbf{x}}k$ and $\\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}k$. The most ideal scenario is that in which $k$ has an analytical formula and $\\mathcal{L}$ is a linear differential operator so that these expressions be computed in closed form by hand or with a symbolic computational software such as Mathematica. This was the approach used for the examples in ,  and , including for the heat equation, Burgers' equation, Korteweg-de Vries Equation, and Navier-Stokes equations. The nonlinear PDEs listed here were treated using an appropriate linearization. An example of $k$ being parametrized by a neural network (which allows derivatives to be computed using backpropagation) was also considered in  for the Burgers' equation. \nClosed form expressions for the covariance kernel \\eqref{eq:joint_covariance} greatly simplify the implementation compared to numerical approximation of $\\mathcal{L}_{\\mathbf{x}}k$ and $\\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}k$ using finite-differences or series expansions. As the size of the dataset and therefore size of the covariance matrix \\eqref{eq:joint_covariance} increases, our numerical experiments suggest that any numerical errors in the approximation of the action of $\\mathcal{L}$ rapidly lead to ill-conditioning of the covariance matrix. This in turn can lead to artifacts in the predictions or failure of maximum likelihood estimation with the constrained GP. Ill-conditioning can be reduced by adding an ad-hoc regularization on the diagonal of \\eqref{eq:joint_covariance} at the cost of reducing the accuracy of the regression, potentially negating the benefit of the added constraint.\nFor more general constraints of the form \\eqref{eq:pde_constraint}, depending on the form of $k$ or $\\mathcal{L}$, numerical methods may be unavoidable. For example, in  and , fractional-order PDE constraints (amounting to $\\mathcal{L}$ being a nonlocal integral operator with singular kernel) were considered. For these constraints, the kernel blocks $\\mathcal{L}_{\\mathbf{x}}k$ and $\\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}k$ had no closed formula. To approximate these terms, a series expansion was used in , and in  a numerical method was developed involving Fourier space representations of $\\mathcal{L}_{\\mathbf{x}}k$ and $\\mathcal{L}_{\\mathbf{x}} \\mathcal{L}_{\\mathbf{x'}}k$ with Gaussian quadrature for Fourier transform inversion. \nA second problem is that the formulation \\eqref{eq:joint_GP_uf} requires enforcing the constraint \\eqref{eq:pde_constraint} at discrete points of ${X}_f$. Therefore, even if we have complete knowledge of the constraining equation \\eqref{eq:pde_constraint} and the forcing term $f$, enhancing the GPR for $u$ by including a high number of virtual data points makes inference as well as maximum likelihood estimation computationally expensive and prone to ill-conditioning. In this regard, the computational approaches discussed in Section \\ref{sec:low_rank}, particularly the subset of data approaches in Section \\ref{sec:subset_of_data}, may be helpful.\nFigure \\ref{fig:pde_example_1} shows an example of a one-dimensional GP with squared-exponential kernel constrained to satisfy the differential equation $1 = d^2 u / dx^2$ on the interval $[0,1]$. Data is generated from sampling the solution $u = \\frac{1}{8} [(2x-1)^2-1]$ at 10 points between 0.2 and 0.8. Both the constrained and unconstrained GPs give a reasonably accurate reconstruction on $[0.2, 0.8]$, but the unconstrained GP has poor accuracy outside this subinterval. On the other hand, the constrained GP is augmented by data $f = d^2 u / dx^2 = 1$ at 10 additional points between $0$ and $1$, leading to an improved reconstruction of $u$ outside $[0.2,0.8]$. \n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth]{figs/pde_constrained_example_cropped.pdf}\n  \\caption{Comparison of unconstrained and PDE constrained GP. \\emph{Left:} Reconstruction of $u$ (red line) with an unconstrained GP (black line) using 10 data points (red dots) in $[0.2, 0.8]$. \\emph{Center:} Reconstruction of $u$ (red line) with a PDE constrained GP (black line) using the same 10 data points (red dots) in $[0.2, 0.8]$. \\emph{Right:} Right-hand side $f$ of the PDE, with 10 additional data points in $[0,1]$ used for the PDE constraint. Note the improved accuracy of the constrained GP outside $[0.2, 0.8]$ due to this constraint data.}\n  \\label{fig:pde_example_1}\n\\end{figure}", "cites": [2929, 7626, 2930], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key mathematical principles from cited papers to explain the block covariance kernel approach for incorporating differential equation constraints in GPR. It provides equations and theoretical justifications for the joint GP formulation, demonstrating integration across works. However, it lacks deeper critical evaluation of these methods' limitations or trade-offs, and while it identifies generalizable kernel-based patterns, it stops short of offering meta-level insights."}}
{"id": "84bac780-15ce-4747-95da-3c67a3d3e503", "title": "Specialized Kernel Construction", "level": "subsection", "subsections": [], "parent_id": "fbf12205-4c0f-46ca-a510-8409ad72bca8", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Differential Equation Constraints"], ["subsection", "Specialized Kernel Construction"]], "content": "\\label{sec:expansion}\n developed an approach that is suited for GPR with linear PDE constraints of the form \\eqref{eq:pde_constraint} for the case of vanishing or localized source terms $f$. In the latter case, the solution $u$ is represented as a solution to the homogeneous equation $\\mathcal{L}u = f$ and an inhomogeneous contribution obtained as a linear model over fundamental solutions corresponding to the point sources.\nFocusing on the GPR for the solution $u$ to the homogeneous equation $\\mathcal{L} u = 0$, a specalized kernel function $k$ is derived from $\\mathcal{L}$ such that the GPR prediction satisfies the equation exactly. In this sense, the approach is similar to that of Section \\ref{transformed_covariance}, although the kernel is not obtained from a transformation of a prior kernel but rather is constructed from solutions to the problem $\\mathcal{L}u = 0$.  show that such a covariance kernel must satisfy\n\\begin{equation}\\label{eq:albert_condition}\n\\mathcal{L}_{\\mathbf{x}} k(\\mathbf{x}, \\mathbf{x}')\\mathcal{L}^\\top_{\\mathbf{x}'} = 0\n\\end{equation}\nin the notation of Section \\ref{transformed_covariance}, and seek kernels $k$ in the form of a Mercer series\n\\begin{equation}\\label{eq:albert_kernel}\nk(\\mathbf{x}, \\mathbf{x}') = \\sum_{i,j} \\phi_i(\\mathbf{x}) \\Sigma^{i,j}_p \\phi_j(\\mathbf{x}')\n\\end{equation}\nfor basis functions $\\phi_i$ and matrix $\\Sigma_p$. They point out that convolution kernels can also be considered.  study the Laplace, heat, and Hemholtz equations, performing MLE and inferring the solution $u$ from the PDE constraint and scattered observations. They construct kernels of the form \\eqref{eq:albert_kernel} which satisfy \\eqref{eq:albert_condition} by selecting $\\{\\phi_i\\}$ to be an orthogonal basis of solution to the corresponding equations. Although the resulting kernels are not stationary and require analytical construction, they result in improved reconstructions of solutions from the observations compared to squared-exponential kernels. We note that similar constructions of kernels -- as expansions in a suitable basis -- are utilized in the approach of  in the following section to enforce boundary conditions. \n}", "cites": [7628], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of specialized kernel construction for GPR with PDE constraints, integrating key concepts from the cited paper and linking them to related methods like boundary condition enforcement. It synthesizes the approach with the broader context of kernel design but does not deeply compare or critique these methods beyond noting their non-stationarity and analytical requirements."}}
{"id": "a2e9aeb9-9082-41e6-90a9-b86682ec0062", "title": "Spectral Expansion Approach", "level": "subsection", "subsections": [], "parent_id": "65d986b6-b11e-421e-971a-bb294bcbf1db", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Boundary Condition Constraints"], ["subsection", "Spectral Expansion Approach"]], "content": "\\label{spectral_expansion_approach}\nThe work of  introduced a method based on the spectral expansion of a desired stationary isotropic covariance kernel \n\\begin{equation}\\label{e:stationary_isotropic_kernel}\nk(\\mathbf{x},\\mathbf{x}') = k(|\\mathbf{x}-\\mathbf{x}'|)\n\\end{equation}\nin eigenfunctions of the Laplacian. For enforcing zero Dirichlet boundary values on a domain $\\Omega$,  use the \\emph{spectral density} (Fourier transform) of the kernel \\eqref{e:stationary_isotropic_kernel},\n\\begin{equation}\n\\label{e:spectral_density}\ns(\\bm{\\omega}) = \\int_{\\mathbb{R}^d}\ne^{-i \\bm{\\omega} \\cdot \\mathbf{x}} \nk(|\\mathbf{x}|)\nd\\mathbf{x}. \n\\end{equation}\nThis enters into the approximation of the kernel:\n\\begin{equation}\\label{e:spectral_expansion_kernel}\nk(\\mathbf{x},\\mathbf{x}') \\approx \\sum_{\\ell=1}^m \ns(\\lambda_\\ell) \\phi_\\ell(\\mathbf{x}) \\phi_\\ell(\\mathbf{x}'),\n\\end{equation}\nwhere $\\lambda_j$ and $\\phi_j$ are the Dirichlet eigenvalues and eigenfunctions, respectively, of the Laplacian on the domain $\\Omega$.\nIn \\eqref{e:spectral_expansion_kernel}, $s(\\cdot)$ is thought of as a function of a scalar variable; since $k$ is isotropic in \\eqref{e:stationary_isotropic_kernel}, so is the Fourier transform $s(\\bm{\\omega}) = s(|\\bm{\\omega}|)$.  \nNote that the expansion \\eqref{e:spectral_expansion_kernel} yields a covariance that is zero when $\\mathbf{x} \\in \\partial\\Omega$ or $\\mathbf{x}' \\in \\partial\\Omega$.  \nThus if the mean of the GP satisfies the zero boundary conditions, Gaussian process predictions using the series \\eqref{e:spectral_expansion_kernel} will satisfy the boundary condition as well.", "cites": [7628], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the spectral expansion approach for enforcing zero Dirichlet boundary conditions in Gaussian processes, based on a single cited paper. It explains the mathematical formulation and how the method ensures boundary compliance, but lacks synthesis with other approaches, critical evaluation of the method's limitations, or broader abstraction to overarching principles. The content is informative but not insight-rich."}}
{"id": "ae4bdd86-cc7b-49ce-872b-76349b98b032", "title": "Implementation", "level": "subsection", "subsections": [], "parent_id": "65d986b6-b11e-421e-971a-bb294bcbf1db", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Boundary Condition Constraints"], ["subsection", "Implementation"]], "content": "The first implementation task that presents itself is computation of the Dirichlet spectrum $(\\lambda_\\ell, \\phi_\\ell)$ of the Laplacian\n\\begin{align}\n\\Delta \\phi_\\ell &= \\lambda_\\ell \\phi_\\ell \\quad\\text{ in } \\Omega \\\\\n\\phi_\\ell &= 0 \\quad\\text{ on } \\partial\\Omega\n\\end{align}\nFor basic domains, such as rectangles, cylinders, or spheres, this can be solved in closed form. For general domains, the problem must be discretized and an approximate spectrum computed. \n obtain an approximate spectrum by discretizing the Laplace operator with a finite difference formula and applying a correction factor to the eigenvalues of the resulting matrix. There are many other approaches for computing the spectrum of the Laplacian with various boundary conditions; see, e.g.,  for an approach using the spectral element method for calculating both Dirichlet and Neumann spectrum in complex geometries. \nEvaluation of $s(\\lambda_\\ell)$, where $s$ denotes the spectral density \\eqref{e:spectral_density} in \\eqref{e:spectral_expansion_kernel}, is typically not difficult since $s$ is available in closed form for many stationary kernels, such as the squared exponential (SE) and Mat\\'ern ($M_{\\nu}$) kernels:\n\\begin{align}\ns_{\\text{SE}}(|\\bm{\\omega}|; \\gamma, \\theta) &=\n\\gamma^2 (2\\pi\\theta^2)^{\\frac{d}{2}}e^{-\\frac{|\\bm{\\omega}|^2 \\theta^2}{2}},\\\\\ns_{M_{\\nu}}(|\\bm{\\omega}|; \\gamma, \\theta) &= \n\\gamma^2 \\frac{2^d \\pi^{\\frac{d}{2}} (2\\nu)^\\nu \\Gamma(\\nu + \\frac{d}{2})}\n{\\theta^{2\\nu}\\Gamma(\\nu) }\n\\left(\n\\frac{2\\nu}{\\ell^2} + |\\bm{\\omega}|^2\n\\right)^{-\\frac{2\\nu + d}{2}}. \n\\end{align}\nNext, we review from  and  how the formulas for Gaussian processes regression and training can be expressed using the the formulation \\eqref{e:spectral_expansion_kernel}. \nGiven $n$ data points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, \nthe covariance matrix is approximated\nusing \\eqref{e:spectral_expansion_kernel} as\n\\begin{equation}\nK_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j) \\approx \n\\sum_{\\ell=1}^m \n\\phi_{\\ell}(\\mathbf{x}_i) s(\\lambda_\\ell) \n\\phi_{\\ell}(\\mathbf{x}_j).\n\\end{equation}\nIntroducing the $n \\times m$ matrix $\\Phi$,\n\\begin{equation}\n{\\Phi}_{i \\ell} = \n\\phi_{\\ell}(\\mathbf{x}_i), \n\\quad 1 \\le i \\le n,\n\\quad 1 \\le \\ell \\le m, \n\\end{equation}\nand the $m \\times m$ matrix \n$\\Lambda = \\text{diag}(s(\\lambda_{\\ell})), 1 \\le \\ell \\le m,$\nthis can be written\n\\begin{equation}\\label{e:big_k_spectral_matrix}\n{K} \\approx {\\Phi} \\Lambda {\\Phi}^\\top.  \n\\end{equation}\nThus, the covariance matrix $K$ is diagonalized and, for a point $\\mathbf{x}^*$, we can write the $n \\times 1$ vector\n\\begin{equation}\\label{e:little_k_spectral_matrix}\n\\mathbf{k}_* = \\left[k(\\mathbf{x}^*, \\mathbf{x}_i)\\right]_{i=1}^n \\approx \n\\left[\n\\sum_{\\ell=1}^m \n\\phi_{\\ell}(\\mathbf{x}_i) s(\\lambda_\\ell) \n\\phi_{\\ell}(\\mathbf{x}^*)\n\\right]_{i=1}^n\n=\n\\Phi \\Lambda \\bm{\\Phi}_*, \n\\end{equation}\nwhere the $m \\times 1$ vector $\\bm{\\Phi}_*$ is defined by\n\\begin{equation}\n\\left[\\bm{\\Phi}_*\\right]_{\\ell} = \\phi_\\ell(\\mathbf{x}^*),\n\\quad\n1 \\le \\ell \\le m.\n\\end{equation}\nThe Woodbury formula can be used to\nobtain the following expressions for the posterior mean and variance over\na point $\\mathbf{x}^*$ given a Gaussian likelihood \n$y_i = f(x_i)+\\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ :\n\\begin{align}\\label{e:regression_formulas_bc}\n\\begin{split}\n\\mathbb{E}[f(\\mathbf{x}^*)] &= \\mathbf{k}_*^\\top \n(K + \\sigma^2 I)^{-1} \\mathbf{y} \\\\\n&=\n\\bm{\\Phi}_*^\\top \n(\\Phi^\\top \\Phi + \\sigma^2 \\Lambda^{-1} )^{-1}\n\\Phi^\\top \\mathbf{y}. \\\\\n\\mathbb{V}[f(\\mathbf{x}^*)] &= \nk(\\mathbf{x}^*,\\mathbf{x}^*) - \n\\mathbf{k}_*^\\top (K + \\sigma^2 I)^{-1} \\mathbf{k}_* \\\\\n&=\n\\sigma^2 \\bm{\\Phi}_*^\\top \n(\\Phi^\\top \\Phi + \\sigma^2 \\Lambda^{-1})^{-1} \\bm{\\Phi}_*.\n\\end{split}\n\\end{align}\nStrategies for using this method with non-Gaussian likelihoods are also discussed by , although we do not go over them here. \nFor use in hyperparameter training, the following formulas were derived in  and  for the \nnegative log-marginal-likelihood \n\\begin{align}\n\\label{e:nlml_bc}\n&\\phantom{\\partial}\\begin{multlined} \n-p(\\bold{y}|X,\\bm{\\theta}) =\n\\frac{n-m}{2} \\log \\sigma^2\n+\\frac{1}{2} \\sum_{\\ell = 1}^{m} \n{\\color{black}\\log\\Big(}\n\\Lambda_{\\ell,\\ell}\n{\\color{black}\\Big)}\n+\\frac{1}{2} \\log \\det \\left( \\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)\n+\\frac{n}{2} \\log(2\\pi) \\\\\n+\\frac{1}{2\\sigma^2}\\left[\n\\mathbf{y}^\\top \\mathbf{y} - \\mathbf{y}^\\top \\Phi\n\\left(\\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1} \\Phi^{\\color{black}\\top}\n \\mathbf{y}\n\\right],\n\\end{multlined}\n\\end{align}\nand in  for its derivative:\n\\begin{align}\n\\label{e:nlml_bc_derivatives}\n\\begin{split}\n&\\begin{multlined}\n-\\frac{\\partial p(\\bold{y}|X,\\bm{\\theta})}{\\partial \\theta_k} = \n\\frac{1}{2} \\sum_{\\ell = 1}^m \\frac{1}{\\Lambda_{\\ell,\\ell}} \n\\frac{\\partial \\Lambda_{\\ell,\\ell}}{\\partial \\theta_k}\n-\n\\frac{\\sigma^2}{2} \\text{Tr}\\left(\n\\left(\\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi\\right)^{-1}\n\\Lambda^{-2} \\frac{\\partial \\Lambda}{\\partial \\theta_k}\n\\right) \\\\\n-\\mathbf{y}^\\top \\Phi \n\\left( \\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1}\n\\left( \\Lambda^{-2} \\frac{\\partial \\Lambda}{\\partial \\theta_k} \\right)\n\\left( \\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1}\n\\Phi^\\top \\mathbf{y}\n,\\end{multlined}\n\\\\\n&\\begin{multlined}\n-\\frac{\\partial p(\\bold{y}|X,\\bm{\\theta})}{\\partial \\sigma^2} = \n\\frac{n-m}{2\\sigma^2}\n+\n\\frac{1}{2}\n\\text{Tr}\n\\left(\n\\left( \\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1}\n\\Lambda^{-1}\n\\right)\n\\\\ + \\frac{1}{2\\sigma^2}\n\\mathbf{y}^\\top \\Phi \\left( \\sigma \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1}\n\\Lambda^{-1}\n\\left( \\sigma \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1}\n\\Phi^\\top \\mathbf{y} \\\\\n\\REVISION{-\n\\frac{1}{2\\sigma^4} \\left[\n\\mathbf{y}^\\top \\mathbf{y} - \\mathbf{y}^\\top \\Phi\n\\left(\\sigma^2 \\Lambda^{-1} + \\Phi^\\top \\Phi \\right)^{-1} \\Phi^{\\color{black}\\top}\n \\mathbf{y}\n\\right].}\n\\end{multlined}\n\\end{split}\n\\end{align}\nNote that $\\Lambda$ is defined by the spectral density $s$ of the kernel $k$, which clearly depends on the kernel hyperparameters $\\bm{\\theta} = [\\theta_i]$, however $\\Phi$ does not. Typically, derivatives of $\\Lambda$ with respect to $\\theta_i$ can be computed in closed form which along with the formulas \\eqref{e:nlml_bc} and \\eqref{e:nlml_bc_derivatives} enable accurate first-order optimization of the kernel hyperparameters.", "cites": [7628], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a mathematically grounded synthesis of how boundary condition constraints are implemented in Gaussian process regression, drawing on the cited paper's framework. It abstracts the method using spectral expansions and derives key expressions for posterior inference and hyperparameter training. However, the critical analysis is limitedthere is little discussion of the limitations or trade-offs of the method compared to alternatives, nor is there a broader evaluation of the cited approaches."}}
{"id": "247a6cf6-5ff8-4e9d-98a9-4c41b81a81af", "title": "Extensions", "level": "subsection", "subsections": [], "parent_id": "65d986b6-b11e-421e-971a-bb294bcbf1db", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Boundary Condition Constraints"], ["subsection", "Extensions"]], "content": "The expansion  was originally developed for the computational advantages of using a low rank approximation to a kernel (see Section \\ref{sec:spectral_low_rank} for a discussion of this aspect) rather than for boundary condition constraints. Consequently, the discussions in  and  focused only on periodic and zero Dirichlet boundary conditions. One possible way \nto constrain a Gaussian process $f$ to satisfy nonzero Dirichlet conditions would be to write $f = (f - g) + g$, where $g$ is a harmonic function that satisfies a given nonzero Dirichlet condition, and model $f-g$ as a Gaussian processes that satisfying a zero Dirichlet condition using the above approach.  remark that the method could also be extended to Neumann boundary conditions by using the Neumann eigenfunctions of the Laplacian, although no examples are given. Another limitation is that spectral expansions in  and  are only considered for isotropic kernels, but they suggest that the approach can be extended to the nonisotropic case.", "cites": [7628], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by discussing how the spectral expansion method originally intended for low-rank approximation can be adapted for nonzero Dirichlet and Neumann boundary conditions. It integrates the cited work with general GP constraint strategies but lacks deeper comparative or evaluative insights across multiple methods. The discussion offers some abstraction by identifying the potential for nonisotropic kernel extensions but remains focused on a single approach."}}
{"id": "7de0b200-1765-49ff-92e9-88eb4c96cc6c", "title": "Constrained Maximum Likelihood Estimation for Splines", "level": "subsection", "subsections": [], "parent_id": "d8114b96-b4e0-4478-b43f-0ac0ea15b28a", "prefix_titles": [["title", "A Survey of Constrained Gaussian Process Regression: \\\\ Approaches and Implementation Challenges"], ["section", "Computational Considerations"], ["subsection", "Constrained Maximum Likelihood Estimation for Splines"]], "content": "\\label{sec:mle}\nWe review the work of  which discusses maximum likelihood estimation of hyperparameters within the spline approach \\REVISION{discussed in Sections \\ref{sec:splines} and \\ref{sec:splines_monotonic}}. \nThe starting point is the constrained log-marginal-likelihood function given the constraints $\\bm{\\xi} \\in \\mathcal{C}$, \\REVISION{where we have denoted $\\mathcal{C} = \\{\\mathbf{a} \\le \\bm{\\xi} \\le \\mathbf{b}\\}$}.\nThis is based on the posterior \\REVISION{density} $p_{\\bm{\\theta}}(\\mathbf{y} | \\bm{\\xi} \\in \\mathcal{C})$ of $\\mathbf{y}$ given the constraint $\\bm{\\xi} \\in \\mathcal{C}$, which by Bayes' rule can be expressed as\n\\begin{equation}\np_{\\bm{\\theta}}(\\mathbf{y} | \\bm{\\xi} \\in \\mathcal{C}) = \n\\frac{p_{\\bm{\\theta}}(\\mathbf{y}) P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C} | \\Phi \\bm{\\xi} = \\mathbf{y})}{P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C})}.\n\\end{equation}\nTaking the logarithm yields a constrained log-marginal-likelihood function:\n\\begin{align}\n\\begin{split}\n\\label{e:cmle}\n\\mathcal{L}_{\\text{cMLE}}\n&=\n\\log p_{\\bm{\\theta}}(\\mathbf{y} | \\bm{\\xi} \\in \\mathcal{C}) \\\\\n&= \n\\log p_{\\bm{\\theta}}(\\mathbf{y}) + \\log P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C} | \\Phi \\bm{\\xi} = \\mathbf{y})  - \\log P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C}) \\\\\n&=\\mathcal{L}_{\\text{MLE}} + \\log P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C} | \\Phi \\bm{\\xi} = \\mathbf{y})  - \\log P_{\\bm{\\theta}}(\\bm{\\xi} \\in \\mathcal{C}).\n\\end{split}\n\\end{align}\nIn the first term, $p_{\\bm{\\theta}}(\\mathbf{y})$ refers to the probability density function of the random variable $\\mathbf{y}$ with hyperparameters $\\bm{\\theta}$; thus, the first term is simply the unconstrained log-marginal-likelihood \\eqref{eq:log_like} which we denote $\\mathcal{L}_{\\text{MLE}}$. In the second and third terms, $P_{\\bm{\\theta}}$ refers to the probability of the indicated events. \nAs $\\bm{\\xi}$ and  $\\bm{\\xi} | \\{\\Phi \\bm{\\xi} = \\mathbf{y}\\}$ are both normally distributed by equations \\eqref{eq:multivar_norm} and \\eqref{eq:gpreg}, respectively, the two last terms in \\eqref{e:cmle} can be expressed as integrals of a normal \\REVISION{density} over \n$\\mathcal{C}$, just like the normalization constant \\eqref{e:truncated_normalization}.\n\\REVISION{Such integrals can be reduced to integrals over orthants, so the last two terms in \\eqref{e:cmle} are referred in  as Gaussian orthant probabilities.}\nUnlike the sampling of \\eqref{e:truncated_normal}, for which computing such integrals can be avoided with MCMC, calculation of Gaussian orthant probabilities is unavoidable if the user wants to train the kernel hyperparameters using the constrained objective function \\eqref{e:cmle}, which we refer to as cMLE. A thorough discussion of numerical approaches to truncated Gaussian integrals is .  utilize the minimax exponential tilting method of , reported to be feasible for quadrature of Gaussian integrals in dimensions as high as 100, to compute the Gaussian orthant probabilities in \\eqref{e:cmle} and compare cMLE with MLE. Another current drawback of cMLE is that the gradient of $\\mathcal{L}_{\\text{cMLE}}$ is not available in closed form, unlike the gradient of \n$\\mathcal{L}_{\\text{MLE}}$ . \nThus, in , MLE was performed using a L-BFGS optimizer, while cMLE was performed using the method of moving asymptotes. This involved a numerical approximation to the gradient of $\\mathcal{L}_{\\text{cMLE}}$, which in our experience can impact the accuracy of the optimization. Although these numerical differences hamper direct comparison of MLE and cMLE, it was found by  that for the case of limited data, cMLE can provide more accurate estimation of hyperparameter values and confidence intervals than MLE. \n also studied under which conditions MLE and cMLE yield consistent predictions of certain hyperparameters. This was further studied in , in which the authors perform an analysis of MLE and cMLE for the case of fixed-domain asymptotics, i.e., data in a fixed domain, as the number of data points tends to infinity. In this regime of dense data, the effect of constraints is expected to diminish. The authors show that MLE and cMLE yield consistent hyperparameters in this limit for the case of boundedness, monotonicity, and convexity constraints, and suggest quantitative tests to determine if the number of data points is sufficient to suggest unconstrained MLE as opposed to the more expensive cMLE.", "cites": [8605], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple sources, particularly connecting the minimax exponential tilting method to the calculation of constrained likelihoods. It provides critical insights by highlighting limitations, such as the inavailability of a closed-form gradient for cMLE, and evaluates the trade-offs between cMLE and MLE. While it identifies broader patterns (e.g., the diminishing effect of constraints with dense data), the abstraction is not extended to a meta-level framework."}}
