{"id": "00c8b60c-73a6-4956-8904-087ba9cccdbd", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "1746dcc5-851c-4521-ab0d-90b718fe1c00", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Introduction"]], "content": "Network analysis has attracted increasing attention over the recent years due to the ubiquity of network data in real world. The graph-structured network is a information carrier commonly used in complex systems, such as semantic networks , protein-protein interaction networks , social networks  and criminal networks . In order to construct the feature representations that can be applied to various tasks on graph-structured networks, network embedding is proposed to embed each node in the network into low-dimensional space . However, the real-world networks are usually evolving, and simply exploiting static network embedding techniques for dynamic network embedding will increase the consumption of time and space, since the training model will be retrained once the network changes. Furthermore, in order to capture the temporal information in network evolution, many dynamic network embedding models are proposed to embed the unstructured data into a low-dimensional space, and predict the trend of network evolution. Fig. \\ref{figure1} shows an illustrative example of embedding two snapshots of a dynamic network into a 2D space. Nowadays, dynamic network representation learning has been successfully applied to machine learning tasks on complex networks, such as visualization, node classification, node clustering, and link prediction.\nThrough modeling the interactions between entities, dynamic network embedding methods can facilitate practical applications, e.g., community detection , recommender system  and so on. Based on social networks whose interactions are constructed with the relationships, we can discover communities, build interpersonal networks, as well as predict interaction and behavior of users.\n\\begin{figure}[!htb]\n\t\\centering\n\t\\tiny\n\t\\subfloat{\n\t\t\\includegraphics[width=0.45\\linewidth]{dynetwork1.eps}\n\t\t\\includegraphics[width=0.45\\linewidth]{dynetwork2.eps}}\n\t\\\\\n\t\t\\subfloat{\n\t\t\\includegraphics[width=0.45\\linewidth]{n-plt-1.eps}\n\t\t\\includegraphics[width=0.45\\linewidth]{n-plt-3.eps}}\n\t\\caption{Two snapshots of the datasets are collected in the MathOverflow social network in 2011-2012, an online community of interactive mathematics. For illustration, 20 nodes are selected randomly, and we draw the original network structure graph and its 2-D layout after embedding.}\n\t\\label{figure1}\n\\end{figure}\nConsiderable effort has been committed to develop static network embedding techniques. DeepWalk  formulates static network embedding as a sequence modeling problem, which generates node sequences by random walk and adopts Skip-Gram to predict the context from input nodes. Based on the DeepWalk model , node2vec  integrates depth-first search and breadth-first search strategies for exploring more flexible network structure. LINE  carefully designs a loss function for characterizing first-order and second-order proximities, and this method is applicable to large-scale information networks of arbitrary types. In addition, SDNE  proposes a structural deep graph embedding model, which combines the advantages of first-order and second-order proximities, and maintains the highly non-linear structure. DNE-APP  proposes a deep network embedding model, which adopts a semi-supervised stacked auto-encoder to obtain the compact representation. Furthermore, in order to prevent the manifold fracturing problem, ANE  utilizes an adversarial auto-encoder for variational inference to generate network representations by matching the posterior of node embeddings with an arbitrary prior distribution. Moreover, APNE  uses matrix decomposition to integrate structure and node label information. However, static networks lack the generalization ability in network evolution. For example, users can randomly create and delete their friendships on YouTube or Facebook, or active neurons tend to develop new relationships with other neurons in brain networks. The newly added or deleted edge has an impact on network analysis, while the existing static network embedding methods can not extract  rich temporal information. In order to overcome these obstacles, a great many dynamic network embedding methods are proposed.\nIn graph theory, a static network is composed of a set of nodes and a set of connected edges, which is indicated by an adjacency matrix. Dynamic networks are engaged with definitions which contain history information, snapshots, timestamps, nodes and edges. These definitions are concluded into two categories: snapshot and continuous-time networks. Snapshot is divided from the dynamic network at equal-interval in time sequence, so as to obtain the discrete sequence of network evolution through decomposing the dynamic network into a sequence of static networks. Nevertheless, dividing the network into equal-interval snapshots in time series places particular emphasis on global changes roughly and cannot retain the evolution information of the network elaborately. In contrast, the continuous-time mode can make up for this deficiency by marking each edge with multiple timestamps to predict whether the connection between nodes has just changed, in which each edge is given specific timestamps. Based on the two definitions mentioned above, several methods are positioned to make contributions to both research and practice in dynamic network embedding fields. In specific, Zhou \\textit{et al.}  designed a triadic unit, namely the dynamicTriad, which models how the triadic unit is closed from an open one, thus preserving both structural information and dynamics in network evolution. Nguen \\textit{et al.}  incorporated temporal dependencies into node embedding and deep graph models, thereby integrating temporal information of dynamic networks. In , the evolving patterns in dynamic network are generated by using evolving random walks, and the current embedding vectors are initialized with previous embedding vectors. Ahmed \\textit{et al.}  introduced a technology based on non-negative matrix factorization to obtain latent features from temporal snapshots of dynamic networks. Zhu \\textit{et al.}  proposed a generalized eigen perturbation model which can incorporate the variation of network links.\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{conclude.eps}\n\t\\caption{The content system of dynamic network embedding are constructed from two aspects, including embedding model and preprocessing process. In specific, the network dataset with attribute and label are necessarily preprocessed into uniform linear arrays, and then the normalized matrices are generated in the dynamic network evolution integrated with time series. After that, the network representations are constructed by updating the normalized matrices through the optimization algorithm.}\n\t\\label{figure2}\n\\end{figure*}\nAccording to both the attributes of the network and the architecture of the model, we categorize the existing dynamic network embedding techniques into five categories, including embedding based on eigenvalue factorization, embedding based on Skip-Gram, embedding based on auto-encoder, embedding based on graph convolution and embedding based on other methods. Fig. \\ref{figure2} shows how to construct a system of dynamic network embedding by adopting a two-stage execution method. Different embedding methods have various criteria for assessing model performance and weaknesses. Theoretically, methods based on neural networks may suffer high computation and space cost.\nGeneralized from the classification scheme mentioned above, we introduce what challenges these categories of dynamic network embedding techniques confronted with, and how to address them. Although several surveys , ,  summarize network embedding methods, they have two major limitations. On one hand, the existing graph embedding surveys only involve the relevant research based on traditional methods, and many recent innovative frameworks are not confirmed for inclusion in the review. For instance,  mainly introduces three categories of approaches and lacks introductions to the novel methods. Moreover,  only shows solicitude for knowledge graph embedding. On the other hand, they only concern on static network embedding while ignoring the critical properties of dynamic networks. Despite remarkable advancements, the domain of dynamic network embedding still requires further research to make summaries on full-scale directions of novel techniques.\nTo summarize, we make the following contribution:\n\\begin{itemize}\n\t\\item We propose a new taxonomy of dynamic network embedding according to different models and describe the challenges in these models, which open up new perspectives for understanding existing works. Furthermore, we systematically categorize the applications that dynamic network embedding supports.\n\t\\item Our work follows the dynamic embedding approaches, but differs from previous works in listing the summarized insights on why dynamic graph embedding can be performed in a certain way, which can provide feasible guidance for future researches.\n\t\\item To facilitate the timely and potential research, we point out six promising future research directions in dynamic network embedding domain, which consist of dynamic embedding models, large-scale networks, heterogeneous networks, attributed networks, task-oriented dynamic network embedding and more embedding spaces.\n\\end{itemize}\nThe rest of our paper is arranged as follows. In Section \\ref{ProForm}, the problem definitions commonly used in the dynamic network embedding strategies are introduced, then we provide a formalized definition of dynamic network embedding. In Section \\ref{Categories}, different dynamic network embedding techniques are elaborated in detail. The applications that dynamic network embedding enables are presented in Section 4. After that, Section 5 summarizes the challenges and points out several potential future research directions. In the end of this survey, we conclude the paper in Section 6.", "cites": [282, 1010, 218, 217, 6199, 219, 479], "cite_extract_rate": 0.3181818181818182, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of dynamic network embedding, listing both static and dynamic methods, including key cited papers. It makes some basic connections between static and dynamic approaches but lacks deeper synthesis or a novel framework. There is minimal critical analysis of the cited works, and the abstraction is limited to a few high-level categorizations without identifying overarching principles or trends."}}
{"id": "c9923275-f18f-4f7f-a4d6-1c7676fc7974", "title": "Embedding Based on Matrix Factorization", "level": "subsection", "subsections": [], "parent_id": "851ef6e4-9ca8-4f4c-bb96-ebaf5e57bbd4", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Categories"], ["subsection", "Embedding Based on Matrix Factorization"]], "content": "\\label{Eigenvalue}\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{SVD.eps}\n\t\\caption{An illustration of singular value decomposition (SVD). $k$ is the rank of matrix $D_{\\mathcal{SVD}}$.}\n\t\\label{figure3}\n\\end{figure}\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{dhpe.eps}\n\t\\caption{The model utilizes eigen decomposition to construct the high-order proximity matrix of the network, and then dynamically updates the node representations of the network in next time step based on the matrix perturbation theory.}\n\t\\label{figure4}\n\\end{figure}\nMatrix factorization  is the most general dimensionality reduction method in the field of network embedding. Singular value decomposition (SVD) is a representative algorithm of matrix factorization. The SVD of a matrix $D_{\\mathcal{SVD}}\\in R^{m \\times n}$ is defined as:\n\\begin{equation}\nD_{\\mathcal{SVD}}=U \\Sigma Q^{T}\n\\end{equation}\nwhere $U \\in R^{m \\times m}$, $Q \\in R^{n \\times n}$, and $\\Sigma \\in R^{m \\times n}$. $\\Sigma$ is a diagonalized matrix, in which each element on the main diagonal is a singular value, all other elements are 0. Besides, matrices $U$ and $Q$ satisfy the equation $U^{T}U=I$ and $Q^{T}Q=I$. The  definition of SVD can be visually illustrated by the following Fig. \\ref{figure3}.\nA traditional but effective embedding method is to decompose the adjacency matrix by SVD and the attribute matrix by eigen decomposition in complex networks, so as to construct the embedding representation of each node. From the perspective of matrix factorization, the dynamic evolution of network is amount to the constant change of original matrix, where the network embedding is updated according to the perturbation theory of matrix. The overall perspective of eigenvalue factorization in dynamic network embedding is sketched in Fig. \\ref{figure4}.\nLi \\textit{et al.}  proposed a dynamic attributed network embedding model which represents dynamic networks as snapshots and considers situations in which adjacency and attribute matrices of the networks evolve over time. The dynamic network representation at the time $t$ is updated based on the matrix variation. For the time $t=1$, the model proposes an embedding algorithm combining the matrix $D$ and $X$ as the hot start. The formula for updating the embedded eigenvalues and eigenvectors is as follows:\t\n\\begin{equation}\n\\begin{small}\n\\begin{array}{c}\n{\\Delta \\lambda_{i}=x_{i}^{\\prime} \\Delta L_{X} x_{i}-\\lambda_{i} x_{i}^{\\prime} \\Delta \\Sigma_{X} x_{i}}\n\\\\ {\\!\\Delta x_{i}\\!=\\!-\\!\\frac{1}{2} x_{i}^{\\prime} \\Delta_{X} x_{i} x_{i}\\!+\\!\\sum_{j\\!=\\!2, j \\neq i}^{k\\!+\\!1}\\left(\\frac{x_{j}^{\\prime} \\Delta L_{X} x_{i}\\!-\\!\\lambda_{i} x_{j}^{\\prime} \\Delta \\Sigma_{X} x_{i}}{\\lambda_{i}-\\lambda_{j}}\\right) x_{j}}\n\\end{array}\n\\end{small}\n\\end{equation}\nwhere $X^{t} \\in \\mathbb{R}^{n \\times n}$ is the adjacency matrix of the attributed network at snapshot $t$ and $\\Sigma_{X}$ is the diagonal matrix with $\\mathrm{\\Sigma}_{X}^{t}(i, i)= \\sum_{j=1}^{n} X^{t}(i, j)$, $x_{1}, x_{2}, ... x_{n}$ are the eigenvectors of the corresponding eigenvalues, in the meantime $0=\\lambda_{1} \\leq \\lambda_{2} \\leq \\ldots \\leq \\lambda_{n}$. $L_X^{t}=\\Sigma_X^{t}-X^{t}$ , and $L_X$ is a Laplacian matrix. Moreover the network representation $Y^{ t+1 }$ is computed by using a correlation maximization method.\t\nCui \\textit{et al.}  devised a embedding model which can capture the high-order proximity shown in Fig. \\ref{figure5}, which is an extension of the asymmetric transitivity preserving graph embedding model in the dynamic network processing. Based on the generalized singular value decomposition factorization (generalized singular value decomposition) and matrix perturbation theory , the node representation of time $t+1$ is rapidly and effectively updated when the network structure changes in the next time step (adding/removing nodes/edges) while preserving the high-order proximity. Remarkably, Katz similarity is transformed into a generalized singular value decomposition factorization to model the high-dimensional structure of a dynamic network, and then the node representation at the time $t+1$ of the given network is dynamically updated by applying matrix perturbation theory.\nSuppose $X$ denote the high-order adjacency matrix of the dynamic network, where $X^{K a t z}\\!=\\!M_{a}^{-1} M_{b}$. The Katz Index  is utilized as the alternate of $X$, which is the most generally used measures of high-order proximity. The formulas for updating the embedded eigenvalues and eigenvectors are shown as follows:\n\\begin{equation}\n\\begin{array}{c}{\\Delta \\lambda_{i}=\\frac{x_{i}^{T} \\Delta M_{b} x_{i}-\\lambda_{i} x_{i}^{T} \\Delta M_{a} x_{i}}{x_{i}^{T} M_{a} x_{i}}} \\\\ {\\Delta x_{i}=\\sum_{j=1, j \\neq i}^{d} \\alpha_{i j} x_{j}}\\end{array}\n\\end{equation}\nwhere $\\left\\{\\lambda_{i}\\right\\}$ are the eigenvalues of $X$ in descending order and $x_{1}, x_{2}, ... x_{n}$ denote the corresponding eigenvectors, where d is the embedding dimension. As for $\\alpha_{i j}$, it represents the coefficient matrix, which indicates the contribution of $x_{j}$ to $\\Delta x_{i}$. Then the final embedded representation $Y$ is obtained by minimizing the distance between $X$ and $YY^{\\mathsf{'T}}$.\n\\begin{figure*}[!htbp]\n\t\t\\centering\n\t\t\\includegraphics[width=1\\linewidth]{usedynamic.eps}\n\t\t\\label{dimension}\n\t\t\\caption{An illustration of eigenvalue factorization in dynamic network embedding. At snapshot $t$, the model generates spectral embedding results by integrating with network structure $A$, and obtain the static embeddings $Y$. Afterwards, at the snapshot $t + 1$, topological change $\\Delta A$ indicates the dynamic information of the network structure. The model integrates matrix perturbation theory for updating $Y$ for a new embedding $Y^{T}$.}\n\t\t\\label{figure5}\n\\end{figure*}\nChen \\textit{et al.}  proposed two online algorithms to track the top eigen pairs in the dynamic network, which are capable of tracking the important network parameters determined by certain eigen functions. Besides, creative methods for analysis of eigen functions with attributions at each time stamp are proposed. Inspired by this, the eigen function can be utilized to map eigen pairs of the network to an attribute matrix:\n\\begin{equation}\n\t\\mathrm{f} :\\left(\\Lambda_{k}, Y_{k}\\right) \\rightarrow \\mathbb{R}^{x}(x \\in \\mathbb{N})\n\\end{equation}\nwhere $\\Lambda_{k}$ is the eigen pairs matrix transformed from the adjacency matrix $X$. $Y_{k}$ denotes the embedding matrix of the given network. After that, through the method of , the eigen function can be estimated by :\n\\begin{equation}\n\tf\\left(\\left(\\Lambda_{k}, Y_{k}\\right)\\right)=\\triangle(G)=\\frac{1}{6} \\sum_{i=1}^{k} \\lambda_{i}^{3}\n\t\\label{fast.1}\n\\end{equation}\nwhere the quantity of triangles in the snapshot $G^{t}$ is $\\triangle(G)$, and $\\Lambda_{k}$ is the specific eigen-pair of $\\Lambda_{k}$.\nAhmed \\textit{et al.}  introduced a technology based on non-negative matrix factorization (NMF) to extract latent features which can strengthen the performance of the link prediction task on dynamic networks. This model focuses on the application of link prediction, and defines a new iterative rule to construct matrix factors with noteworthy network characteristics. In addition, it shows how to foster improvements in high prediction accuracy by adopting the fusion of time and structure information for training methods, and the potential NMF characteristics can effectively express the network dynamic rather than static representation.\nHowever, approaches based on matrix factorization typically operates on more than one $n \\times n$ matrices with a large dimension of $n \\times n$, which suffers from high time complexity. To tackle this obstacle, Zhang \\textit{et al.}  proposed TIMERS to theoretically explore a lower bound of SVD minimum loss and replace the bound with the minimum loss on dynamic networks based on matrix perturbation. TIMERS optimally sets the restart time of SVD in order to reduce the error accumulation in time. Furthermore, driven by treating the maximum tolerated error as a threshold, TIMERS triggers SVD to restart automatically when the margin exceeds rated threshold. Besides, margin is a value between the reconstruction loss of incremental updates and the minimum loss.", "cites": [7231], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes matrix factorization-based dynamic network embedding approaches from multiple papers, connecting ideas like SVD, eigen decomposition, and matrix perturbation theory into a coherent narrative. It also provides some critical analysis by highlighting the high time complexity issue and referencing TIMERS as a solution. The section abstracts the common theme of leveraging matrix perturbation to update embeddings dynamically, offering meta-level insights into the broader strategy used in this category of methods."}}
{"id": "bc4b19f0-3de6-4ed2-a629-7c65b363d13e", "title": "Embedding Based on Skip-Gram", "level": "subsection", "subsections": [], "parent_id": "851ef6e4-9ca8-4f4c-bb96-ebaf5e57bbd4", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Categories"], ["subsection", "Embedding Based on Skip-Gram"]], "content": "\\label{Skip-Gram}\nThe great prevalence of Word2Vec  has facilitated the well-known Skip-Gram model, through which we can predict the context from the input. Perozzi \\textit{et al.}  suggested that nodes are converted into word vectors and a random walk sequence is viewed as sentences. Since then, a great deal of static network embedding algorithms are proposed based on this model. Fig. \\ref{figure6} presents a synthesis framework which integrates temporal information into network embedding methods.\n\\begin{figure*}[!htbp]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{deepwalk.eps}\n\t\\caption{In the dynamic network, each edge is mapped to a corresponding timestamp. An effective walk is represented by a series of nodes connected by edges of an increasing timestamp. The mapping function $\\Phi$ is a matrix, which maps every node into a $d$-dimensional vector with a total of $\\left|V\\right|\\times d$ parameters which can be optimized by using Skip-Gram model or other models.}\n\t\\label{figure6}\n\\end{figure*}\t\nThe following description outlines extension approaches. The classic node2vec  inherits the advantages of DeepWalk and devises a biased random walk to learn node representations and LINE  can preserve the first-order and second-order proximities by utilizing multiple loss functions and optimize the performance by a concat technology. Du \\textit{et al.}  proposed a decomposable objective function on the basis of their previous LINE model to contribute the dynamic network embedding framework, where only a part of nodes are iteratively updated simultaneously and the representation in each iteration has a strong stability compared with retraining whole model. Moreover, the authors also devised the update node selection mechanism, greatly improving the efficiency of iterative update. Experiments on the node multi-label classification task  has performed well on several real networks. Similarly, Sajjad \\textit{et al.}  proposed an efficient unsupervised network representation embedding method based on random walks that also divides networks into snapshots. In order to efficiently calculate the embedding results in the next snapshot which is impacted by the previous snapshot, this paper factorizes the process of generating network representations into two steps. Firstly, the authors proposed a random walk update algorithm responsible for updating the set of walk sequence, with the goal of statistically distinguishing the updated set of random walks from a set of random walks generated from scratch on the new graph. Secondly, the Skip-Gram model is modified to update the node representations based on the random walks.\nStatic network embedding usually obtains training corpus through random walks, and then integrates the corpus to the Skip-Gram model. However, the random walks fail to take the chronological order into account, in which condition the edges appear randomly. For instance, a message propagated in a network is directed, but an unconstrained random walk may result in a reverse corpus. At this point, Nguyen \\textit{et al.}  expressed the dynamic network as a continuous-time network. Each edge has multiple timestamps, indicating the time corresponding to the existence of a connection relationship. On this basis, each random walk must be constrained to conform to the time sequence of occurrence of edges, so as to integrate the time sequence information of the network into the sequence of random walk. Theoretically, the random walk sequence with time series is a subset of which with non-temporal series. According to the view of information theory, the addition of temporal information reduces the uncertainty of random walk and enforces it outperforming DeepWalk and node2vec algorithms in traditional tasks.\nDynnode2vec  modifies node2vec by regarding the prior embedding vectors as the initialization of a Skip-Gram model and employing the random walks in network volution to update the Skip-Gram based on previous timestamps information. Dynnode2vec modifies the static node2vec at time $t$ by capturing historical information from time step $t-1$. In dynnode2vec, only the evolving node embeddings are generated from sets of random walk sequences, rather than considering all nodes in the current timestamps. Hence, novel random walks from local changed regions can efficiently update the embedding vectors in network evolution.\nBesides, dynamic network embedding framework proposed by Zuo \\textit{et al.}  also integrates the Hawkes process based on temporal network embedding and the Skip-Gram models. This technology models the evolution process of nodes through neighbourhood formation sequence, and then captures the influence of historical neighbours on the current neighbourhood formation sequence by using Hawkes process.\nMoreover, Liang \\textit{et al.}  applied dynamic network embedding to user profiling in Twitter , and proposed a dynamic embedding model of user and word (DUWE), coupled with a model of streaming keyword diversification (SKDM). Particularly, DUWE aims at capturing the semantic information of users over time with the Skip-Gram model and attempts to maximize its log likelihood:\n\\begin{equation}\n\\log p\\left(\\mathrm{n}^{ \\pm} | \\mathrm{V}\\right)=\\sum_{k, l=1}^{V} n_{k, I}^{+} \\log s\\left(\\mathrm{v}_{k}^{\\mathrm{T}} \\mathrm{v}_{l}\\right)+n_{k, l}^{-} \\log s\\left(-\\mathrm{v}_{k}^{\\top} \\mathrm{v}_{l}\\right)\n\\end{equation}\nwhere $n^{ \\pm}=\\left(n^{+}, n^{-}\\right)$, $n^{+}, n^{-} \\in \\mathbb{R}^{V \\times V}$ are the positive and negative indicator matrices for all word pairs with $n_{k, l}^{+}$ and $n_{k, l}^{-}$ being their elements respectively. $\\left(v_{k}, v_{l}\\right)$ is a word pair that can be observed in documents of a corpus. $s(x)$ is computed from a sigmoid function $s(x)=\\frac{1}{1+\\exp (-x)}$ and $s(-x)=1-s(x)$. $V=\\left\\{v_{k}\\right\\}_{k=1}^{V}$ is the definition of the embedding result of all the words in the vocabulary. DUWE is based on the Skip-Gram model, which is extended by employing a Kalman filter  to process the evolution information of users and words, aiming to model the dynamic user and word embeddings. In DUWE, the authors utilized the variance of the transformation kernel embedded by all users to represent the diffusion process of the representation in users and words evolution.", "cites": [1010, 479, 8978, 282, 218, 7245], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple Skip-Gram-based dynamic network embedding approaches, connecting their shared reliance on random walks and how they incorporate temporal constraints or evolution mechanisms. It provides some critical analysis by pointing out the limitations of static random walks in capturing chronological order and how temporal methods address this. The abstraction is moderate, as it begins to generalize patterns (e.g., the use of historical information and efficient iterative updates) but does not fully extract overarching theoretical principles or frameworks."}}
{"id": "94953bfc-98da-48d5-929a-9234a880c277", "title": "Embedding Based on Auto-encoder", "level": "subsection", "subsections": [], "parent_id": "851ef6e4-9ca8-4f4c-bb96-ebaf5e57bbd4", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Categories"], ["subsection", "Embedding Based on Auto-encoder"]], "content": "\\label{Auto-encoder}\nAuto-encoder is an artificial neural network that can construct the efficient representation of the input data in an unsupervised manner, which is suitable for dimensionality reduction. The dimension of the learned representation is generally far less than that of input data. In specific, auto-encoder is mainly carried out into two steps, encoder and decoder. In most auto-encoder, the hidden layers of the auto-encoder architecture is realized through the neural network.\n\\begin{figure*}[!t]\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{DDNEDYNGEM.eps}\n\t\\caption{The dynamic network is divided into snapshots. During the training process, the weight parameters of the previous auto-encoder network are retained to initialize the next training network. $Y$ is the set of network representation, and $X$ is the adjacency matrix of networks.}\n\t\\label{figure7}\n\\end{figure*}\t\nSDNE  is a classical network embedding model based on auto-encoder. Due to its unsupervised nature and excellent performance, the embedding model can be extended to dynamic networks straightforwardly. For instance, Goyal \\textit{et al.}  proposed a dynamic embedding model DynGEM, inspired by SDNE, which also represents dynamic networks as snapshots. It retains the embedding information at the previous moment that will be utilized for the next moment, so that the embedding model at the next moment can directly inherit the parameters of the model trained at the previous moment. It's worth noting that the quantity of network nodes and edges have an impact on the architecture of the embedding model. Therefore, this model employs heuristic information to adjust the overall structure of SDNE according to the new network structure, which makes it easy to be applied to the new network. A simplified version of the DynGEM model is shown in Fig. \\ref{figure7}.\nHowever, another work dyngraph2vec  proposed by Goyal \\textit{et al.} reflects that the DynGEM framework and other dynamic embedding algorithms capture the information of the previous step while ignoring the richer historical information. In this case, dyngraph2vec takes the network structure information into account at time step $t, t+1, t+2, \\ldots \\ldots, t+l-1$ when computing the embedding matrix at time $t+l$. To be specific, by expressing the time-ordered network sequence as a corpus, it leverages RNN to encode the historical information for semi-supervised learning and give a simple update rule for the auto-encoder. However, dyngraph2vec employs three models to embed the historical information, which leads to a high level of complexity. Additionally, the NetWalk model proposed by Yu \\textit{et al.}  not only incrementally generates network representations in network evolution, but also detects network anomalies in real time. In particular, the node representation can be obtained through a number of walk sequences extracted from the dynamic network. It unifies local walks and hybridizes it with the hidden layer of a deep auto-encoder to yield node representations, thus, the resultant embedding is capable of reconstructing the original network with less loss. Moreover, the learning process performs over dynamic changes by utilizing a sampling stochastic. Then the model uses a dynamic clustering model to flag anomalous vertices or edges.", "cites": [7232], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple auto-encoder-based dynamic network embedding methods (DynGEM, dyngraph2vec, NetWalk) and connects them under a broader theme of utilizing historical information. It provides some critical remarks, such as the high complexity of dyngraph2vec and the limitations of DynGEM in capturing only the immediate past. While it identifies patterns like the use of RNNs and incremental updates, it lacks deeper abstraction into overarching principles or a novel framework for understanding the space."}}
{"id": "02e0f100-b101-4d55-befe-97bd1fc68f90", "title": "Embedding Based on Neural Networks", "level": "subsection", "subsections": [], "parent_id": "851ef6e4-9ca8-4f4c-bb96-ebaf5e57bbd4", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Categories"], ["subsection", "Embedding Based on Neural Networks"]], "content": "\\label{Applications}\nRecently, neural networks have attracted extensive attention of researchers in network embedding domain. By virtue of the powerful representation ability, neural networks based network embedding models have achieved outstanding results.\nSince neural networks have shown impressive results on dynamic embedding, an inductive deep representation learning framework called DyRep , is decomposed into two dynamic processes on the network: association process and communication process. The former handles the change of network topology, while the latter deals with the change of network dynamics. In this model, event is defined to uniformly represent the changes of the above two processes. Furthermore, DyRep views the change of node representations as an intermediate bridge between the two processes mentioned above, so as to continuously update the node representation according to new events. When an event occurs, the new representation of the node is aggregated by the event-related neighbour nodes. In addition, the model which aims to aggregate neighbours of nodes dynamically adopts the attention mechanism.\nAfter that, Chen \\textit{et al.}  proposed a novel optimization method on neural networks, which assigns different sensitive weights for samples and selects the samples via their weights when computes gradients. The related samples are updated in a diffusion strategy, as the embedding of the selected sample is reconstructed. Meanwhile, in order to decrease the computational cost during selecting samples, the authors presented a nested segment tree for weighted sample selection. This model enforces dynamic embedding algorithms more adequate in analyzing highly dynamic and recency-sensitive data, which can overcome the problem that the optimization process of traditional is complicated.\nAnother example using neural network structure is Know-Evolve , it proposes a deep recurrent architecture for generating non-linearly network representations over time. It models multi-relational timestamped edges to reflect the evolving process. Besides, the information captured by node embeddings only depend on the edge-level information. Sankar \\textit{et al.}  proposed DySAT, a dynamic self-attention network embedding model. Specifically, DySAT constructs node representations which incorporates self-attention mechanism into two aspects: structural neighbors and temporal dynamics. Through the dynamic self-attention architecture, it provides dynamic representations for nodes which capture both structural properties and temporal evolutionary patterns.\nRecurrent neural networks (RNN)  are commonly utilized for continuous sequence learning by capturing the dynamic information in time series. Motivated by the insights of RNN, several dynamic network embedding methods combine the RNN and other neural networks to learn the node representations. For example, EvolveGCN  contains two components, the RNN component and the graph convolutional network (GCN) component. The RNN model updates the weight matrix in the next time step and the GCN model uses the updated weight matrix and the current node embedding matrix to update the subsequent node embedding matrix. EvolveGCN focuses on the evolution of the GCN parameters at every time step rather than the node representations, whose model is adaptive and flexible for preserving network dynamics.\nIn addition, BurstGraph  uses two RNN-based variational autoencoders framework to model graph evolution in the vanilla evolution and bursty links�� occurrences at every time step. Furthermore, the encoders share the same GraphSAGE  to capture node attributed information. BurstGraph encodes both the structural evolution and attributed evolution in the latent embedding vectors, which can fully exploit the recurrent and consistent patterns in dynamic networks.", "cites": [7241, 302, 7236, 8113, 242], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of neural network-based dynamic network embedding methods, mentioning several models and their key components. It integrates some ideas across works (e.g., attention mechanisms, RNN usage) but lacks deeper synthesis or a unifying framework. Critical evaluation is minimal, and while it hints at broader patterns (e.g., modeling both structure and temporal dynamics), it does not abstract to a meta-level or provide deep comparative insights."}}
{"id": "5b1aec33-9ba6-4316-bdd3-4d6ba38743ce", "title": "Dynamic Network Embedding for Large-scale Networks", "level": "subsection", "subsections": [], "parent_id": "4e095d52-e7d7-4248-b7f5-ecbc944db33c", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Future Research Directions"], ["subsection", "Dynamic Network Embedding for Large-scale Networks"]], "content": "For network embedding, Tang \\textit{et al.}  proposed LINE to analyze complex networks with large-scale. However, LINE is not applicable for dynamic networks. Due to the complexity of performing network evolution in dynamic networks, existing dynamic network embedding methods can not adopt for more complex real-world networks. By the insight of the above survey, dynamic networks can be represented as snapshots or continuous-time networks marked by timestamps. The more the number of snapshots or timestamps, the higher complexity of network evolution. Therefore, the efficiency of dynamic network embedding can be improved from two aspects, i.e., reducing the complexity of network evolution or improving embedding models. To conclude, large-scale dynamic network embedding is a difficult research point without effective work reaching it.", "cites": [282], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces LINE and mentions its inapplicability to dynamic networks, but lacks synthesis of multiple ideas or sources. It provides minimal critical evaluation of existing methods and does not offer a deeper analysis or comparison of approaches. The abstraction is limited, with only superficial observations about the challenges of large-scale dynamic networks."}}
{"id": "3f4484a2-f4d4-4af6-830e-5758889ea708", "title": "Dynamic Network Embedding on Attributed Networks", "level": "subsection", "subsections": [], "parent_id": "4e095d52-e7d7-4248-b7f5-ecbc944db33c", "prefix_titles": [["title", "A Survey on Dynamic Network Embedding"], ["section", "Future Research Directions"], ["subsection", "Dynamic Network Embedding on Attributed Networks"]], "content": "Attributed networks contain extra attributed information, such as texts or contents . Dynamic embedding of attributed networks emphasizes the information that both the structures of networks and the attributes of nodes provide as networks evolve. A little work about this research direction has been proposed. Only Li \\textit{et al.}  proposed DANE that updates both the adjacency and attribute matrices when a network changes over time. Nevertheless, the work of Li \\textit{et al.}  only researches the situation where a dynamic network is divided into sequential snapshots. There is no research to pay attention to the other situation in which the dynamic network is represented as continuous-time networks marked by timestamps.", "cites": [7231], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a brief analytical perspective by identifying a gap in the literature, particularly the lack of attention to continuous-time attributed networks. It synthesizes the cited work (DANE by Li et al.) and contrasts it with other unexplored scenarios. However, it remains limited in depth, as it cites only one paper and does not abstract broader principles or compare multiple approaches in detail."}}
