{"id": "a54f5638-9281-403f-b2b2-144a868dc1ee", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "f1956d5f-f2e1-4651-99ed-4a43b7fcee12", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Introduction"]], "content": "Machine learning (ML) models offer the great benefit that they can deal with\nhardly specifiable problems as long as these can be exemplified by data samples.\nThis has opened up a lot of opportunities for promising automation and assistance\nsystems, like highly automated driving, medical assistance systems, text summaries\nand question-answer systems, just to name a few.\nHowever, many types of models that are automatically learned from data will not\nonly exhibit high performance, but also be black-box---\\idest, they hide information on the\nlearning progress, internal representation, and final processing in a format\nnot or hardly interpretable by humans.\nThere are now diverse use-case specific motivations for allowing humans to\n\\emph{understand} a given software component, \\idest, to build up a mental\nmodel approximating the algorithm in a certain way.\nThis starts with legal reasons, like the General Data Protection Regulation~\nadopted by the European Union in recent years.\nAnother example are domain specific standards, like the functional safety standard\nISO\\,26262~ requiring accessibility of software components\nin safety-critical systems. This is even detailed to an explicit requirement for explainability\nof machine learning based components in the ISO/TR\\,4804~\ndraft standard.\nMany further reasons of public interest, like fairness or security, as well as\nbusiness interests like ease of debugging, knowledge retrieval, or appropriate user trust\nhave been identified .\nThis need to translate behavioral or internal aspects of black-box algorithms\ninto a human interpretable form gives rise to the broad research field of\nexplainable artificial intelligence (XAI).\nIn recent years, the topic of XAI methods has received an exponential boost in\nresearch interest .\nFor practical application of XAI in human-AI interaction systems, it is important to \nensure a choice of XAI method(s) appropriate for the corresponding use case.\nWithout question, thorough use-case analysis including the main goal and derived requirements\nis one essential ingredient here .\nNevertheless, we argue that a necessary foundation for choosing correct requirements is\na complete knowledge of the different\n\\emph{aspects} (traits, properties) of XAI methods that may influence their applicability.\nWell-known aspects are, \\forexample,\nportability, \\idest, whether the method requires access to the model internals or not, \nor locality, \\idest, whether single predictions of a model are explained or some global properties.\nAs will become clear from our literature analysis in \\autoref{sec:taxonomy},\nthis only just scratches the surface of aspects of XAI methods that are relevant for practical application.\nThis paper aims to\n(1)~help beginners in gaining a good initial overview and starting point for a deeper dive,\n(2)~support practitioners seeking a categorization scheme for choosing an appropriate XAI method for their use-case, and\n(3)~to assist researchers in identifying desired combination of aspects that have not or little been considered so far.\nFor this, we provide a complete collection and a structured overview in the form of a\ntaxonomy of XAI method aspects in \\autoref{sec:taxonomy}, together with method examples for each aspect.\nThe method aspects are obtained from an extensive literature survey on categorization\nschemes for explainability and interpretability methods,\nresulting in a meta-study on XAI surveys presented in \\autoref{sec:sos}.\nOther than similar work, we do not aim to provide a survey on XAI methods.\nHence, our taxonomy is not constructed as a means of a sufficient chapter scheme. Instead, we try to compile a taxonomy that is complete with respect to existing valuable work.\nWe believe that this gives a good starting point for an in-depth understanding\nof sub-topics of XAI research, and research on XAI methods themselves.\nOur main contributions are:\n\\begin{itemize}\n    \\item \\emph{Complete XAI method taxonomy:}\n        A structured, detailed, and deep taxonomy of XAI method aspects (see \\autoref{fig:taxonomy});\n        In particular, the taxonomy is complete with respect to application relevant\n        XAI method and evaluation aspects that have so far been considered in literature, according to our structured literature search.\n    \\item \\emph{Extensive XAI method meta-study:}\n        A survey-of-surveys of more than 50 works on XAI related topics that may serve\n        to pick the right starting point for a deeper dive into XAI and XAI sub-fields.\n        To our knowledge, this is the most extensive and detailed meta-study specifically on XAI and XAI methods\n        available so far.\n    \\item \\emph{Broad XAI method review:}\n        A large collection, review, and categorization of more than 50 hand-picked diverse XAI methods\n        (see \\autoref{tab:methods.overview}).\n        This evidences the practical applicability of our taxonomy structure and the identified method aspects.\n\\end{itemize}\nThe rest of the paper reads as follows:\nIn \\autoref{sec:background} we reference some related work, recapitulate major milestones in the history of XAI and provide important definitions of terms.\nThis is meant for those readers less familiar with the topic of XAI.\nAfter that, we detail our systematic review approach in \\autoref{sec:approach}.\nThe results of the review are split into the following chapters:\nThe detailed review of the selected surveys is presented in \\autoref{sec:sos},\nincluding their value for different audiences and research focus;\nand \\autoref{sec:taxonomy} details collected XAI method aspects and our proposal of a taxonomy thereof.\nEach considered aspect is accompanied by illustrative example methods, a summary of which can be found in \\autoref{tab:methods.overview}. We conclude our work in \\autoref{sec:conclusion}.", "cites": [6976], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The introduction section effectively synthesizes the importance of XAI from cited works, particularly the influence of the GDPR and ISO standards, and connects these to broader motivations like fairness, security, and trust. It provides a critical framing of the need for a comprehensive taxonomy, highlighting the limitations of fragmented approaches. The section abstracts the field by identifying key method aspects and proposing a structured framework for understanding XAI."}}
{"id": "5d1c14f2-7002-4a18-b5b7-35742e53ee74", "title": "Related work", "level": "subsection", "subsections": [], "parent_id": "bb35dbce-3bde-4148-997f-da6cb3250e88", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Background"], ["subsection", "Related work"]], "content": "\\label{sec:relatedwork}\n\\paragraph*{Meta-studies on XAI methods}\nShort meta-studies collecting surveys of XAI methods are often contained in the\nrelated work section of XAI reviews like \\cite[Sec.\\,2.3]{linardatos_explainable_2021}.\nThese are, however, by nature restricted in length and usually concentrate on\nmost relevant and cited reference works like \\longcite{gilpin_explaining_2018,adadi_peeking_2018,arrieta_explainable_2020}.\nWe here instead consider a very broad and extensive collection of surveys.\nBy now, also few dedicated meta-studies can be found in literature.\nOne is the comprehensive and recent systematic literature survey\nconducted by \\longcite[Chap.~4]{vilone_explainable_2020}.\nThis reviews 53 survey articles related to XAI topics,\nwhich are classified differentially according to their focus.\nTheir literature analysis reaches further back in time and targets even more general topics in XAI than ours.\nHence, several of their included surveys are very specific, quite short, and do not provide any structured\nnotions of taxonomies. Also, the focus of their review lies in research topics,\nwhile the slightly more detailed survey-of-surveys presented here also takes into account\naspects that relate to suitability for beginners and practitioners.\nAnother dedicated survey-of-surveys is the one by \\longcite{chatzimparmpas_survey_2020}.\nWith 18 surveys on visual XAI this is smaller compared to ours, and has quite a different focus:\nThey review the intersection of XAI with visual analytics.\nLastly, due to their publication date they only include surveys from 2018 or earlier,\nwhich misses on important recent work as will be seen in \\autoref{fig:surveydist}.\nThe same holds for the very detailed DARPA report by \\longcite{mueller_explanation_2019} from 2019. Hence, both papers miss many recent works covered in our meta-study (\\cf~\\autoref{fig:surveydist}).\nFurther, the DARPA report has a very broad focus, also including sibbling research fields related to XAI. And, similar to Vilone et al., they target researchers,\nhence give no estimation of what work is especially suitable for beginners or practitioners.\n\\paragraph*{(Meta-)Studies on XAI method traits}\nRelated work on taxonomies is mostly shallow, focused on a sub-field, or is\npurely functional in the sense that taxonomies merely serve as a survey chapter structure.\nA quite detailed, but less structured collection of XAI method traits can be found\nin the courses of discussion in \\longcite{murdoch_definitions_2019,carvalho_machine_2019,burkart_survey_2021,mueller_explanation_2019,li_quantitative_2020}.\nThe focus of each will be discussed later in \\autoref{sec:sos}.\nBut despite their detail, we found that each article features unique aspects that are not included in the others.\nThe only systematic meta-review on XAI method traits known to us is the mentioned\nsurvey by \\longcite{vilone_explainable_2020}. They, however, generally review\nnotions related to XAI, not concretely XAI method traits. Only a shallow taxonomy is\nderived from parts of the retrieved notions (\\cf~\\cite[Sec.\\,5]{vilone_explainable_2020}). \n\\paragraph*{Use-case analysis}\nA related, wide and important field which is out-of-scope of this paper is that of use-case\nand requirements analysis. This, \\forexample, includes analysis of the background of the\nexplanation receiver .\nInstead, we here concentrate on finding aspects of the XAI methods themselves\nthat may be used for requirements analysis and formulation,\n\\forexample, whether the explainability method must be model-agnostic or the\namount of information conveyed to the receiver must be small.\nFor further detail on use-case specific aspects the reader may be referred to one of\nthe following surveys further discussed in \\autoref{sec:sos}\n.", "cites": [7303, 1798], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a thoughtful synthesis of existing meta-studies and taxonomies, contrasting their scope, focus, and depth with the authors' own work. It includes critical evaluations of limitations, such as outdated survey inclusion and narrow target audiences. The abstraction level is strong, as the authors generalize over survey trends and emphasize the need for a broader, more practice-oriented taxonomy."}}
{"id": "6c3b4d09-ab72-4a3a-9a86-8b20b9d6d102", "title": "History of XAI", "level": "subsection", "subsections": [], "parent_id": "bb35dbce-3bde-4148-997f-da6cb3250e88", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Background"], ["subsection", "History of XAI"]], "content": "\\label{sec:history}\nXAI is not a new topic, although the number of papers published in recent years might suggest it. The abbreviation XAI for the term explainable artificial intelligence was first used by \\longcite{vanlent_explainable_2004} (see~\\cite[Sec.\\,2.3]{carvalho_machine_2019}). According to \\longcite{belle2017logic}, the first mention of the underlying concept already dates back to the year 1958, when McCarthy described his idea of how to realize AI systems. In his seminal paper he promoted a declarative approach, based on a formal language and a problem-solving algorithm that operates on data represented in the given formal language. Such an approach could be understood by humans and the system's behaviour could be adapted, if necessary . Basically, McCarthy described the idea of inherently transparent systems that would be explainable by design.\nInherently transparent approaches paved the way for expert systems that were designed to support human decision-makers (see~). Due to the big challenge to integrate, often implicit, expert knowledge, these systems lost their popularity in the 90's of the last century. Meanwhile, deep neural networks (DNNs) have become a new and powerful approach to solve sub-symbolic problems. The first approaches aiming at making neural network decisions transparent for debugging purposes date back to the mid 90's. For example in 1992, Craven and Shavlik presented several methods to visualize numerical data, such as decision surfaces .\nDue to the introduction of the new European General Data Protection Regulation (GDPR) in May 2018, transparency of complex and opaque approaches, such as neural networks, took on a new meaning. Researchers and companies started to develop new AI frameworks, putting more emphasis on the aspect of accountability and the \\enquote{right of explanation} . Besides debugging and making decisions transparent to developers or end-users, decisions of a system now also had to be comprehensible for further stakeholders.\nAccording to \\longcite{adadi_peeking_2018} the term XAI gained popularity in the research community after the Defense Advanced Research Projects Agency (DARPA) published its paper about explainable artificial intelligence, see~\\longcite{gunning2019darpa}.\nIn their definition, XAI efforts aim for two main goals. The first one is to create machine learning techniques that produce models that can be explained (their decision-making process as well as the output), while maintaining a high level of learning performance. The second goal is to convey a user-centric approach, in order to enable humans to understand their artificial counterparts. As a consequence, XAI aims for increasing the trust in learned models and to allow for an efficient partnership between human and artificial agents .\nIn order to reach the first goal, DARPA proposes three strategies: deep explanation, interpretable models and model induction, which are defined in \\autoref{sec:def}. Among the most prominent XAI methods that implement this goal for deep learning, especially in computer vision, are for example LIME , LRP  and Grad-CAM .\nThe second, more user-centric, goal defined by DARPA requires a highly inter-disciplinary perspective. This is based on fields such as computer science, social sciences as well as psychology in order to produce more explainable models, suitable explanation interfaces, and to communicate explanations effectively under consideration of psychological aspects.\n\\longcite{miller_explanation_2019}, \\forexample, made an important contribution to the implementation of the user-centric perspective with his paper on artificial intelligence from the viewpoint of the social sciences. He considers philosophical, psychological, and interaction-relevant aspects of explanation, examining different frameworks and requirements. An important turn toward a user-centric focus of XAI was also supported by \\longcite{rudin_stop_2019} in her paper from 2019, where she argues for using inherently interpretable models instead of opaque models, motivated by the right to explanation and the societal impact of intransparent models.\nAnother milestone in the development of XAI is the turn toward evaluation metrics for explanations . The XAI community now acknowledges more in depth that it is not enough to generate explanations, but that it is also crucial to evaluate how good they are with respect to some formalized measure.", "cites": [7507, 7506, 6976], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes historical developments and key papers to present a coherent narrative of XAI's evolution. It abstracts the concept of explainability from specific examples like expert systems and LIME to broader goals and design principles. While it provides some critical perspective by highlighting the shift toward user-centricity and the importance of evaluation, it could offer deeper critique of individual approaches or limitations in the cited works."}}
{"id": "023f46f2-3164-4343-bff7-2dba25c1df95", "title": "Basic definitions", "level": "subsection", "subsections": [], "parent_id": "bb35dbce-3bde-4148-997f-da6cb3250e88", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Background"], ["subsection", "Basic definitions"]], "content": "\\label{sec:def}\nThis section introduces some basic terms related to XAI which are used throughout this paper.\nDetailed definitions of the identified XAI method aspects will be given in \\autoref{sec:taxonomy}.\nImportant work that is concerned with definitions for XAI can be found in, \\forexample, \\longcite{lipton_mythos_2018}, \\longcite{adadi_peeking_2018}, and the work of \\longcite{doshi-velez_rigorous_2017} who are often cited as base work for formalizing XAI terms (\\cf~\\forexample ). Some definitions are taken from \\longcite{bruckert2020next}, who present explanation as a process involving recognition, understanding and explicability respectively explainability, as well as interpretability.\nThe goal of the process is to achieve making an AI system's actions and decisions transparent as well as comprehensible.\nAn overview is given in Fig. \\ref{fig:cAI}.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{Bilder_interpretabilityVSexplainability.pdf}\n    \\caption{A framework for comprehensible artificial intelligence~.}\n    \\label{fig:cAI}\n\\end{figure}\nIn the following we will assume we are given an AI system that should (partly) be explained to a human. The system encompasses one (or several) AI models and any pre- and post-processing. We use the following nomenclature:\n\\begin{description}[font=\\normalfont\\bfseries]\n    \\item[Understanding] is described as the human ability to recognize correlations, as well as the context of a problem and is a necessary precondition for explanations . The concept of understanding can be divided into mechanistic understanding (\\emph{\"How does something work?\"}) and functional understanding (\\emph{\"What is its purpose?\"})~.\n    \\item[Explicability] refers to making properties of an AI model inspectable~.\n    \\item[Explainability] goes one step further than \\emph{explicability} and aims for making\n    (a)~the context of an AI system's reasoning,\n    (b)~the model, or\n    (c)~the evidence for a decision output accessible,\n    such that they can be \\emph{understood} by a human~.\n    \\item[Transparency] is fulfilled by an AI model, if its algorithmic behaviour with respect to decision outputs or processes can be \\emph{understood} by a human \\emph{mechanistically}~.\n    Transparency will be discussed more closely in \\autoref{sec:taxonomy.problem.interpretability}.\n    \\item[Explaining] means utilizing \\emph{explicability} or \\emph{explainability} to allow a human to \\emph{understand} a model and its purpose~.\n    \\item[Global explanations] \\emph{explain} the model and its logic as a whole (\\enquote{How was the conclusion derived?}).\n    \\item[Local explanations] \\emph{explain} individual decisions or predictions of a model (\\enquote{Why was this example classified as a car?}).\n    \\item[Interpretability] means that an AI model's decision can be \\emph{explained globally} or \\emph{locally} (with respect to \\emph{mechanistic understanding}), and that the model's purpose can be \\emph{understood} by a human actor (\\idest \\emph{functional understanding}).\n    \\item[Correctability] means that an AI system can be adapted by a human actor in a targeted manner in order to ensure correct decisions~.\n    Adaptation refers either to re-labelling of data~ or to changing of a model by constraining the learning process~.\n    \\item[Interactivity] applies if one of the following is possible:\n    (a)~interactive explanations, meaning a human actor can incrementally explore the internal working of a model and the reasons behind its decision outcome; or\n    (b)~the human actor may adapt the AI system (\\emph{correctability}).\n    \\item[Comprehensibility] relies, similar to \\emph{interpretability}, on local and global \\emph{explanations} and \\emph{functional understanding}. Additionally, \\emph{comprehensible} artificial intelligence fulfills \\emph{interactivity}~. \n    Both, \\emph{interpretable} presentation and intervention are considered as important aspects for in depth \\emph{understanding} and therefore preconditions to \\emph{comprehensibility} (see also~).\n    \\item[Human-AI system]\n    is a system that contains both algorithmic components and a human actor,\n    which have to cooperate to achieve a goal .\n    We here consider in specific \\textbf{explanation systems}, \\idest, such\n    human-AI systems in which the cooperation involves \\emph{explanations}\n    about an algorithmic part of the system (the \\emph{explanandum}) by\n    an \\emph{explanator} component, to the human interaction partner (the \\emph{explainee})\n    resulting in an action of the human~.\n    \\item[Explanandum]\n    (\\emph{what is to be explained}, \\cf \\autoref{sec:taxonomy.problem})\n    refers to what is to be \\emph{explained} in an \\emph{explanation system}.\n    This usually encompasses a model (\\forexample, a deep neural network),\n    We here also refer to an explanandum as the object of explanation.\n    \\item[Explanator]\n    (\\emph{the one that explains}, \\cf \\autoref{sec:taxonomy.explanator})\n    is the \\emph{explanation system} component providing \\emph{explanations}.\n    \\item[Explainee]\n    \\emph{(the one to whom the explanandum is explained)}\n    is the receiver of the \\emph{explanations} in the \\emph{explanation system}.\n    Note that this often but not necessarily is a human. \\emph{Explanations}\n    may also be used \\forexample, in multi-agent systems for communication\n    between the agents and without a human in the loop in most of the information\n    exchange scenarios.\n    \\item[Interpretable models] are defined as machine learning techniques that learn more structured representations, or that allow for tracing causal relationships. They are \\emph{inherently interpretable} (\\cf definition in  \\autoref{sec:taxonomy.explanator}), \\idest, no additional methods need to be applied to \\emph{explain them}, unless the structured representations or relationship are too complex to be processed by a human actor at hand.\n    \\item[Interpretable machine learning (iML)] is the area of research concerned with the creation of \\emph{interpretable} AI systems (\\emph{interpretable models}).\n    \\item[Model induction] (also called model distillation, student-teacher approach, or reprojection~) is a strategy that summarizes techniques which are used to infer an approximate \\emph{explainable} model---the (\\emph{explainable}) \\emph{proxy} or \\emph{surrogate model}---by observing the input-output behaviour of a model that is \\emph{explained}.\n    \\item[Deep explanation] refers to combining deep learning with other methods in order to create hybrid systems that produce richer representations of what a deep neural network has learned, and that enable extraction of underlying semantic concepts .\n    \\item[Comprehensible artificial intelligence (cAI)] is the result of a process that unites \\emph{local interpretability} based on \\emph{XAI} methods and \\emph{global interpretability} with the help of \\emph{iML} . The ultimate goal of such systems would be to reach \\emph{ultra-strong machine learning}, where machine learning helps humans to improve in their tasks. For example,  examined the \\emph{comprehensibility} of programs learned with Inductive Logic Programming, and  showed that the \\emph{comprehensibility} of such programs could help laymen to \\emph{understand} how and why a certain prediction was derived.\n    \\item[Explainable artificial intelligence (XAI)] is the area of research concerned with \\emph{explaining} an AI system's decision.\n\\end{description}", "cites": [1799], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key definitions from multiple foundational XAI papers and integrates them into a coherent framework, particularly around understanding, explainability, and interpretability. It abstracts these concepts into a structured taxonomy, providing a clear, unifying perspective. However, the critical analysis is limited, as it primarily presents definitions without evaluating their strengths, weaknesses, or inconsistencies across sources."}}
{"id": "dac278bc-b588-446a-b712-023d07d33fd0", "title": "Conceptual reviews", "level": "subsection", "subsections": [], "parent_id": "a480a2e5-d2ca-470e-a11f-7c9f851904be", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "A survey of surveys on XAI methods and aspects"], ["subsection", "Conceptual reviews"]], "content": "\\label{sec:sos.conceptual}\nBy now, many works have gathered and formulated important concepts related to XAI research.\nWe here roughly divided the available literature by their main focus:\nbroadly focused surveys, surveys from the perspectives of stakeholders and human-computer interaction,\nand finally surveys with an explicit focus on XAI metrics.\nAn overview can be found in \\autoref{tab:conceptualsurveys}.\n\\begin{table}\n    \\centering\n    \\caption{Overview on clusters of reviewed conceptual surveys on XAI}\n    \\label{tab:conceptualsurveys}\n    \\begin{tabulary}{\\linewidth}{@{}l @{~~}L @{}}\n        \\toprule\n        \\textbf{Focus} & \\\\\n        \\secseprule{2}\n        Broad &\n            \\longcite{gunning_xai_2019},\n            \\longcite{lipton_mythos_2018},\n            \\longcite{mueller_explanation_2019},\n            \\longcite{guidotti_principles_2021} \\\\\n\t    \\midrule\n\t    Stakeholder perspective & \n\t        \\longcite{gleicher_framework_2016},\n\t        \\longcite{langer_what_2021} \\\\\n\t    \\midrule\n        HCI perspective &\n            \\longcite{miller_explanation_2019},\n            \\longcite{chromik_taxonomy_2020},\n            \\longcite{ferreira_what_2020},\n            \\longcite{mueller_principles_2021} \\\\\n        \\midrule\n\t    XAI method evaluation &\n\t        \\longcite{doshi-velez_rigorous_2017},\n\t        \\longcite{zhou_evaluating_2021} \\\\\n\t    \\bottomrule\n    \\end{tabulary}\n\\end{table}\n\\paragraph*{Broad conceptual surveys}\nA very short, high-level, and beginner-friendly introduction to XAI can be found\nin the work of \\longcite{gunning_xai_2019}.\nThey derive four open challenges for the field: user-centric explanations, tradeoff between accuracy and interpretability, automated abstraction, and appropriate end-user trust. \nRegarding an XAI method taxonomy, a base for many later surveys was by\n\\longcite{lipton_mythos_2018}.\nIn this medium-length work, Lipton provides a small and high-level XAI taxonomy.\nThe focus, namely motivation and desiderata for interpretability, are held broad\nand suitable for beginners and interdisciplinary discussion.\nIn contrast, the very broad and long DARPA report by \\longcite{mueller_explanation_2019} provided later in 2019 is targeted at researchers of the field.\nThe goal of the report is to broadly capture all relevant developments and topics in the field of XAI.\nThis resulted in a detailed meta-study on state-of-the-literature, and\na detailed list of XAI method aspects and metrics (chapters 7 and 8).\nMore recently, \\longcite{guidotti_principles_2021} illustrate some key dimensions to distinguish XAI approaches in a beginner-friendly book section. They present a broad collection of most common explanation types and state-of-the-art explanators respectively, and discuss their usability and applicability. This results in a conceptual review of taxonomy aspects and evaluation criteria.\n\\paragraph*{XAI from a stakeholder perspective}\nThe early work by \\longcite{gleicher_framework_2016} highlights\nthe stakeholder perspective on XAI problems.\nIn the medium-length survey, Gleicher suggests a framework of general considerations\nfor practically tackling an explainability problem.\nA similar focus is set by \\longcite{langer_what_2021} in their long recent work from 2021.\nThey review in detail XAI from the point of view of satisfying stakeholder desiderata.\nFor this they collect standard goals of XAI problems and propagate that XAI design\nchoices should take into account all stakeholders in an interdisciplinary manner.\n\\paragraph*{XAI from a HCI perspective}\nWhen humans interact with AI-driven machines, this human-machine-system\ncan benefit from explanations obtained by XAI.\nHence, there are by now several surveys concentrating on XAI\nagainst the background of human-computer-interaction (HCI).\nAn important and well-received base work in this direction is that of\n\\longcite{miller_explanation_2019}.\nHe conducts a long, detailed, and extensive survey of work from philosophy, psychology, and\ncognitive science regarding explanations and explainability.\nThe main conclusions for XAI research are:\n(a)~(local) explanations should be understood contrastively,\n    \\idest, they should clarify why an action was taken instead of another;\n(b)~explanations are selected in a biased manner,\n    \\idest, do not represent the complete causal chain but few selected causes;\n(c)~causal links are more helpful to humans than probabilities and statistics; and\n(d)~explanations are social\n    in the sense that the background of the explanation receiver matters.\nA much shorter paper was provided by \\longcite{chromik_taxonomy_2020}. They rigorously develop a taxonomy for\nevaluating black-box XAI methods with the help of human subjects.\nFurthermore, they make some concrete suggestions for study design.\nThe related survey by \\longcite{ferreira_what_2020} is slightly longer, and\nmay serve as an entry point to the topic for researchers.\nThey present a taxonomy of XAI that links with computer science and HCI communities,\nwith a structured and dense collection of many related reviews.\nVery recent work on XAI metrics is provided by Müller \\etal\nin their 2021 paper .\nThey collect concrete and practical design principles for XAI in human-machine-systems.\nSeveral relevant XAI metrics are recapitulated, and their broad collection of\nrelated surveys may serve as an entry point to the research field of human-AI-systems.\n\\paragraph*{Surveys with a focus on evaluation}\nContinuing on the HCI perspective, a hot topic in the field of XAI are metrics for measuring the quality\nof explanations for human receivers.\nAn early and base work on XAI metric categorization is by \\longcite{doshi-velez_rigorous_2017}. The medium-length survey collects latent dimensions of\ninterpretability with recommendations on how to choose and evaluate an XAI method.\nTheir taxonomy for XAI metrics is adapted by us, classifying metrics into human grounded, functionally grounded, and application grounded ones (\\cf \\autoref{sec:taxonomy.metrics}).\nA full focus on metrics for evaluating XAI methods is set in the\nrecent work by \\longcite{zhou_evaluating_2021}.\nThis medium-length, detailed meta-study reviews diverse XAI metrics,\naligned with shallow taxonomies both for methods and metrics.\n\\FloatBarrier", "cites": [7506], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes conceptual XAI surveys by grouping them thematically and highlighting their contributions and distinctions. It abstracts broader patterns such as the shift from high-level overviews to stakeholder- and HCI-centric frameworks. Some critical analysis is present, particularly in identifying the practical limitations of algorithm-focused XAI, though deeper comparative critique across works is limited."}}
{"id": "6cd9ed1f-688f-4a42-9e7c-eef2c1adf602", "title": "Broad method collections", "level": "subsection", "subsections": [], "parent_id": "a480a2e5-d2ca-470e-a11f-7c9f851904be", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "A survey of surveys on XAI methods and aspects"], ["subsection", "Broad method collections"]], "content": "\\label{sec:sos.general}\nThere by now exists an abundance of surveys containing broad collections of XAI methods,\nserving as helpful starting points for finding the right method.\nThe following shortly highlights surveys that feature a generally broad focus\n(for specifically focused surveys see \\autoref{sec:sos.specific}).\nThis summary shall help to find a good starting point for diving deeper into methods.\nHence, the surveys are first sorted by their length (as a proxy for the amount of information),\nand then by their reception (citations per year).\nThe latter was found to strongly correlate with age.\n\\begin{table}\n    \\centering\n    \\caption{\n        Overview on focus points/specialties and audience (Aud.) of discussed general XAI method collections.\n        Sorted first by length, then reception (citations per year).\n        Detail is manually ranked in values from 1 (short discussion per method) to 5 (detailed discussion per method).\n        The audience can be beginner (B), practitioner (P), or researcher (R).\n    }\n    \\label{tab:generalsurveys}\n    \\footnotesize\n    \\begin{tabulary}{\\linewidth}{@{} l @{~}l@{~~} L@{}c @{}}\n    \\toprule\n    \t\t&\t\\textbf{Aud.}\t&\t\\textbf{Focus / Specialty}\t&\t\\textbf{Detail}\n\\\\\\secseprule{4}\\multicolumn{4}{@{}l}{\\textbf{Short}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\\\\ \\midrule \\longcite{dosilovic_explainable_2018}\t&\tB, R\t&\tXAI for supervised learning\t&\t1\n\t\\\\ \\midrule \\longcite{biran_explanation_2017}\t&\tR\t&\tLocal explanations and interpretable models\t&\t1\n\t\\\\ \\midrule \\longcite{benchekroun_need_2020}\t&\tB, P\t&\tIndustry point of view\t&\t5\n\\\\\\secseprule{4}\\multicolumn{4}{@{}l}{\\textbf{Medium}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\\\\\\midrule \\longcite{gilpin_explaining_2018}\t&\tR\t&\tXAI research aspects\t&\t1\n\t\\\\ \\midrule\t\\longcite{du_techniques_2019}\t&\tB\t&\tPerturbation based attention visualization\t&\t3\n\t\\\\ \\midrule \\longcite{murdoch_definitions_2019}\t&\tB, P\t&\tPredictive, descriptive, relevant desiderata for explanations, practical examples\t&\t5\n\t\\\\ \\midrule \\longcite{goebel_explainable_2018}\t&\t\t&\tNeed for XAI\t&\t3\n\t\\\\ \\midrule \\longcite{islam_explainable_2021}\t&\tB, R\t&\tDemonstration of XAI methods in case-study (credit scoring)\t&\t5\n\\\\\\secseprule{4}\\multicolumn{4}{@{}l}{\\textbf{Long}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\\\\ \\midrule \\longcite{adadi_peeking_2018}\t&\tP\t&\tXAI research landscape (methods, metrics, cognitive aspects, human-machine-systems)\t&\t1\n\t\\\\ \\midrule \\longcite{carvalho_machine_2019}\t&\tB, R\t&\tAspects associated with XAI (esp.\\ motivation, properties, desirables)\t&\t2\n\t\\\\ \\midrule \\longcite{xie_explainable_2020}\t&\tB\t&\tExplanation of DNNs\t&\t3\n\t\\\\ \\midrule \\longcite{das_opportunities_2020}\t&\tR\t&\tVisualization methods for visual DNNs\t&\t3\n\t\\\\ \\midrule \\longcite{linardatos_explainable_2021}\t&\tP, R\t&\tTechnical, with source code\t&\t3\n\t\\\\ \\midrule \\longcite{bodria_benchmarking_2021}\t&\tB, P\t&\tComparative studies of XAI methods\t&\t4\n\\\\\\secseprule{4}\\multicolumn{4}{@{}l}{\\textbf{Very long}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\\\\ \\midrule \\longcite{molnar_interpretable_2020}\t&\tB\t&\tFundamental concepts of interpretability\t&\t5\n\t\\\\ \\midrule \\longcite{arrieta_explainable_2020}\t&\tP, R\t&\tNotion of responsible AI: Terminology, broad collection of examples\t&\t1\n\t\\\\ \\midrule \\longcite{burkart_survey_2021}\t&\tR\t&\tSupervised machine learning; linear, rule-based methods and decision trees, data analytics and ontologies\t&\t3\n\t\\\\ \\midrule \\longcite{vilone_explainable_2020}\t&\tR\t&\tSystematic and broad review of XAI surveys, theory, methods, and method evaluation\t&\t3\n\t\\\\\\bottomrule\n    \\end{tabulary}\n\\end{table}\n\\paragraph*{Short surveys}\nA short overview mostly on explanations of supervised learning approaches\nwas provided by Došilović \\etal in 2018\t.\nThis quite short but broad and beginner-friendly introductory survey\nshortly covers diverse approaches and open challenges towards XAI.\nSlightly earlier in 2017, Biran and Cotton published their XAI survey .\nThis is a short and early collection of explainability concepts. Their general focus\nlies on explanations of single predictions, and diverse model types like rule-based\nsystems and Bayesian networks, which are each shortly discussed.\nMost recently, in 2020 Benchekroun \\etal collected and presented XAI methods\nfrom an industry point of view .\nThey present a preliminary taxonomy that includes pre-modelling explainability as\nan approach to link knowledge about data with knowledge about the used model and its results.\nRegarding the industry perspective, they specifically motivate standardization.\n\\paragraph*{Medium-length surveys}\nAn important and very well received earlier work on XAI method collections was provided\nby \\longcite{gilpin_explaining_2018}.\nTheir extensive survey includes very different kinds of XAI methods, including, e.g., rule extraction.\nIn addition, for researchers they provide references to further more specialized surveys in the field.\nIt is very similar to the long survey by \\longcite{adadi_peeking_2018}, only shortened by skipping detail on the methods.\nMore detail, but a slightly more specialized focus, is provided in the survey by \\longcite{du_techniques_2019}.\nThe beginner-friendly high-level introduction to XAI features few, in detail discussed examples.\nThese concentrate on perturbation based attention visualization.\nSimilarly, the examples in  also mostly focus on\nvisual domains and explanations. This review by Murdoch \\etal in 2019 is also beginner-friendly,\nand prefers detail over covering a high number of methods. The examples are embedded into a comprehensive short\nintroductory review of key categories and directions in XAI.\nTheir main focus is on the proposal of three simple practical desiderata for explanations:\nThe model to explain should be predictive (predictive accuracy),\nthe explanations should be faithful to the model (descriptive accuracy), and\nthe information presented by the explanations should be relevant to the receiver.\nSlightly earlier, \\longcite{goebel_explainable_2018} concentrate in their work more on multimodal explanations and question-answering systems.\nThis survey contains a high-level review of the need for XAI, and discussion of some\nexemplary state-of-the-art methods.\nA very recent and beginner-friendly XAI method review is by \\longcite{islam_explainable_2021}. Their in-depth discussion of example methods is aligned with\na shallow taxonomy, and many examples are practically demonstrated in a common simple case study.\nAdditionally, they present a short meta-study of XAI surveys, and a collection of future perspectives for XAI.\nThe latter includes formalization, XAI applications for fair and accountable ML, XAI for human-machine-systems,\nand more interdisciplinary cooperation in XAI research.\n\\paragraph*{Long surveys}\nA well-received lengthy and extensive XAI literature survey was conducted by\n\\longcite{adadi_peeking_2018}.\nThey reviewed and shortly discuss 381 papers related to XAI to provide a holistic\nview on the XAI research landscape at that time. This includes methods, metrics,\ncognitive aspects, and aspects related to human-machine-systems.\nAnother well-received, but more recent, slightly less extensive and more\nbeginner-friendly survey is that by \\longcite{carvalho_machine_2019}.\nThey collect and discuss in detail different aspects of XAI, especially motivation,\nproperties, desirables, and metrics. Each aspect is accompanied by some examples.\nSimilarly beginner-friendly is the introductory work of \\longcite{xie_explainable_2020}.\nTheir field guide explicitly targets newcomers to the field with a general\nintroduction to XAI, and a wide variety of examples of standard methods,\nmostly for explanations of DNNs.\nA more formal introduction is provided by \\longcite{das_opportunities_2020} in their survey.\nThey collect formal definitions of XAI related terms, and develop a shallow taxonomy.\nThe focus of the examples is on visual local and global explanations of DNNs based on\n(model-specific) backpropagation or (model-agnostic) perturbation-based methods.\nMore recent and much broader is the survey of \\longcite{linardatos_explainable_2021}.\nIt provides an extensive technical collection and review of XAI methods with code\nand toolbox references, which makes it specifically interesting for practitioners.\nSimilarly and in the same year, \\longcite{bodria_benchmarking_2021} review in detail more than 60 XAI methods for visual, textual, and tabular data models.\nThese are selected to be most recent and widely used, and cover a broad range of explanation types. Several comparative benchmarks of methods are included, as well as a short review of toolboxes.\n\\paragraph*{Very long surveys}\nBy now there are a couple of surveys available that aim to give a broad, rigorous, and\nin-depth introduction to XAI.\nA first work in this direction is the regularly updated book on XAI by\n\\longcite{molnar_interpretable_2020}, first published in 2017\\footnote{\\url{https://github.com/christophM/interpretable-ml-book/tree/v0.1}}.\nThis book is targeted at beginners, and gives a basic and detailed introduction on\ninterpretability methods, including many transparent and many model-agnostic ones.\nThe focus lies more on fundamental concepts of interpretability and\ndetail on standard methods than on the amount of discussed methods.\nMeanwhile, \\longcite{arrieta_explainable_2020} put up a very long and broad collection of XAI methods\nin their 2020 review.\nThe well-received survey can also be considered a base work of state-of-the-art XAI,\nas they introduce the notion of \\emph{responsible AI}, \\idest, development of AI models\nrespecting fairness, model explainability, and accountability.\nThis is also the focus of their work, in which they provide terminology,\na broad but less detailed selection of example methods, and practical discussion\nfor responsible AI.\nA very recent extensive XAI method survey is that of \\longcite{burkart_survey_2021}. They review in moderate detail explainability\nmethods, primarily for classification and regression in supervised machine learning.\nSpecifically, they include many rule-based and decision-tree based explanation methods,\nas well as aspects on data analysis and ontologies for formalizing input domains.\nThis is preceded by a deep collection of many general XAI aspects.\nFinally and a little earlier, \\longcite{vilone_explainable_2020} published in 2020 an\nequally extensive systematic literature survey on XAI in general.\nThe systematic literature search was similar to ours, only with a different focus.\nThey include a broad meta-study of reviews, as well as reviews and discussion of works on\nXAI theory, methods, and method evaluations.\n\\FloatBarrier", "cites": [7508], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various XAI surveys, categorizing them by length and reception. It synthesizes some level of information by grouping surveys and briefly describing their focus and audience. However, it lacks deeper critical evaluation of the works and does not abstract or generalize beyond the specific attributes of the surveys listed, making the insights more factual than analytical."}}
{"id": "865a0ee3-ba29-4ef4-b2fe-0107b5b43f58", "title": "Method collections with specific focus", "level": "subsection", "subsections": [], "parent_id": "a480a2e5-d2ca-470e-a11f-7c9f851904be", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "A survey of surveys on XAI methods and aspects"], ["subsection", "Method collections with specific focus"]], "content": "\\label{sec:sos.specific}\nBesides the many broad method collections, there by now are numerous ones specifically\nconcentrating on an application domain, specific input or task types, certain surrogate model types,\nor other traits of XAI methods.\nWe here manually clustered surveys by similarity of their main focus points.\nAn overview on the resulting clusters is given in \\autoref{tab:specificsurveys}.\n\\begin{table}\n    \\centering\n    \\footnotesize\n    \\caption{Overview on surveys clusters reviewed \\autoref{sec:sos.specific} with restricted (restr.) focus}\n    \\label{tab:specificsurveys}\n    \\begin{tabularx}{\\linewidth}{@{}>{\\raggedright}p{6em} >{\\raggedright}p{8.5em} X @{}}\n        \\toprule\n        \\textbf{Restr. by:} & \\textbf{Restr. to:} & \\\\\n        \\secseprule{3}\n        Application domain\n                      & NLP & \\longcite{danilevsky_survey_2020} \\\\\\cmidrule{2-3}\n\t                  & Medicine & \\longcite{singh_explainable_2020,tjoa_survey_2020} \\\\\\cmidrule{2-3}\n\t                  & Recommendation systems & \\longcite{nunes_systematic_2017,zhang_explainable_2020} \\\\\n\t    \\midrule\n\t    Application type\n\t                  & Interactive ML & \\longcite{anjomshoae_explainable_2019,baniecki_grammar_2020,amershi_power_2014} \\\\\n\t    \\midrule\n        Task          \n                      & Visual tasks  &  \\longcite{samek_explainable_2019a,samek_explainable_2019,nguyen_understanding_2019,ancona_gradientbased_2019,alber_software_2019,li_quantitative_2020,zhang_visual_2018}\\\\\\cmidrule{2-3}\n                      & Reinforcement ML &  \\longcite{puiutta_explainable_2020,heuillet_explainability_2021} \\\\\n        \\midrule\n\t    Explanator output type\n\t                  & Rule-based XAI & \\longcite{cropper_turning_2020,vassiliades_argumentation_2021,hailesilassie_rule_2016,calegari_integration_2020} \\\\\\cmidrule{2-3}\n\t                  & Counterfactual explanations & \\longcite{byrne_counterfactuals_2019,artelt_computation_2019,verma_counterfactual_2020,keane_if_2021,mazzine_framework_2021,karimi_survey_2021,stepin_survey_2021} \\\\\n\t    \\midrule\n\t    Other XAI method traits\n\t                  & Model-agnostic methods &  \\longcite{guidotti_survey_2018} \\\\\n        \\bottomrule\n    \\end{tabularx}\n\\end{table}\n\\paragraph*{XAI for specific application domains}\nSome method surveys focus on concrete practical application domains.\nOne is by \\longcite{danilevsky_survey_2020}, who survey\nXAI methods for \\emph{natural language processing} (NLP). This includes a taxonomy, a\nreview of several metrics, and a dense collection of XAI methods.\nAnother important application domain is the \\emph{medical domain}.\nFor example, \\longcite{singh_explainable_2020} provided a survey and taxonomy of XAI methods for image classification with a focus on medical applications.\nA slightly broader focus on general XAI methods for medical applications was selected by \\longcite{tjoa_survey_2020} in the same year.\nTheir long review shortly discusses more than sixty methods, and sorts them into a shallow taxonomy.\nAnother domain sparking needs for XAI is that of \\emph{recommendation systems},\n\\forexample, in online shops.\nAn example here is the long, detailed, and practically oriented survey by \\longcite{nunes_systematic_2017}. Amongst others, they present a detailed taxonomy\nof XAI methods for recommendation systems (\\cf \\cite[Fig.\\,11]{nunes_systematic_2017}).\nA similar, but even more extensive and lengthy survey was provided by\n\\longcite{zhang_explainable_2020}.\nThey generally review recommendation systems also in a practically oriented manner,\nand provide a good overview on models that are deemed explainable.\n\\paragraph*{XAI for interactive ML applications}\nSeveral studies concentrate on XAI to realize interactive machine learning.\nFor example, \\longcite{anjomshoae_explainable_2019} reviewed explanation generation, communication and\nevaluation for autonomous agents and human-robot interaction.\n\\longcite{baniecki_grammar_2020} instead directly concentrated on interactive machine learning\nin their medium-length survey on the topic.\nThey present challenges in explanation, traits to overcome these, as well as a taxonomy\nfor interactive explanatory model analysis.\nThe longer but earlier review by \\longcite{amershi_power_2014}\nmore concentrates on practical case studies and research challenges.\nThey motivate incremental, interactive and practical human-centered XAI methods.\n\\paragraph*{XAI for visual tasks}\nSome of the earlier milestones for the current field of XAI were methods\nto explain input importance for models with image inputs ,\nsuch as LIME~ and LRP~.\nResearch on interpretability and explainability of models for visual tasks is still\nvery active, as several surveys with this focus show.\nOne collection of both methods and method surveys with a focus on visual explainability\nis the book edited by \\longcite{samek_explainable_2019a}.\nThis includes the following surveys:\n\\begin{itemize}\n    \\item \\longcite{samek_explainable_2019}:\n    A short introductory survey on visual explainable AI for researchers, giving an overview on important developments;\n    \\item \\longcite{nguyen_understanding_2019}:\n    A survey specifically on feature visualization methods.\n    These are methods to find prototypical input patterns that most activate parts of a DNN.\n    The survey includes a mathematical perspective on the topic and a practical overview on applications.\n    \\item \\longcite{ancona_gradientbased_2019}:\n    A detailed survey on gradient-based methods to find attribution of inputs to outputs; and\n    \\item \\longcite{alber_software_2019}:\n    A detailed collection of implementation considerations regarding different methods\n    for highlighting input attribution.\n    The review includes code snippets for the TensorFlow deep learning framework.\n\\end{itemize}\nSimilar to \\longcite{ancona_gradientbased_2019},\n\\longcite{li_quantitative_2020} focus on XAI methods to obtain\nheatmaps as visual explanations. They in detail discuss seven examples of methods,\nand conduct an experimental comparative study with respect to five specialized metrics.\nAnother survey focusing on visual interpretability is the earlier work\nby \\longcite{zhang_visual_2018}. This well-received medium-length\nsurvey specializes on visual explanation methods for convolutional neural networks.\n\\paragraph*{XAI for reinforcement learning tasks}\nJust as for visual tasks, there are some studies specifically focusing on\nexplanations in tasks solved by reinforcement learning.\nOne is that by \\longcite{puiutta_explainable_2020}.\nThis medium long to lengthy review provides a short taxonomy on XAI methods for reinforcement learning.\nIt reviews more than 16 methods specific to reinforcement learning in a beginner-friendly way.\nA comparable and more recent, but slightly longer, more extensive, and more technical\nsurvey on the topic is by \\longcite{heuillet_explainability_2021}.\n\\paragraph*{XAI methods based on rules}\nOne type of explanation outputs is that of (formal) symbolic rules.\nBoth generation of interpretable rule-based models, as well as extraction of\napproximate rule sets from less interpretable models have a long history.\nWe here collect some more recent reviews on these topics.\nOne is the historical review by \\longcite{cropper_turning_2020} on the developments in Inductive Logic Programming. Inductive logic programming summarizes methods to automatically construct\nrule-sets for solving a task given some formal background knowledge and few examples.\nThe mentioned short survey aims to look back at the last 30 years of development in the field,\nand serve as a good starting point for beginners.\nOn the side of inherently interpretable rule-based models, \\longcite{vassiliades_argumentation_2021} recently \nreviewed argumentation frameworks in detail.\nAn argumentation framework provides an interpretable logical argumentation line that formally\ndeduces a statement from (potentially incomplete) logical background knowledge.\nThis can, \\forexample, be used to find the most promising statement from some choices,\nand, hence, as an explainable (potentially interactive) model on symbolic data.\nThe long, detailed, and extensive survey formally introduces standard argumentation frameworks,\nreviews existing methods and applications, and promotes argumentation frameworks as\npromising interpretable models.\nWhile the previous surveys concentrate on directly training inherently\ninterpretable models consisting of rules, \\longcite{hailesilassie_rule_2016} shortly reviewed rule extraction methods.\nRule extraction aims to approximate a trained model with symbolic rule sets or decision trees.\nThese methods have a long history but faced their limits when applied to large-sized models\nlike state-of-the-art neural networks, as discussed in the survey.\nA more recent survey that covers both rule extraction as well as integration of symbolic\nknowledge into learning processes is the review by\n\\longcite{calegari_integration_2020}.\nTheir long and in-depth overview covers the main symbolic/sub-symbolic integration\ntechniques for XAI, including rule extraction methods for some steps.\n\\paragraph*{Counterfactual and contrastive explanations}\nWhile the previous surveys were mostly concentrated on a specific type of task,\nthere are also some that are restricted to certain types of explainability methods.\nOne rising category is that of contrastive and counterfactual explanations (also \\emph{counterfactuals}).\nThe goal of these is to explain for an instance \\enquote{why was the output $P$ rather than $Q$?} , and, in particular for counterfactual explanations, how input features can be changed to achieve $Q$ . To do this, one or several other instances are provided to the user that produce the desired output.\nOne is the short survey by \\longcite{byrne_counterfactuals_2019}.\nThis reviews specifically counterfactual explanations with respect to evidence from human reasoning.\nThe focus here lies on additive and subtractive counterfactual scenarios.\nAlso as early as 2019, \\longcite{artelt_computation_2019} provide a beginner-friendly review on model-specific counterfactual explanation methods. They consider a variety of standard ML models, and provide detailed mathematical background for each of them.\nA more broad survey on counterfactuals was conducted by \\longcite{verma_counterfactual_2020}. The mid-length survey reviews 39 methods and discusses common desiderata, a taxonomy, evaluation criteria, and open challenges for this XAI subtopic. In particular, they suggest the following desiderata:\ncounterfactual examples should be valid inputs that are similar to the training data;\nthe example should be as similar as possible to the original (proximity), while changing as few features as possible (sparsity);\nfeature changes should be actionable, \\idest, the explainee should be able to achieve them (\\forexample, increase age, not decrease);\nand they should be \\emph{causal}, acknowledging known causal relationships in the model.\nLater studies confirm these desirables.\nIn particular, \\longcite{keane_if_2021} review in total 100 methods with respect to common motivations for counterfactual explanations and typical shortcomings thereof, in order to guide researchers. In their short survey, they find that better psychological grounding of counterfactuals as well as their evaluation is required, in specific for validity and feature selection. Also, methods up to that point in time are often missing user studies and comparative tests.\nA closer look at how to tackle comparative studies was taken by \\longcite{mazzine_framework_2021} in their extensive survey.\nThey benchmarked open source implementations of 10 strategies for counterfactual generation for DNNs on 22 different tabular datasets. The controlled experimental environment may serve as a role model for researchers for future evaluations.\nIn contrast to functional aspects of counterfactual methods, \\longcite{karimi_survey_2021} in parallel focused on the use-case perspective: The mid-length survey highlights the impact of the mentioned desiderata on the use-case of algorithmic recourse.\nAlgorithmic recourse here means to provide explanations and actionable recommendations for individuals who encountered an unfavorable treatment by an automated decision-making system.\nLastly, the broad and extensive survey by \\longcite{stepin_survey_2021} by unites research perspectives on conceptual and methodological work for advanced researchers.\nThey rigorously survey 113 studies on counterfactual and contrastive explanations reaching back as far as the 1970s. The reader is provided with a review of terms and definitions used throughout the primary literature, as well as a detailed taxonomy. The obtained conceptual insights are then related to the methodological approaches.\n\\paragraph*{Model-agnostic XAI methods}\nA very well received, long and extensive survey for model-agnostic XAI methods on tabular data is by\n\\longcite{guidotti_survey_2018}.\nBesides the method review, they also develop a formal approach to define XAI use-cases\nthat is especially useful for practitioners.\n\\FloatBarrier", "cites": [7507, 7509, 1800], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section organizes and groups XAI surveys by their specific focus areas, providing a structured overview. While it connects related works thematically, it largely summarizes each survey without deeper analysis or synthesis into a novel framework. There is limited critique or identification of broader trends beyond the described categorization."}}
{"id": "86359e9f-3bad-42a0-8497-843d47b39288", "title": "Toolboxes", "level": "subsection", "subsections": [], "parent_id": "a480a2e5-d2ca-470e-a11f-7c9f851904be", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "A survey of surveys on XAI methods and aspects"], ["subsection", "Toolboxes"]], "content": "\\label{sec:sos.toolbox}\nA single XAI method often does not do the full job of making all relevant\nmodel aspects clear to the explainee.\nHence, toolboxes have become usual that implement more than one explainability\nmethod in a single library with a common interface.\nFor detailed lists of available toolboxes the reader is referred to, e.g.,\nthe related work in \\cite[Tab.\\,1]{arya_one_2019}, the repository links in\n\\cite[Tabs.\\,A.1, A.2]{linardatos_explainable_2021}, and the toolbox review in \\cite[Sec.\\,7]{bodria_benchmarking_2021}.\nFor implementation considerations in the case of visual interpretability\nthe review  is a suitable read.\nWe here provide some examples of publications presenting toolboxes published since 2019 that we analyzed for taxonomy aspects, summarized in \\autoref{tab:toolboxes}.\n\\begin{table}\n    \\centering\n    \\caption{Overview on the sub-selection of papers introducing explainability toolboxes that were considered in \\autoref{sec:sos.toolbox} with their respective code repository;\n    more extensive overviews can be found, e.g., in\n    \\cite[Tab.\\,1]{arya_one_2019}, \\cite[Tabs.\\,A.1, A.2]{linardatos_explainable_2021}, and \\cite[Sec.\\,7]{bodria_benchmarking_2021}.\n    }\n    \\label{tab:toolboxes}\n    \\begin{tabular}{@{}l l l @{}}\n        \\toprule\n        \\textbf{Toolbox} & \\textbf{Publication} \n        & \\textbf{Code repository} \\\\\n        \\secseprule{3}\n        Skater & \\longcite{choudhary_interpreting_2018} \n        & \\url{https://github.com/oracle/Skater} \\\\\n        InterpretML & \\longcite{nori_interpretml_2019} \n        & \\url{https://github.com/interpretml/interpret} \\\\\n        iNNvestigate & \\longcite{alber_innvestigate_2019} \n        & \\url{https://github.com/albermax/innvestigate} \\\\\n        AI Fairness 360 & \\longcite{arya_one_2019} \n        & \\url{https://github.com/Trusted-AI/AIF360} \\\\\n        explAIner & \\longcite{spinner_explainer_2020} \n        & \\url{https://github.com/dbvis-ukon/explainer} \\\\\n        FAT Forensics & \\longcite{sokol_fat_2020} \n        & \\url{https://github.com/fat-forensics/fat-forensics} \\\\\n        Alibi & \\longcite{klaise_alibi_2021} \n        & \\url{https://github.com/SeldonIO/alibi} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\nAlready in 2018, first toolboxes like Skater  were available for several XAI tasks. Skater, in specific, provides in total seven XAI methods for both global and local explanation of different kinds of trained models, including DNNs for visual and textual inputs.\nThe beginner-friendly Microsoft toolbox InterpretML~\nimplements five model-agnostic and four transparent XAI methods\nthat are shortly introduced in the paper.\nAnother toolbox from that year is iNNvestigate by \\longcite{alber_innvestigate_2019}.\nThey specifically concentrate on some standard post-hoc heatmapping methods for visual explanation.\n\\longcite{arya_one_2019} presented the IBM AI explainability 360 toolbox\nwith 8 diverse XAI methods in 2019.\nThe implemented workflow follows a proposed practical, tree-like taxonomy of XAI methods.\nImplemented methods cover a broad range of explainability needs, including\nexplainability of data, inherently interpretable models, and post-hoc explainability\nboth globally and locally.\nMore recently, \\longcite{spinner_explainer_2020} presented their toolbox explAIner.\nThis is realized as a plugin to the existing TensorBoard\\footnote{TensorBoard toolkit: \\url{https://www.tensorflow.org/tensorboard}} toolkit for the TensorFlow deep learning framework.\nThe many post-hoc DNN explanation and analytics tools are aligned with the suggested\npipeline phases of model understanding, diagnosis, and refinement.\nTarget users are both researchers, practitioners, and beginners.\nIn 2020 also the FAT Forensics (for Fairness, Accountability, Transparency) toolbox was published . Based on scikit-learn, several applications towards model and data quality checks and local and global explainability are implemented.\nThe focus is on black-box methods, providing a generic interface for two use-cases: research on new fairness metrics (for researchers) and monitoring of ML models during pre-production (for practitioners).\nAt the time of writing, methods for image and tabular data are supported.\nThe most recent toolbox considered here is the Alibi explainability kit by \\longcite{klaise_alibi_2021}. It features nine diverse, mostly black-box XAI methods for classification and regression. This includes both local and (for tabular data) global state-of-the-art analysis methods for image or tabular input data, with some additionally supporting textual inputs.\nAlibi targets practitioners, aiming to be an extensively tested, production-ready, scalable, and easy to integrate toolbox for explanation of machine learning models.", "cites": [1801, 7510], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of various XAI toolboxes and their features, citing specific examples and linking them to code repositories. While it synthesizes information by grouping the toolboxes with brief descriptions, it lacks deeper comparative analysis or evaluation of their strengths and weaknesses. Some abstraction is present in mentioning common functionalities, but no overarching framework or novel insights are derived."}}
{"id": "3f42b222-2a41-470c-8df8-bc7b124ec494", "title": "Task type", "level": "paragraph", "subsections": [], "parent_id": "05ac423b-a15c-4f44-b1c6-a1b4319bc484", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Task"], ["paragraph", "Task type"]], "content": "\\label{sec:taxonomy.problem.task.type}\nTypical task categories are\nunsupervised clustering (clu),\nregression,\nclassification (cls),\ndetection (det),\nsegmentation (seg), either semantic, which is pixel-wise classification,\nor segmentation of instances.\nMany XAI methods that target a question for classification,\n\\forexample, \\enquote{Why this class?}\ncan be extended to det, seg, and temporal resolution.\nThis can be achieved by snippeting of the new dimensions:\n\\enquote{Why this class in this spatial/temporal snippet?}.\nIt must be noted that XAI methods working on classifiers often require\naccess to the prediction of a continuous classification score instead of the\nfinal discrete classification. Such methods can also be used on regression\ntasks to answer questions about local trends, \\idest,\n\\enquote{Why does the prediction tend in this direction?}.\nExamples of regression predictions are bounding box dimensions in object detection.\n\\begin{examples}\n    \\examplemethod{RISE~}\n    RISE (Randomized Input Sampling for Explanation) by \\longcite{petsiuk_rise_2018}\n    is a model-agnostic attribution analysis method specializing in image classification.\n    For an input image, it produces a heatmap that highlights those superpixels in the image,\n    which, when deleted, have the greatest influence on the class confidence.\n    High-attribution superpixels are found by randomly dimming superpixels of the input image.\n    \\examplemethod{D-RISE~}\n    The method D-RISE by \\longcite{petsiuk_blackbox_2021} extends this to object detection.\n    It considers not a one-dimensional class confidence but the total prediction vector of a detection. The influence of dimming is measured as the distance between the prediction vectors.\n    \\examplemethod{ILP~}\n    The mentioned image-specific (\\idest, local) explanation methods use the\n    continuous class or prediction scores of the explanandum,\n    and, hence, are in principle also applicable to regressors.\n    In contrast, surrogate models produced using inductive logic programming (ILP)  require the binary classification output of a model.\n    ILP frameworks require input background knowledge (logical theory),\n    together with positive and negative examples. From this, a logic program in the form\n    of first-order rules is learned that covers as many of the samples as possible.\n    \\examplemethod{CA-ILP~}\n    An example of an ILP-based XAI method for convolutional image classifier is CA-ILP (Concept Analysis for ILP) by \\longcite{rabold_expressive_2020}. In order to explain parts of the classifier with logical rules,\n    they first train small global models that extract symbolic features from the DNN intermediate outputs.\n    These feature outputs are then used to train an ILP surrogate model.\n    Lastly, clustering tasks can often be explained by providing examples or prototypes\n    of the final clusters, which will be discussed in \\autoref{sec:taxonomy.explanator}.\n\\end{examples}", "cites": [1802, 1803, 7511], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple XAI methods by linking their applicability to different task types, such as classification, detection, and segmentation, and explains how methods like RISE and D-RISE extend across these. It also provides a critical view by highlighting differences in method requirements (e.g., continuous vs. binary outputs), and abstracts by discussing broader principles of adaptability and generalization of XAI techniques. This analytical approach identifies patterns and conditions under which methods can be applied, contributing to a more meta-level understanding of XAI."}}
{"id": "d6a2199e-e164-4cc6-bd71-5296a7f2d3c8", "title": "Input data type", "level": "paragraph", "subsections": [], "parent_id": "05ac423b-a15c-4f44-b1c6-a1b4319bc484", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Task"], ["paragraph", "Input data type"]], "content": "\\label{sec:taxonomy.problem.task.data}\nNot every XAI method supports every input and output \\emph{signal type},\nalso called data type~. One input type is\ntabular (symbolic) data, which encompasses numerical, categorical, binary, and ordinary (ordered) data.\nOther symbolic input types are natural language or graphs, and\nnon-symbolic types are images and point clouds (with or without temporal resolution),\nas well as audio.\n\\begin{examples}\n    Typical examples for image explanations are methods producing heatmaps.\n    These highlight parts of the image that were relevant to the decision or a part thereof.\n    This highlighting of input snippets can also be applied to textual inputs\n    where single words or sentence parts may serve as snippets.\n    \\examplemethod{LIME~}\n    A prominent example of heatmapping that is both applicable to images and text inputs\n    is the model-agnostic LIME~ method\n    (Local Interpretable Model-agnostic Explanations).\n    It locally approximates the explanandum model by a linear model on feature snippets of\n    the input. For training of that linear model, randomly selected snippets are removed.\n    In the case of textual inputs, the words are considered as snippets, and for images pixels are grouped into superpixels.\n    The removal of superpixels is here realized by coloring them with a neutral color, e.g., black.\n    While LIME is suitable for image or textual input data,\n    \\longcite{guidotti_survey_2018} provide a broad overview of model-agnostic XAI methods\n    for tabular data.\n\\end{examples}", "cites": [7507, 7303], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information about input data types in XAI and integrates the LIME method from Paper 1 with the broader context of model-agnostic methods for tabular data from Paper 2. It abstracts beyond individual papers by categorizing input types and discussing their implications for explanation methods. While it provides a clear analytical perspective on how different data types influence explanation approaches, it offers limited critical evaluation of the limitations or trade-offs of these methods."}}
{"id": "c1fac81b-b9e1-48c0-b4a9-baa24865f6a6", "title": "Model interpretability", "level": "subsubsection", "subsections": ["db103d41-acd8-4389-95f7-4e14ffd105ca", "e07b35d1-b96b-4678-aeb3-7fcad6eab983", "28e83527-9c84-4061-915f-9357d3910e47", "41e8ed7e-ba16-48f8-8df2-f1f87ea34a49"], "parent_id": "e4e707c0-92c6-4c7c-bb00-5c1e8e2e6ac2", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Model interpretability"]], "content": "\\label{sec:taxonomy.problem.interpretability}\nModel interpretability here refers to the level of interpretability of the explanandum,\n\\idest, the model used to solve the original task of the system.\nA model is interpretable if it gives rise not only to mechanistic understanding (transparency) but also to a functional understanding by a human~.\nExplainability of (aspects of) the explanation system can be achieved by one of the following choices:\n\\begin{itemize}\n    \\item start from the beginning with an \\emph{\\hyperref[sec:taxonomy.problem.interpretability.intrinsic]{intrinsically interpretable}} explanandum model\n    (also called \\emph{ante-hoc interpretable}~ or \\emph{intrinsically interpretable}) or a\n    \\item \\emph{\\hyperref[sec:taxonomy.problem.interpretability.blended]{blended}}, \\idest, partly interpretable model\n    (also called \\emph{interpretable by design} );\n    \\item design the explanandum model to include  \\emph{\\hyperref[sec:taxonomy.problem.interpretability.self]{self-explanations}} as additional output; or\n    \\item \\emph{\\hyperref[sec:taxonomy.problem.interpretability.posthoc]{post-hoc}}\n    find an interpretable helper model without changing the trained explanandum model.\n\\end{itemize}", "cites": [1799, 1804], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of model interpretability by integrating definitions and distinctions from the cited papers, presenting a coherent and structured classification of interpretability approaches. It abstracts these ideas into a broader framework by categorizing models into intrinsic, blended, self-explanatory, and post-hoc. However, the analysis remains largely descriptive in tone with limited critical evaluation of the approaches or identification of deeper limitations in the cited works."}}
{"id": "db103d41-acd8-4389-95f7-4e14ffd105ca", "title": "Intrinsic or inherent interpretability", "level": "paragraph", "subsections": [], "parent_id": "c1fac81b-b9e1-48c0-b4a9-baa24865f6a6", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Model interpretability"], ["paragraph", "Intrinsic or inherent interpretability"]], "content": "\\label{sec:taxonomy.problem.interpretability.intrinsic}\nAs introduced by \\longcite{lipton_mythos_2018},\none can further differentiate between different levels of model transparency.\nThe model can be understood as a whole, \\idest, a human can adopt it as a mental model \n(\\emph{simulatable}~).\nAlternatively, it can be split up into simulatable parts\n(\\emph{decomposable}~).\nSimulatability can either be measured based on the size of the model\nor based on the needed length of computation (\\cf discussion of metrics in \\autoref{sec:taxonomy.metrics}).\nAs a third category, \\emph{algorithmic transparency} is considered,\nwhich means the model is mathematically understood,\n\\forexample, the shape of the error surface is known.\nThis is considered the weakest form of transparency, because the algorithm may not be simulatable as a mental model.\nThe following models are considered inherently transparent in the literature\n(\\cf \\cite[Chap.~4]{molnar_interpretable_2020}, \\cite[Sec.~5]{guidotti_survey_2018},\n):\n\\begin{subparagraphs}\n    \\item[Decision tables and rules] as experimentally evaluated by \\longcite{huysmans_empirical_2011,allahyari_useroriented_2011,freitas_comprehensible_2014};\n        This encompasses boolean rules as can be extracted from decision trees\n        or fuzzy or first-order logic rules.\n        For further insights into inductive logic programming approaches to find the latter kind\n        of rules, see, \\forexample, the recent survey by \\longcite{cropper_turning_2020}.\n    \\item[Decision trees] as empirically evaluated by \\longcite{huysmans_empirical_2011,freitas_comprehensible_2014,allahyari_useroriented_2011};\n    \\item[Bayesian networks and naïve Bayes models] as of \\longcite{burkart_survey_2021};\n        interpretability of Bayesian network classifiers was, \\forexample, experimentally evaluated by \\longcite{freitas_comprehensible_2014}.\n    \\item[Linear and logistic models] as of, \\forexample, \\longcite{molnar_interpretable_2020};\n    \\item[Support vector machines]  as of \\longcite{singh_explainable_2020};\n        as long as the used kernel function is not too complex, non-linear SVMs give interesting insights into the decision boundary.\n    \\item[General linear models (GLM)] to inherently provide weights for the importance of input features;\n        Here, it is assumed that there is a transformation, such that there is a linear relationship between the transformed input features and\n        the expected output value.\n        For example, in logistic regression, the transformation is the logit.\n        See, \\forexample, \\cite[Sec.~4.3]{molnar_interpretable_2020} for a basic introduction\n        and further references.\n    \\item[General additive models (GAM)] also inherently come with feature importance weights;\n        Here, it is assumed that the expected output value is the sum of transformed features.\n        See the survey by \\longcite{chang_how_2020} for more details and further references.\n        \\begin{examples}\n            \\examplemethod{Additive Model Explainer~}\n            One concrete example of general additive models is the\n            Additive Model Explainer by \\longcite{chen_explaining_2019}.\n            They train predictors for a given set of features, and another small DNN\n            predicts the additive weights for the feature predictors.\n            They use this setup to learn a GAM surrogate models for a DNN,\n            which also provides a prior to the weights: They should correspond to the\n            sensitivity of the DNN with respect to the features.\n        \\end{examples}\n    \\item[Graphs] as of, \\forexample, \\longcite{xie_explainable_2020};\n    \\item[Finite state automata] as of \\longcite{wang_empirical_2018};\n    \\item[Simple clustering and nearest neighbors approaches] as of \\longcite{burkart_survey_2021};\n        \\begin{examples}\n            Examples are $k$-nearest neighbors (supervised) or $k$-means clustering (unsupervised).\n            \\examplemethod{$k$-means clustering~}\n            The standard $k$-means clustering method introduced by \\longcite{hartigan_algorithm_1979}\n            works with an intuitive model, simply consisting of $k$ prototypes and\n            a proximity measure, with inference associating new samples to\n            the closest prototype representing a cluster.\n            \\examplemethod{$k$-NN~}\n            $k$-nearest neighbors ($k$-NN) determines for a new input the $k$\n            samples from a labeled database that are most similar to the new input sample.\n            The majority vote of the nearest labels is then used to assign a label\n            to the new instance.\n            As long as the proximity measure is not too complex, these methods\n            can be regarded as unsupervised respectively supervised inherently interpretable models. $k$-NN was experimentally evaluated for interpretability by \\longcite{freitas_comprehensible_2014}.\n        \\end{examples}\n    \\item[Diagrams] as of \\longcite{heuillet_explainability_2021}.\n\\end{subparagraphs}", "cites": [7510, 1805], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of inherently interpretable models by listing examples and referencing relevant studies. While it integrates some terminology from Lipton (2018) and Molnar (2020), it does so in a largely additive manner without deeper synthesis or a novel framework. Critical evaluation of the cited works is minimal, and abstraction is limited to general categories like 'decomposable' or 'simulatable' without identifying overarching principles or trends."}}
{"id": "e07b35d1-b96b-4678-aeb3-7fcad6eab983", "title": "Blended models", "level": "paragraph", "subsections": [], "parent_id": "c1fac81b-b9e1-48c0-b4a9-baa24865f6a6", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Model interpretability"], ["paragraph", "Blended models"]], "content": "\\label{sec:taxonomy.problem.interpretability.blended}\nBlended models (also called \\emph{interpretable by design}~)\nconsist partly of intrinsically transparent, symbolic models\nthat are integrated in sub-symbolic non-transparent ones.\nThese kinds of hybrid models are especially interesting for neuro-symbolic\ncomputing and similar fields combining symbolic with sub-symbolic models\n.\n\\begin{examples}\n    \\examplemethod{Logic Tensor Nets~}\n    An example of a blended model is the Logic Tensor Network.\n    Their idea is to use fuzzy logic to encode logical constraints\n    on DNN outputs, with a DNN acting as a fuzzy logic predicate.\n    The framework by \\longcite{donadello_logic_2017} allows additionally to learn\n    semantic relations subject to symbolic fuzzy logic constraints.\n    The relations are represented by simple linear models.\n    \\examplemethod{\n    FoldingNet~,\n    Neuralized clustering~}\n    Unsupervised deep learning can be made interpretable by several approaches, \\forexample,\n    combining autoencoders with visualization approaches. Another approach explains\n    choices of \\enquote{neuralized} clustering methods \n    (\\idest, clustering models translated to a DNN) with saliency maps.\n    Enhancing an autoencoder was applied, for example, in the FoldingNet~ architecture on point clouds.\n    There, a folding-based decoder allows for viewing the reconstruction\n    of point clouds, namely the warping from a 2D grid into the point cloud surface.\n    A saliency-based solution can be produced by algorithms such as layer-wise relevance propagation, which will be discussed in later examples.\n\\end{examples}", "cites": [1804, 1806, 7512], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of blended models in XAI, citing specific examples such as Logic Tensor Networks and FoldingNet. It integrates some ideas across the cited works by highlighting the common theme of combining symbolic and sub-symbolic models but lacks deeper comparative or critical analysis. The generalization is limited to a few examples without broader conceptual abstraction."}}
{"id": "28e83527-9c84-4061-915f-9357d3910e47", "title": "Self-explaining models", "level": "paragraph", "subsections": [], "parent_id": "c1fac81b-b9e1-48c0-b4a9-baa24865f6a6", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Problem definition"], ["subsubsection", "Model interpretability"], ["paragraph", "Self-explaining models"]], "content": "\\label{sec:taxonomy.problem.interpretability.self}\nSelf-explaining models provide additional outputs that explain the output\nof a single prediction.\nAccording to \\longcite{gilpin_explaining_2018}, there are three standard types of\noutputs of explanation generating models:\n\\emph{attention maps},\n\\emph{disentangled representations}, and\n\\emph{textual or multi-modal explanations}.\n\\begin{subparagraphs}[font=\\appenddot]\n    \\item[Attention maps]\n    These are heatmaps that highlight the relevant parts of a given single input\n    for the respective output.\n    \\begin{examples}\n    The work by \\longcite{kim_interpretable_2017} adds an attention module to a DNN\n    that is processed in parallel to, and later multiplied with, convolutional outputs.\n    Furthermore, they suggest a clustering-based post-processing of the\n    attention maps to highlight the most meaningful parts.    \n    \\end{examples}\n    \\item[Disentangled representations]\n    Representations in the intermediate output of the explanandum are called disentangled\n    if single or groups of dimensions therein directly represent symbolic (also called semantic) concepts.\n    \\begin{examples}\n        One can, by design, force one layer of a DNN to exhibit a disentangled representation.\n        \\examplemethod{Capsule Nets~}\n        One example is the capsule network by \\longcite{sabour_dynamic_2017},\n       where groups of neurons,\n        the capsules, characterize each an individual entity, \\forexample, an object\n        or object part.\n        The length of a capsule vector is interpreted as the probability that\n        the corresponding object is present, while the rotation encodes the properties\n        of the object (\\forexample, rotation or color).\n        Later capsules get as input the weighted sum of transformed previous capsule outputs,\n        with the transformations learned and the weights obtained in an iterative\n        routing process.\n        A simpler disentanglement than an alignment of semantic concepts with groups\n        of neurons is the alignment of single dimensions.\n        \\examplemethod{ReNN~}\n        This is done, \\forexample, in the ReNN architecture developed by \\longcite{wang_renn_2018}.\n        They explicitly modularize their DNN to ensure semantically meaningful\n        intermediate outputs.\n        \\examplemethod{Semantic Bottlenecks~}\n        Other methods rather follow a post-hoc approach that fine-tunes a trained\n        DNN towards more disentangled representations, as suggested for\n        Semantic Bottleneck Networks~.\n        These consist of the pretrained backbone of a DNN, proceeded by a layer\n        in which each dimension corresponds to a semantic concept, called semantic bottleneck,\n        and finalized by a newly trained front DNN part. During fine-tuning,\n        first, the connections from the backend to the semantic bottleneck are trained,\n        then the parameters of the front DNN.\n        \\examplemethod{Concept Whitening~}\n        Another interesting fine-tuning approach is that of\n        concept whitening by \\longcite{chen_concept_2020}, which supplements\n        batch-normalization layers with a linear transformation that learns to align\n        semantic concepts with unit vectors of an activation space.\n    \\end{examples}\n    \\item[Textual or multi-model explanations]\n    These provide the explainee with a direct verbal or combined explanation as part of the model output.\n    \\begin{examples}\n        \\examplemethod{}\n        An example are the explanations provided by \\longcite{kim_textual_2018}\n        for the application of end-to-end steering control in autonomous driving.\n        Their approach is two-fold: They add a custom layer that produces\n        attention heatmaps similar to those from ;\n        a second custom part uses these heatmaps to generate\n        textual explanations of the decision, which are (weakly)\n        aligned with the model processing.\n        \\examplemethod{ProtoPNet~}\n        ProtoPNet by \\longcite{chen_this_2019} for image classification provides visual\n        examples rather than text. The network architecture is based on first\n        selecting prototypical image patches and then inserting a prototype layer\n        that predicts similarity scores for patches of an instance with prototypes.\n        These can then be used for explanation of the final result in the manner of\n        \\enquote{This is a sparrow as its beak looks like that of other sparrow examples}.\n        \\examplemethod{}\n        A truly multi-modal example is that by \\longcite{hendricks_generating_2016}, which\n        trains alongside a classifier a long-short term memory DNN (LSTM) to generate\n        natural language justifications of the classification.\n        The LSTM uses both the intermediate features and predictions of the image classifier\n        and is trained towards high-class discriminativeness of the justifications.\n        The explanations can optionally encompass bounding boxes for features\n        that were important for the classification decision, making it multi-modal.\n    \\end{examples}\n\\end{subparagraphs}", "cites": [7067, 1807, 1810, 1808, 1811, 306, 1809, 7513], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear descriptive overview of self-explaining models by categorizing them into types and referencing relevant papers. It synthesizes information by grouping methods under three output categories, but integration of ideas across sources is limited. There is little critical evaluation or identification of broader patterns or limitations in the cited works."}}
{"id": "d4305231-938b-4075-a964-a88b0ab91cba", "title": "Portability", "level": "paragraph", "subsections": [], "parent_id": "4da4bd26-4547-46a7-bf46-e0568217eb34", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Input"], ["paragraph", "Portability"]], "content": "\\label{sec:taxonomy.explanator.input.portability}\nAn important practical aspect of post-hoc explanations is whether or how far\nthe explanation method is dependent on access to the internals of the explanandum model.\nThis level of dependency is called portability, translucency, or transferability.\nIn the following, we will not further differentiate between the strictness of\nrequirements of model-specific methods.\nTransparent and self-explaining models are always model-specific, as\nthe interpretability requires a special model type or model architecture (modification).\nHigher levels of dependency are:\n\\begin{subparagraphs}\n    \\item[Model-agnostic]\n    also called \\emph{pedagogical}~ or black-box:\n    This means that only access to model input and output is required.\n    \\begin{examples}\n        A prominent example of model-agnostic methods is the previously discussed\n        LIME~ method for local approximation via a linear model.\n        \\examplemethod{SHAP~}\n        Another method to find feature importance weights without any access\n        to model internals is SHAP (SHapley Additive exPlanation) by \\longcite{lundberg_unified_2017}.\n        Their idea is to axiomatically ensure: local fidelity; features missing\n        from the original input have no effect; an increase in weight\n        also means an increased attribution of the feature to the final output\n        and uniqueness of the weights. Just as LIME, SHAP just requires a\n        definition of \\enquote{feature} or snippet on the input in order to\n        be applicable.\n    \\end{examples}\n    \\item[Model-specific]\n    also called \\emph{decompositional}~ or white-box:\n    This means that access is needed to the internal processing or architecture of\n    the explanandum model, or even constraints apply.\n    \\begin{examples}\n        Methods relying on gradient or relevance information for the generation of\n        visual attention maps are strictly model-specific.\n        \\examplemethod{Sensitivity Analysis~}\n        A gradient-based method is Sensitivity Analysis by \\longcite{baehrens_how_2010}.\n        They pick the vector representing the steepest ascension in the gradient\n        tangential plane of a sample point. This method is independent of the\n        type of input features but can only analyze a single\n        one-dimensional output at once.\n        \\examplemethod{\n        Deconvnet~,\n        Backprop~,\n        Guided Backprop~}\n        Deconvnet by \\longcite{zeiler_visualizing_2014} instead is agnostic to the type of output, but depends on a convolutional architecture and image inputs.\n        The same holds for its successors\n        Backpropagation~ and Guided Backpropagation~.\n        They approximate a reconstruction of input by defining inverses of pool\n        and convolution operations. This allows for backpropagating the activation of \n        single filters back to input image pixels\n        (see  for a good overview).\n        \\examplemethod{LRP~}\n        The idea of Backpropagation is generalized axiomatically by LRP\n        (Layer-wise Relevance Propagation): \\longcite{bach_pixelwise_2015} require that the sum of linear relevance weights\n        for each neuron in a layer should be constant throughout the layers. The rationale behind this is that relevance is neither created nor extinguished from layer to layer.\n        Methods that achieve this are, \\forexample, Taylor decomposition or\n        the backpropagation of relevance weighted by the forward-pass weights.\n        \\examplemethod{PatternAttribution~}\n        The advancement PatternAttribution by \\longcite{kindermans_learning_2018} fulfills\n        the additional constraint to be sound on linear models.\n    \\end{examples}\n    \\item[Hybrid]\n    also called \\emph{eclectic}~ or \\emph{gray-box}:\n    This means the explanator only depends on access to parts of the model\n    intermediate output, but not the full architecture.\n    \\begin{examples}\n        \\examplemethod{DeepRED~}\n        The rule extraction technique DeepRED\n        (Deep Rule Extraction with Decision tree induction) by \\longcite{zilke_deepred_2016} is an example of\n        an eclectic method, so neither fully model-agnostic nor totally\n        reliant on access to model internals. The approach conducts a backward\n        induction over the layer outputs of a DNN, between each two applying\n        a decision tree extraction. While they enable rule extraction for arbitrarily\n        deep DNNs, only small networks will result in rules of decent length\n        for explanations.\n    \\end{examples}\n\\end{subparagraphs}", "cites": [7507, 1812, 1813, 499, 1814, 8498], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to define and categorize the concept of portability in XAI methods. It abstracts key distinctions between model-agnostic, model-specific, and hybrid methods while providing a nuanced discussion of their respective dependencies and constraints. Critical analysis is present, though limited, as it identifies trade-offs and specific limitations (e.g., DeepRED works better on small networks)."}}
{"id": "df11d573-116a-4e14-8275-ae760f36eb45", "title": "Explanation locality", "level": "paragraph", "subsections": [], "parent_id": "4da4bd26-4547-46a7-bf46-e0568217eb34", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Input"], ["paragraph", "Explanation locality"]], "content": "\\label{sec:taxonomy.explanator.input.locality}\nLiterature differentiates between different ranges of validity of an explanation\nrespectively surrogate model. A surrogate model is valid in the ranges where high\nfidelity can be expected (see \\autoref{sec:taxonomy.metrics}).\nThe range of input required by the explanator depends on the targeted validity range,\nso whether the input must represent a \\emph{local} or the \\emph{global} behavior\nof the explanandum.\nThe general locality types are:\n\\begin{subparagraphs}[font=\\appenddot]\n    \\item[Local]\n    An explanation is considered local if the explanator is valid in a neighborhood of one or\n    a group of given (valid) input samples.\n    Local explanations tackle the question of \\emph{why} a given decision\n    for one or a group of examples was made.\n    \\begin{examples}\n        Heatmapping methods are typical examples for local-only explanators, such as\n        the discussed perturbation-based model-agnostic methods\n        RISE~,\n        D-RISE~,\n        LIME~,\n        SHAP~,\n        as well as the model-specific sensitivity and backpropagation based methods\n        LRP~,\n        PatternAttribution~,\n        Sensitivity Analysis~,\n        and Deconvnet and its successors~.\n    \\end{examples}\n    \\item[Global]\n    An explanation is considered global if the explanator is valid in the complete (valid) input space.\n    Other than the \\emph{why} of local explanations, global interpretability can\n    also be described as answering \\emph{how} a decision is made.\n    \\begin{examples}\n        \\examplemethod{Explanatory Graphs~}\n        A graph-based global explanator is generated by \\longcite{zhang_interpreting_2018}.\n        Their idea is that semantic concepts in an image usually\n        consist of sub-objects to which they have a constant relative spatial relation\n        (\\forexample, a face has a nose in the middle and two eyes next to each other) and that\n        the localization of concepts should not only rely on high filter activation patterns,\n        but also on their sub-part arrangement.\n        To achieve this, they translate the convolutional layers of a DNN into a tree of nodes (concepts),\n        the \\emph{explanatory graph}.\n        Each node belongs to one filter, is anchored at a fixed spatial position in the image,\n        and represents a spatial arrangement of its child nodes.\n        The graph can also be used for local explanations via heatmaps.\n        For localizing a node in an input image, the node is assigned the position closest\n        to its anchor for which (1)~its filter activation is highest, and (2)~the\n        expected spatial relation to its children is best fulfilled.\n        \\examplemethod{Feature Visualization~}\n        While most visualization-based methods provide only local visualizations,\n        Feature Visualizations as reviewed by \\longcite{olah_feature_2017} give a global, prototype-based, visual explanation.\n        The goal here is to visualize the functionality of a DNN's part.\n        It is achieved by finding prototypical input examples that strongly activate that part.\n        These can be found via picking, search, or optimization.\n        \\examplemethod{VIA~}\n        Other than visualizations, rule extraction methods usually only provide\n        global approximations.\n        An example is the well-known model-agnostic rule extractor VIA\n        (Validity Interval Analysis) by \\longcite{thrun_extracting_1995}, which iteratively refines or generalizes pairs of\n        input- and output-intervals.\n        \\examplemethod{SpRAy~}\n        An example of getting from local to global explanations is\n        SpRAy (Spectral Relevance Analysis) by \\longcite{lapuschkin_unmasking_2019}.\n        They suggest to apply spectral clustering~\n        to local feature attribution heatmaps of data samples in order to find\n        spuriously distinct global behavioral patterns.\n        The heatmaps were generated via LRP~.\n    \\end{examples}\n\\end{subparagraphs}", "cites": [7511, 7507, 1812, 1813, 7514, 1802, 499, 1814, 8499, 8498], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of explanation locality by integrating examples and methods from multiple papers, such as RISE, LIME, SHAP, and LRP, to illustrate how different approaches achieve local or global explanations. It provides a nuanced analysis by explaining the differences in the scope of explanations and their implications for interpretability. While it offers clear categorizations and examples, a more explicit critique of the limitations or trade-offs between local and global methods could have further enhanced its critical depth."}}
{"id": "b90faf58-9272-4f6e-90d5-ecd7c05d00b7", "title": "Object of explanation", "level": "paragraph", "subsections": [], "parent_id": "720b79ec-0cac-4d3d-a72b-5021735355d7", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Output"], ["paragraph", "Object of explanation"]], "content": "\\label{sec:taxonomy.explanator.output.object}\nThe object (or scope~) of an explanation describes\nwhich item of the development process should be explained.\nItems we identified in the literature:\n\\begin{subparagraphs}[font=\\appenddot]\n    \\item[Processing]\n    The objective is to understand the (symbolic) processing pipeline of the model,\n    \\idest, to answer parts of the question \\enquote{How does the model work?}.\n    This is the usual case for model-agnostic analysis methods.\n    Types of processing to describe are, \\forexample,\n    the \\emph{decision boundary} and \\emph{feature attribution} (or feature importance).\n    Note that these are closely related, as highly important features usually\n    locally point out the direction to the decision boundary.\n    In case a symbolic explanator is targeted, one may need to first find a\n    symbolic representation of the input, output, or the model's internal representation.\n    Note that model-agnostic methods that do not investigate the\n    input data usually target explanations of the model processing.\n    \\begin{examples}\n        Feature attribution methods encompass all the discussed\n        attribution heatmapping methods (\\forexample, \n        RISE~,\n        LIME~,\n        LRP~).\n        LIME can be considered a corner case: In addition to explaining feature\n        importance it approximates the decision boundary using a\n        linear model on superpixels. The linear model itself may already serve as an\n        explanation.\n        Typical ways to describe decision boundaries are decision trees\n        or sets of rules, as extracted by the discussed\n        VIA~, and\n        DeepRED~ approaches.\n        \\examplemethod{TREPAN~}\n        Standard candidates for model-agnostic decision tree extraction are\n        TREPAN by \\longcite{craven_extracting_1995}, and C4.5 by \\longcite{quinlan_c4_1993}.\n        TREPAN uses M-of-N rules at the split points of the extracted decision tree.\n        \\examplemethod{C4.5~}\n        C4.5 uses interval-based splitting points, and generates shallower but wider trees compared to TREPAN.\n        \\examplemethod{Concept Tree~}\n        Concept tree is a recent extension of TREPAN by \\longcite{renard_concept_2019} that adds\n        automatic grouping of correlated features into the candidate concepts to use for\n        the tree nodes.\n    \\end{examples}\n    \\item[Inner representation]\n    Machine learning models learn new representations of the input space, like the\n    latent space representations found by DNNs.\n    Explaining these inner representations answers, \\enquote{How does the model see the world?}.\n    A more fine-grained differentiation considers whether\n    \\emph{layers}, \\emph{units}, or \\emph{vectors} in the feature space \n    are explained.\n    \\begin{examples}\n        \\begin{itemize}\n            \\item Units:\n            One example of unit analysis is the discussed Feature Visualization~.\n            \\examplemethod{NetDissect~}\n            In contrast to this unsupervised assignment of convolutional filters to\n            prototypes, NetDissect (Network Dissection) by \\longcite{bau_network_2017}\n            assigns filters to pre-defined semantic concepts in a supervised manner:\n            For a filter, that semantic concept (color, texture, material, object, or object part)\n            is selected for which the ground truth segmentation masks have the highest overlap\n            with the upsampled filter's activations. The authors also suggest that concepts that\n            are less entangled, so less distributed over filters, are more interpretable,\n            which is measurable with their filter-to-concept-alignment technique.\n            \\item Vectors:\n            \\examplemethod{Net2Vec~}\n            Other than NetDissect, Net2Vec by \\longcite{fong_net2vec_2018} also wants to assign concepts\n            to their possibly entangled representations in the latent space.\n            Given a concept, they train a linear $1\\times1$-convolution on the output of a layer to segments the respective concept with an image.\n            The weight vector of the linear model for a concept can be understood as\n            a prototypical representation (embedding) for that concept in the DNN\n            intermediate output. They found that such embeddings behave like vectors in\n            a word vector space: Concepts that are semantically similar feature embeddings\n            with high cosine similarity.\n            \\examplemethod{TCAV~}\n            Similar to Net2Vec, TCAV\n            (Testing Concept Activation Vectors) also aims to find embeddings of NetDissect concepts.\n            \\longcite{kim_interpretability_2018} are not interested in embeddings that are represented as a linear combination of\n            convolutional filters, but instead in embedding vectors lying in the space of\n            the complete layer output. In other words, they do not segment concepts,\n            but make an image-level classification of whether the concept is present.\n            These are found by using an SVM model instead of the $1\\times1$-convolution.\n            Additionally, they suggest using partial derivatives along those concept vectors\n            to find the local attribution of a semantic concept to a certain output.\n            \\examplemethod{ACE~}\n            Other than the already mentioned supervised methods,\n            ACE (Automatic Concept-based Explanations) by by \\longcite{ghorbani_automatic_2019}\n            does not learn a linear classifier but does an unsupervised clustering\n            of concept candidates in the latent space.\n            The cluster center is then selected as the embedding vector.\n            A superpixeling approach is used together with outlier removal to obtain\n            concept candidates.\n            \\item Layers:\n            \\examplemethod{Concept completeness~, IIN~}\n            The works of \\longcite{yeh_completenessaware_2020} and\n            the IIN (invertible interpretation networks) approach by \\longcite{esser_disentangling_2020}\n            extend on the previous approaches and analyze a complete layer output\n            space at once. For this, they find a subspace with a basis of concept embeddings,\n            which allows an invertible transformation to a disentangled representation space.\n            While IIN uses invertible DNNs for the bijection of concept space to latent space,\n            \\longcite{yeh_completenessaware_2020} use linear maps in their experiments.\n            These approaches can be seen as a post-hoc version of the\n            Semantic Bottleneck~ architecture,\n            only not replacing the complete later part of the model, but just learning\n            connections from the bottleneck to the succeeding trained layer.\n            \\longcite{yeh_completenessaware_2020} additionally introduce the notion of\n            completeness of a set of concepts as the maximum performance of the model\n            intercepted by the semantic bottleneck.\n        \\end{itemize}\n    \\end{examples}\n    \\item[Development (during training)]\n    Some methods focus on assessing the effects during training \\cite[Sec.~2.3]{molnar_interpretable_2020}:\n    \\enquote{How does the model evolve during the training? What effects do new samples have?}\n    \\begin{examples}\n        \\examplemethod{}\n        One example is the work of \\longcite{shwartz-ziv_opening_2017}, who inspect the model\n        during training to investigate the role of depth in neural networks.\n        Their findings indicate that depth actually is of computational benefit.\n        \\examplemethod{Influence Functions~}\n        An example which can be used to provide, \\forexample, prototypical explanations\n        are Influence Functions by \\longcite{koh_understanding_2017}.\n        They gather the influence of training samples during the training to later\n        assess the total impact of samples on the training.\n        They also suggest using this information as a proxy to estimate the influence of the samples on model decisions.\n    \\end{examples}\n    \\item[Uncertainty] \\longcite{molnar_interpretable_2020}\n    suggests to capture and explain (\\forexample, visualize) the uncertainty of a prediction of the model.\n    This encompasses the broad field of Bayesian deep learning \n    and uncertainty estimation .\n    Several works argue why it is important\n    to make the uncertainty of model decisions accessible to users.\n    For example, \\longcite{poceviciute_survey_2020} argues this for medical applications, and \\longcite{mcallister_concrete_2017} for autonomous driving.\n    \\item[Data]\n    Pre-model interpretability  is the point where explainability\n    touches the large research area of data analysis and feature mining.\n    \\begin{examples}\n        \\examplemethod{PCA~}\n        Typical examples for projecting high-dimensional data into easy-to-visualize\n        2D space are component analysis methods like PCA (Principal Component Analysis) which was introduced by \\longcite{jolliffe_principal_2002}.\n        \\examplemethod{t-SNE~}\n        A slightly more sophisticated approach is t-SNE (t-Distributed Stochastic Neighbor Embedding) by \\longcite{maaten_visualizing_2008}.\n        In order to visualize a set of high-dimensional data points,\n        they try to find a map from these points into a 2D or 3D space\n        that is faithful to pairwise similarities.\n        \\examplemethod{Spectral Clustering~}\n        And also clustering methods can be used to generate prototype-\n        or example-based explanations of typical features in the data.\n        Examples here are k-means clustering~\n        and graph-based spectral clustering~.\n    \\end{examples}\n\\end{subparagraphs}", "cites": [1816, 1818, 1811, 7516, 8499, 7515, 1815, 1819, 7068, 7507, 7511, 1817], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to define and categorize the 'object of explanation' in XAI, integrating model-agnostic and symbolic approaches. It provides a structured comparison between methods (e.g., NetDissect vs. Net2Vec vs. ACE) and highlights conceptual differences, such as layer-wise analysis and the use of semantic concepts. While not deeply critical of individual papers, it abstracts broader patterns in how models encode and represent information, contributing to a principled understanding of XAI methods."}}
{"id": "058d7afe-5515-4683-b6d7-5d065085247d", "title": "Output type", "level": "paragraph", "subsections": [], "parent_id": "720b79ec-0cac-4d3d-a72b-5021735355d7", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Output"], ["paragraph", "Output type"]], "content": "\\label{sec:taxonomy.explanator.output.type}\nThe output type, also considered the actual explanator~,\ndescribes the type of information presented to the explainee.\nNote that this (\\enquote{what} is shown) is mostly independent of\nthe presentation form (\\enquote{how} it is shown).\nTypical types are:\n\\begin{subparagraphs}\n    \\item[By example instance,]\n    \\forexample, closest other samples, word cloud;\n    \\begin{examples}\n        The discussed ProtoPNet~ is based on selecting\n        and comparing relevant example snippets from the input image data.\n    \\end{examples}\n    \\item[Contrastive / counterfactual / near miss examples,]\n    including adversarial examples;\n    The goal here is to explain for an input why the respective output was as obtained instead of a desired output. This is done by presenting how the input features have to change in order to obtain the alternative output.\n    Counterfactual examples are sometimes seen as a special case of more general contrastive examples .\n    Desirables associated specifically with counterfactual examples are that they are valid inputs close to the original examples and with few features changed (\\emph{sparsity}) that are actionable for the explainee and that they adhere to known causal relations .\n    \\begin{examples}\n        \\examplemethod{CEM~}\n        The perturbation-based feature importance heatmapping approach of RISE is extended in\n        CEM (Contrastive, Black-box Explanations Model) by \\longcite{dhurandhar_explanations_2018}.\n        They do not only find positively contributing features but also the\n        features that must minimally be absent to not change the output.\n    \\end{examples}\n    \\item[Prototype,]\n    \\forexample, generated, concept vector;\n    \\begin{examples}\n        A typical prototype generator is used in the discussed\n        Feature Visualization method :\n        images are generated, \\forexample, via gradient descent, that represent the\n        prototypical pattern for activating a filter.\n        While this considers prototypical inputs, concept embeddings as collected in\n        TCAV~ and Net2Vec~\n        describe prototypical activation patterns for a given semantic concept.\n        The concept mining approach ACE~ combines\n        prototypes with examples: They search a concept embedding as a prototype for\n        an automatically collected set of example patches, that, in turn, can be used to\n        explain the prototype.\n    \\end{examples}\n    \\item[Feature importance] that will highlight features with high attribution or influence on the output;\n    \\begin{examples}\n        A lot of feature importance methods producing heatmaps have been discussed\n        before, such as\n        RISE~,\n        D-RISE~,\n        CEM~,\n        LIME~,\n        SHAP~,\n        LRP~,\n        PatternAttribution~,\n        Sensitivity Analysis~,\n        Deconvnet and successors~\n        .\n        \\examplemethod{}\n        One further example is the work by \\longcite{fong_interpretable_2017}, who follow a\n        perturbation-based approach.\n        Similar to RISE, their idea is to find a minimal occlusion mask that, if used to\n        perturb the image (\\forexample, blur, noise, or blacken), maximally changes the outcome.\n        To find the mask, backpropagation is used, making it a model-specific method.\n        \\examplemethod{\n        CAM~,\n        Grad-CAM~}\n        Some older but popular and simpler example methods are\n        Grad-CAM by \\longcite{selvaraju_gradcam_2017} and its predecessor\n        CAM (Class Activation Mapping) by \\longcite{zhou_learning_2016}.\n        While Deconvnet and its successors can only consider the feature importance\n        with respect to intermediate outputs, (Grad-)CAM produces class-specific heatmaps,\n        which are the weighted sum of the filter activation maps for one (usually the last)\n        convolutional layer.\n        For CAM, it is assumed the convolutional backend is finalized by a global\n        average pooling layer that densely connects to the final classification output.\n        Here, the weights in the sum are the weights connecting the neurons of the global average pooling layer to the class outputs. For Grad-CAM, the weights in the sum are the averaged derivation of the class output by each activation map pixel.\n        \\examplemethod{Concept-wise Grad-CAM~}\n        This is also used in the more recent work of \\longcite{zhou_interpretable_2018},\n        who do not apply Grad-CAM directly to the output but to each of a minimal set\n        of projections from a convolutional intermediate output of a DNN that predict\n        semantic concepts.\n        \\examplemethod{SIDU~}\n        Similar to Grad-CAM, SIDU (Similarity Distance and Uniqueness) by \\longcite{muddamsetty_introducing_2021}\n        also adds up the filter-wise weighted activations of the last convolutional layer.\n        The weights encompass a combination of a similarity score and a uniqueness score for the prediction output under each filter activation mask.\n        The scores aim for high similarity of a masked prediction with the original\n        one and low similarity to the other masked prediction, leading to masks\n        capturing more interesting object regions.\n    \\end{examples}\n    \\item[Rule-based,]\n    \\forexample, decision tree; or if-then, binary, m-of-n, or  hyperplane rules (\\cf );\n    \\begin{examples}\n        The mentioned exemplary rule-extraction methods\n        DeepRED~ and\n        VIA~, as well as \n        decision tree extractors\n        TREPAN~,\n        Concept Tree~, and\n        C4.5~\n        all provide global, rule-based output.\n        For further rule extraction examples, we refer the reader to the comprehensive surveys\n        \\longcite{hailesilassie_rule_2016,wang_comparative_2018,augasta_rule_2012} on the topic\n        and the survey by \\longcite{wang_empirical_2018} for recurrent DNNs.\n        \\examplemethod{LIME-Aleph~}\n        An example of a local rule-extractor is the recent LIME-Aleph\n        approach by \\longcite{rabold_explaining_2018}, which generates a local explanation in the form of first-order logic rules.\n        This is learned using inductive logic programming (ILP)~\n        trained on the symbolic knowledge about a set of semantically similar examples.\n        Due to the use of ILP, the approach is limited to tabular input data and\n        classification outputs, but just like LIME, it is model-agnostic.\n        \\examplemethod{NBDT~}\n        A similar approach is followed by NBDT (Neural-Backed Decision Trees).\n        Here, \\longcite{wan_nbdt_2020} assume that the concept embeddings of super-categories are\n        represented by the mean of their sub-category vectors\n        (\\forexample, the mean of \\enquote{cat} and \\enquote{dog} should be\n        \\enquote{animal with four legs}).\n        This is used to infer from bottom-to-top a decision tree where the nodes are\n        super-categories, and the leaves are the classification classes.\n        At each node, it is decided which of the sub-nodes best applies to the image.\n        As embedding for a leaf concept (an output class), they suggest taking\n        the weights connecting the penultimate layer to a class output,\n        and as similarity measure for the categories, they use dot-product\n        (\\cf Net2Vec and TCAV).\n    \\end{examples}\n    \\item[Dimension reduction,]\n    \\idest, sample points are projected to a sub-space;\n    \\begin{examples}\n        Typical dimensionality reduction methods mentioned previously are\n        PCA~ and\n        t-SNE~.\n    \\end{examples}\n    \\item[Dependence plots]\n    which plot the effect of an input feature on the final output of a model (\\cf );\n    \\begin{examples}\n        \\examplemethod{PDP~}\n        PDP (Partial Dependency Plots, \\cf \\cite[sec.~5.1]{molnar_interpretable_2020})\n        by \\longcite{friedman_greedy_2001}\n        calculate for one input feature, and for each value of this feature,\n        the expected model outcome is averaged over the dataset.\n        This results in a plot (for each output) that indicates the\n        global influence of the respective feature on the model.\n        \\examplemethod{ICE~}\n        The local equivalent by \\longcite{goldstein_peeking_2015}, ICE\n        (Individual Conditional Expectation, \\cf \\cite[sec.~5.2]{molnar_interpretable_2020}) plots,\n        obtain the PDP for generated data samples locally around a given sample.\n    \\end{examples}\n    \\item[Graphs] as of \\forexample \\longcite{mueller_explanation_2019,mueller_principles_2021,linardatos_explainable_2021};\n    \\begin{examples}\n        The previously discussed Explanatory Graph~\n        method provides, amongst others, a graph-based explanation output.\n    \\end{examples}\n    \\item[Combinations] of mentioned model types.\n\\end{subparagraphs}", "cites": [1816, 1813, 737, 1803, 499, 1814, 1820, 1821, 1810, 7303, 7515, 1822, 8498, 7511, 7507, 1812, 1802, 1817], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping diverse XAI methods under the 'output type' dimension and linking them with their conceptual goals and techniques. It provides critical insights by discussing method-specific limitations (e.g., sparsity, model specificity, interpretability constraints). The abstraction is evident through the categorization of outputs and highlighting of underlying principles, such as the distinction between 'what is shown' and 'how it is shown.'"}}
{"id": "2178b417-ee92-4f6b-badc-2c63838cff58", "title": "Presentation", "level": "paragraph", "subsections": [], "parent_id": "720b79ec-0cac-4d3d-a72b-5021735355d7", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Output"], ["paragraph", "Presentation"]], "content": "\\label{sec:taxonomy.explanator.output.presentation}\nThe presentation of information can be characterized by two categories of\nproperties:\nthe used \\emph{presentation form} and\nthe \\emph{level of abstraction} used to present available information.\nThe presentation form simply summarizes the human sensory input channels\nutilized by the explanation, which can be:\nvisual (the most common one including diagrams, graphs, and heatmaps),\ntextual in either natural language or formal form,\nauditive, and\ncombinations thereof.\nIn the following, the aspects influencing the level of abstraction are elaborated.\nThese can be split up into\n(1) aspects of the smallest building blocks of the explanation, the \\emph{information units},\nand\n(2) the \\emph{accessibility} or level of \\emph{complexity} of their combinations (the information units).\nLastly, further filtering may be applied before finally presenting the explanation,\nincluding privacy filters. \n\\begin{subparagraphs}[font=\\appenddot]\n    \\item[Information units]\n    The basic units of the explanation, cognitive chunks , or information units,\n    may differ in the level of processing applied to them.\n    The simplest form are unprocessed \\emph{raw features}, as used\n    in explanations by example.\n    \\emph{Derived features} capture some indirect information contained in\n    the raw inputs, like superpixels or attention heatmaps.\n    These need not necessarily have semantic meaning to the explainee,\n    in contrast to explicitly \\emph{semantic features}, \n    \\forexample, concept activation vector attributions.\n    The last type of information unit are \\emph{abstract semantic features}\n    not directly grounded in any input, \\forexample, generated prototypes.\n    \\emph{Feature interactions} may occur as information units or be left unconsidered\n    for the explanation.\n    \\begin{examples}\n        Some further notable examples of heatmapping methods for feature attribution\n        are SmoothGrad by \\longcite{smilkov_smoothgrad_2017} and \n        Integrated Gradients by \\longcite{sundararajan_axiomatic_2017}.\n        One drawback of the methods described so far is that they linearly approximate the loss surface in a point-wise manner.\n        Hence, they struggle with \\enquote{rough} loss surfaces that\n        exhibit significant variation in the point-wise values, gradients, and thus feature importance .\n        \\examplemethod{SmoothGrad~}\n        SmoothGrad\n        aims to mitigate this by averaging the gradient from random samples\n        within a ball around the sample to investigate.\n        \\examplemethod{Integrated Gradients~}\n        Integrated gradients do the averaging\n        (to be precise: integration) along a path between two points in the input space.\n        \\examplemethod{Integrated Hessians~}\n        A technically similar approach but with a different goal is\n        Integrated Hessians~.\n        They intend not to grasp and visualize the sensitivity of the model for\n        one feature (as a derived feature), but their information units are interactions\n        of features, \\idest, how much the change of one feature changes the\n        influence of the other on the output.\n        This is done by having a look at the Hessian matrix, which is obtained\n        by two subsequent Integrated Gradients calculations.\n    \\end{examples}\n    \\item[Accessibility]\n    The accessibility, level of detail, or level of complexity describes\n    how much intellectual effort the explainee has to bring up in order to\n    understand the simulatable parts of the explanation.\n    Thus, the perception of complexity heavily depends on the end-user, which is mirrored in the human-grounded complexity / interpretability metric discussed later in \\autoref{sec:taxonomy.metrics}.\n    In general, one can differentiate between representations\n    that are considered \\emph{simpler} and such that are more \\emph{expressive but complex}.\n    Because accessibility is a precondition to simulating the parts,\n    it is not the same as the transparency level.\n    For example, very large, transparent decision trees or very high-dimensional\n    (general) linear models may be perceived as globally complex by the end-user.\n    However, when looking at the simulatable parts of the explanator,\n    like small groups of features or nodes,\n    they are easy to grasp.\n    \\begin{examples}\n        Accessibility can indirectly be assessed by the complexity and expressivity\n        of the explanation (see \\autoref{sec:taxonomy.metrics}).\n        To give some examples:\n        Simple presentations are, \\forexample,\n        linear models, general additive models,\n        decision trees and Boolean decision rules,\n        Bayesian models, or\n        clusters of examples (\\cf \\autoref{sec:taxonomy.problem});\n        generally, more complex are, \\forexample,\n        first-order or fuzzy logical decision rules.\n    \\end{examples}\n    \\item[Privacy awareness]\n    Sensible information like names may be contained in parts of the explanation,\n    even though they are not necessary for understanding the actual decision.\n    In such cases, an important point is privacy awareness :\n    Is sensible information removed if unnecessary or properly anonymized if needed?\n\\end{subparagraphs}", "cites": [1823, 7517, 8500, 1824], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to define and categorize aspects of XAI presentation, such as information units and accessibility. It provides critical analysis by identifying limitations of certain methods (e.g., linear approximations in feature attribution). The abstraction level is high as it constructs a general framework for understanding how explanations are structured and perceived."}}
{"id": "8fe870e9-2ade-4856-a801-85afbb0f63dc", "title": "Interactivity", "level": "subsubsection", "subsections": [], "parent_id": "533b7e4e-c152-49a1-a09a-c98cbde9a783", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Explanator"], ["subsubsection", "Interactivity"]], "content": "\\label{sec:taxonomy.explanator.interactivity}\nThe interaction of the user with the explanator may either be static, so the explainee\nis once presented with an explanation, or interactive, meaning an iterative process\naccepting user feedback as explanation input.\nInteractivity is characterized by the \\emph{interaction task} and the \\emph{explanation process}. \n\\begin{subparagraphs}[font=\\appenddot]\n    \\item[Interaction task]\n    The user can either inspect explanations or \\emph{correct} them.\n    Inspecting takes place through \\emph{exploration} of different parts of one explanation\n    or through consideration of various alternatives and complementing explanations,\n    such as implemented in the \\emph{iNNvestigate} toolbox .\n    Besides, the user can be empowered within the human-AI partnership to provide corrective feedback to the system via an explanation interface, in order to adapt the explanator and thus the explanandum.\n    \\begin{examples}\n        State-of-the-art systems\n        \\begin{itemize}\n            \\item\n            enable the user to perform \\emph{corrections on labels} and to act upon wrong explanations through interactive machine learning (intML),\n            such as implemented in the approach\n            \\inlineexamplemethod{CAIPI },\n            \\item\n            they allow for \\emph{re-weighting of features} for explanatory debugging, like the system\n            \\inlineexamplemethod{EluciDebug~},\n            \\item\n            \\emph{adaption of features} as provided by\n            \\inlineexamplemethod{Crayons~},\n            and\n            \\item\n            correcting generated verbal explanations through user-defined constraints,\n            such as implemented in the medical-decision support system\n            \\inlineexamplemethod{Learn\\-With\\-ME~}.\n        \\end{itemize}\n    \\end{examples}\n    \\item[Explanation process]\n    As mentioned above, explanation usually takes place in an iterative fashion.\n    Sequential analysis allows the user to query further information in an\n    iterative manner and to understand the model and its decisions over time,\n    in accordance with the users' capabilities and the given context .\n    \\begin{examples}\n        \\examplemethod{Multi-modal explanations~}\n        The explanation process includes combining different methods to create multi-modal explanations and involving the user through dialogue. It can be realized in a phrase-critic model as presented by \\longcite{Hendricks_2018_ECCV}, or with the help of an explanatory dialogue such as proposed by \\longcite{finzel2021explanation}.\n    \\end{examples}\n\\end{subparagraphs}", "cites": [1825], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes interactivity in XAI by categorizing it into 'interaction task' and 'explanation process,' effectively integrating examples from multiple systems. It identifies broader patterns such as user-driven correction and iterative explanation. While it includes some critical insights (e.g., the importance of trust in explanation systems), the critique of the cited works is not as deep or nuanced as it could be."}}
{"id": "9e9b8548-b345-4055-9d71-cd316320246c", "title": "Functionally-grounded metrics", "level": "subsubsection", "subsections": [], "parent_id": "47001ea6-392e-4a75-8870-1bf110582f17", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Metrics"], ["subsubsection", "Functionally-grounded metrics"]], "content": "\\label{sec:taxonomy.metrics.functional}\nMetrics are considered functionally-grounded if they do not require any human feedback\nbut instead measure the formal properties of the explanator.\nThis applies to the following metrics:\n\\begin{subparagraphs}\n    \\item[Faithfulness] , fidelity~, soundness~, or causality~,\n    measures how accurately the behavior of the explanator conforms with that of the actual object of explanation.\n    If a full surrogate model is used, this is the accuracy of the surrogate model outputs with respect to the explanandum outputs.\n    And the fidelity of inherently interpretable models also serving as explanators is naturally 100\\%.\n    Note that fidelity is more often used if the explanator consists of a complete surrogate model (\\forexample, a linear proxy like LIME~)\n    and faithfulness in more general contexts .\n    Other works use the terms interchangeably like \\longcite{du_techniques_2019,carvalho_machine_2019}, which we adopt here.\n    More simplification usually comes along with less faithfulness since corner cases\n    are not captured anymore, also called the \\emph{fidelity-interpretability trade-off}.\n    \\item[Localization accuracy] with respect to some ground truth\n    (\\cf  for visual feature importance maps)\n    means how well an explanation correctly localizes\n    certain points of interest.\n    These points of interest must be given by a ground truth that is preferably\n    provided by mathematical properties, such as\n    certainty, bias, feature importance, and outliers (\\cf ).\n    In which way such points are highlighted for the explainee depends on the explanation type.\n    For example, they could be highlighted in a feature importance heatmap  or expressed by the aggregated relevance within regions of interest .\n    Note that localization capability is closely related to faithfulness but refers to specific properties of interest.\n    \\item[Completeness,] or coverage,\n    measures how large the validity range of an explanation is, so in which subset\n    of the input space high fidelity can be expected.\n    It can be seen as a generalization of fidelity to the distribution of fidelity.\n    Note that coverage can also be calculated for parts of an explanation, such as\n    single rules of a rule set, as considered by \\longcite{burkart_survey_2021}.\n    \\item[Overlap]\n    is considered by \\longcite{burkart_survey_2021} for rule-based explanations.\n    It measures the number of data samples that satisfy more than one rule of the rule set,\n    \\idest, measures the size of areas where the validity ranges of different rules overlap.\n    This can be seen as a measure of redundancy in the rule set\n    and may sometimes correlate with perceived complexity .\n    \\item[Accuracy] of the surrogate model\n    ignores the prediction quality of the original model and only\n    measures the prediction quality of the surrogate model for the original task (using the standard accuracy metric).\n    This only applies to post-hoc explanations.\n    \\item[Architectural complexity] \n    can be measured using metrics specific to the explanator type.\n    The goal is to approximate the subjective, human-grounded complexity that\n    a human perceives using purely architectural properties like size measures.\n    Such architectural metrics can be, e.g., the number of used input features for feature importance,\n    the number of changed features for counterfactual examples (also called \\emph{sparsity} ),\n    the sparsity of linear models ,\n    the width or depth of decision trees ,\n    or, in case of rules, the number of defined rules and the number of unique used predicates .\n    \\item[Algorithmic complexity] and scalability\n    measure the information-theoretic complexity of the algorithm used to derive\n    the explanator. This includes the time to convergence (to an acceptable solution)\n    and is especially interesting for complex approximation schemes like rule extraction.\n    \\item[Stability] or robustness~\n    measures the change of explanator (output) given a change in the input samples.\n    This analogon to (adversarial) robustness of deep neural networks and a stable\n    algorithm is usually also better comprehensible and desirable.\n    \\item[Consistency]\n    measures the change of the explanator (output) given a change in the model to explain.\n    The idea behind consistency is that functionally equivalent models should produce the\n    same explanation. This assumption is important for model-agnostic approaches, while\n    for model-specific ones, a dependency on the model architecture may even be desirable.\n    (\\forexample, for architecture visualization).\n    \\item[Sensitivity]\n    measures whether local explanations change if the model output changes strongly.\n    A big change in the model output usually comes along with a change in the discrimination strategy of the model between the differing samples .\n    Such changes should be reflected in the explanations.\n    Note that this may be in conflict with stability goals for regions in which\n    the explanandum model behaves chaotically.\n    \\item[Expressiveness] or the level of detail\n    refers to the level of detail of the formal language used by the explanator.\n    It is interested in approximating the expected information density perceived by the user.\n    It is closely related to the level of abstraction of the presentation.\n    Several functionally-grounded proxies were suggested to obtain comparable\n    measures for expressivity:\n    \\begin{itemize}\n        \\item the depth or amount of \\emph{added information},\n        also measured as the mean number of used information units per explanation;\n        \\item \\emph{number of relations} that can be expressed; and\n        \\item the \\emph{expressiveness category} of used rules, namely\n        mere conjunction, boolean logic, first-order logic, or fuzzy rules\n        (\\cf ).\n    \\end{itemize}\n\\end{subparagraphs}", "cites": [7507, 1804, 1826], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing diverse metrics from multiple XAI surveys into a coherent framework, particularly by grouping them under the concept of 'functionally-grounded' metrics. It offers some critical perspectives by noting potential conflicts (e.g., fidelity-interpretability trade-off, stability vs. sensitivity), and by identifying redundancy and scalability concerns. The abstraction is high as it generalizes common themes and principles across the cited papers, such as the relationship between complexity and comprehensibility."}}
{"id": "772cdd9d-9722-423a-8f6e-c49262454490", "title": "Human-grounded metrics", "level": "subsubsection", "subsections": [], "parent_id": "47001ea6-392e-4a75-8870-1bf110582f17", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Metrics"], ["subsubsection", "Human-grounded metrics"]], "content": "\\label{sec:taxonomy.metrics.human}\nOther than functionally-grounded metrics, human-grounded metrics require\nto involve a human directly on proxy tasks for their measurement.\nHuman involvement can be measured through observation of a person's reactions but also through direct human feedback.\nOften, proxy tasks are considered instead of the final application to avoid\na need for expensive experts or application runtime (think of medical domains).\nThe goal of an explanation always is that the receiver of the explanation\ncan build a \\emph{mental model} of (aspects of) the object of explanation .\nHuman-grounded metrics aim to measure some fundamental psychological properties\nof the XAI methods, namely quality of the \\emph{mental model}.\nThe following are counted as such in literature:\n\\begin{subparagraphs}\n    \\item[Interpretability] or comprehensibility, or complexity\n    measures how accurately the mental model approximates the explanator model.\n    This measure mostly relies on subjective user feedback on whether they\n    \\enquote{could make sense} of the presented information.\n    It depends on background knowledge, biases, and cognition of the subject\n    and can reveal the use of vocabulary inappropriate to the user\n    .\n    \\item[Effectiveness]\n    measures how accurately the mental model approximates the object of explanation.\n    In other words, one is interested in how well a human can simulate the\n    (aspects of interest of the) object after being presented with the explanations.\n    Proxies for effectiveness can be fidelity and accessibility\n    \\cite[Sec.~2.4]{molnar_interpretable_2020}.\n    This may serve as a proxy for interpretability.\n    \\item[(Time) efficiency]\n    measures how time efficient an explanation is, \\idest, how long it takes\n    a user to build up a viable mental model.\n    This is especially of interest in applications with a limited time frame\n    for user reaction, like\n    product recommendation systems~ or\n    automated driving applications~.\n    \\item[Degree of understanding]\n    measures in interactive contexts the current status of understanding.\n    It helps to estimate the remaining time or measures needed to reach\n    the desired extent of the explainee's mental model.\n    \\item[Information amount]\n    measures the total subjective amount of information conveyed by one explanation.\n    Even though this may be measured on an information-theoretic basis, it\n    usually is subjective and thus requires human feedback.\n    Functionally-grounded related metrics are the (architectural) complexity of the object of explanation, together with fidelity and coverage.\n    For example, more complex models have a tendency to contain more information,\n    and thus require more complex explanations if they are to be approximated\n    widely and accurately.\n\\end{subparagraphs}", "cites": [8501, 1809, 8502], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited surveys by identifying common themes such as mental model building and subjective human feedback. It abstracts key metrics like interpretability, effectiveness, and information amount into a coherent framework for human-grounded evaluation. While it offers a structured analysis and conceptual generalization, it lacks deeper critical evaluation of the limitations or contradictions among the cited works."}}
{"id": "89c2e10f-3e68-4a98-9295-40af51b6eee2", "title": "Application-grounded metrics", "level": "subsubsection", "subsections": [], "parent_id": "47001ea6-392e-4a75-8870-1bf110582f17", "prefix_titles": [["title", "A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts"], ["section", "Taxonomy"], ["subsection", "Metrics"], ["subsubsection", "Application-grounded metrics"]], "content": "\\label{sec:taxonomy.metrics.application}\nOther than human-grounded metrics, application-grounded ones work on human feedback\nfor the final application.\nThe following metrics are considered application-grounded:\n\\begin{description}\n    \\item[Satisfaction]\n    measures the direct content of the explainee with the system. It implicitly measures the benefit of explanations for the explanation system user.\n    \\item[Persuasiveness]\n    assesses the capability of the explanations to nudge an explainee into a certain\n    direction.\n    This is foremostly considered in recommendation systems \n    but has high importance when it comes to analysis tasks, where false positives and\n    false negatives of the human-AI system are undesirable.\n    In this context, a high persuasiveness may indicate a miscalibration of indicativeness.\n    \\item[Improvement of human judgment] \n    measures whether the explanation system user develops an appropriate level of\n    trust in the decisions of the explained model.\n    Correct decisions should be trusted more than wrong decisions,\n     \\forexample because explanations of wrong decisions are illogical.\n    \\item[Improvement of human-AI system performance]\n    considers the end-to-end task to be achieved by all of the following: explanandum, explainee, and explanator.\n    This can, \\forexample, be the diagnosis quality of doctors assisted by a recommendation system\n    .\n    \\item[Automation capability]\n    gives an estimate of how much of the manual work conducted by the human\n    in the human-AI system can be automatized.\n    Especially for local explanation techniques, automation may be an important\n    factor for feasibility if the number of samples a human needs to scan can\n    be drastically reduced .\n    \\item[Novelty]\n    estimates the subjective degree of novelty of information provided to the\n    explainee .\n    This is closely related to efficiency and satisfaction:\n    Especially in exploratory use cases, high novelty can drastically\n    increase efficiency (no repetitive work for the explainee) and\n    keep satisfaction high (decrease the possibility of boredom for the explainee).\n\\end{description}\n\\begin{landscape}\n\\begin{figure}\n    \\raggedright\n    \\includegraphics[scale=\\thetaxonomygraphicscale]{xai_taxonomy}\n    \\caption{Overview of the complete taxonomy that is detailed in \\autoref{sec:taxonomy}}\n    \\label{fig:taxonomy}\n\\end{figure}\n\\end{landscape}\n\\begingroup\n\\footnotesize\n\\newcolumntype{f}{@{~}c}\n\\newcommand{\\cluster}[1]{\\\\\\secseprule{9}\\multicolumn{9}{@{}l@{}}{\\textbf{#1}}}\n\\let\\cite\\longcite\n\\begin{tabularx}{\\linewidth}{@{} >{\\raggedright}p{8em} >{\\raggedright}X @{} c@{}cfffff@{}}\n    \\caption{Review of an exemplary selection of XAI techniques according\n    to the defined taxonomy aspects (without inherently transparent models from \\autoref{sec:taxonomy.problem.interpretability}).\n    Abbreviations by column:\n    \\abbr{image data}{img}, \\abbr{point cloud data}{pcl};\n    \\abbr{Trans.}{transparency}, \\abbr{post-hoc}{p}, \\abbr{transparent}{t}, \\abbr{self-explaining}{s}, \\abbr{blended}{b};\n    \\abbr{processing}{p}, \\abbr{representation}{r}, \\abbr{development during training}{t} \\abbr{data}{d};\n    \\abbr{visual}{vis}, \\abbr{symbolic}{sym}, \\abbr{plot}{plt};\n    \\abbr{feature importance}{fi}, \\abbr{contrastive}{con}, \\abbr{prototypical}{proto}, \\abbr{decision tree}{tree}, \\abbr{distribution}{dist}}\n    \\label{tab:methods.overview}\\\\ \n    \t\\rot{Name}\t&\t\\rot{Cite}\t&\t\\rot{Task}\t&\t\\rot{Model-agnostic?}\t&\t\\rot{Transp.}\t&\t\\rot{Global?}\t&\t\\rot{Obj. Expl.}\t&\t\\rot{Form}\t&\t\\rot{Type}\t\n\\endfirsthead\\caption{continued from page~\\pageref{tab:methods.overview}}\\\\\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\rot{Name}\t&\t\\rot{Cite}\t&\t\\rot{Task}\t&\t\\rot{Model-agnostic?}\t&\t\\rot{Transp.}\t&\t\\rot{Global?}\t&\t\\rot{Obj. Expl.}\t&\t\\rot{Form}\t&\t\\rot{Type}\t\n\\endhead\t\n\\cluster{Self-explaining and blended models}\n\\\\ \\midrule\t-\t&\t\t&\tcls\t&\t\t&\ts\t&\t\t&\tp\t&\tsym/vis\t&\trules/fi\t\n\\\\ \\midrule\t-\t&\t\t&\tany\t&\t\t&\ts\t&\t\t&\tp\t&\tsym/vis\t&\trules/fi\t\n\\\\ \\midrule\tProtoPNet\t&\t\t&\tcls,img\t&\t\t&\ts\t&\t\t&\tp/r\t&\tvis\t&\tproto/fi\t\n\\\\ \\midrule\tCapsule Nets\t&\t\t&\tcls\t&\t\t&\ts\t&\t\t&\tr\t&\tsym\t&\tfi\t\n\\\\ \\midrule\tSemantic Bottlenecks, ReNN, Concept Whitening\t&\t\t&\tany\t&\t\t&\ts\t&\t\t&\tr\t&\tsym\t&\tfi\t\n\\\\ \\midrule\tLogic Tensor Nets\t&\t\t&\tany\t&\t\t&\tb\t&\t\\checkmark\t&\tp/r\t&\tsym\t&\trule\t\n\\\\ \\midrule\tFoldingNet\t&\t\t&\tany,pcl\t&\t\t&\tb\t&\t\t&\tp\t&\tvis\t&\tfi/red\t\n\\\\ \\midrule\tNeuralized clustering\t&\t\t&\tany\t&\t\t&\tb\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\cluster{Black-box heatmapping}\n\\\\ \\midrule\tLIME, SHAP\t&\t\t&\tcls\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi/con\t\n\\\\ \\midrule\tRISE\t&\t\t&\tcls,img\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tD-RISE\t&\t\t&\tdet,img\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tCEM\t&\t\t&\tcls,img\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi/con\t\n\\cluster{White-box heatmapping}\n\\\\ \\midrule\tSensitivity analysis\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tDeconvnet, (Guided) Backprop.\t&\t\t&\timg\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tCAM, Grad-CAM\t&\t\t&\tcls,img\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tSIDU\t&\t\t&\tcls,img\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tConcept-wise Grad-CAM\t&\t\t&\tcls,img\t&\t\t&\tp\t&\t\t&\tp/r\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tSIDU\t&\t\t&\tcls,img\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tLRP\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tPattern Attribution\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\t-\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tSmoothGrad, Integrated Gradients\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tIntegrated Hessians\t&\t\t&\tcls\t&\t\t&\tp\t&\t\t&\tp\t&\tvis\t&\tfi\t\n\\cluster{Global representation analysis}\n\\\\ \\midrule\tFeature Visualization\t&\t\t&\timg\t&\t\t&\tp\t&\t\\checkmark\t&\tr\t&\tvis\t&\tproto\t\n\\\\ \\midrule\tNetDissect\t&\t\t&\timg\t&\t\t&\tp\t&\t\\checkmark\t&\tr\t&\tvis\t&\tproto/fi\t\n\\\\ \\midrule\tNet2Vec\t&\t\t&\timg\t&\t\t&\tp\t&\t(\\checkmark)\t&\tr\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tTCAV\t&\t\t&\tany\t&\t\t&\tp\t&\t\\checkmark\t&\tr\t&\tvis\t&\tfi\t\n\\\\ \\midrule\tACE\t&\t\t&\tany\t&\t\t&\tp\t&\t\\checkmark\t&\tr\t&\tvis\t&\tfi\t\n\\\\ \\midrule\t-\t&\t\t&\tany\t&\t\t&\tp\t&\t\\checkmark\t&\tr\t&\tvis\t&\tproto\t\n\\\\ \\midrule\tIIN\t&\t\t&\tany\t&\t\t&\tp\t&\t(\\checkmark)\t&\tr\t&\tvis/sym\t&\tfi\t\n\\\\ \\midrule\tExplanatory Graph\t&\t\t&\timg\t&\t\t&\tp\t&\t(\\checkmark)\t&\tp/r\t&\tvis\t&\tgraph\t\n\\cluster{Dependency plots}\n\\\\ \\midrule\tPDP\t&\t\t&\tany\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tvis\t&\tplt\t\n\\\\ \\midrule\tICE\t&\t\t&\tany\t&\t\\checkmark\t&\tp\t&\t\\checkmark\t&\tp\t&\tvis\t&\tplt\t\n\\cluster{Rule extraction}\n\\\\ \\midrule\tTREPAN, C4.5, Concept Tree\t&\t\t&\tcls\t&\t\\checkmark\t&\tp\t&\t\\checkmark\t&\tp\t&\tsym\t&\ttree\t\n\\\\ \\midrule\tVIA\t&\t\t&\tcls\t&\t\\checkmark\t&\tp\t&\t\\checkmark\t&\tp\t&\tsym\t&\trules\t\n\\\\ \\midrule\tDeepRED\t&\t\t&\tcls\t&\t\t&\tp\t&\t\\checkmark\t&\tp\t&\tsym\t&\trules\t\n\\\\ \\midrule\tLIME-Aleph\t&\t\t&\tcls\t&\t\\checkmark\t&\tp\t&\t\t&\tp\t&\tsym\t&\trules\t\n\\\\ \\midrule\tCA-ILP\t&\t\t&\tcls\t&\t\t&\tp\t&\t\\checkmark\t&\tp\t&\tsym\t&\trules\t\n\\\\ \\midrule\tNBDT\t&\t\t&\tcls\t&\t\t&\tp\t&\t\\checkmark\t&\tp\t&\tsym\t&\ttree\t\n\\cluster{Interactivity}\n\\\\ \\midrule\tCAIPI\t&\t\t&\tcls,img\t&\t\\checkmark\t&\tp\t&\t\t&\tr\t&\tvis\t&\tfi/con\t\n\\\\ \\midrule\tEluciDebug\t&\t\t&\tcls\t&\t\\checkmark\t&\tp\t&\t\t&\tr\t&\tvis\t&\tfi,plt\t\n\\\\ \\midrule\tCrayons\t&\t\t&\tcls,img\t&\t\\checkmark\t&\tt\t&\t\t&\tp\t&\tvis\t&\tplt\t\n\\\\ \\midrule\tLearnWithME\t&\t\t&\tcls\t&\t\\checkmark\t&\tt\t&\t\\checkmark\t&\tp, r\t&\tsym\t&\trules\t\n\\\\ \\midrule\tMulti-modal phrase-critic model\t&\t\t&\tcls,img\t&\t\t&\tp\t&\t\\checkmark\t&\tp\t&\tvis,sym\t&\tplt,rules\t\n\\cluster{Inspection of the training}\n\\\\ \\midrule\t-\t&\t\t&\tany\t&\t\t&\tp\t&\t\\checkmark\t&\tt\t&\tvis\t&\tdist\t\n\\\\ \\midrule\tInfluence functions\t&\t\t&\tcls\t&\t\t&\tp\t&\t\\checkmark\t&\tt\t&\tvis\t&\tfi/dist\t\n\\cluster{Data analysis methods}\n\\\\ \\midrule\tt-SNE, PCA\t&\t\t&\tany\t&\t\\checkmark\t&\tp\t&\t\\checkmark\t&\td\t&\tvis\t&\tred\t\n\\\\ \\midrule\tk-means, spectral clustering\t&\t\t&\tany\t&\t\\checkmark\t&\tp\t&\t\\checkmark\t&\td\t&\tvis\t&\tproto\t\n\\\\ \\bottomrule\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\\end{tabularx}\n\\normalsize\n\\endgroup", "cites": [1816, 1813, 1823, 1818, 737, 1824, 7067, 499, 7517, 1815, 1811, 1809, 1814, 1820, 7516, 8499, 1810, 7515, 306, 1819, 7068, 1806, 1822, 7512, 8498, 1825, 7507, 7511, 1812, 1802, 1807, 7506, 1817, 8501, 7513], "cite_extract_rate": 0.5645161290322581, "origin_cites_number": 62, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes application-grounded metrics for XAI, listing their definitions and examples. It provides limited synthesis of the cited papers, and instead focuses on categorizing methods and their characteristics. There is minimal critical analysis or abstraction, as the discussion remains mostly at the level of definitions and does not explore deeper connections, trade-offs, or overarching principles."}}
