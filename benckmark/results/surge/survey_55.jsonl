{"id": "f9bc4b42-58de-468c-aa34-a7fafef466f0", "title": "Introduction", "level": "section", "subsections": ["e07ad078-3971-4d39-be8b-bc99cd7f9940", "fcf8b532-0bf6-44b4-8136-b7602b1b6449", "4bbf405c-f271-483c-9f3d-b320ee98d639"], "parent_id": "1912a489-c536-4d4e-8859-51a6be4f79c3", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Introduction"]], "content": "Applications of stream processing technology have gone through a resurgence, penetrating multiple and very diverse industries. Nowadays, virtually all Cloud vendors offer first-class support for deploying managed stream processing pipelines, while streaming systems are used in a variety of use-cases that go beyond the classic streaming analytics (windows, aggregates, joins, etc.). For instance, web companies are using stream processing for dynamic car-trip pricing, banks apply it for credit card fraud detection, while traditional industries apply streaming technology for real-time harvesting analytics. \nAt the moment of writing we are witnessing a trend towards using stream processors to build more general event-driven architectures , large-scale continuous ETL and analytics, and microservices .\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=.75\\linewidth]{Figures/overview.pdf}\n\\caption{\\small An overview of the evolution of stream processing and respective domains of focus.}\n\\label{fig:overview}\n\\end{figure*}\nDuring the last 20 years, streaming technology has evolved significantly, under the influence of database and distributed systems. The notion of streaming queries was first introduced in 1992 by the Tapestry system~, and was followed by lots of research on stream processing in the early 00s. Fundamental concepts and ideas originated in the database community and were implemented in prototypical systems such as TelegraphCQ~, Stanford's STREAM, NiagaraCQ~, Auroral/Borealis~, and Gigascope~. Although these prototypes roughly agreed on the data model, they differed considerably on querying semantics . This research period also introduced various systems challenges, such as sliding window aggregation , fault-tolerance and high-availability , as well as load balancing and shedding . This first wave of research was highly influential to commercial stream processing systems that were developed in the following years (roughly during 2004 -- 2010), such as IBM System S, Esper, Oracle CQL/CEP and TIBCO. These systems focused -- for the most part -- on streaming window queries and Complex Event Processing (CEP). This era of systems was mainly characterized by scale-up architectures, processing ordered event streams.\nThe second generation of streaming systems was a result of research that started roughly after the introduction of MapReduce~ and the popularization of Cloud Computing. The focus shifted towards distributed, data-parallel processing engines and shared-nothing architectures on commodity hardware. Lacking well-defined semantics and a proper query language, systems like Millwheel , Storm~, Spark Streaming~, and Apache Flink~ first exposed primitives for expressing streaming computations as hard-coded dataflow graphs and transparently handled data-parallel execution on distributed clusters. With very high influence, the Google Dataflow model  re-introduced older ideas such as out-of-order processing  and punctuations~, proposing a unified parallel processing model for streaming and batch computations. Stream processors of this era are converging towards fault-tolerant, scale-out processing of massive out-of-order streams.\nFigure~\\ref{fig:overview} presents a schematic categorization of influential streaming systems into three generations and highlights each era's domains of focus. Although the foundations of stream processing have remained largely unchanged over the years, stream processing systems have transformed into sophisticated and scalable engines, producing correct results in the presence of failures. Early systems and languages were designed as extensions of relational execution engines, with the addition of windows. Modern streaming systems have evolved in the way they reason about completeness and ordering (e.g., out-of-order computation) and have witnessed architectural paradigm shifts that constituted the foundations of processing guarantees, reconfiguration, and state management. At the moment of writing, we observe yet another paradigm shift towards general event-driven architectures, actor-like programming models and microservices~, and a growing use of modern hardware~.\nThis survey is the first to focus on the evolution of streaming systems rather than the state of the field at a particular point in time. To the best of our knowledge, this is also the first attempt at understanding the underlying reasons why certain early techniques and designs prevailed in modern systems while others were abandoned. Further, by examining how ideas survived, evolved, and were often re-invented, we reconcile the terminology used by the different generations of streaming systems.", "cites": [4757], "cite_extract_rate": 0.03571428571428571, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the evolution of stream processing systems over two decades, integrating insights from multiple cited works and framing them within a generational progression. It shows abstraction by identifying overarching trends such as architectural shifts, querying semantics, and the influence of hardware and distributed systems. While it provides a clear analytical perspective, it does not deeply critique specific papers or identify major limitations, which limits its critical score slightly."}}
{"id": "fcf8b532-0bf6-44b4-8136-b7602b1b6449", "title": "Related surveys and research collections", "level": "subsection", "subsections": [], "parent_id": "f9bc4b42-58de-468c-aa34-a7fafef466f0", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Introduction"], ["subsection", "Related surveys and research collections"]], "content": "We view the following surveys as complementary to ours and recommend them to readers interested in diving deeper into a particular aspect of stream processing or those who seek a comparison between streaming technology and advances from adjacent research communities.\n\\setlength{\\tabcolsep}{8pt}\n\\begin{table*}[ht]\n\\centering\n\\caption{Evolution of streaming systems}\n\\label{tab:evolution-streaming}\n\\begin{tabular}{p{0.3\\textwidth}\n                        p{0.4\\textwidth}\n                        p{0.4\\textwidth}\n                        }\n\\hline\n\\multicolumn{1}{l}{\\textbf{}} &\n\\multicolumn{1}{l}{\\textbf{1st generation}} &\n\\multicolumn{1}{l}{\\textbf{2nd-3rd generation}}\n\\\\\n\\hline\n\\multicolumn{1}{l}{\\textbf{Results}}\n& \\multicolumn{1}{e}{approximate or exact}\n& \\multicolumn{1}{d}{exact}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Language}}\n& \\multicolumn{1}{e}{SQL extensions, CQL}\n& \\multicolumn{1}{d}{UDF-heavy -- Java, Scala, Python, SQL-like, etc.}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Query plans}}\n& \\multicolumn{1}{e}{global, optimized, with pre-defined operators}\n& \\multicolumn{1}{d}{independent, with custom operators}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Execution}}\n& \\multicolumn{1}{e}{(mostly) scale-up}\n& \\multicolumn{1}{d}{distributed}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Parallelism}}\n& \\multicolumn{1}{e}{pipeline}\n& \\multicolumn{1}{d}{data, pipeline, task}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Time \\& progress}}\n& \\multicolumn{1}{e}{heartbeats, slack, punctuations}\n& \\multicolumn{1}{d}{low-watermark, frontiers}\n\\\\\n\\multicolumn{1}{l}{\\textbf{State management}}\n& \\multicolumn{1}{e}{shared synopses, in-memory}\n& \\multicolumn{1}{d}{per query, partitioned, persistent, larger-than-memory}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Fault tolerance}}\n& \\multicolumn{1}{e}{HA-focused, limited  correctness guarantess}\n& \\multicolumn{1}{d}{distributed snapshots, exactly-once}\n\\\\\n\\multicolumn{1}{l}{\\textbf{Load management}}\n& \\multicolumn{1}{e}{load shedding, load-aware scheduling}\n& \\multicolumn{1}{d}{backpressure, elasticity}\n\\\\\n\\hline\n\\end{tabular}\n\\end{table*}\nCugola and Margara~ provide a view of stream processing with regard to related technologies, such as active databases and complex event processing systems, and discuss their relationship with data streaming systems. Further, they provide a categorization of streaming languages and streaming operator semantics. The language aspect is further covered in another recent survey~, which focuses on the languages developed to address the challenges in very large data streams. It characterizes  streaming languages in terms of data model, execution model, domain, and intended user audience. R{\\\"o}ger and Mayer~ present an overview of recent work on parallelization and elasticity approaches of streaming systems. They define a general system model which they use to introduce operator parallelization strategies and parallelism adaptation methods. Their analysis also aims at comparing elasticity approaches originating in different research communities. Hirzel et al.~ present an extensive list of logical and physical optimizations for streaming query plans. They present a categorization of streaming optimizations in terms of their assumptions, semantics, applicability scenarios, and trade-offs. They also present experimental evidence to reason about profitability and guide system implementers in selecting appropriate optimizations. To, Soto, and Markl~ survey the concept of state and its applications in big data management systems, covering also aspects of streaming state.\nFinally, Dayarathna and Perera~ present a survey of the advances of the last decade with a focus on system architectures, use-cases, and hot research topics. They summarize recent systems in terms of their features, such as what types of operations they support, their fault-tolerance capabilities, their use of programming languages, and their best reported performance.\nTheoretical foundations of streaming data management and streaming algorithms are out of the scope of this survey. A comprehensive collection of influential works on these topics can be found in Garofalakis et al.~. The collection focuses on major contributions of the first generation of streaming systems. It reviews basic algorithms and synopses, fundamental results in stream data mining, streaming languages and operator semantics, and a set of representative applications from different domains.", "cites": [4758, 4759], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparison between the 1st and 2nd-3rd generations of stream processing systems using a table, which helps highlight the evolution of key features. This synthesis is useful and connects the cited surveys to broader system design trends. However, it lacks deeper critical analysis of the cited works' limitations or biases, and while it identifies some patterns (e.g., in language use and fault tolerance), it does not elevate them to meta-level principles."}}
{"id": "eda7c585-042b-490a-b5d3-31fa35f1927c", "title": "Relational Streaming Model", "level": "subsubsection", "subsections": [], "parent_id": "27e5bea0-0d7e-4c63-95d8-7c8ef9f7fda3", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Preliminaries"], ["subsection", "Streaming data models"], ["subsubsection", "Relational Streaming Model"]], "content": "In the relational streaming model as implemented by first-generation systems~, a stream is interpreted as describing a changing relation over a common schema. Streams are either produced by external sources and update relation tables or are produced by continuous queries and update materialized views. An operator outputs event streams that describe the changing view computed over the input stream according to the relational semantics of the operator. Thus, the semantics and schema of the relation are imposed by the system. \nSTREAM~ defines streams as bags of tuple-timestamp pairs and relations as time-varying bags of tuples. The implementation unifies both types as sequences of timestamped tuples, where each tuple also carries a flag that denotes whether it is an insertion or a deletion. Input streams consist of insertions only, while relations may also contain deletions. TelegraphCQ~ uses a similar data model. Aurora~ models streams as append-only sequences of tuples, where a set of attributes denote the key and the rest of the attributes denote values. Borealis~ generalizes this model to support insertion, deletion, and replacement messages. Messages may also contain additional fields related to QoS metrics. Gigascope~ extends the sequence database model. It assumes that stream elements bear one or more timestamps or sequence numbers, which generally increase (or decrease) with the ordinal position of a tuple in a stream. Ordering attributes can be (strictly) monotonically increasing or decreasing, monotone non-repeating, or increasing within a group of records. In CEDR~, stream elements bear a valid timestamp, $V_{s}$, after which they are considered valid and can contribute to the result. Alternatively, events can have validity intervals. The contents of the relation at time $t$ are all events with $V_{s} \\leq t$.", "cites": [8837], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the relational streaming model as used in first-generation systems and briefly mentions several systems (STREAM, Aurora, Borealis, etc.) along with their data model features. While it organizes information thematically, it lacks deeper synthesis of how these models relate or evolve. There is minimal critical analysis or evaluation of the models' strengths and limitations, and the abstraction remains at a basic level, focusing more on individual system characteristics than broader principles."}}
{"id": "23e6dc62-b2ef-4514-8ef1-515f92d2667c", "title": "Revision processing", "level": "subsubsection", "subsections": ["17e6dc27-9fbe-4121-bb9f-0ecdf4cbe9c8", "f78a88b0-9a38-416b-89d2-f3897c778384", "6beb97a7-3c2e-44d8-bab6-efbe80c5cfe1"], "parent_id": "ad45beb9-c999-4e67-9a7a-bf9201d42e7a", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Managing Event Order and Timeliness"], ["subsection", "Mechanisms for Managing Disorder"], ["subsubsection", "Revision processing"]], "content": "\\label{subsubsec:revision-processing}\nRevision processing is the update of computations in face of late, updated, or retracted data, which require the modification of previous outputs in order to provide correct results.\nRevision processing made its debut in Borealis~.\nFrom there on, it has been combined with in-order processing architectures~, as well as\nout-of-order processing architectures~.\nIn some approaches revision processing works by \\textit{storing} incoming data and \\textit{revising} computations in face of late, updated, or retracted data~.\nOther approaches \\textit{replay} affected data, \\textit{revise} computations, and propagate the revision messages to update all affected results until the present~.\nFinally, a third line of approaches maintain multiple \\textit{partitions} that capture events with different levels of lateness and \\textit{consolidate} partial results~.", "cites": [8837], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes revision processing and categorizes approaches (storing, replaying, partitioning) but lacks integration of ideas from the cited papers. It does not critically evaluate the strengths or weaknesses of these mechanisms or identify broader trends or principles in the field."}}
{"id": "17e6dc27-9fbe-4121-bb9f-0ecdf4cbe9c8", "title": "Store and revise.", "level": "paragraph", "subsections": [], "parent_id": "23e6dc62-b2ef-4514-8ef1-515f92d2667c", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Managing Event Order and Timeliness"], ["subsection", "Mechanisms for Managing Disorder"], ["subsubsection", "Revision processing"], ["paragraph", "Store and revise."]], "content": "Microsoft's CEDR~ and StreamInsight~, and Google's Dataflow~ buffer or store stream data and process late events, updates, and deletions incrementally by revising the captured values and updating the computations.\nThe dataflow model~ divides the concerns for out-of-order data into three dimensions: the event time when late data are processed, the processing time when corresponding results are materialized, and how later updates relate to earlier results.\nThe mechanism that decides the emission of updated results and how the refinement will happen is called a \\textit{trigger}.\nTriggers are signals that cause a computation to be repeated or updated when a set of specified rules fire.\nOne important rule regards the arrival of late input data.\nTriggers ensure output correctness by incorporating the effects of late input into the computation results.\nTriggers can be defined based on watermarks, processing time, data arrival metrics,\nand combinations of those; they can also be user-defined.\nTriggers support three refinement policies, accumulating where new results overwrite older ones, discarding where new results complement older ones, and accumulating and retracting where new results overwrite older ones and older results are retracted.\nRetractions, or compensations, are also supported in StreamInsight~.", "cites": [8837], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information about stream processing systems by discussing the 'store and revise' mechanism, referencing both Microsoft and Google systems, and connecting it to the dataflow model. It provides a coherent explanation of triggers and refinement policies, showing some synthesis of concepts. However, it lacks deeper critical evaluation of the cited works and primarily focuses on summarizing features rather than offering a meta-level analysis of broader patterns or limitations."}}
{"id": "6beb97a7-3c2e-44d8-bab6-efbe80c5cfe1", "title": "Partition and consolidate.", "level": "paragraph", "subsections": [], "parent_id": "23e6dc62-b2ef-4514-8ef1-515f92d2667c", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Managing Event Order and Timeliness"], ["subsection", "Mechanisms for Managing Disorder"], ["subsubsection", "Revision processing"], ["paragraph", "Partition and consolidate."]], "content": "Both \\emph{order-independent processing}~ and \\emph{impatience sort}~ are based on partial processing of independent partitions in parallel and consolidation of partial results.\nIn order-independent processing, when a tuple is received after its corresponding progress indicator a new partition is opened and a new query plan instance processes this partition using standard out-of-order processing techniques.\nOn the contrary, in impatience sort, the latest episode of the vision of CEDR~, an online sorting operator incrementally orders the input arriving at each partition so that it is emitted in order.\nThe approach uses punctuations to bound the disorder as opposed to order-independent processing which can handle events arriving arbitrarily late.\nIn order-independent processing, partitioning is left for the system to decide while in impatience sort it is specified by the users.\nIn order-independent processing, tuples that are too old to be considered in their original partition are included in the partition which has the tuple with the closest data.\nWhen no new data enter an ad-hoc partition for a long time, the partition is closed and destroyed by means of a heartbeat.\nAd-hoc partitions are window-based; when an out-of-order tuple is received that does not belong to one of the ad-hoc partitions, a new ad-hoc partition is introduced.\nAn out-of order tuple with a more recent timestamp than the window of an ad-hoc partition causes that partition to flush results and close.\nOrder-independent processing is implemented in Truviso.\nOn the contrary, in impatience sort, users specify reorder latencies, such as $1ms$, $100ms$, and $1s$, that define the buffering time for ingesting and sorting out-of-order input tuples.\nAccording to the specified reorder latencies, the system creates different partitions of in-order input streams.\nAfter sorting, a union operator merges and synchronizes the output of a partition $P$ with the output of a partition $L$ that features lower reorder latency than $P$.\nThus, the output will incorporate partial results provided by $L$ with later updates that $P$ contains.\nThis way applications that require fast but partial results can subscribe to a partition with small reorder latency and vice versa.\nBy letting applications choose the desired extent of reorder latency this design provides for different trade-offs between completeness and freshness of results.\nImpatience sort is implemented in Microsoft Trill.", "cites": [8837], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively compares 'order-independent processing' and 'impatience sort' by highlighting their mechanisms, user control, and system behavior. It synthesizes details from the cited paper on CEDR and integrates them into a coherent explanation of how these two approaches manage disorder differently. While there is some abstraction in identifying how reorder latency affects result completeness and freshness, the analysis remains descriptive in nature with limited critique of the approaches or deeper theoretical insights."}}
{"id": "642969c0-7ad3-4b8d-a803-892c464c2e58", "title": "1st generation vs. 2nd generation", "level": "subsection", "subsections": [], "parent_id": "2af1dcb8-7a81-4897-bd68-275168901eef", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Managing Event Order and Timeliness"], ["subsection", "1st generation vs. 2nd generation"]], "content": "\\label{sub:vintage}\nThe importance of event order in data stream processing became obvious since its early days~ leading to the first wave of simple intuitive solutions. Early approaches involved buffering and reordering arriving tuples using some measure for adjusting the frequency and lateness of data dispatched to a streaming system in order~.\nA few years later, the introduction of out-of-order processing~ improved throughput, latency, and scalability for window operations by keeping track of processing progress without ordering tuples.\nIn the meantime, revision processing~ was proposed as a strategy for dealing with out-of-order data reactively.\nIn the years to come, in-order, out-of-order, and revision processing were extensively explored, often in combination with one another~.\nModern streaming systems implement a refinement of these original concepts.\nInterestingly, concepts devised several years ago, like low-watermarks, punctuations, and triggers, which advance the original revision processing, were popularized recently by streaming systems such as Millwheel~ and the Google Dataflow model~, Flink~, and Spark~. Table~\\ref{tab:disorder-management-streaming} presents how both 1st generation and modern streaming systems implement out-of-order data management.", "cites": [8837], "cite_extract_rate": 0.07142857142857142, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a basic chronological overview of event order management strategies but lacks in-depth synthesis or comparison across the cited works. While it mentions the evolution of ideas, it does not clearly integrate insights from multiple sources or critically assess their merits and shortcomings. The abstraction is limited to surface-level generalizations rather than identifying deeper principles or frameworks."}}
{"id": "4d611f34-13ad-4d1b-b9b9-ecaa134619ef", "title": "State Persistence at Epoch Granularity", "level": "subsubsection", "subsections": [], "parent_id": "12be1f86-2bed-464d-829f-537405a4329f", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "State Management"], ["subsection", "Managing Consistency and Persistence"], ["subsubsection", "State Persistence at Epoch Granularity"]], "content": "Instead of adopting state persistence on a per-record granularity processing, epoch-level approaches divide computation into a series of mini-batches, also known as ``epochs''.\nIn \\autoref{fig:epochcommits} we depict the overall approach, marking input, system states and outputs with a distinct epoch identifier. Epochs can be defined through markers at the logged input of the streaming application. A system execution can be instrumented to process each epoch and commit the state of the entire task graph after each epoch is processed. If a failure or other reconfiguration action happens during the execution of an epoch then the system can roll back to a previously committed epoch and recover its execution. The term ``exactly-once processing'' in this context relates to each epoch being atomically committed. In Section~\\ref{sub:fault-tolerance} where we present the different  levels of processing semantics in streaming we call this flavor \\textit{exactly-once processing on state}. The rest of this section focuses on various approaches used to commit stream epochs.\n\\stitle{Strict Two-Phase Epoch Commits.} A common coordinated protocol to commit epochs is a strict two-phase commit where: Phase-1 corresponds to the full processing of an epoch and the Phase-2 ensures persisting the state of the system at the end of the computation. \nThis approach was popularized by Apache Spark  through the use of periodic ``micro-batching'' and it is an effective strategy when batch processing systems are used for unbounded processing. The main downside of this approach is the risk of low task utilization due to synchronous execution, since tasks have to wait for all other tasks to finish their current epoch. Drizzle~ mitigates this problem by chaining multiple epochs in a single atomic commit. A similar approach was also employed by S-Store~, where each database transaction corresponds to an epoch of the input stream that is already stored in the same database.\n\\stitle{Asynchronous Two-Phase Epoch Commits.} \nFor pure dataflow systems, strict two-phase committing is problematic since tasks are uncoordinated and long-running. Furthermore, it is feasible to achieve the same functionality asynchronously through consistent snapshotting algorithms, known from classic distributed systems literature . Consistent snapshotting algorithms exhibit beneficial properties because they not require pausing a streaming application. Furthermore, they acquire a snapshot of a consistent cut in a distributed execution . In other words, they capture the global states of the system during a ``valid'' execution. Throughout different implementations we can identify i) unaligned and ii) aligned snapshotting protocols.\n\\para{I. Unaligned / Chandy-Lamport  snapshots} provide one of the most efficient methods to obtain a consistent snapshot. This approach is currently supported by several stream processors, such as IBM Streams and Flink. The core idea is to make use of a punctuation or ``marker'', into the regular stream of events and use that marker to separate all actions that come before and after the snapshot while the system is running. A caveat of unaligned snapshots is the need to record input (a.k.a. in-flight) events that arrive to individual tasks until the protocol is complete. In addition to space overhead for logged inputs, unaligned snapshots require more processing during recovery, since logged inputs need to be replayed (similarly to redo logs in database recovery with fuzzy checkpoints).\n\\para{II. Aligned Snapshots}\nAligned snapshots aim to improve performance during recovery and minimize reconfiguration complexity exhibited by unaligned snapshots. The main differentiation is to prioritize input streams that are expected before the snapshot and thus, end up solely with states that reflect a complete computation of an epoch and no in-flight events as part of a snapshot. For example, Flink's epoch snapshotting mechanism  resembles the Chandy Lamport algorithm in terms of using markers to identify epoch frontiers. However, it additionally employs an alignment phase that synchronizes markers within tasks before disseminating further. This is achieved through partially blocking input channels where markers were previously received until all input channels have transferred all messages corresponding to a particular epoch. \nIn summary, unaligned snapshots are meant to offer the best runtime performance but sacrifice recovery times due to the redo-phase needed upon recovery. Whereas, aligned snapshots can lead to slower commit times due to the alignment phase while providing a set of beneficial properties. First, aligned snapshots reflect a complete execution of an epoch which is useful in use-cases where snapshot isolated queries need to be supported on top of data streaming . Furthermore, aligned snapshots yield the lowest reconfiguration footprint as well as setting the basis for live reconfiguration within the alignment phase as exhibited by Chi .", "cites": [7843, 4760], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from the cited papers to explain epoch-level state persistence mechanisms in stream processing systems. It provides a critical evaluation of trade-offs between unaligned and aligned snapshots, noting performance and recovery implications. The abstraction level is strong as it generalizes these mechanisms into a broader conceptual framework, linking them to broader distributed systems concepts like consistent snapshotting."}}
{"id": "eee37e9d-c1e5-4718-8d08-45f165bb37aa", "title": "Open Problems", "level": "subsection", "subsections": [], "parent_id": "859d816d-87a6-416f-a8b8-81462a1d1466", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "State Management"], ["subsection", "Open Problems"]], "content": "Data streaming covers many data management needs today that go beyond real-time analytics, which was the original purpose of the stream processing technology. New needs include support for more complex data pipelines with implicit transactional guarantees. Furthermore, modern applications involve Machine Learning, Graph Analysis and Cloud Apps, all of which have a common denominator: complex state and new access patterns. These needs have cultivated novel research directions in the emerging field of stream state management.\nThe decoupling of state programming from state persistence resembles the concept of data independence in databases. Systems are converging in terms of semantics and operations on state while, at the same time many new methods employed on embedded databases (e.g., LSM-trees, state indexing, externalized state) are helping stream processors to evolve in terms of performance capabilities. A recent study~ showcases the potential of workload-aware state management, adapting state persistence and access to the individual operators of a dataflow graph. To this end, an increasing number of ``pluggable'' systems~ for local state management with varying capabilities are being adopted by stream processors. This opens new capabilities for optimization and sophisticated, yet transparent state management that can automate the process of selecting the right physical plan and reconfigure that plan while continuous applications are executed.", "cites": [4761], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by identifying emerging needs and research directions in stream state management, including workload-aware approaches and pluggable systems. It synthesizes concepts from the cited paper and database principles, such as data independence and state indexing. While it mentions limitations in system designs (e.g., transactional vs. analytical trade-offs), it stops short of in-depth evaluation or comparison of the cited works, and the abstraction is moderate but not at a meta-level."}}
{"id": "c8f9a49c-fc34-48ed-a538-db893d951b19", "title": "High availability", "level": "subsection", "subsections": [], "parent_id": "60b7aebd-8cd3-47cb-a168-613a863dea95", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Fault Tolerance \\& High Availability"], ["subsection", "High availability"]], "content": "\\label{sub:high-availability}\nEmpirical studies of high availability in stream processing~ propose an active replication approach~, a passive replication approach~, a hybrid active-passive replication approach~, or model multiple approaches and evaluate them with simulated experiments~.\n\\para{Active replication.}\nFlux~ implements active replication by duplicating the computation and coordinating the progress of the two replicas.\nFlux restores operator state and in-flight data of a failed partition while the other partition continues to process input.\nA new primary dataflow that runs following a failure quiesces when a new secondary dataflow is ready in a standby machine in order to copy the state of its operators to the new secondary.\nContrastingly, Borealis~ has nodes address upstream node failures by switching to a live replica of the failed upstream node.\nIf a replica is not available, the node can produce tentative output for incomplete input to avoid the recovery delay.\nThe approach sacrifices consistency to optimize availability, but guarantees eventual consistency.\n\\para{Passive replication.}\nHwang et al.~ propose that a server in a cluster has another server as backup where it ships independent parts of its checkpointed state.\nWhen a node fails, its backup servers that hold parts of its checkpointed state initiate recovery in parallel by starting to execute the operators of the failed node whose state they have and collecting the input tuples they have missed from the checkpointed state they possess.\nSGuard~ and Clonos~ save computational resources in another way by checkpointing state asynchronously to a distributed file system.\nUpon a failure a node is selected to run a failed operator.\nThe operator's state is loaded from the file system and its in-memory state is reconstructed before it can join the job.\nBeyond asynchronous checkpointing,\na new checkpoint mechanism~ preserves output tuples until an acknowledgment is received from all downstream operators.\nNext, an operator trims its output tuples and takes a checkpoint.\nThe authors show that passive replication still requires longer recovery time than active replication, but with 90\\% less overhead due to reduced checkpoint size.\n\\para{Hybrid replication.}\nZwang et al.~ propose a hybrid approach to replication, which operates in passive mode under normal operation, but switches to active mode using a suspended pre-deployed secondary copy when a transient failure occurs.\nAccording to the provided experiment results, their approach saves 66\\% recovery time\ncompared to passive replication\nand produces 80\\% less message overhead than active replication.\nAlternatively, Heinze et al.~ propose to dynamically choose the replication scheme for each operator, either active replication or upstream backup, in order to reduce the recovery overhead of the system by limiting the peak latency under failure below a threshold.\nSimilarly, Su et al.~ counter correlated failures by passively replicating processing tasks except for a dynamically selected set that is actively replicated.\n\\para{Modeling and simulations.}\nIn their seminal work Hwang et al.~\nmodel and evaluate the recovery time and runtime overhead of four recovery approaches, active standby, passive standby, upstream backup, and amnesia, across different types of query operators.\nThe simulated experiments suggest that active standby achieves near-zero recovery time at the expense of high overhead in terms of resource utilization, while passive standby produces worse results in terms of both metrics compared to active standby.\nHowever, passive standby poses the only option for arbitrary query networks.\nUpstream backup has the lowest runtime overhead at the expense of longer recovery time.\nWith a similar goal, Shrink~, a distributed systems emulator,\nevaluates the models of five different resiliency strategies\nwith respect to uptime \\textsc{sla} and resource reservation.\nThe strategies differ across three axes, single-node vs multi-node, active vs passive replication, and checkpoint vs replay.\nAccording to the experiments with real queries on real advertising data using\nTrill~, active replication with periodic checkpoints\nis proved advantageous in many streaming workloads,\nalthough no single strategy is appropriate for all of them.", "cites": [4762], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple replication approaches (active, passive, hybrid, and simulations) across several systems and studies, connecting them into a coherent narrative about high availability in stream processing. It includes critical analysis by comparing trade-offs such as recovery time, overhead, and consistency guarantees. The section also abstracts from individual implementations to identify overarching design principles and patterns, such as the balance between consistency and availability or the impact of checkpointing strategies on recovery performance."}}
{"id": "c48c9908-5343-4d71-9072-8d2e3ca530f6", "title": "Elasticity policies", "level": "subsubsection", "subsections": [], "parent_id": "8918a236-3757-4be5-8e98-bad1687de8b5", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Load management, elasticity, \\& reconfiguration"], ["subsection", "Elasticity"], ["subsubsection", "Elasticity policies"]], "content": "A \\emph{scaling policy} involves two individual decisions. First, it needs to detect the symptoms of an unhealthy computation and decide whether scaling is necessary. Symptom detection is a well-understood problem and can be addressed using conventional monitoring tools.\nSecond, the policy needs to identify the causes of exhibited symptoms (e.g. a bottleneck operator) and propose a scaling action. This is a challenging task which requires performance analysis and prediction.\nIt is common practice to place the burden of scaling decisions on application users who have to face conflicting incentives. They can either plan for the highest expected workload, possibly incurring high cost, or they can choose to be conservative and risk degraded performance.\nAutomatic scaling refers to scaling decisions transparently handled by the streaming system in response to load.\nCommercial streaming systems that support automatic scaling include Google Cloud Dataflow~, Heron~, and IBM System S~, while DS2~, Seep~ and StreamCloud~ are recent research prototypes.\nIn Table~\\ref{tbl:scaling-comparison}, we categorize policies into \\emph{heuristic} and \\emph{predictive}. Heuristic policies rely on empirically predefined rules and are often triggered by thresholds or observed conditions while predictive policies make scaling decisions guided by analytical performance models.\nHeuristic policy controllers gather coarse-grained metrics, such as CPU utilization, observed throughput, queue sizes, and memory utilization, to detect suboptimal scaling. CPU and memory utilization can be inadequate metrics for streaming applications deployed in cloud environments due to multi-tenancy and performance interference~. StreamCloud~ and Seep~ try to mitigate the problem by separating user time and system time, but\npreemption can make these metrics misleading. For example, high CPU usage caused by a task running on the same physical machine as a dataflow operator can trigger incorrect scale-ups (false positives) or prevent correct scale-downs (false negatives). Google Cloud Dataflow~ relies on CPU utilization for scale-down decisions only but still suffers false negatives.\nDhalion~ and IBM Streams~ also use congestion and back-pressure signals to identify bottlenecks. These metrics are helpful for identifying bottlenecks but they cannot detect resource over-provisioning.\n\\setlength{\\tabcolsep}{4pt}\n\\begin{table*}\n\\smaller\\centering\n\\caption{Elasticity policies and mechanisms in streaming systems}\n\\label{tbl:scaling-comparison}\n\\begin{tabular}{p{0.15\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.06\\textwidth}\n                        p{0.08\\textwidth}\n                        }\n\\hline\n\\multicolumn{1}{c}{\\textbf{System}} &\n\\multicolumn{2}{c}{\\textbf{Policy}} &\n\\multicolumn{2}{c}{\\textbf{Objective}} &\n\\multicolumn{3}{c}{\\textbf{Reconfiguration}} &\n\\multicolumn{2}{c}{\\textbf{State Migration}}\n\\\\\n\\rowcolor{white}\n& \\multicolumn{1}{c}{Heuristic}\n& \\multicolumn{1}{c}{Predictive}\n& \\multicolumn{1}{c}{Latency}\n& \\multicolumn{1}{c}{Throughput}\n& \\multicolumn{1}{c}{Stop-and-Restart}\n& \\multicolumn{1}{c}{Partial Pause}\n& \\multicolumn{1}{c}{Live} \n& \\multicolumn{1}{c}{At-Once}\n& \\multicolumn{1}{c}{Progressive} \\\\\n\\hline\nBorealis~\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{3}{a}{n/a}\n& \\multicolumn{2}{b}{n/a}\n\\\\\nStreamCloud~ \n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nSeep~ \n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nIBM Streams~ \n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n&  \\multicolumn{1}{b}{}\n\\\\\nFUGU~ \n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nNephele~ \n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{}\n\\\\\nDRS~ \n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{}\n\\\\\nMPC~ \n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nCometCloud~ \n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{2}{b}{n/a}\n\\\\\nChronostream~ \n& \\multicolumn{2}{a}{n/a}\n& \\multicolumn{2}{b}{n/a}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nACES~ \n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{3}{a}{n/a}\n& \\multicolumn{2}{b}{n/a}\n\\\\\nStella~ \n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{}\n\\\\ \nGoogle~Dataflow~\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{}\n\\\\ \nDhalion~\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\ \nDS2~\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\ \nSpark Streaming~\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nMegaphone~\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n\\\\\nTurbine~\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\nRhino~\n& \\multicolumn2{a}{n/a}\n& \\multicolumn{2}{b}{n/a}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{a}{\\checkmark}\n& \\multicolumn{1}{a}{}\n& \\multicolumn{1}{b}{\\checkmark}\n& \\multicolumn{1}{b}{}\n\\\\\n\\hline\n\\end{tabular}\n\\end{table*}\nPredictive policy controllers build an analytical performance model of the streaming system and formulate the scaling problem as a set of mathematical functions. Predictive approaches include queuing theory~, control theory~, and instrumentation-driven linear performance models~. Thanks to their closed-form analytical formulation, predictive policies are capable of making multi-operator decisions in one step.", "cites": [4763], "cite_extract_rate": 0.041666666666666664, "origin_cites_number": 24, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section organizes and compares various systems based on their use of heuristic versus predictive elasticity policies and their associated mechanisms. It provides some synthesis by categorizing these systems in a table, which helps identify patterns. However, it lacks deeper analysis of limitations or broader theoretical implications, and the abstraction remains limited to the categorization rather than identifying higher-level principles or trends."}}
{"id": "24a60a5e-5309-48db-a30e-2b25c7c7f6b1", "title": "Elasticity mechanisms", "level": "subsubsection", "subsections": [], "parent_id": "8918a236-3757-4be5-8e98-bad1687de8b5", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Load management, elasticity, \\& reconfiguration"], ["subsection", "Elasticity"], ["subsubsection", "Elasticity mechanisms"]], "content": "Elasticity mechanisms are concerned with realizing the actions indicated by the policy. \nThey need to ensure correctness and low-latency redistribution of accumulated state when effecting a reconfiguration. To ensure correctness, many streaming systems rely on the fault-tolerance mechanism to provide reconfiguration capabilities. When adding new workers to a running computation, the mechanism needs not only re-assign work to them but also migrate any necessary state these new workers will now be in charge of. \nElasticity mechanisms need to complete a reconfiguration as quickly as possible and at the same time minimize performance disruption. We review the main methods for state redistribution, reconfiguration, and state transfer next. We focus on systems with embedded state, as reconfiguration mechanisms are significantly simplified when state is external.\n\\stitle{State redistribution.}\nState redistribution must preserve key semantics, so that existing state for a particular key and all future events with this key are routed to the same worker. For that purpose, most systems use hashing methods. \\emph{Uniform hashing} evenly distributes keys across parallel tasks. It is fast to compute and requires no routing state but might incur high migration cost. When a new node is added, state is shuffled across existing and new workers. It also causes random I/O and high network communication. Thus, it is not particularly suitable for adaptive applications. \\emph{Consistent hashing} and variations are more often preferred. Workers and keys are mapped to multiple points on a ring using multiple random hash functions. Consistent hashing ensures that state is not moved across workers that are present before and after the migration. When a new worker joins, it becomes responsible for data items from multiple of the existing nodes. When a worker leaves, its key space is distributed over existing workers. \nApache Flink~ uses a variation of consistent hashing in which state is organized into \\emph{key groups} and those are mapped to parallel tasks as ranges. On reconfiguration, reads are sequential within each key group, and often across multiple key groups. The metadata of key group to task assignments are small and it is sufficient to store key-group range boundaries. The number of key groups limits the maximum number of parallel tasks to which keyed state can be scaled.\nHashing techniques are simple to implement and do not require storing any routing state, however, they do not perform well under skewed key distributions. \\emph{Hybrid partitioning}~ combines consistent hashing and an explicit mapping to generate a compact hash function that provides load balance in the presence of skew. The main idea is to track the frequencies of the partitioning key values and treat normal keys and popular keys differently. The mechanism uses the lossy counting algorithm~  in a sliding window setting to estimate heavy hitters, as keeping exact counts would be impractical for large key domains.\n\\stitle{Reconfiguration strategy.}\nRegardless of the re-partitioning strategy used, if the elasticity policy makes a decision to change an application's resources, the mechanism will have to transfer some amount of state across workers on the same or different physical machines.\nThe \\emph{stop-and-restart} strategy halts the computation, takes a state snapshot of all operators, and then restarts the application with the new configuration. Even though this mechanism is simple to implement and it trivially guarantees correctness, it unnecessary stalls the entire pipeline even if only one or few operators need to be rescaled. As shown in Table~\\ref{tbl:scaling-comparison}, this strategy is very common in modern systems.\n\\emph{Partial pause and restart}, introduced by FLUX~, is a less disruptive strategy that only blocks the affected dataflow subgraph temporarily. The affected subgraph contains the operator to be scaled, as well as upstream channels and upstream operators. Figure~\\ref{fig:partial-pause-and-restart} shows an example of the protocol. To migrate state from operator $a$ to operator $b$, the mechanism will execute the following steps:\n(1) First, it \\emph{pauses} $a$'s upstream operators and stops pushing tuples to $a$. Paused operators start buffering input tuples in their local buffers. operator $a$ continues processing tuples in its buffers until they are empty. (2) Once $a$'s buffers are empty, it extracts its state and sends it to operator $b$. (3) Operator $b$ loads the state and (4) sends a \\emph{restart} signal to upstream operators. Once upstream operators receive the signal they can start processing tuples again.\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=.9\\linewidth]{Figures/sec6/patial-pause-and-restart.pdf}\n   \\caption{An example of the partial-pause-and-restart protocol. To move state from operator $a$ to $b$, the mechanism executes the following steps: (1) Pause $a$'s upstream operators, (2) extract state from $a$, (3) load state into $b$, and (4) send a restart signal from $b$ to upstream operators.}\n   \\label{fig:partial-pause-and-restart}\n\\end{figure}\nThe \\emph{pro-active replication} strategy maintains state backup copies in multiple nodes so that reconfiguration can be performed in a nearly live manner when needed. The state is organized into smaller partitions, each of which can be transferred independently. Each node has a set of primary state slices and a set of secondary state slices. Figure~\\ref{fig:proactive} shows an example of the protocol as implemented by ChronoStream~. \n\\begin{figure}[b]\n  \\centering\n  \\includegraphics[width=.9\\linewidth]{Figures/sec6/proactive-replication.pdf}\n   \\caption{An example of the proactive replication protocol. To move slice \\#1 from $N_{src}$ to $N_{dest}$, the mechanism executes the following steps: (1) the leader instructs $N_{dest}$ to load slice \\#1, (2) $N_{dest}$ loads slice \\#1 and sends ack to the leader, (3) the leader notifies upstream operators to replay events, (4) upstream start rerouting events to $N_{dest}$, (5) the leader notifies $N_{src}$ that the transfer is complete and $N_{src}$ moves slice \\#1 to the backup group.}\n   \\label{fig:proactive}\n\\end{figure}\n\\stitle{State transfer.} Another important decision to make when migrating state from one worker to another is whether the state is moved \\emph{all-at-once} or in a \\emph{progressive} manner. If a large amount of state needs to be transferred, moving it in one operation might cause high latency during re-configuration. \nAlternatively, \\emph{progressive} migration~ moves state in smaller pieces and flattens latency spikes by interleaving state transfer with processing. On the downside, progressive state migration might lead to longer migration duration.", "cites": [4763], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes elasticity mechanisms across different stream processing systems, integrating hashing techniques, reconfiguration strategies, and state transfer approaches. It provides a critical comparison of methods like uniform hashing versus consistent hashing, highlighting trade-offs such as migration cost and performance impact. The abstraction is moderate, as it identifies general patterns in state redistribution and transfer but does not propose a novel framework or meta-level insights."}}
{"id": "ac790ee5-82fb-4e06-9845-dd189aefb008", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "1912a489-c536-4d4e-8859-51a6be4f79c3", "prefix_titles": [["title", "A Survey on the Evolution of Stream Processing Systems"], ["section", "Conclusion"]], "content": "\\label{sec:conclusion}\nWhile early streaming systems strove to extend relational execution engines with time-based window processing, modern systems have evolved significantly in terms of architecture and capabilities. Table~\\ref{tab:evolution-streaming} summarizes the evolution of major streaming system aspects over the last three decades.\nWhile approximate results were mainstream in early systems, modern systems have primarily focused on results correctness and have largely rejected the notion of approximation.\nIn terms of languages, modern systems favor general-purpose programming languages, however, we recently witness a trend to return to extensions for streaming SQL~. Over the years, execution has also gradually transitioned from mainly centralized to mainly distributed, exploiting data, pipeline, and task parallelism. At the same time, most modern systems construct independent execution plans per query and apply little optimization and sharing.\nRegarding time, order, and progress, many of the inventions of the past proved to have survived the test of time, since they continue to hold a place in modern streaming systems.\nEspecially Millwheel and the Google Dataflow Model popularized punctuations, watermarks, the out-of-order architecture, and triggers for revision processing. Streaming state management witnessed a major shift, from specialized in-memory synopses to large partitioned and persistent state supported today. As a result, fault tolerance and high availability also shifted towards passive replication and exactly-once processing. Finally, load management approaches have transitioned from load shedding and scheduling methods to elasticity and backpressure coupled with persistent inputs.\nIn state management we identify the most radical changes seen in data streaming so far. The most obvious advances relate to the scalability of state and long-term persistence in unbounded executions. Yet, today's systems have invested thoroughly in providing transactional guarantees that are in par with those modern database management systems can offer today. Transactional stream processing has pivoted data streaming beyond the use for data analytics and has also opened new research directions in terms of efficient methods for backing and accessing state that grows in unbounded terms. Stream state and compute are gradually being decoupled and this allows for better optimizations, wider interoperability with storage technologies as well as novel semantics for shared and external state having stream processors as the backbone of modern continuous applications and live scalable data services.\nWe believe the road ahead is still long for streaming systems. Emerging streaming applications in the areas of Cloud services~, machine learning~, and streaming graph analytics~ present new requirements and are already shaping the key characteristics of the future generation of data stream technology. We expect systems to evolve further and exploit next-generation hardware~, focus on transactions and iteration support, improve their reconfiguration capabilities, and take state management a step further by leveraging workload-aware backends~, shared state and versioning.\n\\bibliographystyle{abbrv}\n\\interlinepenalty=10000\n\\bibliography{references}\n\\balance\n\\end{document}", "cites": [4757], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The conclusion effectively synthesizes the evolution of stream processing systems by integrating concepts from multiple eras and connecting them to broader trends. It offers some critical evaluation by highlighting shifts in priorities (e.g., from approximation to correctness) and identifying limitations like minimal optimization and sharing in modern execution plans. The section abstracts well, identifying overarching themes such as transactional guarantees, decoupling of state and compute, and the influence of emerging application domains on future system design."}}
