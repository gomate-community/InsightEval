{"id": "f4a56e15-85a6-4b24-bb06-4bbd14f51176", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ce58463f-0ca0-47ed-a2b5-be52614e56ea", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Introduction"]], "content": "Deep learning has shown outstanding performance in a wide range of computer vision tasks in the past years, especially image tasks. Meanwhile, in many practical applications, such as autonomous vehicles (Figure \\ref{fig:1} shows a point cloud collected by an autonomous vehicle), we need more information than only images to obtain a better sense of the environment. 3D data from lidar or RGB-D cameras are considered to be a good supplement here. These devices generate 3D geometric data in the form of point clouds. With the growing demand from industry, utilization of point clouds with deep learning models is becoming a research hotspot recently. \\par\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.5in]{pc1.png}\n\\includegraphics[width=1.5in]{pc2.png}\n\\end{center}\n   \\caption{Point cloud data collected from outdoor scene, shown from two distinct angles.}\n\\label{fig:1}\n\\end{figure}\nIn constrast to image data, point clouds do not directly contain spatial structure, and deep models on point clouds must therefore solve three main problems: (1) how to find a representation of high information density from a sparse point cloud, (2) how to build a network satisfying necessary restrictions like size-variance and permutation-invariance, (3)  how to process large volumes of data with lower time and computing resource consumption. PointNet  is one of the representative early attempts to design a novel deep network for comsumption of unordered 3D point sets by taking advantage of MLP and T-Net. PointNet, together with its improved version PointNet++ , inspired a lot of follow-up works. \\par\nFundamental tasks in images, such as classification, segmentation and object detection also exist in point clouds. Most solutions to these problems benefit from research findings on the image side, while adequate adaptions are inevitable to suit the characteristics of 3D data. In this paper, recent works on point clouds are divided into the following categories: classification, segmentation, detection, matching and registration, augmentation, completion and reconstruction. Detailed descriptions of each category will be provided in the following sections. \\par\nA growing number of datasets are available for different tasks on point clouds. ShapeNet  and ModelNet  are two early datasets consisting of clean 3D models. These early datasets suffer from the lack of generalization. However, it is necessary to consider disturbance including noise and missing points to develop robust models. With that in mind, datasets such as ScanNet  and KITTI  are then created from scans of the actual environment. Datasets designed for autonomous vehicle tasks, like nuScenes  and Lyft , are further generalized by involving various environments at different times. Currently, ever more datasets are being proposed in order to meet the increasing demands of distinct niches. \\par\nThe structure of this paper is as follows. Section 2 introduces existing 3D datasets and corresponding metrics for different tasks. Section 3 includes a survey of 3D shape classification methods. Section 4 reviews methods for 3D semantic segmentation and instance segmentation. Section 5 presents a survey of methods for 3D object detection and its derivative task. Section 6 introduces recent progress in 3D point cloud matching and registration. Section 7 provides a review of methods to improve data quality. Finally, section 8 concludes the paper. \\par", "cites": [1517, 7376, 7385, 7374], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction provides a basic overview of the field, mentioning key challenges in point cloud processing and citing representative papers such as PointNet++ and datasets like nuScenes and ScanNet. However, it does not deeply synthesize ideas across sources, lacks critical evaluation of the cited works, and only offers minimal abstraction by listing categories of tasks and datasets without identifying overarching trends or principles."}}
{"id": "6b0e4133-0919-4fa2-a644-a5fe6f69d78f", "title": "Datasets and metrics", "level": "section", "subsections": ["a6784349-1499-4536-ab1a-cca28de29152", "0d42a89f-4d95-480a-ba76-75c22463e792"], "parent_id": "ce58463f-0ca0-47ed-a2b5-be52614e56ea", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Datasets and metrics"]], "content": "Datasets are of great importance in deep learning methods for 3D point cloud data. First, well-designed datasets provide convictive evaluation and comparison among different algorithms. Second, datasets with richer content and metadata help define more complicated tasks and raise new research topics. In this section, we will briefly introduce some most commonly used datasets and evaluation metrics.\n\\begin{table*}[h]\n\\centering\n\\caption{Commonly used 3D point cloud datasets in recent works}\n\\begin{tabular}{|p{2.2cm}|p{2cm}|p{1cm}|p{2.8cm}|p{5.5cm}|p{0.7cm}|}\n\\hline\nDataset & Task & Classes & Scale & Feature & Year \\\\\n\\hline\nShapeNet  & Classification & 55 & 51300 models & The categories are selected according to WordNet  synset. & 2015\\\\\n\\hline\nModelNet40  & Classification & 40 & 12311 models & The models are collected with online search engines by querying for each established object category. & 2015 \\\\\n\\hline\nS3DIS  & Segmentation & 12 & 215 million points & Points are collected in 5 large-scale indoor scenes from 3 different buildings. & 2016 \\\\\n\\hline\nSemantic3D  & Segmentation & 8 & 4 billion points & Hand-labelled from a range of diverse urban scenes. & 2017 \\\\\n\\hline\nScanNet  & Segmentation & 20 & 2.5 million frames & Collected with a scalable RGB-D capture system with automated surface reconstruction and crowdsourced semantic annotation. & 2017 \\\\\n\\hline\nKITTI  & Detection Tracking & 3 & 80256 objects & Captured by a standard station wagon equipped with two cameras, a Velodyne laser scanner and a GPS localization system driving in different outdoor scenes. & 2012\\\\\n\\hline\nnuScenes  & Detection Tracking & 23 & 1.4M objects & Captured with full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS); 1000 scenes of 20s each. & 2019\\\\\n\\hline\nWaymo Open Dataset  & Detection Tracking & 4 & 12.6M objects with tracking ID & Captured with 1 mid-range lidar, 4 short-range lidars\nand 5 cameras (front and sides); 1,950 segments of 20s each, collected at 10Hz. & 2019 \\\\\n\\hline\n\\end{tabular}\n\\label{table:0}\n\\end{table*}", "cites": [1517, 1518, 7376, 7378, 7374], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily provides a factual summary of commonly used 3D point cloud datasets and their associated tasks, features, and years of release. While it includes a table that organizes information from multiple sources, it lacks deeper synthesis or critical evaluation of the datasets' strengths and weaknesses. There is no abstraction or generalization of trends in dataset design or annotation quality."}}
{"id": "a6784349-1499-4536-ab1a-cca28de29152", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "6b0e4133-0919-4fa2-a644-a5fe6f69d78f", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Datasets and metrics"], ["subsection", "Datasets"]], "content": "Table \\ref{table:0} shows the most commonly used 3D point cloud datasets for three matured tasks (classification, segmentation and detection), which will be mentioned often in the following sections. We will also introduce each of them with more details. \\par \n\\textbf{ShapeNet} ShapeNet  is a rich-annotated dataset with 51300 3D models in 55 categories. It consists of several subsets. ShapeNetSem, which is one of the subsets, contains 12000 models spread over a broader set of 270 categories. This dataset, together with ModelNet40 , are relatively clean and small, so they are usually used to evaluate the capacity of backbones before applied to more complicated\ntasks.\\par\n\\textbf{ModelNet40} The ModelNet  project provides three benchmarks: ModelNet10, ModelNet40 and Aligned40. The ModelNet40 benchmark, where ``40\" indicates the number of classes, is the most widely used. To find the most common object categories, the statistics obtained from the SUN database  are utilized. After establishing the vocabulary, 3D CAD models are collected with online search engines and verified by human workers. \\par\n\\textbf{S3DIS} The Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset is composed of 5 large-scale indoor scenes from three buildings to hold diverse in architectural style and appearance. The point clouds are automatically generated without manual intervention. 12 semantic elements including structural elements (floor, wall, etc.) and common furniture are detected. \\par\n\\textbf{Semantic3D} Semantic3D  is the largest 3D point cloud dataset for outdoor scene segmentation so far. It contains over 4 billion points collected from around 110000$m^2$ area with a static lidar. The natural of outdoor scene, such as the unevenly distribution of points and massive occlusions, makes the dataset challenging. \\par\n\\textbf{ScanNet} ScanNet  is a video dataset consists of 2.5 million frames from more than 1000 scans, annotated with camera poses, surface reconstructions and instance-level semantic segmentation. The dataset provides benchmarks for mutiple 3D scene understanding tasks, such as classification, semantic voxel labeling and CAD model retrieval. \\par\n\\textbf{KITTI} The KITTI  vision benchmark suite is among the most famous benchmarks with 3D data. It covers benchmarks for 3D object detection, tracking and scene flow estimation. The multi-view data are captured with an autonomous driving platform with two high-resolution color and gray cameras, a Velodyne laser scanner and a GPS localization system. Only three \nkinds of objects which are important to autonomous driving are labelled: cars, pedestrians and cyclists.\\par\n\\textbf{Other datasets} There are some other datasets of high quality but not widely used, such as Oakland , iQmulus  and Paris-Lille-3D . 3DMatch  pushed the research in 3D matching and registration, which is a less popular direction in the past period. Recently, the rising demand from industry of autonomous driving has spawned several large-scale road-based datasets, represented by nuScenes , Lyft Level 5  and Waymo Open Dataset . They proposed complicated challenges requiring to leverage multi-view data and related metadata. The development of datasets is helping reduce the gap between research and practical applications.", "cites": [1517, 1518, 7376, 7378, 7374], "cite_extract_rate": 0.3125, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of datasets, listing their characteristics and purposes without critically analyzing or comparing them. It integrates basic information from the cited papers but does not synthesize ideas into a deeper narrative or identify overarching patterns. The level of abstraction is limited to describing the datasets themselves."}}
{"id": "5ff34ce1-d7ee-486f-bea3-6b388d8159c1", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "b8c11e81-fe9c-42d0-8a98-4b5d97607589", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Overview"]], "content": "Classification on point clouds is commonly known as 3D shape classification. Similar to image classification models, models on 3D shape classification usually first generate a global embedding with an aggregation encoder, then pass the embedding through several fully connected layers to obtain the final result. Most 3D shape classification methods are tested with clean 3D models (as in Figure \\ref{fig:2}). Based on the point cloud aggregation method, classification models can be generally divided into two categories: projection-based methods and point-based methods.\n\\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=5in]{shapenet.png}\n\\end{center}\n   \\caption{3D models from ShapeNet . ShapeNet contains large-scale 3D models with manually verified annotation.}\n\\label{fig:2}\n\\end{figure*}", "cites": [7374], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of 3D point cloud classification and introduces the ShapeNet dataset, but it lacks synthesis of multiple sources, critical evaluation of methods, or abstraction to broader principles. It mainly outlines the general structure of classification models and categorizes them into two types without deeper analysis or comparative insights."}}
{"id": "c871e847-7f55-4544-88e5-dc81f63d06e9", "title": "Multi-view representation", "level": "subsubsection", "subsections": [], "parent_id": "42b163ac-a9e7-467b-a483-7553b47897b7", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Projection-based Methods"], ["subsubsection", "Multi-view representation"]], "content": "MvCNN  is a method based on a multi-view representation of point clouds. A 3D point cloud is represented by a group of 2D images by rendering snapshots from different angles. Each image in the group will be passed through a CNN to extract view-based features, pooled across views and passed through another CNN to build a compact descriptor. While MVCNN does not distinguish different views, it is helpful to consider the relationship among views. GVCNN  is a method that takes advantage of this relationship. By quantifying the discrimination of views, we are able to divided the set of views into groups based on their discrimination scores. The view descriptors will be passed through intra-group pooling and cross-group fusion for prediction. Aside from the models mentioned above,  and  also improve the recognition accuracy with multi-view representation.", "cites": [7382], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of multi-view representation methods (MvCNN, GVCNN) but lacks synthesis, critical evaluation, and abstraction. It does not connect the ideas in a meaningful way or generalize to broader principles, nor does it evaluate the strengths and limitations of the approaches. The narrative remains at a surface level summary of the methods."}}
{"id": "3c9732b9-6fbf-4195-aa71-0197b2bcfd1b", "title": "Volumetric representation", "level": "subsubsection", "subsections": [], "parent_id": "42b163ac-a9e7-467b-a483-7553b47897b7", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Projection-based Methods"], ["subsubsection", "Volumetric representation"]], "content": "VoxNet  is an early method using the volumetric representation. In this method, each point $(x, y, z)$ is projected into a corresponding discrete voxel point $(i, j, k)$. Each point cloud will be mapped into an occupancy grid of $32\\times 32\\times 32$ voxels, and the grid will then be passed through two 3D convolutional layers to obtain the final representation. \\par\nVoxNet simply uses adaption of CNN layers for the prediction head, which leads to potential loss of detailed spatial information. 3D ShapeNet  proposed a belief-based deep convolutional network to learn the distribution of point clouds in different 3D shapes. In this method, 3D shapes are represented by the probability distributions of binary variables on grids. \\par\nWhile volumetric methods already achieve satisfactory performance, most suffer from the cubic growth of computation complexity and memory footprint, hence the resolution of the grid is strictly limited. OctNet  improved the efficiency by introducing a hybrid grid-octree structure to hierarchically partition point clouds. A point cloud is represented by several octrees along a regular grid, each octree is encoded as a bit string, and features are generated through naive arithmetic. Inspired by OctNet, OCNN  then proposed a method that introduces 3D-CNNs to extract features from octrees. \\par\nMethods based on volumetric representations as mentioned above are naturally coarse as only a small fraction of voxels are non-empty and the detailed context inside each voxel is hardly collected. The balance between resolution and computation is difficult to achieve in practice. \\par", "cites": [7384], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates multiple papers into a narrative about the evolution of volumetric methods, showing some synthesis by linking them through their use of 3D representations and efficiency considerations. It also provides limited critical analysis by pointing out the computational limitations and loss of spatial detail. However, abstraction is modest, with general patterns mentioned but not deeply explored."}}
{"id": "0dc9f82c-8178-47e5-b20a-61b5bc7512c3", "title": "Basis point set", "level": "subsubsection", "subsections": [], "parent_id": "42b163ac-a9e7-467b-a483-7553b47897b7", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Projection-based Methods"], ["subsubsection", "Basis point set"]], "content": "BPS  proposed a new approach that breaks the convention that point clouds, even with various sizes, are usually projected onto a grid of same size. In BPS, input points are first normalized into a unit ball, then a group of points is randomly sampled to make up a basis point set (BPS). The sampled BPS is constant for all point clouds in a dataset. For a given point cloud $X$, each point $x_i$ is represented by the Euclidean distance between itself and its nearest neighbor in BPS. By passing such representation through the last two fully connected layers of PointNet, the model achieves performance similar to that of the original PointNet design.", "cites": [1533], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of the Basis Point Set (BPS) method, describing its approach and results. It integrates the core idea from the cited paper but lacks broader synthesis with other methods or deeper critical evaluation. There is minimal abstraction or identification of general principles, keeping the discussion concrete and limited to a single technique."}}
{"id": "f6700f84-ee95-4da9-ae52-94abea28420f", "title": "MLP networks", "level": "subsubsection", "subsections": [], "parent_id": "3846ef1e-65fc-4432-8abb-f1d3943ada2e", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "MLP networks"]], "content": "PointNet  is a famous architecture that takes advantage of multi-layer perceptrons (MLPs). The input (an $n \\times 3$ 2D tensor) is first multiplied by an affine transformation matrix\npredicted by a mini-network (T-Net) to hold invariance under geometric transformations. The point set is then passed through a group of MLPs followed by another joint alignment network, and a max-pooling layer to obtain the final global feature. This backbone can be used for both classification and segmentation prediction. For classification, the global feature is passed through an MLP for output scores. For segmentation, the concatenations of the global feature and different levels of intermediate features from each point are passed through an MLP for the classification result of each point. Conventional CNNs take features at different scales by a stack of convolutional layers; inspired by that, PointNet++  is proposed. In this work, the local region of a point $x$ is defined as the points within a sphere centered at $x$. One set abstraction level here contains a sampling layer, a grouping layer to identify local regions and a PointNet layer. Stacking such set abstraction levels allows us to extract features hierarchically as CNNs for image tasks do. \\par\nThe simple implementation and promising performance of PointNet  and PointNet++  inspired a lot of follow-up work. PointWeb  is adapted from PointNet++ and improves quality of features by introducing Adaptive Feature Adjustment (AFA) to make use of context information of local neighborhoods. In addition, SRN  proposed Structural Relation Network (SRN) to equip PointNet++, and obtained better performance.", "cites": [7385], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear, factual overview of PointNet and PointNet++ and their extensions, integrating the basic concepts from the cited works. It connects PointNet++ to the hierarchical design of CNNs, showing some synthesis. However, it lacks in-depth critical evaluation of the methods' limitations or trade-offs and offers only limited abstraction beyond the specific models discussed."}}
{"id": "8c9309b4-e422-48d1-acef-f71face8459e", "title": "Convolutional networks", "level": "subsubsection", "subsections": [], "parent_id": "3846ef1e-65fc-4432-8abb-f1d3943ada2e", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "Convolutional networks"]], "content": "Convolution kernels on 2D data can be extended to work on 3D point cloud data. As mentioned before, VoxNet  is an early work that directly takes advantage of 3D convolution. \\par\nA-CNN  proposed another way to apply convolution on point clouds. In order to prevent redundant information from overlapped local regions (the same group of neighboring points might be repeatedly included in regions at different scales), A-CNN proposed a ring-based scheme instead of spheres. To convolve points within a ring, points are projected on a tangent plane at a query point $q_i$, then ordered in clockwise or counter-clockwise direction by making use of cross product and dot product, and eventually a 1-D convolution kernel will be applied to the ordered sequence. The output feature can be used for both classification and segmentation as in PointNet.\\par\nRS-CNN  is another convolutional network based on relation-shape convolution. An RS-Conv kernel takes a neighborhood around a certain point as its input, and learns the mapping from naive relations (e.g. Euclidean distance, relative position) to high-level relations among points, and encodes the spatial structure within the neighborhood with the learned mapping.\\par\nIn PointConv , the convolution operation is defined as finding a Monte Carlo estimation of the hidden continuous 3D convolution w.r.t. an importance sampling. The process is composed with a weighting function and a density function, implemented by MLP layers and a kernelized density estimation. Furthermore, the 3D convolution is reduced into matrix multiplication and 2D convolution for memory and computational efficiency and easy deployment. A similar idea is used in MCCNN , where convolution is replaced by a Monte Carlo estimation based on the density function of the sample. \\par\nGeo-CNN  proposed another way to model the geometric relationship among neighborhood points. By taking six orthogonal bases, the space will be separated into eight quadrants, and all vectors in a specific quadrant can be composed by three of the bases. Features are extracted independently along each direction with corresponding direction-associated weight matrices, and are aggregated based on the angle between the geometric vector and the bases. The feature of some specific point at the current layer is the sum of features of the given point and its neighboring edge features from the previous layer. \\par\nIn SFCNN , the input point cloud is projected onto regular icosahedral lattices with discrete sphere coordinates, hence convolution can be implemented by maxpooling and convolution on the concatenated features from vertices of spherical lattices and their neighbors. SFCNN holds rotation invariance and is robust to perturbations.", "cites": [7389, 1524, 1525], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a factual summary of convolutional network approaches for point cloud classification, citing several methods and briefly describing their mechanisms. It shows minimal synthesis by connecting only surface-level ideas (e.g., ring-based vs. spherical structures), lacks critical evaluation of the methods, and offers only basic abstraction by highlighting features like rotation invariance or efficiency in some cases. The narrative remains largely descriptive with limited insight into broader trends or comparative advantages."}}
{"id": "71547927-40bd-486d-8485-8398a77bed30", "title": "Graph networks", "level": "subsubsection", "subsections": [], "parent_id": "3846ef1e-65fc-4432-8abb-f1d3943ada2e", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "Graph networks"]], "content": "Graph networks consider a point cloud as a graph and the vertices of the graph as the points, and edges are generated based on the neighbors of each point. Features will be learned in spatial or spectral domains. \\par\nECC  first proposed the idea of considering each point as a vertex of the graph and connected edges between pairs of points that are ``neighbors\". Then, edge conditioned convolution (ECC) is applied with a filter generating network such as MLP. Neighborhood information is aggregated by maxpooling and coarsened graph will be generated with VoxelGrid  algorithm. After that, DGCNN  uses a MLP to implement EdgeConv, followed by channel-wise symmetric aggregation on edge features from the neighborhood of each point, which allows the graph to be dynamically updated after each layer of the network. \\par\nInspired by DGCNN, Hassani and Haley  proposed an unsupervised multi-task approach to learn shape features. The approach consists of an encoder and an decoder, where the encoder is constructed from multi-scale graphs, and the decoder is constructed for three unsupervised tasks (clustering, self-supervised classification and reconstruction) trained by a joint loss. \\par\nClusterNet  uses rigorously rotation-invariant (RRI) module to generate rotation-invariant features from each point, and an unsupervised agglomerative hierarchical clustering method to construct hierarchical structures of a point cloud. Features of sub-clusters at each level are first learned with an EdgeConv block, then aggregated by maxpooling. \\par", "cites": [278, 1529, 7215], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of graph-based methods for 3D point cloud classification, listing key papers and their main components. It integrates information across the cited works to some extent by showing a progression from ECC to DGCNN and then to ClusterNet, but lacks deeper synthesis into a novel framework. There is minimal critical analysis or identification of broader patterns or principles."}}
{"id": "1deca089-7643-4c4c-8fd7-c16ec96b7423", "title": "Other networks", "level": "subsubsection", "subsections": [], "parent_id": "3846ef1e-65fc-4432-8abb-f1d3943ada2e", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "Other networks"]], "content": "Aside from OctNet , which uses octrees on voxel grids to hierarchically extract features from point clouds, Kd-Net  makes use of K-d trees to build a bottom-up encoder. Leaf node representations are normalized 3D coordinates (by setting the center of mass as origin and rescaled to $[-1,1]^3$), and non-leaf node representations are calculated from its children nodes with MLP. The parameters of MLPs are shared within each level of the tree. Moreover, 3DContextNet  proposed another method based on K-d trees. While non-leaf representations are still computed with MLP from its children, the aggregation at each level is more complicated for considering both local cues and global cues. The local cues concern points in the corresponding local region, and the global cues concern the relationship between current position and all positions in the input feature map. The representation at the root will be used for prediction. \\par\nRCNet  introduced RNN to point cloud embedding. The ambient space is first partitioned into parallel beams, each beam is then fed into a shared RNN, and the output subregional features are considered as a 2D feature map and processed by a 2D CNN. \\par\nSO-Net  is a method based on the self-organized map (SOM). A SOM is a low-dimensional (two-dimensional in the paper) representation of the input point cloud, initialized by a proper guess (dispersing nodes uniformly in a unit ball), and trained with unsupervised competitive learning. A k-nearest-neighbor set is searched over the SOM for each point, and the normalized KNN set is then passed through a series of fully connected layers to generate individual point features. The point features are used to generate node features by maxpooling according to the association in KNN search, and the node features are passed through another series of fully connected layers and aggregated into a global representation of the input point cloud.", "cites": [1531, 7401, 7398], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of several point-based classification methods (Kd-Net, 3DContextNet, RCNet, SO-Net) but does so in a largely descriptive manner without significant synthesis of ideas across the papers. It mentions differences in hierarchical feature extraction and aggregation strategies but does not offer a deeper comparative analysis or critique of their strengths and weaknesses. Some minimal abstraction is present in noting the use of tree structures and self-organizing maps, but broader patterns or principles are not clearly identified."}}
{"id": "ebe31351-7c66-45ec-a89b-9a05f235439a", "title": "Experiments", "level": "subsection", "subsections": [], "parent_id": "b8c11e81-fe9c-42d0-8a98-4b5d97607589", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Classification"], ["subsection", "Experiments"]], "content": "Different methods choose to test their models on various datasets. In order to obtain a better comparison among methods, we select datasets that most methods are tested on, and list the experiment results for them in Table \\ref{table:1}.\n\\begin{table*}[htbp]\n\\centering\n\\caption{Experiment results on ModelNet40 classification benchmark. ``OA\" stands for overall accuracy and ``mACC\" stands for mean accuracy.}\n\\begin{tabular}{|c|c|c|}\n\\hline\nMethods & ModelNet40(OA) & ModelNet40(mAcc) \\\\\n\\hline\nPointNet  & 89.2\\% & 86.2\\% \\\\\n\\hline\nPointNet++  & 90.7\\% & 90.7\\% \\\\\n\\hline\nPointWeb  & 92.3\\% & 89.4\\% \\\\\n\\hline\nSRN  & 91.5\\% & - \\\\\n\\hline\nPointwise-CNN  & 86.1\\% & 81.4\\% \\\\\n\\hline\nPointConv  & 92.5\\% & - \\\\\n\\hline\nRS-CNN  & 92.6\\% & - \\\\\n\\hline\nGeoCNN  & 93.4\\% & 91.1\\% \\\\\n\\hline\nA-CNN  & 92.6\\% & 90.3\\% \\\\\n\\hline\nHassani and Haley  & 89.1\\% & - \\\\\n\\hline\nECC  & 87.4\\% & 83.2\\% \\\\\n\\hline\nSFCNN  & 91.4\\% & - \\\\\n\\hline\nDGCNN  & 92.2\\% & 90.2\\% \\\\\n\\hline\nClusterNet  & 87.1\\% & - \\\\\n\\hline\nBPS  & 91.6\\% & - \\\\\n\\hline\nKD-Net  & 91.8\\% & 88.5\\% \\\\\n\\hline\n3DContextNet & 91.1\\% & - \\\\\n\\hline\nRCNet  & 91.6\\% & - \\\\\n\\hline\nSO-Net  & 90.9\\% & 87.3\\% \\\\\n\\hline\n\\end{tabular}\n\\label{table:1}\n\\end{table*}", "cites": [1527, 7215, 1533, 1531, 7398, 7389, 278, 1524, 7401, 7385, 1529], "cite_extract_rate": 0.5789473684210527, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, listing experimental results from various cited papers without offering a synthesis of their approaches or insights. There is minimal critical analysis or abstraction, as it does not evaluate performance trade-offs, compare methodological strengths/weaknesses, or identify broader trends in the literature."}}
{"id": "3aae4c00-5be1-4a48-9cea-289a5901b47a", "title": "Projection-based methods", "level": "subsubsection", "subsections": [], "parent_id": "7030fb51-f32e-4a26-8247-1e32a88328fd", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Semantic Segmentation"], ["subsubsection", "Projection-based methods"]], "content": "Huang and You  project the input point cloud into occupancy voxels, which are then fed into a 3D convolutional network to generate voxel-level labels. All points within a voxel are assigned with the same semantic label as the voxel. ScanComplete  utilizes fully convolutional networks to adapt to different input data sizes, and deploys a coarse-to-fine strategy to improve the resolution of predictions hierarchically. VV-Net  also transfers unordered points into regular voxel grids as the first step. After that, the local geometry information of each voxel will be encoded with a kernel-based interpolated variational auto-encoder (VAE). In each voxel, a radial basis function (RBF) is computed to generate a local continuous representation to deal with sparse distributions of points.  \\par\nF. Jaremo-Lawin et al.  proposed a multi-view method that first projects a 3D cloud to 2D planes from multiple camera views, then pixel-wise scores on synthetic images are predicted with a multi-stream FCN, and the final labels are obtained by fusing scores over different views. PolarNet , however, proposed a polar BEV representation. By implicitly aligning attention with the long-tailed distribution, this representation reduces the imbalance of points across grid cells along the radial axis.\\par \nSome other methods leverage scans in multiple modalities. 3DMV  proposed a joint 3D-multi-view network that combines features from RGB images and point cloud. Features are extracted with a 3D CNN stream and a group of 2D streams respectively. MVPNet  proposed another aggregation to fuse features (from images and point cloud) in 3D canonical space with a point-based network. \\par\n\\par", "cites": [7428, 7426, 8932], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of projection-based methods for 3D point cloud semantic segmentation but lacks deeper synthesis of ideas across the cited works. It does not offer critical analysis or highlight limitations of the methods. There is minimal abstraction beyond individual papers, with only surface-level observations about data distribution and multi-view approaches."}}
{"id": "f5ec5905-3927-4494-ab05-6eacfca8d355", "title": "Point-based methods", "level": "subsubsection", "subsections": [], "parent_id": "7030fb51-f32e-4a26-8247-1e32a88328fd", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Semantic Segmentation"], ["subsubsection", "Point-based methods"]], "content": "First of all, PointNet  and PointNet++  can predict semantic labels with corresponding prediction branches attached. Engelmann et al.  proposed a method to define neighborhoods in both world space and feature space with k-means clustering and KNN. A pairwise distance loss and centroid loss are introduced to feature learning based on the assumption that points with the same semantic label are supposed to be closer. PointWeb , as mentioned in classification, can also be adapted to predict segmentation labels. PVCNN  proposed a comprehensive method that leverages both point and voxel representation to obtain memory and computation efficiency simultaneously.\\par\nSome extensions of the convolution operator are introduced for feature extraction on point cloud. PCCN  introduces parametric continuous convolutional layers. These layers are parameterized by MLPs and span full continuous vector spaces. The generalization allows models to learn over any data structure where the support relationship is computable. Pointwise-CNN  introduced a point-wise convolution where the neighbor points are projected into kernel cells and convolved with corresponding kernel weights. Engelmann et al.  proposed Dilated Point Convolution (DPC) to aggregate dilated neighbor features, instead of the conventional k-nearest neighbors. \\par\nGraph networks are also used in some segmentation models to obtain the underlying geometric structures of the input point clouds. SPG  introduced a structure called superpoint graph (SPG) to capture the organization of point clouds. The idea is further extended in , which introduces a oversegmentation (into pure superpoints) of the input point cloud. Aside from that, Graph Attention Convolution  (GAC) is proposed to learn relevant features from local neighborhoods selectively. By dynamically assigning attention weights to different neighbor points and different feature channels based on their spatial positions and feature differences, the model is able to learn discriminative features from the most relevant part of the neighbor point sets.\\par\nCompared with projection-based methods, point-based methods usually require more computation and therefore have more trouble dealing with large-scale data. Tatarchenko et al.  introduced tangent convolutions to solve this. A fully-convolutional network is designed based on the tangent convolution and successfully improved the performance on large-scale point clouds. RandLA-Net  attempted to reduce computation by replace conventional complex point sampling approaches with random sampling. And to avoid random sampling from discarding crucial information, a novel feature aggregation module is introduced to enlarge receptive fields of each point. \\par\nBased on the fact that the production of point-level labels is labor-intensive and time-consuming, some methods explored weakly supervised segmentation. Xu and Lee  proposed a weakly supervised approach which only requires a small fraction of points to be labelled at training stage. By learning gradient approximation and smoothness constraints in geometry and color, competitive results can be obtained with as few as 10\\% points labelled. On the other hand, Wei et al.  introduced a multi-path region mining module, which can provide pseudo point-level labels by a classification network over weak labels. The segmentation network is then trained with these pseudo labels in a fully supervised manner.", "cites": [1539, 7437, 7431, 1542, 1527, 7440, 7450, 270, 1541, 7385], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes various point-based segmentation methods by categorizing them into approaches like PointNet-based models, extended convolution operators, and graph-based techniques. It connects ideas across papers (e.g., comparing PVCNN with traditional point/voxel methods) and highlights design choices such as neighborhood definitions, attention mechanisms, and weak supervision. While it provides a structured overview, it lacks deeper comparative evaluation or identification of overarching theoretical principles, remaining primarily focused on methodological descriptions and adaptations."}}
{"id": "a335255c-fa10-431a-931f-f76142a75321", "title": "Proposal-based methods", "level": "subsubsection", "subsections": [], "parent_id": "505df953-187e-46bf-a5dc-d79729c9ceca", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Instance Segmentation"], ["subsubsection", "Proposal-based methods"]], "content": "Proposal-based instance segmentation methods can be considered as the combination of object detection and mask prediction. 3D-SIS  is a fully convolutional network for 3D semantic instance segmentation where geometry and color signals are fused. For each image, 2D features for each pixel are extracted by a series of 2D convolutional layers, and then backprojected to the associated 3D voxel grids. The geometry and color features are passed through a series of 3D convolutional layers respectively and concatenated into a global semantic feature map. Then a 3D-RPN and a 3D-RoI layer are applied to generate bounding boxes, instance masks and object labels. Generative Shape Proposal Network (GSPN)  generates proposals by reconstructing shapes from the scene instead of directly regresses bounding boxes. The generated proposals are refined with a region-based PointNet (R-PointNet), and the labels are determined with a point-wise binary mask prediction over all class labels. 3D-BoNet  is a single-stage method that adapts PointNet++  as backbone network to global features and local features at each point. Two prediction branches follow to generate instance-level bounding box and point-level mask respectively. Zhang el al.  proposed a method for large-scale outdoor point clouds. The point cloud is first encoded into a high-resolution BEV representation augmented by KNN, and features are then extracted by voxel feature encoding (VFE) layers and self-attention blocks. For each grid, a horizontal object center and its height limit are predicted, objects that are closed enough will be merged, and eventually these constraints will be leveraged to generate instance prediction.", "cites": [7385], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of proposal-based methods for 3D point cloud instance segmentation, listing several approaches and their components. It integrates information from the cited works but does so in a surface-level manner without deeper synthesis or comparison. There is minimal abstraction or critical evaluation of the methods, making the section largely descriptive."}}
{"id": "28ec03bc-cabc-45e8-bc6d-ebd0697c9df7", "title": "Proposal-free methods", "level": "subsubsection", "subsections": [], "parent_id": "505df953-187e-46bf-a5dc-d79729c9ceca", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Instance Segmentation"], ["subsubsection", "Proposal-free methods"]], "content": "Proposal-free methods tend to generate instance-level label based on semantic segmentation by algorithms like clustering. Similarity Group Proposal Network (SGPN)  is a representative work that learns a feature and semantic map for each point, and a similarity matrix to estimate the similarity between pairs of features. A heuristic non-maximal suppression method follows to merge points into instances. Lahoud et al.  adopted multi-task metric learning to (1) learn a feature embedding such that voxels with the same instance label are close and those with different labels are separated in the feature space and (2) predict the shape of instance at each voxel. Instance boundaries are estimated with mean-shift clustering and NMS. \\par\nZhang et al.  introduced a probabilistic embedding to encode point clouds. The embedding is implemented with multivariate\nGaussian distribution, and the Bhattacharyya kernel is adopted to esimate the similarity between points. Proposal-free methods do not suffer from the computational complexity of region-proposal layers; however, it is usually difficult for them to produce discriminative object boundaries from clustering. \\par\nThere are also several instance segmentation methods based on projection. SqueezeSeg  is one of the pioneer works in this direction. In this method, points are first projected onto a sphere for a grid-based representation. The transformed representation is of size $H\\times W\\times C$, where in practice $H$=64 is the number of vertical channels of lidar, $W$ is manually picked to be 512, and $C$ equals to 5 (3 dimensional coordinates + intensity measurement + range). The representation is then fed through a conventional 2D CNN and a conditional random field (CRF) for refined segmentation results. This method is afterwards improved by SqueezeSegv2  with a context aggregation module and a domain adaptation pipeline. \\par\nThe idea of projection-based methods is further explored by Lyu et al. . Inspired by graph drawing algorithms, they proposed a hierarchical approximate algorithm to project point clouds into image representations with abundant local geometric information preserved. The segmentation will then be generated by a multi-scale U-Net from the image representation. With this innovative projection algorithm, the method obtained significant improvement. \\par\nPointGroup  proposed a bottom-up framework with two prediction branches. For each point, its semantic label and relative offset to its respective instance centroid are predicted. The offset branch helps better grouping of points into objects as well as separation of objects with the same semantic label. During the clustering stage, both original positions and shifted positions are considered, the association of these two results turns out to have a better performance. Along with NMS based on the newly designed ScoreNet, this method outperforms other works of the day by a great margin. \\par", "cites": [5601, 7446, 7447, 9114], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes well by grouping methods into a coherent narrative around proposal-free instance segmentation, connecting different papers that use clustering, embeddings, and projection. It offers a critical note on the limitations of clustering in producing discriminative boundaries and highlights the computational advantages of proposal-free approaches. The abstraction is moderate, as it identifies general trends in the use of embeddings and projection but does not present overarching theoretical principles."}}
{"id": "5d9e5d6a-201a-404c-b9a3-221cdaf3f661", "title": "Joint Training", "level": "subsection", "subsections": [], "parent_id": "c3bca14c-25b7-4a48-b146-f9ab060305a2", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Joint Training"]], "content": "As mentioned above, some recent works jointly address more than one problems to better realized the power of models. The unsupervised multi-task approach proposed by Hassani and Haley  is an example in which clustering, self-supervised classification and reconstruction are jointly trained. The two tasks under segmentation, semantic segmentation and instance segmentation, are also proven to likely benefit from simultaneous training. \\par\nThere are two naive ways to solve semantic segmentation and instance segmentation at the same time: (1) solve semantic segmentation first, run instance segmentation on points of certain labels based on the result of semantic segmentation, (2) solve instance segmentation first, and directly assign semantic labels with instance labels. These two step-wise paradigms highly depend on the output quality of the first step, and are not able to make full use of the shared information between two tasks. \\par \nJSIS3D  develops a pointwise network that predicts the semantic label of each point and high-dimensional embeddings at the same time. After these steps, instances of the same class will have similar embeddings, then a multi-value conditional random field model is applied to synthesize semantic and instance labels, formulating the problem as jointly optimizing labels in the field model. ASIS  is another method that makes the two tasks benefit from each other. Specifically, instance segmentation benefits from semantic segmentation by learning semantic-aware instance embedding at point level, while semantic features of the point set from the same instance will be fused together to generate accurate semantic predictions for every point.", "cites": [1545, 1529], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the concept of joint training for semantic and instance segmentation in point clouds and references two key papers (JSIS3D and ASIS) to illustrate different approaches. It synthesizes these methods into a broader discussion of how simultaneous training can leverage shared information, but does not go beyond to offer a novel framework. Critical analysis is limited, and while there is some abstraction in framing joint training as a beneficial paradigm, deeper generalization or meta-insights are not presented."}}
{"id": "237d2b48-4b1a-4345-a3c5-b69822303b73", "title": "Experiments", "level": "subsection", "subsections": [], "parent_id": "c3bca14c-25b7-4a48-b146-f9ab060305a2", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Segmentation"], ["subsection", "Experiments"]], "content": "We select the benchmarks on which most methods are tested, S3DIS, to compare the performance of different methods. The performances are summarized in Table \\ref{table:2}.\n\\begin{table*}[!htbp]\n\\centering\n\\caption{Experiment results on on semantic segmentation in S3DIS benchmark. Only results that are reported in the original papers are listed, those reported as a reference by other papers are excluded because they are sometimes conflicting.}\n\\begin{tabular}{|c|c|c|c|c|}\n\\hline\nMethods & Area5(mACC) & Area5(mIoU) & 6-fold(mACC) & 6-fold(mIoU) \\\\\n\\hline\nPointCNN  & 63.9 & 57.3 & 75.6 & 65.4 \\\\\n\\hline\nPointWeb  & 66.6 & 60.3 & 76.2 & 66.7 \\\\\n\\hline\nA-CNN  & - & - & - & 62.9 \\\\\n\\hline\nDGCNN  & - & - & - & 56.1 \\\\\n\\hline\nVV-Net  & - & - & 82.2 & 78.2 \\\\\n\\hline\nPCCN  & - & 58.3 & - & - \\\\\n\\hline\nGAC  & - & 62.9 & - & - \\\\\n\\hline\nDPC  & 68.4 & 61.3 & - & - \\\\\n\\hline\nSSP+SPG  & - & - & 78.3 & 68.4\\\\\n\\hline\nJSIS3D  & - & - & 78.6 & - \\\\\n\\hline\nASIS  & 60.9 & 53.4 & 70.1 & 59.3 \\\\\n\\hline\nXu and Lee  & - & 48.0 & - & - \\\\\n\\hline\nRandLA-Net  & - & - & 82.0 & 70.0 \\\\\n\\hline\nTatarchenko et al.  & 62.2 & 52.8 & - & - \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\label{table:2}\n\\end{table*}", "cites": [1539, 1545, 278, 7431, 1542, 1541], "cite_extract_rate": 0.4, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily presents a table of results from various methods on the S3DIS benchmark without synthesizing or connecting ideas between the cited papers. There is no critical evaluation of the methods or their limitations, and no abstraction or generalization to broader patterns or principles in point cloud segmentation."}}
{"id": "31c66996-74a1-437f-a6e9-79bf4ad3e520", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "aba1eb45-d555-4844-b690-5d316715bfce", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Overview"]], "content": "Object detection is a recent research hotspot as the basis of many practical applications. It aims to locate all the objects in the given scene. 3D object detection methods can be generally divided into three categories: multi-view methods, projection-based methods and point-based methods. Figure 4.1 shows an example of 3D object detection on multiple (lidar and camera) views. Aside from image object detection models, the exclusive characteristics of point cloud data provide more potential of optimization. Also, since 3D object tracking and scene flow estimation are two derivative tasks that highly depend on object detection, they will be discussed together in this section.\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=3.2in]{nuscene.png}\n\\end{center}\n   \\caption{An outdoor scene from nuScenes , annotations in multi-view (lidar/camera) are provided.}\n\\label{fig:4}\n\\end{figure}", "cites": [1517], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of 3D object detection, mentioning its importance and categorizing methods into three types, but it does not deeply integrate or synthesize information from the cited paper. The reference to nuScenes is minimal and serves only to illustrate an example without elaborating on the dataset's contributions or comparing it to other data sources. There is little abstraction or critical analysis of the methods or challenges involved."}}
{"id": "066b5a3a-86ad-4012-9ee1-3cd429f222ae", "title": "Projection-based methods", "level": "subsubsection", "subsections": [], "parent_id": "4f03dc8f-ba67-4f7f-8e7f-4213f6a952be", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Object Detection"], ["subsubsection", "Projection-based methods"]], "content": "The success of convolutional neural networks in image object detection inspired attempts to apply 3D CNN on projected point cloud data. VoxelNet  proposed an approach that applies random sampling to the point set within each voxel, and passes them through a novel voxel feature encoding (VFE) layer based on PointNet  and PointNet++  to extract point-wise features. A region proposal network is used to produce detection results. Similar to classification models with volumetric representation, VoxelNet runs at a relatively low speed due to the sparsity of voxels and 3D convolutions. SECOND  then proposed an improvement in inference efficiency by taking advantage of sparse convolution network. \\par\nPointPillars  utilizes point cloud data in another way. Points are organized in vertical columns (called Pillars), and the features of pillars are extracted with PointNet to generate a pseudo image. The pseudo image is then considered as the input of a 2D object detection pipeline to predict 3D bounding boxes. PointPillars is more accurate than previous fusion approaches, and it is capable of real-time applications with a running speed of 62 FPS. Wang et al.  further proposed another anchor-free bounding box prediction based on a cylindrical projection into multi-view features. \\par\nProjection-based methods suffer from spatial information loss inevitably. Aside from using point-based networks instead, He et al.  proposed a structure-aware method to mitigate the problem. The convolutional layers are explicitly supervised to contain structural information by an auxiliary network. The auxiliary network converts the convolutional features from the backbone network to point-level representations and is jointly optimized. After the training process is finished, the auxiliary network can be detached to speed up the inference. \\par", "cites": [7963, 7385], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key projection-based methods by connecting their design choices (e.g., VoxelNets VFE layer, PointPillars pillar-based pseudo image, and Wang et al.s cylindrical projection). It also critically points out common limitations, such as spatial information loss and inference speed issues. While it identifies patterns like the use of PointNet for feature extraction, it does not fully generalize to overarching principles or provide a deep evaluative critique of the methods."}}
{"id": "e7887261-f260-40a7-a8d4-53b77b396360", "title": "Point-based methods", "level": "subsubsection", "subsections": [], "parent_id": "4f03dc8f-ba67-4f7f-8e7f-4213f6a952be", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Object Detection"], ["subsubsection", "Point-based methods"]], "content": "Most point-based methods attempt to minimize information loss during feature extraction, and they are the group with the best performance so far. STD  introduced the idea of using sphere anchors for proposal generation, which achieves a high recall with significantly less computation than previous methods. Each proposal is passed through a PointsPool layer that converts proposal features from sparse expression to compact representation, and is robust under transformation. In addition to the regular regression branch, STD has another IoU branch to replace the role of classification score in NMS.\\par\nSome methods use foreground-background classification to improve the quality of proposals. PointRCNN  is such a framework, in which points are directly segmented to screen out foreground points, while semantic features and spatial features are then fused to produce high-quality 3D boxes. Compared with multi-view methods above, segmentation-based methods perform better for complicated scenes and occluded objects.\\par\nFurthermore, Qi et al. proposed VoteNet . A group of points are sampled as seeds, and each seed independently generates a vote for potential center points of objects in the point cloud with the help of PointNet++ . By taking advantage of voting, VoteNet outperforms previous approaches on two large indoor benchmarks. However, as the center point prediction of virtual center points is not as stable, the method performs less satisfactorily in wild scenes. As a follow-up work, ImVoteNet  inherited the idea of VoteNet and achieved prominent improvement by fusing 3D votes with 2D votes from images.\\par\nThere are also attempts that consider domain knowledge as an auxiliary to enhance features. Associate-3Ddet  introduced the idea of perceptual-to-conceptual association. To enrich perception features that might be incomplete due to occlusion or sparsity, a perceptual-to-conceptual module is proposed to generate class-wise conceptual models from the dataset. The perception and conceptual features will be associated for feature enhancement. \\par\nYang et al.  proposed a point-based anchor-free method 3DSSD. This method attempts to reduce computation by abandoning the upsampling layers (e.g. feature propagation layers in ) and refinement stages that are widely used in previous point-based methods. Previous set abstraction layers for downsampling only leverage furthest-point-sampling based on Euclidean distance (D-FPS), instances with a small number of interior points are easily lost under this strategy. In this case, removing upsampling layers could lead to huge performance drop. 3DSSD proposed F-FPS, a new sampling strategy based on feature distances, to preserve more foreground points for instances. The fusion of F-FPS and D-FPS, together with the candidate generation layer and 3D center-ness assignment in the prediction head, help this method outperform previous single-stage methods with a considerable margin. \\par\nGraph neural networks have also been introduced to 3D object detection for its ability to accommodate intrinsic characteristics of point clouds like sparsity. PointRGCN  is an early work that introduce graph-based representation for 3D vehicle detection refinement. After that, HGNet  introduces a hierarchical graph network based on shape-attentive graph convolution (SA-GConv). By capturing object shapes with relative geometric information and reasoning on proposals, the method obtained a significant improvement on previous results. Besides, Point-GNN  proposed a single-shot method based on graph neural networks. It first builds a fixed radius near-neighbors graph over the input point cloud. Then, the category and the bounding box of affiliation are predicted with the point graph. Finally, a box merging and scoring operation is used to obtain accurate combination of detection results from multiple vertices. \\par", "cites": [7414, 5602, 7404, 7407, 7418, 7405, 7411, 7385], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key point-based object detection methods by highlighting common themes like minimizing information loss and using voting or graph-based strategies. It provides some critical evaluation, such as noting VoteNet's performance issues in wild scenes or the risks of removing upsampling layers in 3DSSD. While not reaching the highest level of abstraction, it identifies patterns like the use of graph neural networks and sampling strategies, offering meaningful analytical insights into the field."}}
{"id": "5f059aec-ea02-4892-b548-1904863343ff", "title": "Multi-view methods", "level": "subsubsection", "subsections": [], "parent_id": "4f03dc8f-ba67-4f7f-8e7f-4213f6a952be", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Object Detection"], ["subsubsection", "Multi-view methods"]], "content": "MV3D  is a pioneering method in multi-view object detection methods on point clouds. In this approach, candidate boxes are generated from BEV map and projected into feature maps of multiple views (RGB images, lidar data, etc.), then the region-wise features extracted from different views are combined to produce the final oriented 3D bounding boxes. While this approach achieves satisfactory performance, much like many other early multi-view methods, its running speed is too slow for practical use. \\par\nAttempts to improve multi-view methods generally take one of two directions. First, we could find a more efficient way to fuse information from different views. Liang et al.  use continuous convolutions to effectively fuse feature maps from images and lidar at different resolutions. Image features for each point in BEV space are utilized to generate a dense BEV feature map by bi-linear interpolation with projections of image features within the BEV plane. Experiments show that dense BEV feature maps perform better than discrete image feature maps and sparse point cloud feature maps. Second, many methods propose innovative feature extraction approaches to obtain representations of input data with higher robustness. SCANet  introduced a Spatial Channel Attention (SCA) module to make use of multi-scale contextual information. The SCA module captures useful features from the global and multi-scale context of given scene, while an Extension Spatial Unsample (ESU) module helps combine multi-scale low-level features to generate high-level features with rich spatial information, which then leads to accurate 3D object proposals. In RT3D , the majority of convolution operations prior to the RoI pooling module are removed. With such optimization, RoI convolutions only need to be performed once for all proposals, accelerating the method to run at 11.1 FPS, which is five times faster than MV3D . \\par\nAnother approach to detect 3D objects is to generate candidate regions on 2D plane with 2D object detectors, then extract a 3D frustum proposal for each 2D candidate region. In F-PointNets , each 2D region generates a frustum proposal, and the features of each 3D frustum are learned with PointNet  or PointNet++  and used for 3D bounding box estimation. PointFusion  uses both 2D image region and corresponding frustum points for more accurate 3D box regression. A fusion network is proposed to directly predict corner locations of boxes by fusing image features and global features from point clouds. \\par", "cites": [7385], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple multi-view detection methods, highlighting their shared goals and architectural differences. It also provides critical analysis by pointing out limitations (e.g., speed of MV3D) and improvements (e.g., efficiency in RT3D). The section identifies broader patterns in how 2D-3D information is fused and how feature extraction strategies evolve, offering meta-level insights into the design choices in this subfield."}}
{"id": "8fc41ea6-8659-4e1f-9793-aa7c0f9ed3e9", "title": "Object Tracking", "level": "subsection", "subsections": [], "parent_id": "aba1eb45-d555-4844-b690-5d316715bfce", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Object Tracking"]], "content": "Object tracking targets estimating the location of a certain object in subsequent frames given its state in the first frame. The success of Siamese networks  in 2D image object tracking inspired 3D object tracking, and Giancola et al.  extend Siamese networks to 3D. In this method, candidates are first generated by a Kalman filter, then passed through an encoding model to generate compact representations with shape regularization, and match the detected objects by cosine similarity. Zarzar et al.  proposed another method that captures target objects more efficiently by leveraging a 2D Siamese network to detect coarse object candidates on BEV representation. The coarse candidates are then refined by cosine similarity in the 3D Siamese network.\\par\nChiu et al.  introduced the Kalman filter to encode the hidden states of objects. The state of an object is represented by a tuple of 11 variables, including position, orientation, size and speed. A Kalman filter is adopted to predict the object in next frame based on previous information, and a greedy algorithm is used for data association with Mahalanobis distance.\\par\nBesides, Qi et al.  proposed P2B, a point-to-box method for 3D object tracking. It divides the task into two parts. The first part is target-specific feature augmentation, seeds from the template and the search area are generated with a PointNet++ backbone, and the search area seeds will be enriched with target clues from the template. The second is target proposal and verification, candidate target centers are regressed and seed-wise targetness is evaluated for joint target proposal and verification. \\par", "cites": [7422, 7414, 7964, 7423, 9113], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by grouping them under common themes such as Siamese networks and Kalman filters. However, the connections are largely methodological and do not offer a deeper unifying framework. There is limited critical analysis, with no discussion of trade-offs, limitations, or evaluation of the effectiveness of these methods. Abstraction is minimal, as the focus remains on describing specific techniques without extracting broader principles or trends."}}
{"id": "92247a73-3e6f-4cbe-a797-99e688c47ae1", "title": "Scene Flow Estimation", "level": "subsection", "subsections": [], "parent_id": "aba1eb45-d555-4844-b690-5d316715bfce", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Scene Flow Estimation"]], "content": "Similar to optical flow estimation on images, 3D scene flow estimation works on a sequence of point clouds. FlowNet3D  is a representative work that directly estimates scene flows from pairs of consecutive point clouds. The flow embedding layer is used to learn point-level features and motion features. The experiment results of FlowNet3D shows that it performs less than satisfactorily in non-static scenes, and the angles of predicted motion vectors sometimes significantly differ from the ground truth. FlowNet3D++  is proposed to fix these issues by introducing a cosine distance loss in angles, and a point-to-plane distance loss to improve accuracy in dynamic scenes. HPLFlowNet , on the other hand, proposed a series of bilateral convolutional layers to fuse information from two consecutive frames and restore structural information from unconstructed point clouds. \\par\nIn addition, MeteorNet  introduced direct grouping and chained-flow grouping to group temporal neighbors, and adopted information aggregation over neighbor points to generate representation for dynamic scenes. Derived from recurrent models in images, Fan and Yang  proposed PointRNN, PointGRU and PointLSTM to encode dynamic point clouds by capturing both spatial and temporary information.", "cites": [7424, 7425], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a coherent narrative by linking different approaches to scene flow estimation, such as FlowNet3D, FlowNet3D++, HPLFlowNet, MeteorNet, and PointRNN, and highlights how each addresses specific limitations. It includes some critical analysis by pointing out performance issues of FlowNet3D and improvements made by its successors. However, it lacks deeper abstraction or a meta-level discussion of the broader implications or design principles across these methods."}}
{"id": "43510d23-52bd-4bae-8e66-0f34e53be874", "title": "Experiments", "level": "subsection", "subsections": [], "parent_id": "aba1eb45-d555-4844-b690-5d316715bfce", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Detection, Tracking and Flow Estimation"], ["subsection", "Experiments"]], "content": "KITTI  is one of the most popular benchmarks for many computer vision tasks, including those in images, point clouds, and multi-views. By taking advantage of autonomous driving platforms, KITTI provides raw data of real-world scenes, and allows evaluation on multiple tasks. Table \\ref{table:3} shows experimental results of different methods on KITTI. Some methods, such as VoteNet , which does not provide detailed test results on KITTI, are not listed.\n\\begin{table*}[htbp]\n\\centering\n\\caption{Experiment results on KITTI 3D detection benchmark, E/M/H stands for easy/medium/hard samples.}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\multirow{2}*{Method} & \\multirow{2}*{Category} & \\multirow{2}*{Speed} & \\multicolumn{3}{|c|}{Car} & \\multicolumn{3}{|c|}{Pedestrians} & \\multicolumn{3}{|c|}{Cyclists} \\\\\n\\cline{4-12}\n~ & ~ & ~ & E & M & H & E & M & H & E & M & H\\\\\n\\hline\nMV3D  & multi-view & 2.8 & 74.8 & 63.6 & 54.0 & - & - & - & - & - & - \\\\\n\\hline\nAVOD  & multi-view & 12.5 & 89.8 & 85.0 & 78.3 & 42.6 & 33.6 & 30.1 & 64.1 & 48.1 & 42.4 \\\\\n\\hline\nSCANet  & multi-view & 12.5 & 76.4 & 66.5 & 60.2 & - & - & - & - & - & - \\\\\n\\hline\nPIXOR  & projection & 28.6 & 84.0 & 80.0 & 74.3 & - & - & - & - & - & - \\\\\n\\hline\nVoxelNet  & projection & 2.0 & 77.5 & 65.1 & 57.7 & 39.5 & 33.7 & 31.5 & 61.2 & 48.4 & 44.4 \\\\\n\\hline\nSECOND  & projection & 26.3 & 83.3 & 72.6 & 65.8 & 49.0 & 38.8 & 34.9 & 71.3 & 52.1 & 45.8 \\\\\n\\hline\nPointPillars  & projection & 62.0 & 82.6 & 74.3 & 69.0 & 54.5 & 41.2 & 38.9 & 77.1 & 85.7 & 52.0 \\\\\n\\hline\nPointRCNN  & point & 10.0 & 87.0 & 75.6 & 70.7 & 48.0 & 39.4 & 36.0 & 75.0 & 58.8 & 52.5 \\\\\n\\hline\nPointRGCN  & point & 3.8 & 86.0 & 95.6 & 70.7 & - & - & - & - & - & - \\\\\n\\hline\nSTD  & point & 12.5 & 88.0 & 79.7 & 75.1 & 53.3 & 42.5 & 38.3 & 78.7 & 61.6 & 55.3 \\\\\n\\hline\nPoint-GNN  & point & - & 88.3 & 79.5 & 72.3 & 52.0 & 43.8 & 40.1 & 78.6 & 63.5 & 57.0 \\\\\n\\hline\nPV-RCNN  & point & - & 90.2 & 81.4 & 76.8 & 52.1 & 43.3 & 40.3 & 78.6 & 63.7 & 57.7 \\\\\n\\hline\n3DSSD  & point & 26.3 & 88.4 & 79.6 & 74.6 & 54.6 & 44.3 & 40.2 & 82.5 & 64.1 & 56.9 \\\\\n\\hline\n\\end{tabular}\n\\label{table:3}\n\\end{table*}", "cites": [7414, 7407, 7405, 7965, 7411], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a table comparing various methods on the KITTI benchmark but lacks synthesis of ideas or broader themes across the cited works. It provides factual results and minimal context, with no deeper analysis of trends, trade-offs, or limitations. The abstraction is limited to the raw performance metrics without drawing generalizable insights about the methodologies."}}
{"id": "46cc7191-7f21-4e73-be21-4474fbd5a61f", "title": "Traditional Methods", "level": "subsection", "subsections": [], "parent_id": "bad9e291-62b3-4660-9835-00c69e0db7ab", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Registration"], ["subsection", "Traditional Methods"]], "content": "The Iterative Closest Point (ICP) algorithm  is a pioneering work that solves 3D point set registration. The basic pipeline of ICP and its variants is as follows: (1) Sample a point set $P$ from the source point cloud. (2) Compute the closest point set $Q$ from the target point cloud. (3) Calculate the registration (transformation) with $P$ and $Q$. (4) Apply the registration, and if the error is above some threshold, go back to step (2), otherwise terminate. A global refinement step is usually required for better performance. The performance of ICP highly depends on the quality of initialization and whether the input point clouds are clean. Generalized-ICP  and Go-ICP  are two representative follow-up works that mitigate the problems of ICP in different ways. \\par\nCoherent Point Drift (CPD) algorithm  considers the alignment as a problem of probability density estimation. Concretely, the algorithm consider the first point set as the Gaussian mixture model centroids, and the transformation is estimated by maximizing the likelihood in fitting them to the second point set. The movement of these centroids are forced to be coherent to preserve the topological structure. \\par\nRobust Point Matching (RPM)  is another influential point matching algorithm. The algorithm starts with soft assignments of the point correspondences, and these soft assignments will get hardened through deterministic annealing. RPM is generally more robust than ICP, but still sensitive to initialization and noise.\\par\nIglesias et al.  focused on the registration of several point clouds to a global coordinate system. In other words, with the original set of $n$ points, we want to find the correspondences between (subsets of) the original set and $m$ local coordinate systems respectively. Iglesias et al. consider the problem as a Semidefinite Program (SDP), and attempt to analyze it with the application of Lagrangian duality. \\par", "cites": [8933, 5603], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of traditional point cloud registration methods, including ICP, CPD, RPM, and a method by Iglesias et al. It integrates the main ideas and pipelines from the cited papers but stops short of developing a deeper synthesis or novel framework. There is minimal critical evaluation of the limitations or trade-offs between methods. The abstraction is limited to general observations like robustness and initialization sensitivity, without identifying broader principles or trends."}}
{"id": "ae7b3393-90ed-49b9-9852-b0f301a88998", "title": "Learning-based Methods", "level": "subsection", "subsections": [], "parent_id": "bad9e291-62b3-4660-9835-00c69e0db7ab", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Registration"], ["subsection", "Learning-based Methods"]], "content": "DeepVCP  is the first end-to-end learning-based framework in point cloud registration. Given the source and target point cloud, PointNet++  is applied to extract local features. A point weighting layer then helps select a set of $N$ keypoints, after which $N\\times C$ candidates from the target point cloud are selected and passed through a deep feature embedding operation together with keypoints from the source. Finally, a corresponding point generation layer takes the embeddings and generates the final result. Two losses are incurred: (1) the Euclidean distance between  the estimated corresponding points and ground truth under the ground truth transformation, and (2) the distance between the target under the estimated transformation and ground truth. These losses are combined to consider both global geometric information and local similarity. \\par\n3DSmoothNet  is proposed to perform 3D point cloud matching with a compact learned local feature descriptor. Given two raw point clouds as input, the model first computes the local reference frame (LRF) of the neighborhood around the randomly sampled interest points. Then the neighborhoods are transformed into canonical representations and voxelized by Gaussian smoothing, and the local feature of each point is then generated by 3DSmoothNet. The features will then be utilized by a RANSAC approach to produce registration results. The proposed smooth density value (SDV) voxelization outperforms traditional binary-occupancy grids by reducing the impact of boundary effects and noise, and provides greater compactness. Following 3DSmoothNet, Gojcic et al.  proposed another method that formulates conventional two-stage approaches in an end-to-end structure. Earlier methods solve the problem in two steps, the pairwise alignment and the globally consistent refinement, by jointly learning both parts. Gojcic et al.'s method outperforms previous ones with higher accuracy and less computational complexity. \\par\nRPM-Net  inherits the idea of RPM  algorithm, and takes advantage of deep learning to enhance robustness against noise, outliers and bad initialization. In this method, the initialization assignments are generated based on hybrid features from a network instead of spatial distances between points. The parameters of annealing is predicted by a secondary network, and a modified Chamfer distance is introduced to evaluate the quality of registration. This method outperforms previous methods no matter the input is clean, noisy, or even partially visible.", "cites": [5604, 8934, 7385], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of the cited learning-based point cloud registration methods, including their architectures and key components. It attempts some synthesis by mentioning how each method improves upon prior approaches (e.g., end-to-end learning, handling noise/outliers), but lacks deeper integration into a broader framework. There is minimal critical analysis or identification of overarching patterns, focusing largely on summarizing individual contributions."}}
{"id": "9deffdf0-bf5f-4033-8c19-fb03bc2f910b", "title": "Discriminative Methods", "level": "subsection", "subsections": [], "parent_id": "d2332784-c38f-4aa0-a8a9-a2e7c7e7ee0d", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Augmentation and Completion"], ["subsection", "Discriminative Methods"]], "content": "Noise in point clouds collected from outdoor scenes is naturally inevitable. To prevent noise from influencing the encoding of point clouds, some denoising methods shall be applied in pre-processing. Conventional methods include local surface fitting, neighborhood averaging and guessing the underlying noise model. PointCleanNet  proposed a data-driven method to remove outliers and reduce noise. With a deep neural network adapted from PCPNet, the model first classifies outliers and discards them, then estimates a correction projection that projects noise to original surfaces.\\par\nHermosilla et al.  proposed Total Denoising that achieved unsupervised denoising of 3D point clouds without additional data. The unsupervised image denoisers are usually built based on the assumption that the value of a noisy pixel follows a distribution around a clean pixel value. Under this assumption, the original clean value can be recovered by learning the parameters of the random distribution. However, such an idea cannot be directly extended to point clouds because there are multiple formats of noise in point clouds, such as a global position deviation where no reliable reference point exists. Total Denoising introduces a spatial prior term that finds the closest of all possible modes on a manifold. The model achieves competitive performance against supervised models.\\par\nWhile a lot of models benefit from rich information in dense point clouds, some others are suffering from the low efficiency with large amounts of points. Conventional downsampling approaches usually have to risk dropping critical points. Nezhadarya et al.  proposed the critical points layer (CPL) that learns to reduce the number of points while preserving the important ones. The layer is deterministic, order-agnostic and also efficient by avoiding neighbor search. Aside from that, SampleNet  proposed a differentiable relaxation of point sampling by approximating points after sampling as a mixture of original points. The method has been tested as a front to networks on various tasks, and obtains decent performance with only a small fraction of the raw input point cloud. \\par", "cites": [5606, 5605, 7966], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of key discriminative methods for point cloud augmentation and completion, connecting ideas from different works (e.g., PointCleanNet and Total Denoising) and placing them in the context of noise types and challenges. It includes some critical evaluation, such as the limitations of conventional methods and the assumptions made in Total Denoising. However, the abstraction is limited to identifying general trends rather than offering deep meta-level insights or overarching frameworks."}}
{"id": "886c3c58-af17-412e-a68e-955ac1862a91", "title": "Generative Methods", "level": "subsection", "subsections": [], "parent_id": "d2332784-c38f-4aa0-a8a9-a2e7c7e7ee0d", "prefix_titles": [["title", "Deep Learning for 3D Point Cloud Understanding: A Survey"], ["section", "Augmentation and Completion"], ["subsection", "Generative Methods"]], "content": "Generative adversarial networks are widely studied for 2D images and CNNs, as they help locate the potential defects of networks by generating false samples. While typical applications of point cloud models, such as autonomous driving, consider safety as a critical concern, it is helpful to study how current deep neural networks on point clouds are affected by false samples.\\par\nXiang et al.  proposed several algorithms to generate adversarial point clouds against PointNet. The adversarial algorithms work in two ways: point perturbation and point generation. Perturbation is implemented by shifting existing points negligibly, and generation is implemented by either adding some independent and scattered points or a small number of point clusters with predefined shapes. Shu et al.  proposed tree-GAN, a tree-structured graph convolution network. By performing graph convolution within a tree, the model takes advantage of ancestor information to enrich the capacity of features. Along with the development of adversarial networks, DUP-Net  is proposed to defend 3D adversarial models. The model contains a statistical outlier removal (SOR) module as denoiser and a data-driven upsampling network as upsampler. \\par\nAside from adversarial generation, generative models are also used for point cloud upsampling. There are generally two motivations to upsample a point cloud. The first is to reduce the sparseness and irregularity of data, and the second is to restore missing points due to occlusion. \\par\nFor the first aim, PU-Net  proposed upsampling in the feature space. For each point, multi-level features are extracted and expanded via a multi-branch convolution unit; after that, the expanded feature is split into multiple features and reconstructed to upsample the input set. Inspired by image super-resolution models, Wang et al.  proposed a cascade of patch-based upsampling networks, learning different levels of details at different steps, where at each step the network focuses only on a local patch from the output of the previous step. The architecture is able to upsample a sparse input point set to a dense set with rich details. Hui et al.  also proposed a learning-based deconvolution network that generates multi-resolution point clouds based on low-resolution input with bilateral interpolation performed in both the spatial and feature spaces.\\par\nMeanwhile, early methods in completion, such as , tend to voxelize the input point cloud at the very beginning. PCN  was the first framework to work on raw point clouds and in a coarse-to-fine fashion. Wang et al.  improved the results with a two-step reconstruction design. Besides, Huang et al.  proposed PF-Net that preserves the spatial structure of the original incomplete point cloud, and predicts the missing points hierarchically a multi-scale generating network. GRNet, on the other hand, proposed a gridding-based which retrieve structural context by performing cubic feature sampling per grid, and complete the output with \"Gridding Reverse\" layers and MLPs. \\par\nLan et al.  proposed a probabilistic approach to optimize outliers by applying EM algorithm with Cauchy-Uniform mixture model to suppress potential outliers. More generally, PU-GAN  proposed a data-driven generative adversarial network to learn point distributions from the data and upsample points over patches on the surfaces of objects. Furthermore, RL-GAN-Net  uses a reinforcement learning (RL) agent to provide fast and reliable control of a generative adversarial network. By first training the GAN on the dimension-reduced latent space representation, and then finding the correct input to generate the representation that fits the current input form the uncompleted point cloud with a RL agent, the framework is able to convert noisy, partial point cloud into a completed shape in real time.\\par", "cites": [5611, 5612, 5607, 5609, 5613, 5610, 5608, 7968, 7967], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes generative methods for point cloud augmentation and completion effectively, grouping works by their aims (e.g., reducing sparseness vs. completing missing parts) and highlighting architectural innovations such as feature space upsampling, cascaded refinement, and gridding-based approaches. It provides a critical perspective by contrasting methods (e.g., PF-Net preserves structure vs. others that change existing points) and identifying limitations such as detail loss in MLP-based methods. The section abstracts key design principles and trends in generative point cloud modeling, such as the shift from voxelization to raw point cloud processing and the use of adversarial learning and interpolation techniques."}}
