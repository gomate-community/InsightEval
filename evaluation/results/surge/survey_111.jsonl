{"id": "cde5e9dc-556a-489c-8400-af776f3af7b8", "title": "Introduction", "level": "section", "subsections": ["8c4057d1-01d8-489a-89ad-d33f198b61f8", "7eeb464d-45a5-4117-9380-47e8cc4f5aa9", "6d22fd18-7538-4ae7-ae2e-7ef1308188eb", "6ee875b7-2355-4ad2-9101-387bc242ccfe"], "parent_id": "68e5db2c-bcee-48f2-8fe1-51da0b40c965", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Introduction"]], "content": "\\label{sec:Introduction}\nIn the age of data deluge, where users face a new form of information explosion, recommender systems (RS) have emerged as a paradigm of information push to lessen decision anxieties and consumer confusion by over-choice. RS enhance users' decision-making process and support sales\nby personalizing item recommendations for each user and helping them discover novel products. RS are a pervasive part of user experience online today and serve as the primary choice for many consumer-oriented companies such as Amazon, Netflix, and Google (e.g., YouTube~).\nAmong different recommendation techniques, collaborative filtering (CF) methods have been the mainstream of recommendation research both in academia and industry due to their superb recommendation quality. CF builds on the fundamental assumption that users who have expressed similar interests in the past will maintain similar choices in future~, and infers target user preference over unseen items by leveraging behavioral data of other users and exploiting similarities in their behavioral patterns.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width = 0.90\\linewidth]{Pictures/From_CF_to_SecureCF.pdf}\n    \\caption{Milestones of CF recommender models.}\n    \\label{fig:milestones}\n\\end{figure}\nMilestones in CF models over the last three decades are illustrated in Figure~\\ref{fig:milestones}. We can identify two major eras in development of CF models based on their main objective:\n\\begin{enumerate}\n    \\item The era focused on maximizing/enhancing the recommendation accuracy and beyond-accuracy; \n    \\item The post neural era, the transition era from classical learning to adversarial machine learning.\n\\end{enumerate}\n\\noindent \\textbf{Accuracy maximization and beyond-accuracy enhancement era.} In this era, the main effort of research and practitioner-scholars was concentrated on the \\dquotes{golden objective} of \\textit{maximizing recommendation accuracy}. Consequently, machine-learned models tend to use any available signal in the data to reach this goal, even though some of the data contained noise as the results of users' misoperations. We distinguish between two classes of CF techniques in this era: (i) classical non-neural CF, (ii) deep neural CF, each described in the following.\n\\begin{itemize} \n    \\item \\textbf{Classical non-neural CF.} The starting of this era dates back to the 1990s and is still progressing. Over these three decades, the study on CF methods has been the subject of active research by the RS community resulting in a diverse set of models and evaluation measures to assess the effectiveness of these models. We can classify these CF approaches based on various dimensions. For example, from a \\textit{learning paradigm} perspective, CF models can be classified according to (i) \\textit{memory-based CF} and (ii) \\textit{model-based CF} models, in which the former category makes recommendation based on the similarity of users-user interactions (i.e., user-based neighborhood model) or item-item interactions (i.e., item-based neighborhood model) while the latter category predicts users' feedback of unseen items using latent factor models such as matrix-factorization (MF)~. From the \\textit{model training} perspective, it is possible to categorize these models based on the loss functions employed according to (i) \\textit{point-wise} loss where the goal is to optimize towards a predefined ground-truth (e.g., matrix factorization approach based on SVD), (ii) \\textit{pairwise ranking loss} where the goal is to optimize personalized ranking  (e.g., matrix factorization based on BPR) and (iii) \\textit{list-wise} loss where the objective is to reflect the distance between the reference list and the output list ~.\n\t\\item \\textbf{Deep neural CF.}  Another milestone is concerned with the success of  \\dquotes{neural} technology in machine learning (ML). DNNs have shown to be capable of providing remarkable accuracy in several predictive tasks and domains such as image classification~ and speech recognition~ among others. In the field of RS, DNNs have been shown useful for the recommendation in several ways such as extracting deep features (via using CNNs), modeling item content in CF models by integrating side item information, building CF models by parameterizing latent factor models into layers of a DNN (deep CF), and modeling sequential relations (via using RNNs). As for deep-CF approaches, while MF assumes that the \\textit{linear interaction} between user and item latent factors can explain observed feedback, deep CF models can model a more complex representation of hidden latent factors by \\textit{parametrization of MF via a DNN}.   \n\\end{itemize}\n\\updated{The above system types have been redesigned to use a wealth of side information beyond the URM into the recommendation models to make RS adapted in specific domains. The surveys~ provide a good frame of reference for CF methods leveraging rich side information.} \n\\noindent \\textbf{The post neural era, the transition era from classical learning to adversarial machine learning.} Despite the significant success of DNNs to solve a variety of complex prediction tasks on non-structured data such as images, recently, they have been demonstrated to be vulnerable to \\textit{adversarial examples}. Adversarial examples (or adversarial samples) are subtle but non-random perturbations \\textit{designed} to dictate a ML model to produce erroneous outputs (e.g., to misclassify an input sample). The subject started booming after the pioneering work by~ reported the vulnerability of DNNs against adversarial samples for the image classification task. It has been shown that by adding a negligible amount of adversarial perturbation on an image (e.g., a panda), a CNN classifier could misclassify the image in another class (e.g., a gibbon) with high confidence.  These results were quite shocking since it was expected that state-of-the-art DNNs that generalize well on unknown data do not change the label of a test image that is slightly perturbed and is human-imperceptible. Algorithms that aim to find such adversarial perturbations are referred to as \\textit{adversarial attacks}. As ML models are involved in many consumer safety and security-intensive tasks such as autonomous driving, facial recognition, and camera surveillance, adversarial attacks pose significant concerns to the security and integrity of the deployed ML-models.\nIn the field of RS, numerous works have reported the failure of machine-learned recommendation models, i.e., latent-factor models (LFM) based on CF like MF and deep CF methods widely adopted in modern RS, against adversarial attacks. For instance,   showed that by exposing\nthe model parameters of BPR~ to both adversarial and random perturbations of the BPR model parameters, the value of nDCG is decreased by -21.2\\% and -1.6\\% respectively, which is equal to a staggering impact of approximately 13 times difference. One main explanation for such behavior is that adversarial attacks exploit the imperfections and approximations made by the ML model during the training phase to control the models' outcomes in an engineered way~.\nAdversarial machine learning (AML) is an emerging research field that combines the best practices in the areas of ML, robust statistics, and computer security~. It is concerned with the design of learning algorithms that can resist adversarial attacks, studies the capabilities and limitations of the attacker, and investigates suitable countermeasures to design more secure learning algorithms~.\nThe pivotal distinguishing characteristic of AML is the notion of \\dquotes{min-max} game, in which two competing players play a zero-sum differential game, one --- i.e., the attacker --- tries to \\textit{maximize} the likelihood of the attack success, while the other --- i.e., the defender --- attempts to \\textit{minimize} the risk in such a worst-case scenario. In the context of RS, the defender players can be a machine-learned model such as BPR or a neural network, while the attacker is the adversarial model.\nTo protect models against adversarial attacks, \\textit{adversarial training} has been proposed. It is a defensive mechanism whose goal is not to detect adversarial examples, instead to build models that perform equally well with adversarial and clean samples. Adversarial training consists of injecting adversarial samples ---generated via a specific attack model such as FGSM~ or BIM~--- into each step of the training process. It has been reported ---both in RS~ and ML~--- that this process leads to robustness against adversarial samples (based on the specific attack type on which the model was trained on), and better generalization performance on clean samples. For instance, in~, the authors show that the negative impact of adversarial attacks measured in terms of nDCG is reduced from -8.7\\% to -1.4\\% when using adversarial training instead of classical training.\nThe above discussion highlights the failure of classical ML models (trained on clean data) in adversarial settings and advocates the importance of AML as a new paradigm of learning to design more secure models. Nevertheless, the attractiveness of AML that exploits the power of two adversaries within a \\dquotes{min-max} game is not limited to security applications and has been exploited to build novel \\textit{generative} models, namely generative adversarial networks (GANs). The key difference is as follows: the models used in AML for security (or attack and defense) focus only on a class of discriminative models (e.g., classifiers), whereas GANs build upon both discriminative and generative models.\nA GAN is composed of two components: the generator $G$ and the discriminator $D$. The training procedure of a GAN is a min-max game between $G$,  optimized to craft fake samples such that $D$ cannot distinguish them from real ones, and $D$, optimized to classify original samples from generated ones correctly. \nThrough the interplay between these two components, the model reaches the Nash equilibrium where $G$ has learned to mimic the ground-truth data distribution, e.g., a profile of a particular user. In the present survey, we identified different application for GAN-based RS that include, improving negative sampling step in learning-to-rank objective function~, fitting the generator to predict missing ratings by leveraging both temporal~ and side-information~, or augmenting training dataset~.", "cites": [314, 891, 7318, 892, 1189, 923, 1187, 1186, 1188], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a range of foundational papers on adversarial machine learning and applies them to the context of recommender systems, particularly collaborative filtering. It integrates concepts like adversarial examples, min-max games, and robustness into a coherent narrative about the evolution and vulnerabilities of recommendation models. While it provides some critical commentary (e.g., the vulnerability of BPR to adversarial perturbations), the critique is more descriptive than deeply evaluative, though it still contributes to an analytical understanding of the field."}}
{"id": "6d22fd18-7538-4ae7-ae2e-7ef1308188eb", "title": "Survey Context and Related Surveys", "level": "subsection", "subsections": [], "parent_id": "cde5e9dc-556a-489c-8400-af776f3af7b8", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Introduction"], ["subsection", "Survey Context and Related Surveys"]], "content": "\\updated{While there exist several survey articles on general RS topics, for example~,\nto the best of our knowledge, none of them focuses on the application of AML techniques in the recommendation task. In contrast, we provide a comprehensive literature review and extended taxonomy on the application of AML for security purposes (i.e., adversarial attacks and defenses) and generative models (i.e., in GAN-based systems). This classification is further accompanied with the identification of main application trends, possible future directions, and novel open challenges.}\n\\updated{The current literature review can be seen nonetheless related to other surveys such as~.  In particular,~ introduce AML as a novel application of DL by pointing out to very few works of the field such as~ without providing a detailed study on the topic of AML for RS. The work by~, on the other hand, is centered around the application of AML for graph learning in general ML setting. Although, link prediction techniques can be adapted from graph-learning based system to perform item recommendation task (e.g., by predicting a user's connection with an item), this work remains far from the focus of the current survey. We would like also to acknowledge existence of related surveys on the application of AML in other tasks, not directly related to RS, for example for the CV field by~, on classical ML models by~, and GAN applications in CV and NLP tasks by~.}\nWe can also highlight that part of the material presented in this survey has been presented as a tutorial at the WSDM'20~ and the RecSys'20 conference.\\footnote{Tutorial slides at ~\\url{https://github.com/sisinflab/amlrecsys-tutorial}}", "cites": [1187, 1190, 313, 7336], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates information from related surveys and papers to situate the current work within the broader context of adversarial learning. It differentiates the focus of existing surveys, particularly in graph learning and computer vision, from the specific application in recommender systems, indicating an effort to synthesize and contrast different domains. While it identifies gaps and limitations in other surveys, the analysis remains somewhat high-level and could benefit from deeper critical engagement or more detailed comparative insights."}}
{"id": "8a2feeca-6f33-4ae6-8808-36429930926a", "title": "\\updated{Adversarial attacks on ML models", "level": "subsubsection", "subsections": [], "parent_id": "37d5f95e-65ad-40e8-a04c-82ef99007910", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Foundations of adversarial machine learning"], ["subsubsection", "\\updated{Adversarial attacks on ML models"]], "content": "}\nIn recent years, the advances made in deep learning (DL) have considerably advanced the intelligence of ML models in a unique number of predictive tasks such as classification of images and other unstructured data. Notwithstanding their great success, recent studies have shown that ML/DL models are not immune to security threats from adversarial use of AI. We can classify attacks against a ML model along three main dimensions, attack \\textit{timing} and \\textit{goal}.\n    \\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=0.70\\paperwidth]{Pictures/posion_evasion_attack.pdf}\n    \\caption{A schematic representation of the distinction between \\textit{evasion} attacks and \\textit{poisoning} attacks.}\n    \\label{figure:attack_scheme}\n\\end{figure}\n\\textbf{Attack timing.} As illustrated in Fig.~\\ref{figure:attack_scheme}, an adversary can attack a ML model at two main stages of the learning pipeline, during \\textit{training} or \\textit{production}. These two categories of attacks are respectively known as (i) \\textit{training-time attack} (a.k.a. causative or poisoning attack)~ and, ii) \\textit{inference-time attack} (a.k.a. exploratory or evasion attack)~.\n    \\begin{itemize}\n    \\item \\textit{Poisoning attack.} Data poisoning attacks are realized by injecting false data points into the training data with the goal to corrupt/degrade the model (e.g., the classifier). Poisoning attacks have been explored in the literature for a variety of tasks~, such as (i) attacks on binary classification for tasks such as label flipping or against kernelized SVM~, (ii) attacks on unsupervised learning such as clustering and anomaly detection~, and (iii) attacks on matrix completion task in RS~. As an example, in the pioneering work by~, the authors propose a poisoning attack based on properties of the SVM optimal solution that could significantly degrade the classification test accuracy. \n    \\item \\textit{Evasion attack.} \\updated{Unlike poisoning attacks, evasion attacks do not interfere with training data. They adjust malicious samples during the inference phase. These attacks are also named \\textit{decision-time} attacks referring to their attempt to \\textit{evade} the \\textit{decision} made by the learned model at test time~. For instance, evasion attacks can be used to evade spam~, as well as network intrusion~ detectors. Recently, evasive attacks are conducted by crafting \\textit{adversarial examples}, which are subtle but non-random human-imperceptible perturbations, added to original data to cause the learned model to produce erroneous output.~ were the first to discover that some carefully selected perturbations that are barely perceptible to the human eye, when added to an image, could lead a well-trained DNN to misclassify the adversarial image with high confidence.}\n    \\end{itemize}\n\\textbf{Attack goal.} Attacks are conducted for different goals. We can distinguish between two main classes of attack goals: i) \\textit{untargeted attack} and, ii) \\textit{targeted attack}. To provide the reader with an intuitive insight of the mechanism behind adversarial attacks and defense strategies, we define them formally for a classification task~.\n\\begin{definition}[Untargeted adversarial attack~]\\label{def:untar_attack} The goal of the attacker in \\textbf{untargeted adversarial attack} (misclassification) is to add a minimal amount of perturbation $\\delta$ on the input sample $x$ such that it can cause  incorrect classification.\n Given $f(x;\\theta) = y$, an Untargeted Adversarial Attack is formulated as:\n    \\begin{equation}\n        \\label{eq:att_untrg}\n        \\begin{aligned}\n            & \\min_{\\delta}\n            & & \\left\\lVert \\delta \\right\\rVert \\\\\n            & \\text{s.t.:}\n            & & f(x + \\delta; \\theta) \\neq y , \\ \\ x + \\delta \\in [0,1]^n\n        \\end{aligned}\n    \\end{equation}\nThe second constraint $x + \\delta \\in [0,1]^n$ is a value-clipping constraint needed for images, to bound the adversarial samples into to a predefined range so that the images remain visible after adversarial attack. Alternatively, we can formulate the problem as an unconstrained optimization problem where the goal of the attacker is to \\textit{maximize} the loss between the perturbed sample $x + \\delta$ and true class $y$\n    \\begin{equation}\n        \\label{eq:att_rel_gen}\n        \\begin{aligned}\n            & \\underset{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon }{\\text{max}}\n            & &\\ell(f(x+\\delta; \\theta),y) \n        \\end{aligned}\n    \\end{equation}\nObviously since adding an unbounded amount of noise on the input will eventually lead to a classification error, the goal of the attacker is to minimize a norm-constrained form of noise, that is $\\left\\lVert \\delta \\right\\rVert \\leq \\epsilon$ for some exogenously given $\\delta$. \\qed\n\\end{definition}\nIn the context of DNN, the above attacks are categorized based on the norm used to represent the magnitude of the noise according to the following norm types~: $l_0$, $l_1$ and $l_2$ and $l_{\\infty}$.\n\\begin{definition}[Targeted adversarial attack~]\n    \\label{def:untar_attack}\nThe goal of the attacker in \\textbf{targeted adversarial attack} is to perturb the input by adding a minimum amount of perturbation $\\delta$ such that it can force the model to misclassify the perturbed sample into an illegitimate target class (aka mis-classification label). Given $f(x;\\theta) = y$, with $y \\neq y_t$, we formulate the problem as:\n    \\begin{equation}\n        \\label{eq:att_trg}\n        \\begin{aligned}\n            & \\underset{\\delta}{\\text{min}}\n            & & \\left\\lVert \\delta \\right\\rVert \\\\\n            & \\text{s.t.:}\n            & & f(x + \\delta; \\theta) = y_t\n        \\end{aligned}\n    \\end{equation}\nSimilarly, the above problem can be expressed as a unconstrained optimization problem \n    \\begin{equation}\n        \\label{eq:att_rel_gen}\n        \\begin{aligned}\n            & \\underset{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon }{\\text{min}}\n            & & \\ell(f(x+\\delta; \\theta), y_t) \n        \\end{aligned}\n    \\end{equation}  \n    \\qed \n\\end{definition}\nThe most common attack types so far exploited in the community of RS are fast gradient sign attack (FGSM)~ and Carlini and Wagner (C\\&W) attacks, which belong to $l_{\\infty}$- and $l_2$-norm   attack types respectively. We provide the formal definition of the FGSM and C\\&W attacks here.\n\\begin{definition}[FGSM attack~]\nThe fast gradient sign method (FGSM)~ utilizes the \\textit{sign of the gradient} of the loss function to find perturbation that maximizes the training loss (for untargeted case)\n    \\begin{equation}\n        \\label{eq:att_linf}\n        \\delta = \\epsilon \\cdot \\sign    (\\bigtriangledown_x \\ell(f(x;\\theta),y))\n    \\end{equation}\nwhere $\\epsilon$ (perturbation level) represents the attack strength and $\\bigtriangledown_x$ is the gradient of the loss function w.r.t. input sample $x$. The adversarial example is generated as $x^{adv} = x + \\delta$. FGSM applies an $l_{\\infty}$-bound constraint $||\\delta||_{\\infty} \\leq \\epsilon$ with the original idea to encourage perceptual similarity between the original and perturbed samples. The unconstrained FGSM  aims to find perturbation that would increase/maximize the loss value. The corresponding approach for targeted FSGM~ is\n    \\begin{equation}\n        \\label{eq:att_linf_targeted}\n        \\delta = - \\epsilon \\cdot \\sign    (\\bigtriangledown_x \\ell(f(x;\\theta),y_t))\n    \\end{equation}\nwhere the goal is to maximize the conditional probability $p(y_t|x)$ for a given input $x$.\\qed \n\\end{definition}\nSeveral variants of the FGSM has been proposed in the literature~. For instance, the fast gradient value (FGV) method~, which instead of using the sign of the gradient vector in FGSM, uses the actual value of the gradient vector to modify the adversarial change, or basic iterative method (BIM)~  (a.k.a iterative FGSM) that applies FGSM attack multiple times \\textit{iteratively} using a small step size and within a total acceptable input perturbation level.\n\\begin{definition}[C\\&W attack~]\nThe Carlini and Wagner (C\\&W) attack~ is one of the most effective attack models. The core idea of C\\&W attack is to replace the standard loss function --- e.g., typically cross-entropy --- with an empirically-chosen loss function and use it in an \\textit{unconstrained optimization formulation} given by\n\\begin{equation}\n    \\underset{\\delta}{\\text{min}} \\left\\lVert \\delta \\right\\rVert_p^p + c \\cdot h(x+\\delta, y_t)\n\\end{equation}\nwhere $h(\\cdot)$ is the candidate loss function.\\qed \n\\end{definition}\nThe C\\&W attack has been used with several norm-type constraints on perturbation $l_0$, $l_2$, $l_{\\infty}$ among which the $l_2$-bound constraint has been reported to be most effective~.\n\\textbf{Adversarial attacks on RS - challenges and differences with ML tasks.} In spite of the similarities between ML classification and recommendation learning tasks, there are considerable differences/challenges in adversarial attacks on RS compared with ML and the degree to which the subject has been studied in the respective communities:\n\\begin{itemize}\n    \\item \\textit{Poisoning vs. adversarial attack.} In the beginning, the main focus of RS research community has been on \\textit{hand-engineered} fake user profiles (a.k.a shilling attacks) against rating-based CF~. Given a URM with $n$ real users and $m$ items, the goal of a shilling attack is to augment a fraction of malicious users $\\lfloor{\\alpha n} \\rfloor$ ($\\lfloor{.}\\rfloor$ is the floor operation) to the URM ($\\alpha \\ll 1$) in which each malicious use profile can contain ratings to a maximum number of $C$ items. The ultimate goal is to harvest recommendation outcomes toward an illegitimate benefit, e.g., pushing some targeted items into the top-$K$ list of users for market penetration. Shilling attacks against RS have an established literature and their development face two main milestones: the first one ---since the early 2000s--- where the literature was focused on building hand-crafted fake profiles whose rating assignment follow different strategy according to random, popular, love-hate, bandwagon attacks among others~; the second research direction started in 2016 when the first ML-optimized attack was proposed by~ on factorization-based RS. This work reviews a novel type of data poisoning attack that applies the adversarial learning paradigm for generating poisoning input data. Nonetheless, given their significant impact against modern recommendation models, the research works focusing on \\textit{machine-learned adversarial attacks} against RS have recently received great attention from the research community, \\updated{e.g., consider~.}\n    \\item \\textit{CF vs. classification models:} Attacks against classification tasks focus on enforcing the wrong prediction of individual instances in the data. In RS, however, the mainstream attacks rely on CF principles, i.e., mining similarity in opinions of like-minded users to compute recommendations. This interdependence between users and items can, on the one hand, \\textit{improve robustness} of CF, since predictions depend on a group of instances not on an individual one and, on the other other hand, may cause \\textit{cascade effects}, where attack on individual user may impact other neighbor users~.\n    \\item \\textit{Attack granularity and application type:} Adversarial examples created for image classification tasks are empowered based on continuous real-valued representation of image data (i.e., pixel values), but in RS, the raw values are user/item IDs and ratings that are discrete. Perturbing these discrete entities is infeasible since it may lead to changing the semantics of the input, e.g., loosely speaking applying $ID+\\delta$ can result in a new user $ID$. Therefore, existing adversarial attacks in the field of ML are not transferable to the RS problems trivially. Furthermore, in the context of CV --- attacks against images --- the perturbations often need to be \\dquotes{human-imperceptible} or \\dquotes{inconspicuous} (i.e., may be visible but not suspicious)~. How can we capture these nuances for designing attacks in RS remains as an open challenge.\n\\end{itemize}", "cites": [314, 891, 1193, 890, 1192, 1191, 892, 923, 1196, 7337, 916, 1194, 1195], "cite_extract_rate": 0.6190476190476191, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers on adversarial machine learning, presenting a structured overview of attack timing (poisoning vs. evasion) and goals (untargeted vs. targeted) with formal definitions and equations. It abstracts common attack formulations and discusses their application in recommender systems. However, while it organizes the information effectively, it lacks deeper critical evaluation of the cited works and their limitations, which keeps the critical dimension lower."}}
{"id": "9668c3aa-18b8-4212-b46b-d6733b069086", "title": "Defense against adversarial attacks", "level": "subsubsection", "subsections": [], "parent_id": "37d5f95e-65ad-40e8-a04c-82ef99007910", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Foundations of adversarial machine learning"], ["subsubsection", "Defense against adversarial attacks"]], "content": "From a broad perspective, defense mechanisms against adversarial attacks can be classified as \\textit{detection} methods and methods seeking to increase the \\textit{robustness} of the learning model. The goal of this section is to briefly review approaches that build \\textit{robust ML} models in adversarial settings. The prominent methods used in RS are (i) the robust optimization~ and, (ii) the distillation method~.\n\\textbf{Robust optimization against adversarial attacks.}\nAt the heart of the robust optimization method is the assumption that every sample in the training data $\\mathcal{D}$ can be a source for adversarial behavior. It performs an ERM against a specific adversary on each sample in $\\mathcal{D}$ and applies a zero sum-game between the prediction and attack adversaries\nleading to the following robust optimization framework\n\\begin{equation}\n    \\begin{aligned}\n        \\label{eq:robust_opt_2}\n         \\min_{\\theta}  \\sum_{(x_i,y_i) \\in \\mathcal{D}} \\max_{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon} \\ell(f(x_i+\\delta; \\theta),y_i)\n    \\end{aligned}\n\\end{equation}\nwhere $\\epsilon$ is an upper-bound on the adversarial perturbation level $\\delta$. The ultimate goal in robust optimization is that the prediction model will perform equally well with adversarial and clean inputs.\n\\begin{definition}[Adversarial training~]\nThe goal of adversarial training is to build a robust model from ground-up on a training set augmented with adversarial examples. Adversarial regularization is one of the mostly investigated techniques for adversarial training, which utilizes an approximation of the worst-case loss function, i.e., $\\max_{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon} \\ell(f(x + \\delta; \\theta),y_i)$, as the regularizer.\n\\begin{equation}\n    \\label{eq:robust_opt_adv_reg}\n        \\ell_{T} = \\underbrace{\\min_{\\theta}  \\sum_{i \\in \\mathcal{D}} [\\ell(f(x; \\theta),y_i) + \\lambda \\underbrace{\\max_{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon} \\ell(f(x + \\delta; \\theta),y_i)}_\\text{optimal attack model}}_\\text{optimal robustness-preserving prediction}]\n\\end{equation}\n\\qed \n\\end{definition}\nAs it can be noted, the inner maximization finds the strongest attack  against the prediction model that is subject to adversarial perturbation. The outer minimization estimates the strongest defensive against a given attack by giving up a level of accuracy due to the regularization. The parameter $0<\\lambda<1$ controls the trade-off between accuracy (on clean data) and robustness (on perturbed data).\n\\begin{exmp}[Adversarial training of BPR-MF]\nBPR is the state-of-the-art method for personalized ranking  implicit feedbacks. The main idea behind BPR is to maximize the distance between positively and negatively rated items. Given the training dataset $D$ composed by positive and negative items for each user, and the triple $(u,i,j)$ (user $u$, a positive item $i$ and negative item $j$), the BPR objective function is defined as \n\\begin{equation}\n    \\label{eq:BPR_loss}\n    \\ell_{BPR}(\\mathcal{D} | \\Theta) = \\argmax_{\\Theta} \\sum_{(u,i,j) \\in \\mathcal{D}} ln \\, \\sigma(\\hat{x}_{ui}(\\Theta)-\\hat{x}_{uj}(\\Theta))-\\lambda\\left \\| \\Theta \\right \\|^2\n\\end{equation}\nwhere $\\sigma$ is the logistic function, and $\\hat{x}_{ui}$ is the predicted score for user $u$ on item $i$ and $\\hat{x}_{uj}$ is the predicted score for user $u$ on item $j$; $\\lambda \\left \\| \\Theta \\right \\|^2$ is a regularization method to prevent over-fitting.\\footnote{As it can be noted, BPR can be viewed as a classifier on the triple $(u,i,j)$, where the goal of the learner is to classify the difference $\\hat{x}_{ui}-\\hat{x}_{uj}$ as correct label +1 for a positive triple sample and 0 for a negative instance.} Adversarial training of BPR-MF similar to Eq.~\\ref{eq:robust_opt_adv_reg} can be formulated as\n\\begin{equation}\n    \\label{eq:robust_bpr_adv_reg}\n    \\ell_{APR} = \\underbrace{\\min_{\\theta}  \\sum_{(u,i,j) \\in D} [\\ell_{BPR}(\\mathcal{D} | \\Theta) + \\lambda \\underbrace{\\max_{\\delta: \\left\\lVert \\delta \\right\\rVert \\leq \\epsilon} \\ell_{BPR}(\\mathcal{D} | \\Theta + \\delta)]}_\\text{optimal attack model against BPR}}_\\text{optimal robustness preserving defensive}\n\\end{equation} \\qed \n\\end{exmp}\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=0.7\\paperwidth]{Pictures/adversarial_framework_v2.pdf}\n    \\caption{A notional view of Adversarial Recommendation Framework integrating the adversarial perturbations on users and items, and their side information, model parameters.}\n    \\label{figure:adv_framework}\n\\end{figure}\nWe do not report details on \\textit{distillation}  as defense strategy since it is not very common for RS.", "cites": [892, 1187, 923, 681], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers on adversarial training and robust optimization, integrating them into a clear framework for understanding defense mechanisms in RS. It abstracts the BPR-MF method into a generalized adversarial training formulation, showing broader relevance. However, critical analysis is limited, as it does not deeply evaluate the limitations or trade-offs of the approaches or compare them in a nuanced way."}}
{"id": "6267834f-3f7b-44b2-a329-ab08802823e3", "title": "Adversarial Machine Learning for Attack and Defense on RS", "level": "subsection", "subsections": ["6f7afaec-eb67-418a-bbfd-b446bed85c06"], "parent_id": "853aaac3-266e-4a22-bdef-6c605bd09586", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Adversarial Machine Learning for Attack and Defense on RS"]], "content": "\\label{subsec:AML_RS}\nIn this section, we focus on state-of-the-art approaches to the application of AML in RS research. RS which employ AML for security applications in recommendation tasks, follow the simplified steps sketched in Fig.~\\ref{figure:adv_framework}.\nIn the following, in addition to providing concise summaries of the surveyed works, for a convenient overview, we categorize the reviewed research articles in Table~\\ref{tbl:adl_attack} according to the following dimensions:\n\\begin{itemize}\n     \\item \\textbf{Model.} This column lists the model name and provides the reference to the main paper.\n     \\item \\textbf{Attack and Defense Model.} This column represents the main \\textit{attack} and \\textit{defense} strategies applied on various recommendation models and the \\textit{attack granularity} on the system.\n     \\begin{enumerate}\n         \\item \\textit{Attack model.}\n         Among all attacks strategies proposed in the community of CV~, in RS the most dominant attack approaches to date have been \\textit{FGSM} and \\textit{C\\&W}, \\updated{and \\textit{Other} strategies (e.g., multi-step adversarial attacks~, GAN-based attack models~))}\n         \\item \\textit{Defense model.}  As for the best defensive strategy against attack, we have found the strategy \\textit{adversarial training (a.k.a. adversarial regularization)} as the most commonly-adopted approach irrespective of the attack model, while  \\textit{distillation} is adopted only by a single paper~.\n         \\item \\textit{Attack granularity.} This column represents the level of data on which the adversarial perturbation is added on. It is important to note that while in the computer vision domain, these perturbations are added on raw data (e.g., pixel values), in RS, they are applied on the model parameters of recommendation strategy, as illustrated in Fig.~\\ref{figure:adv_framework}. In particular, adversarial perturbations are added to one of the following data: (i) directly on the \\textit{user profile }(i.e., user rating profile), (ii) \\textit{user and item latent factor model parameters} in an LFM, e.g., according to $\\mathbf{p}'_u = \\mathbf{p}_u + \\delta$, $\\mathbf{q}'_i = \\mathbf{q}_i + \\delta$ in which $\\mathbf{p}_u, \\mathbf{q}_i \\in \\mathbb{R}^F$ are $F$-dimensional embedding factors  whose linear interaction explains an unobserved preference; (iii) and (iv) \\textit{embeddings representing side information of user and items} respectively.\n    \\end{enumerate}\n     \\item \\textbf{Recommendation \\& Learning.}\n     The core recommendation models that we consider in this survey are CBF, CF and CA. We also consider hybrid systems but we do not specify a placeholder for such systems; if an approach use both CBF+CF, we simply mark both corresponding columns, regardless of which hybridization technique it uses~. Instead, given the ML (optimization)-based approach for most of the considered papers, we categorize papers based on the recommendation prediction model according to \\textit{linear LFM} (e.g., MF or variations of that such as PMF), \\textit{linear tensor factorization} (TF), \\textit{non-linear models based on auto-encoder} (NL-AE) and \\textit{neural network} (NL-NN); furthermore we classify the loss function used in the core optimization model of the attack and defense scenarios based on \\textit{BPR}~ and \\textit{cross-entropy}.\n\\end{itemize}\n\\updated{To further help categorization of the research works, we split them according to three adversarial objectives, according to adversarial attacks and defense against: (i) accuracy of recommendations, (ii) privacy of users and (iii) bias and fairness of recommendation models as shown in Table~\\ref{tbl:adl_attack}.}\n\\input{Tables/table_security}", "cites": [313, 902, 917, 1197], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by categorizing attack and defense models, as well as attack granularity, into a structured framework. It attempts to abstract broader patterns in how adversarial methods are adapted for RS, particularly in contrast to computer vision. However, the critical analysis is limited to stating the most common approaches without deeper evaluation of their strengths, weaknesses, or trade-offs."}}
{"id": "6f7afaec-eb67-418a-bbfd-b446bed85c06", "title": "Accuracy of Recommendations.", "level": "paragraph", "subsections": ["040cb700-1d51-4eee-9bf6-c37990ef326a", "593599d6-ed2b-4d65-90d7-83868d6662f2"], "parent_id": "6267834f-3f7b-44b2-a329-ab08802823e3", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Adversarial Machine Learning for Attack and Defense on RS"], ["paragraph", "Accuracy of Recommendations."]], "content": "\\updated{Adversarial attacks against RS models have primarily focused on the degradation of recommendation predictive performance. Thus, in this section we review research works that perform adversarial attacks to undermine the accuracy of recommendation models and measure the impact of the adversarial attack (and defense) via an accuracy evaluation metric.}\nLooking at Table~\\ref{tbl:adl_attack} globally, we note that adversarial personalized ranking (APR)~ by He et. al. was the first work that formally addressed AML to improve the robustness of BPR-MF. After this pioneering work, in the following years, a growing number of works have considered application of AML for different recommendation tasks. Another interesting observation is the co-occurrence of the attack type FGSM and defense model adversarial training (AdReg). In fact, the adversarial training procedure based on FGSM is the first defense strategy proposed by~ to train DNNs resistant to adversarial examples. The authors interpret the improvement in robustness to adversarial examples because the proposed procedure is based on the minimization of the error on adversarially perturbed data.\n\\input{Tables/table_security_datasets_metrics}\nFurthermore, in Table~\\ref{tbl:adl_evaluation}, we provide an overview of the presented approaches under the perspective of experimental evaluation. In particular, we classify the surveyed works according to the \\textit{preference score} used for building/training the recommender models according to implicit and explicit (i.e., rating-based) feedbacks, the prominent \\textit{evaluation metrics} utilized for the offline evaluation of attack success (NDCG, HR, Success Rate, F1, distortion, Precision, and MAP), the \\textit{domain} of focus (e.g., movie, music, social media, business) and \\textit{datasets} used for evaluation. We may notice that, most of the approaches have been tested on an \\textit{implicit} preference type. As for the evaluation metrics, HR is the most adopted one followed by nDCG with a partial overlap among approaches adopting them both. As for the application domain of the datasets used for the evaluation, \\textit{movie} is the most adopted one. This is mainly due to the popularity the Movielens datasets (in their two variants 1M and 100k). Interestingly, \\textit{tourism} is an emerging domain thanks to the availability of the Yelp dataset. Finally, we observe that the high majority of the baselines are based on MF approaches. The following section will provide a detailed description of the most prominent approaches.\n\\setlength{\\parindent}{15pt} [\\textbf{APR}]  are the first to propose an adversarial learning framework for recommendation. The proposed model, called \\textit{adversarial personalized ranking (APR)}, examines the robustness of BPR-MF to adversarial perturbation on users and items embedding of a BPR-MF~. The authors verify the success of using adversarial training as a defense strategy against adversarial perturbations and demonstrate the competitive results in applying adversarial training on BPR-MF. \\updated{Recently,~ studied application of iterative FGSM-based perturbation techniques and demonstrated the inefficacy of defacto defense mechanism APR in protecting the recommender against such multi-step attacks. For instance, the authors showed that the defended model loses more than 60\\% of its accuracy under iterative perturbations, while only less than 9\\% in the case of FGSM-ones.} \n\\\\\\setlength{\\parindent}{15pt} [\\textbf{AMR}]  put under adversarial framework another BPR model, namely visual-BPR (VBPR). VBPR is built upon BPR and extends it by incorporating visual dimensions (originally based on deep CNN feature) by using an embedding matrix. In~, the authors first motivate the importance for adversarial training of VBPR by visually depicting how a surprisingly modest amount of adversarial perturbation ($\\epsilon = 0.007$) added on raw image pixels \\textemdash~where the added noise is barely perceivable to the human eye \\textemdash~can alter recommendation raking outcomes of VBPR and produce erroneous results. The proposed model therefore consists of constructing adversarial perturbations under the FGSM attack model and adding them to the deep latent feature of items' images extracted by CNN (i.e., ResNet50~) with the goal to learn robust image embedding parameters. One of the key insights about this work is that it does not add perturbations directly on raw image pixels for two main reasons: (i) it would require the feature extractor (CNN) component and the recommender model to be trained end-to-end with overfitting issues on the CNN due to the sparsity of user-item feedback data, (ii) it would be a time-consuming operation because at each update of the recommender model it is necessary to update all the CNN parameters.\nIn the above-mentioned works, the authors adopt several steps to validate the effectiveness of the proposed adversarial training framework, which can be summarized according to the following dimensions: (i) the \\textit{generalization} capability, (ii) the comparison of \\textit{adversarial noise v.s. random noise}, and (iii) the \\textit{robustness of models}. Regarding (i), the key insight is that adversarial training approaches (i.e., APR and AMR) can lead to learning model parameters, which can enhance model generalization capability \\textemdash~in other words, improvement of the general performance of recommendation while not being exposed to adversarial perturbation. Concerning (ii), it has been demonstrated that the impact adversarial perturbation on classical recommendation models (e.g., MF-BPR or VBPR) is significantly larger than their random noise counter-part under similar perturbation level. For instance,~ shows that by exposing MF to adversarial and random noise, the test on nDCG is decreased by -21.2\\% and -1.6\\% respectively \\textemdash~i.e., an impact of approximately 13 times difference. Dimension (iii) constitutes the core of the system validations in these works in which compelling evidence has been provided on the vulnerability of classical recommendation models to adversarial examples, or equivalently the robustness of the proposed training framework against adversarial samples. To provide an illustrating example, in~ it has been shown for an experiment on the Amazon dataset, that by changing the perturbation level from $\\epsilon = 0.05$ to $\\epsilon = 0.2$, the amount of decrease in nDCG ranges from -8.7\\% to -67.7\\% whereas for AMR it varies from -1.4\\% to -20.2\\%. These results suggest that approaches using adversarial learning instead of classical learning act significantly in a more robust way against adversarial perturbations.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{AdvIR}] In~, the authors propose a system to address CF recommendation based on implicit feedbacks. The main issue in learning from implicit interaction is characterized by scarcity of negative feedbacks compared with positive ones, regarded as one-class problem. Sampling uniformly from unobserved data, known as \\textit{negative sampling }, has been introduced in prior work to address this issue. The proposed system in~ is called AdvIR, which entails an adversarial sampling and training framework to learn recommendation models from implicit interactions. The system applies adversarial training on both positive and negative interaction separately, to create informative adversarial positive/negative samples. The proposed adversarial training approach works for both discrete and continuous input by adding the adversarial perturbation directly on the input vector (e.g., one-hot encoding user-id).\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{ACAE / FG-ACAE}]  use the adversarial training framework for a neural network-based recommendation model, namely collaborative denoising auto-encoder (CDAE)~, based on which the authors propose two variations, namely: i) the adversarial collaborative auto-encoder (ACAE) and (ii) fine-grained collaborative auto-encoder (FG-ACAE). ACAE applies adversarial noise on encoder and decoder parameters and adopts an adversarial training framework. FG-ACAE considers the impact of adversarial noise in a more fine-grained manner. In particular, in FG-ACAE adversarial noise is added not only on encoder and decoder but also on the user's embedding matrix as well as hidden layers of the network. Furthermore, to increase the flexibility of training, all the noise factors in ACAE and FG-ACAE are controlled by different parameters. The experimental results confirm the trend that AdReg may improve the model's robustness against adversarial perturbed input, as well as the generalization performance of recommenders.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{ATF}]  combine tensor factorization and adversarial learning to improve the robustness of pairwise interaction tensor factorization (PITF)~ for context-aware recommendation. Comparison with standard tensor models in tag recommendations acknowledges that the adversarial framework outperforms state-of-the-art tensor-based recommenders.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{FNCF}]  approach security issues for C\\&W  attacks~. The authors propose to make more robust neural network-based collaborative filtering models (e.g., NCF~) by using knowledge distillation~ instead of the adversarial (re)training. The framework integrates  knowledge distillation with the injection of additive adversarial noise at training time. Experiments demonstrate that this system enhances the robustness of the treated recommender model.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{SACRA}]  propose a novel recommender model, named Click Feedback-Aware Network (CFAN), to provide query suggestions considering the sequential search queries issued by the user and her history of clicks.\nThe authors employ additional adversarial (re)training epochs (i.e., adding adversarial perturbations on item embeddings) to improve the robustness of the model. \\updated{A similar approach has been also implemented by~ to robustify a self-attention sequential recommender model, named [\\textbf{SAO}].}\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{TAaMR}]  explore the influence of targeted adversarial attacks (i.e., FGSM, and PGD~) against original product images used to extract deep features in state-of-the-art visual recommender models (i.e., VBPR~, and AMR~). The authors verify that recommendation lists can be altered such that a low recommended product category can be pushed by adding adversarial noise on product images in a human-imperceptible way.   \\updated{Within a similar scenario,~ verify, in the \\textbf{VAR} framework, the inefficacy of the adversarial robustification of the image feature extractor component in protecting the visual recommender from such adversarial perturbations}.\\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{AIP}] Similar to TAaMR~,~ propose a series of adversarial attacks to increase the recommendability of items by perturbing the products images. The authors model three level of adversary's knowledge (i.e., high, medium, and low) with corresponding adversarial image perturbation strategies. Furthermore, they validate the inefficacy of JPEG compression and bit depth reduction as two possible defense mechanisms.}", "cites": [1199, 890, 917, 97, 681, 1201, 892, 1187, 1186, 1202, 1200, 1198], "cite_extract_rate": 0.5217391304347826, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key adversarial learning approaches in recommender systems, drawing on multiple papers to highlight trends in attack/defense strategies and evaluation practices. It critically evaluates the robustness and limitations of these approaches, particularly noting the vulnerability of models to multi-step attacks. The section also abstracts these findings to present broader patterns in model generalization, robustness, and the impact of adversarial vs. random noise."}}
{"id": "040cb700-1d51-4eee-9bf6-c37990ef326a", "title": "Users' Privacy.", "level": "paragraph", "subsections": [], "parent_id": "6f7afaec-eb67-418a-bbfd-b446bed85c06", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Adversarial Machine Learning for Attack and Defense on RS"], ["paragraph", "Accuracy of Recommendations."], ["paragraph", "Users' Privacy."]], "content": "\\updated{Another main area of concern is user privacy and averting the negative consequences of adversarial attacks on user privacy. Recently, in the light of privacy-violation scandals such as Cambridge Analytica~ privacy-protection laws such as the GDPR, US Congress, and other jurisdictions have been proposed to legislate new disclosure laws. Thus, attempts have been made to build machine-learned recommendation models that offer a privacy-by-design architecture, such as federated learning~, or the ones based on differential privacy~. Nonetheless, several works recently have challenged user data confidentiality via adversarial attacks, for example, in the context of \\textit{social recommenders} that has been widely studied in these scenarios. For instance,~ propose a privacy-preserving framework to protect users from adversaries that want to infer, or reconstruct, their historical interactions and social connections.~ propose to defend users' privacy from inference attacks with a privacy-oriented social media data publishing framework optimized to preserve the recommendation performance, while domain-independent recommendation algorithms have been developed by~ as a MF extension. All these works use the differential privacy~ technique to reduce privacy violations.} \n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{PAT}]~ propose an adversarial training procedure, the Domain-Adversarial Training~, to build a privacy-adversarial method to defeat the data leakage. The intuition is to train a LFM with the classical minimax paradigm where the model learns its parameters by minimizing both the recommendation cost an adversarial regularization component related to the adversarial privacy-violation.   \n}\\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{RAP}]~ propose an adversarial learning procedure to protect users' from attribute-inference attacks. The model, named Recommendation with Attribute Protection (RAP), simultaneously learns to maximize the users' gain from the recommendation while minimizing the adversary capability in inferring users' personal attributes (e.g., gender, age, and occupancy).}", "cites": [1203], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several privacy-preserving approaches in adversarial recommender systems, including federated learning, differential privacy, and adversarial learning (e.g., PAT and RAP). It connects these methods under the umbrella of privacy-by-design and explains their underlying mechanisms. While it provides a coherent narrative, the critical analysis is limitedthere is no explicit comparison of effectiveness or discussion of trade-offs between privacy and recommendation performance. The abstraction is moderate, as it identifies a general trend toward adversarial methods for privacy but does not propose a unifying theoretical or conceptual framework."}}
{"id": "593599d6-ed2b-4d65-90d7-83868d6662f2", "title": "Bias, Fairness.", "level": "paragraph", "subsections": [], "parent_id": "6f7afaec-eb67-418a-bbfd-b446bed85c06", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Machine Learning for Security of RS"], ["subsection", "Adversarial Machine Learning for Attack and Defense on RS"], ["paragraph", "Accuracy of Recommendations."], ["paragraph", "Bias, Fairness."]], "content": "\\updated{Another area of concern is related to biases and fairness of recommendations. RS assist users in many life-affecting scenarios such as medical, financial, or job-related ones. Unfair recommendations could have far-reaching consequences, impacting people's lives and placing minority groups at a significant disadvantage~. From a RS perspective, where users are first-class citizens, fairness is a \\textit{multi-sided concept} and the utility of recommendations needs to be studied by considering the benefits of multiple groups of individual~, for instance based on the user-centered utility and the vendor-centered utility (e.g., profitability). In the literature, a few research works have exploited the adversarial training procedure to reduce the biased/unfair impact of recommendations.} \n\\\\\\updated{\\setlength{\\parindent}{15pt} [\\textbf{DPR}] \nInspired by~,~ propose a debiased personalized ranking (DPR) model composed of two components: an adversary model, i.e., multi-layer perceptron network, and a classical recommender model, i.e., BPR-MF.  During the adversarial training, the adversary tries to infer the group of an item, while the recommender model tries to reduce both the recommendation error and the adversary's capability of identifying the true class of the item. The central intuition is to unbias the recommender model by enhancing the similarity of the predicted score distributions between different item groups. Extensive experiments demonstrate that DPR reduces the under-recommendation bias while retaining accurate recommendations.} \\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{FAN}]  design a fairness-aware news recommendation (FAN) method via adversarial learning. Similar to DPR~, the authors extend the neural news recommendation with a multi-head self-attention (NRMS) model~ with an adversarial component trained to infer the sensitive users' characteristics, while the recommender model is regularized to reduce the adversarial possibility of inferring the users' attributes. The intuition is that adversarial training generates bias-free users' embeddings that can be in turn used to produce fair-aware recommendations.}", "cites": [1204], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of adversarial training approaches aimed at addressing bias and fairness in recommender systems, particularly through the DPR and FAN models. It synthesizes the methods by highlighting the shared adversarial learning framework and their goals of generating fair recommendations. While it offers some abstraction by framing fairness as a multi-sided concept, it lacks deeper critical analysis or evaluation of the limitations and trade-offs of these approaches."}}
{"id": "6dd4510f-4915-4716-a591-27cd3851d8dc", "title": "Foundations of Generative Adversarial Networks (GANs)", "level": "subsection", "subsections": [], "parent_id": "76c138c6-bcb1-42a0-b007-917968172e54", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "Foundations of Generative Adversarial Networks (GANs)"]], "content": "\\label{subsec:found_GAN}\nGANs are deep generative models proposed by~ in 2014. A GAN is composed of two components, a generator $\\mathcal{G}$, and a discriminator $\\mathcal{D}$. The generator works to capture the real data distribution to generate adversarial examples and fool the discriminator,  while the discriminator endeavors to distinguish the fake examples from real ones.\nThis competition, known as adversarial learning, ends when the components reach the Nash equilibrium. The GAN architecture is shown in Figure~\\ref{fig:gan_architecture}.\n\\begin{definition}[Conventional Vanilla GAN~]\\label{def:vanilla-GAN}\nAssume that we are given a dataset of input samples $x \\in \\mathcal{X}$, where $\\mathbb{P}_{\\mathcal{X}}$ represents the probability distribution of the original data and suppose $z \\in \\mathcal{Z}$ denotes a sample from some latent space $\\mathcal{Z}$. We are interested in sampling from $\\mathbb{P}_{\\mathcal{X}}$. The goal of GAN is to train the generator $\\mathcal{G}$ to transform samples $z \\sim \\mathbb{P}_{\\mathcal{Z}}$ into $g_{\\theta}(z) \\sim \\mathbb{P}_{\\theta}$ such that $ \\mathbb{P}_{\\theta} \\approx \\mathbb{P}_{\\mathcal{X}}$. The role of the discriminator $\\mathcal{D}$ is to distinguish $\\mathbb{P}_{\\theta}$ and $\\mathbb{P}_{\\mathcal{X}}$ by training a classifier $f_{\\phi}$. The training involves solving the following min-max objective\n    \\begin{equation}\n         \\label{eq:gan_basic}\n         \\min_{\\theta} \\max_{\\phi} L(\\mathcal{G}_\\theta, \\mathcal{D}_{\\phi}) =  \\, \\mathbb{E}_{x \\sim \\mathbb{P}_{\\mathcal{X}}} \\, \\log f_{\\phi}(x) + \\mathbb{E}_{z \\sim \\mathbb{P}_{\\mathcal{Z}}} \\log (1-f_{\\phi}(g_{\\theta}(z)))\n    \\end{equation}\nwhere $\\theta$ and $\\phi$ are model parameters of the discriminator and generator respectively, learned during the trained phase.  \\qed \n\\end{definition}\nDifferent distance measures $f_{\\theta}$ lead to different GAN models, e.g., Vanilla GAN (based on Jensen-Shannon divergence)~, Wasserstein GAN (based on Wasserstein distance)~, and Conditional GAN (based on class conditioning on both the generator and discriminator)~.\n\\begin{definition}[Conditional-GAN (CGAN)]\nConditional GAN extends the conventional GAN by incorporating an extra condition information term $c$ on both the input of the generator $\\mathcal{G}$ and the discriminator $\\mathcal{D}$, thus conditioning them on this new term\n    \\begin{equation}\n         \\label{eq:gan_basic}\n         \\min_{\\theta} \\max_{\\phi} L(\\mathcal{G}_\\theta, \\mathcal{D}_{\\phi}) =  \\, \\mathbb{E}_{x \\sim \\mathbb{P}_{\\mathcal{X}}} \\, \\log f_{\\phi}(x|c) + \\mathbb{E}_{z \\sim \\mathbb{P}_{\\mathcal{Z}}} \\log (1-f_{\\phi}(g_{\\theta}(z|c)))\n    \\end{equation}\nwhere $c$ can represent any auxiliary information to the networks such as class labels, content features, data from other domains and so forth. \\qed\n\\end{definition}", "cites": [529, 7217, 64], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of GANs and their variants, integrating definitions from multiple cited papers. It synthesizes basic concepts (e.g., Vanilla GAN, Wasserstein GAN, Conditional GAN) but does not deeply connect or contrast their underlying principles. Critical analysis is minimal, as it merely states properties without evaluating strengths, weaknesses, or trade-offs. Some abstraction is present in discussing general adversarial learning objectives, but the focus remains on describing specific models rather than identifying broader patterns or theoretical implications."}}
{"id": "65d97e24-cbc8-4621-a296-e433b8a989c8", "title": "GAN-based Recommendation Framework", "level": "subsection", "subsections": [], "parent_id": "76c138c6-bcb1-42a0-b007-917968172e54", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "GAN-based Recommendation Framework"]], "content": "\\label{subsec:GAN_RF}\nGANs have been successfully applied in start-of-the-art RS to learning recommendation models. Since the first pioneering GAN-based work IRGAN~ in 2017, we have witnessed rapid adoption of these network architectures in many traditional and novel applications and domains. In this section, we provide a conceptual framework that will show how GANs are employed in RS domain and shed light on particularities and differences of GAN application in RecSys and ML. \n\\begin{figure}[!h]\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{Pictures/gan_cf_new2.pdf}\n    \\caption{A conceptual view of GAN-CF incorporating GAN to address item recommendation task.}\n    \\label{fig:GAN-CF}\n\\end{figure}\n\\noindent {\\textbf{GAN-CF problem formulation and conceptual model.}}\\label{subsec:GAN_RS}\nThe prominent recommendation models in the literature that successfully apply GAN~ for the CF task, utilize the two-player min-max game with objective function built on top of Eq.~\\ref{eq:gan_basic}.\n\\begin{definition}[The GAN-CF model~]\nLet $\\mathcal{U}$ and $\\mathcal{I}$ denote a set of users and items in a system, respectively. The training objective is given by\n    \\begin{equation}\n         \\label{eq:irgan_basic}\n          \\min_{\\theta} \\max_{\\phi} L(\\mathcal{G}_\\theta, \\mathcal{D}_{\\phi}) = \\, \\mathbb{E}_{i \\sim \\mathbb{P}_{\\mathcal{X}}(i|u)} \\, \\log f_{\\phi}(i|u) + \\mathbb{E}_{\\hat{i} \\sim \\mathbb{P}_{\\theta}(\\hat{i}|u)} \\log \\, (1-f_{\\phi}(\\hat{i} | u))\n    \\end{equation}\nwhere $i \\in \\mathcal{I}$ is an item receiving implicit (or explicit) feedback by user $u \\in \\mathcal{U}$ (e.g., purchased) and $\\hat{i} \\in \\mathcal{I}$ is a generated item.\n\\qed \n\\end{definition}\nA few observations are important to be made here: (i) the output of generator $\\mathcal{G}$ is a set of item indices deemed relevant to user $u$; (ii) both $\\mathcal{G}$ and $\\mathcal{D}$ are \\textit{user-conditioned}, signifying that model parameters are learnt in a \\textit{personalized fashion}; (iii) the GAN-based CF works do not use the noise term as input (to $\\mathcal{G}$) as the goal is to generate one unique ---yet plausible--- item rather than a set of items. Figure~\\ref{fig:GAN-CF} summarizes these aspects conceptually.\n\\input{Tables/sampling_strategy}\n\\noindent {\\textbf{Discrete outcome and sampling strategies.}} \nThe parameters in the GAN-CF model are learned in an end-to-end fashion. \nHowever, before we can take benefit of this training paradigm, the system needs to solve a critical issue that does not exist on the original GAN presented in Def. 3.1. based on the sampled noise signal. The generation of recommendation lists is a discrete sampling operation, i.e., performed over discrete candidate items (see Figure~\\ref{fig:GAN-CF}). Thus, the gradients that are derived from the objective function in Eq.~\\eqref{eq:irgan_basic} cannot be directly used to optimize the generator via gradient descent as happens in the original GAN formulation, where gradients are applied for differentiable values (e.g.,images and videos). To obtain a differentiable sampling strategy in GAN-CF models, two sampling strategies are proposed in the literature based on reinforcement learning algorithm and the Gumbel-Softmax differentiable sampling procedure~, summarized in Table~\\ref{tbl:smple_strg}.", "cites": [782], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the GAN-based recommendation framework by integrating key concepts from adversarial learning into the collaborative filtering task, highlighting important distinctions such as the lack of noise input and user-conditioned personalization. It introduces a generalized model (GAN-CF) and discusses the challenge of discrete sampling, linking it to the Gumbel-Softmax method from the cited paper. While it offers some abstraction by framing the issue in terms of differentiability in recommendation systems, the critical analysis remains limited, with no clear evaluation or comparison of the strengths and weaknesses of the proposed strategies."}}
{"id": "b30f5353-add0-45bc-a6e6-aa4a2dfe8a78", "title": "Collaborative Recommendation", "level": "subsubsection", "subsections": [], "parent_id": "485abc46-712e-46d8-8a44-993d95b3a599", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "GAN-based Recommendation Models: State of the Art"], ["subsubsection", "Collaborative Recommendation"]], "content": "\\label{subsec:GAN_CF_rec}\nGANs have been shown powerful in generating relevant recommendations --- in particular, using the CF approach --- and capable of successively competing with state-of-the-art models in the field of RS. We have identified the following reasons for the potential of GANs in RS: (i) they are able to generalize well and learn unknown user preference distributions and thus be able to model user preference in complex settings (e.g., IRGAN~ and CFGAN~); (ii) they are capable of generating more negative samples than random samples in pairwise learning tasks (e.g., APL~, DASO~) and (iii) they can be used for data augmentation (e.g., AugCF~  and RAGAN~).\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{IRGAN}] The work by Wang et. al.~ is presumably the first attempt to integrate the generative and discriminative approach to IR under the same roof by proposing a GAN-based IR model. The authors demonstrate the application of IRGAN for web search, item recommendation and question answering tasks where for the item recommendation task, the query is constructed from the user's historical interactions.\nDuring adversarial learning ---the min-max game--- the generator learns the actual distribution of relevant items as much as possible.  It turns out that this novel training idea results in a more satisfactory accuracy in recommendation than optimizing the traditional pure discriminative loss functions based on pointwise, or pairwise, objectives.\n\\input{Tables/gan_rs.tex}\n\\\\ \\setlength{\\parindent}{15pt} [\\textbf{GraphGAN}]  propose GraphGAN --- a graph-based representation learning --- (a.k.a. network embedding) for CF recommendation. Graph-based analysis is gaining momentum in recent years due to their ubiquity in real-world problems such as modeling user preference for item recommendation as well as social graphs in social media (SM) networks, co-occurrence graph in linguistics, citation graph in research, knowledge graph and so forth. The central idea of network embedding is to represent each entity in a graph with a lower-dimensional latent representation to facilitate tasks within the network and prediction over entities. For example, such latent representation makes it possible to perform prediction for supervised tasks, while the distance between node embedding vectors can serve as a useful measure in unsupervised tasks. GraphGAN can be viewed as a graph-based representation of IRGAN, where queries/items are \\textit{nodes} of the graph. For a given node $v_c$, the objective of $\\mathcal{G}$ is to learn the ground-truth connectivity distribution over vertices $p_{true}(v|v_c)$, whereas $\\mathcal{D}$ aims to discern whether or not a connectivity should reside between vertex pairs $(v,v_c)$. GraphGan furthermore proposes the graph softmax as $\\mathcal{G}$ ---instead of traditional softmax--- which appears to boost the computational efficiency of training (graph sampling and embedding learning) performed by  $\\mathcal{G}$.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{GAN-HNBR}] From an application perspective, GAN-based graph representations have also been applied in more niche domains of RS, including personalized citation recommendation. The goal is to recommend research articles for citation by using a content-based and author-based representation~ or learning heterogeneous bibliographic network representation (HBNR).  propose GAN-HNBR ---a GAN-based citation recommendation model--- that can learn the optimal representation of a bibliographic network consisting of heterogeneous vertex content features such as papers and authors into a common shared latent space and provide personalized citation recommendation.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{CFGAN}] CFGAN has been introduced in~ to address a problem with \\textit{discrete items} in IRGAN, where $\\mathcal{G}$ produces at each iteration a single item index, which is a discrete entity in nature. This is different from the original GAN in the CV domain in which the output of $\\mathcal{G}$ is an image (i.e., a vector). The generation of discrete item indices by $\\mathcal{G}$ results in a poor sampling of items from the pool of available alternatives (i.e., samples identical to ground-truth) deteriorating the performance of $\\mathcal{G}$ and $\\mathcal{D}$ ---instated of improvement--- during the min-max training iteration. CFGAN introduces \\textit{vector-wise\ntraining} in which $\\mathcal{G}$ generates continuous-valued vectors to avoid misleading $\\mathcal{D}$, which in turn improves the performance of both $\\mathcal{G}$ and $\\mathcal{D}$. The authors show the improvement of CFGAN over IRGAN and GraphGAN baselines. As an example, with regards to P@20 on the Ciao dataset, the improvement is $100\\%$  for CFGAN vs. IRGAN (0.45 v.s. 0.23) and $160\\%$ for CFGAN vs. GraphGAN (0.45 v.s. 0.17), which turns to be a significant improvement of the recommendation accuracy.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{Chae et al.}]  propose an auto-encoder-based GAN, in which an auto-encoder (AE) is used as $\\mathcal{G}$ to model the underlying distribution of user preferences over items. The primary motivation behind this work is that conventional MF-based approaches are linear. Instead, the proposed system can generate non-linear latent factor models and uncover more complex relationships in the underlying user-item interaction matrix.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{VAE}] An adversarial variational auto-encoder (VAE) is adopted by~, where the authors propose the usage of a GAN to regularize the VAE by imposing an arbitrary prior to the latent representation (based on implicit feedback). Similar works can be found in~, which exploits a VAE to enhance the robustness of adversarial examples. The authors furthermore present the Wasserstein distance with gradient penalty.\\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{GAN-VAE-CF}]  design an adversarial generative network to learn inter-item interactions used to generate informative \\textit{niche} negative samples paired with the positive ones to reduce the popularity bias, and  promote recommendation of long-tail products.}\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{CALF]} Other issues of IRGAN, such as sparsity causing gradient vanishing and update instability and discrete value preventing a training to optimize using gradient descent, are addressed in~. The proposed solution is named convolutional adversarial latent factor model (CALF), which employs a CNN to learn correlations between embeddings and Rao-Blackwell sampling to deal with discrete values optimizing CALF.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{PD-GAN]} The authors of~ propose a solution to improve diversity of CF-based recommendation with GAN based on personalized diversification.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{LambdaGAN]} In~, the authors propose LambdaGAN ---a GAN model with a lambda ranking strategy--- that improves the recommendation performance in a pairwise ranking setting by proposing lambda rank~ function into the adversarial learning of the proposed GAN-based CF framework.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{VAEGAN]} A variant of VAE is introduced in~ to address the limited expressiveness of the inference model and latent features, which reduces the generalization performance of the model. The proposed solution, named adversarial variational autoencoder GAN (VAEGAN), is a more expressive, and flexible model that better approximates the posterior distribution by combining VAEs and GAN. This work is one of the first work to propose the application of adversarial variational Bayes (AVB)~ to perform the adversarial training.", "cites": [1189], "cite_extract_rate": 0.05, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes various GAN-based collaborative recommendation models, connecting their core ideas and demonstrating how they build upon or address limitations of earlier approaches. It provides critical comparisons of performance (e.g., P@20 metrics) and identifies technical issues such as discrete item indices and gradient instability. The discussion also abstracts key principles, such as the benefits of adversarial training and the role of GANs in modeling complex distributions and reducing popularity bias."}}
{"id": "57530d1b-af15-4cf3-a9ed-4da067fd7b5c", "title": "Context-aware Recommendation", "level": "subsubsection", "subsections": [], "parent_id": "485abc46-712e-46d8-8a44-993d95b3a599", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "GAN-based Recommendation Models: State of the Art"], ["subsubsection", "Context-aware Recommendation"]], "content": "\\label{subsec:GAN_CA_rec}\nAlthough long-term preference modeling has proven to be effective in several domains~, recent research indicates that users' preferences are highly variable based on the user's context, e.g., time, location, and mood~. Context provides the background of \\textit{user objective} for using the system and can be exploited to generate more relevant recommendations.\n\\noindent \\textbf{Temporal-aware Recommendation.}\nIn real applications, users' preferences change over time, and modeling such \\textit{temporal evolution} is needed for effective recommendation. While long-term preferences of users change slowly, their \\textit{short-term preferences} can be seen as more dynamic and changing more rapidly. Predicting short-term user preference has been recently studied in the context of \\textit{session-based} and \\textit{sequential recommendations}. A temporal extension of SVD++ towards the modeling of temporal dynamic,  named TimeSVD++, has been proposed in~. It has also been reported that the structure of time-aware inputs (e.g., click-logs, session) are effectively modeled by a recurrent neural network (RNN).\nFor instance,  proposed to model the sequential user clicks to output session-based recommendation with a GRU-gated recurrent unit; while  proposed to integrate an LSTM model, to capture both the user and the item temporal evolution, and MF to model stationary preferences. \n\\updated{\nBoth LSTM and GRU are variants of RNNs deployed to reduce the gradient vanishing problem by including a mechanism to enable the model to learn the historical evolution of users' behavior. In particular, GRUs are generally preferred over LSTMs since they have to learn less model parameters and, consequently, require less memory than LSTMs~. \n}\nInspired by the accuracy improvements of IRGAN, GAN-based models have been combined in temporal frameworks to boost the recommendation performance in sequence-aware recommendation tasks. \n\\setlength{\\parindent}{15pt} [\\textbf{RecGAN}] In~, the authors propose to incorporate in a single framework both the temporal modeling capabilities of RNN and the latent feature modeling power of the \\textit{min-max  game}. The proposed framework, named RecGAN, implements both the generator and the discriminator with the Gated Recurrent Unit (GRU)~, in order to make $\\mathcal{G}$ capable of predicting a sequence of relevant items based on the dynamic evolution of user's preferences.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{PLASTIC \\& LSIC}] Differently from RecGAN that implements only an RNN cell to capture the dynamic evolution of the user's behavior,  propose to combine MF and RNN in an adversarial recommendation framework to model respectively long and short-term user-item associations. The proposed framework, named PLASTIC, adopts MF and LSTM cells into $\\mathcal{G}$ to account for the varying aspect of both users and items,  while a two-input Siamese network ---built manually by using a MF and RNN--- as $\\mathcal{D}$ encodes both the \\textit{long-term} and \\textit{session-based information} in the pair-wise scenario.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{NMRN-GAN}] Recent studies have endorsed that adversarially created close-to-observed negative samples  are capable of improving the user and item representation.  introduce \\textit{GAN-based negative sampling} for streaming recommendation. Instead of using a random sampling strategy, which is static and hardly contributes towards the training of the recommender model, adversarially generated negative samples result more informative. NMRN-GAN uses a key-value memory network~ to keep the model's long-term and short-term memory combined with a GAN-based negative sampling strategy to create more instructive negative samples thus improving the training effectiveness and the quality of the recommendation model.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{GAN-CQDN}] A GAN-based solution has been proposed in~ for sequence-aware recommendation in conjunction with reinforcement learning (RL). The main aim here is that of modeling the dynamic of user's status and long-term performance. The authors propose GAN-CQDN, an RL-based recommender system that exploits GAN to model user behavior dynamics and learn her reward function. The advantages of using GAN is that it improves the representation of the user profile a well as the reward function according to the learned user profile, and it accommodates online changes for new users. \\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{MFGAN}]  implement the generator as a Transformer-based network to predict, for each user, the next relevant item whose importance will be judged from multiple factor-specific discriminators. The discriminators are a set of Transformed-based binary classifiers that measure the recommendation relevance with respect to additional information (e.g., item popularity, semantic information, and price). Similar to MFGAN~,  propose an Adversarial Oracular Seq2seq learning for sequential Recommendation (AOS4Rec) framework to enhance the recommendation performance of Transformer-based, or RNN-based, next-item recommenders.\n}\\\\\n\\noindent \\textbf{Geographical-aware Recommendation.}  Another relevant application of contextual information is point-of-interest (POI) recommendation. In this field, many approaches have been proposed over the year especially after the mobile revolution. Location-based social networks (LBSNs) have attracted millions of users to share rich information, such  as experiences and  tips.  Point-of-Interest  (POI) recommender systems play an important role in LBSNs since they can help users explore attractive locations as well as help social network service providers design location-aware advertisements for Point-of-Interest.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{Geo-ALM}] In~, the authors propose Geo-ALM, a GAN-based POI recommender that integrates geographical features (POI and region features) with a GAN to achieve (better) POI recommendation. In the proposed system, $\\mathcal{G}$ improves the random negative sampling approach in the pairwise POI recommendation scenario that leads to better representation of user and items and enhances recommendation quality with respect to state-of-the-art models.\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{APOIR}] Inspired by the advances of POI recommendation performance under GAN-based framework,  propose adversarial point-of-interest recommendation (APOIR) to learn user-latent representations in a generative manner. The main novelty of the proposed framework is the use of POIs' geographical features and the users' social relations into the reward function used to optimize the $\\mathcal{G}$. The reward function acts like a contextual-aware regularizer of $\\mathcal{G}$, that is the component of APOIR in the proposed POI recommendation model.", "cites": [1207, 1206, 5180, 1205, 243], "cite_extract_rate": 0.29411764705882354, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several GAN-based context-aware recommendation models (RecGAN, PLASTIC, NMRN-GAN, GAN-CQDN, MFGAN, Geo-ALM, APOIR), integrating them into a coherent narrative focused on modeling temporal and geographical dynamics. It identifies patterns in how these models leverage adversarial learning and different neural components (e.g., RNNs, Transformers, memory networks). However, critical analysis is limited, as it does not deeply evaluate the strengths, weaknesses, or trade-offs of the different approaches. Some abstraction is achieved by highlighting the role of context in recommendation, but broader principles could be more explicitly articulated."}}
{"id": "9a0c94e9-c849-443b-a1f3-b16c512135c9", "title": "Cross-domain Recommendation", "level": "subsubsection", "subsections": [], "parent_id": "485abc46-712e-46d8-8a44-993d95b3a599", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "GAN-based Recommendation Models: State of the Art"], ["subsubsection", "Cross-domain Recommendation"]], "content": "\\label{subsec:GAN_CR_rec}\nRecommender models are usually designed to compute recommendations for items belonging to a single domain. Items belonging to a specific domain share characteristics and attributes, which are intrinsically similar, and domain-specific recommendation models allow the designer to study these characteristics individually. However, \\textit{single-domain} recommendation faces numerous challenges. The first challenge refers to the well-known \\textit{cold-start} problem, when insufficient interactions exist in the considered domain. Second, users' interests and needs span across different application areas and large e-commerce sites, like Amazon or eBay, store users' preference scores related to products/services of various domains ---from books and products to online movies and music. As companies strive to increase the diversity of products or services to users, cross-domain recommendation can help such companies to increase sales productivity by offering personalized cross-selling or bundle recommendations for items from multiple domains~. The third aspect is a novel research idea related to discovering relationships between items (e.g., images) of two different domains. For example, can a machine achieve a human-level understanding to recommend a fashion item consistent with user taste/style in another domain such as media or visual scenery? \n\\setlength{\\parindent}{15pt} [\\textbf{FR-DiscoGAN}]\nIn~, the authors propose a cross-domain GAN to generate fashion designs from the sceneries. In the proposed hypothetical scenario, the user can specify via a query her POI to visit (e.g., mountain, beach) together with keywords describing a season (i.e., spring, summer, fall, and winter). The core idea is to automatically generate fashion items (e.g., clothes, handbags, and shoes) whose useful features (i.e., style) match the natural scenery specified by the user. For instance, the system can recommend a collection of fashion items that look cool/bright for visiting a beach in summer, even though the actual preference of the user is black-style clothes. The role of GAN is to learn associations between scenery and fashion images. In the field of ML and CV, the problem is termed as \\dquotes{style transfer} or \\dquotes{image to image translation} problem~. \\\\\n\\setlength{\\parindent}{15pt} [\\textbf{VAE-GAN-CC}]\nAn effective cross-domain recommendation system relies on \\textit{capturing both similarities and differences} among features of domains and exploiting them for improving recommendation quality in multiple domains. Single-domain algorithms have difficulty in uncovering the specific characteristics of each domain. To solve this problem, some approaches extract latent features of the domains by a separate network~. Although these approaches might be successful in capturing characteristic features of each domain, they do not establish the similarity between features of multiple domains. To extract both homogeneous and divergent features in multiple domains,  propose a generic cross-domain recommendation system that takes as input the user interaction history (click vector) in each domain,  maps the vectors to a shared latent space using two AEs and then uses $\\mathcal{G}$ to remap the underlying latent representation to click vectors. The main novelty of this work lies in building/linking shared latent space between domains, which in turn facilitates \\textit{domain-to-domain} translation. In particular, the former is realized by enforcing a weight-sharing constraint related to variational auto-encoders, i.e., the encoder-generator pair $\\{\\mathcal{E}_A, \\mathcal{G}_A\\}$ and $\\{\\mathcal{E}_B, \\mathcal{G}_B\\}$ and using cycle-consistency (CC) as a weight-sharing constraint. Finally, two separate adversarial discriminators are employed to determine whether the translated vectors are realistic. The final system is called VAE-GAN-CC network, which extends the unsupervised image-to-image translation network in the CV domain~ for RS applications and is thus named domain-to-domain translation model (D2D-TM).\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{DASO}]\nInspired by the efficacy of \\textit{adversarial negative sampling} techniques proposed by~,  address the limitation of typical negative sampling in the \\textit{social recommendation} domain in transferring users' information from social domain to item domain. The proposed Deep Adversarial SOcial recommendation (DASO) system, harnesses the power of adversarial learning to dynamically generate difficult negative samples for \\textit{user-item} and \\textit{user-user pairs}, to guide the network to learn better user and item representations. \\\\\n\\updated{\n\\setlength{\\parindent}{15pt} [\\textbf{Asr}]  propose an adversarial social regularization (Asr) framework to improve the item recommendation performance by integrating contextual information, i.e., users' social connections, within a GAN-based approach. Furthermore, the proposed framework is agnostic to the recommender models guaranteeing applicability in several settings and domains. For instance, the authors demonstrate the framework's efficacy in within a large set of models (e.g., VAE).}\\\\\n\\setlength{\\parindent}{15pt} [\\textbf{CnGAN}]\n, propose GAN for cross-network (CnGAN) to address one of the significant shortcomings of cross-network recommendation concerning \\textit{non-overlapping users}  missing preference scores. These users exist in the source domain but not in the target domain, and thus, their preferences about items in the target domain are not available. In the proposed work, $\\mathcal{G}$ learns the mapping of user preferences from \\textit{target to source} and generate more \\textit{informative preferences} on the source domain. $\\mathcal{D}$ uses the synthetically generated preferences (generated from $\\mathcal{G}$) to provide recommendations for users who only have interactions on the target network (not overlapped users). The authors also propose two novel loss functions ---a content-wise and a user-wise loss function--- to guide the min-max training process better. The authors validate the effectiveness of the system against state-of-the-art models both in terms of accuracy and beyond-accuracy measures (novelty, diversity).", "cites": [1189, 8425, 1208], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple GAN-based cross-domain recommendation approaches, connecting their core ideas to form a coherent narrative around domain-to-domain translation. It provides some critical analysis, such as limitations of single-domain methods and how adversarial learning helps overcome them, but lacks deeper comparative evaluation of the cited systems. The abstraction level is moderate, as it identifies recurring themes like shared latent space and adversarial sampling without proposing a unified conceptual framework."}}
{"id": "6e7196de-82aa-4d00-b39f-67020a488ecb", "title": "Fashion Recommendation", "level": "subsubsection", "subsections": [], "parent_id": "485abc46-712e-46d8-8a44-993d95b3a599", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Adversarial Learning for GAN-based Recommendation"], ["subsection", "GAN-based Recommendation Models: State of the Art"], ["subsubsection", "Fashion Recommendation"]], "content": "\\label{subsec:GAN_FA_rec}\nMost conventional RS are not suitable for application in the fashion domain due to unique characteristics hidden in this domain. For instance, people do not follow the crowd blindly when buying clothes or do not buy a fashion item twice~. \nAnother aspect is related to the notion of  \\textit{complementary} relationship for recommending a personalized fashion outfit. It is natural for humans to establish a sense of relationship between products based on their visual appearance. \nRecently, GAN-based models have shown promising performance for outfit recommendation, being able to compete with state-of-the-art fashion recommendation models in the field, such as Siamese-base networks~. Finally, another new application of GANs is related to exploiting the \\textit{generative power of GANs} to synthesize real-looking fashion clothes. This aspect can inspire the aesthetic appeal/curiosity of costumer and designers and motivates them to explore the space of\npotential fashion styles.\n\\setlength{\\parindent}{15pt} [\\textbf{CRAFT}]  address the problem of recommending complementary fashion items based on visual features by using an adversarial process that resembles GAN and uses a conditional feature transformer as $\\mathcal{G}$ and a discriminator $\\mathcal{D}$. One main distinction between this work and the prior literature is that the $\\langle$input, output$\\rangle$ pair for $\\mathcal{G}$ are both features (here features are extracted using pre-trained CNNs~), instead of $\\langle$image, image$\\rangle$ or hybrid types such as $\\langle$image, features$\\rangle$ explored in numerous previous works~. This would allow the network to learn the relationship between items directly on the feature space, spanned by the features extracted. The proposed system is named complementary recommendation using adversarial feature transform (CRAFT) since in the model, $\\mathcal{G}$ acts like a feature transformer that ---for a given query product image $q$--- maps the source feature $s_{q}$ into a complementary target feature $\\hat{t}_{q}$ by playing a min-max game with $\\mathcal{D}$ with the aim to classify fake/real features. For training, the system relies on learning the co-occurrence of item pairs in real images. In summary, the proposed method does not generate new images; instead it learns how to generate features of the complementary items conditioned on the query item. \n\\setlength{\\parindent}{15pt} [\\textbf{DVBPR}] Deep visual Bayesian personalized ranking (DVBPR)~ is presumably one of the first works that exploit the \\textit{visual generative power of the GAN} in the fashion recommendation domain. It aims at generating clothing images based on user preferences. Given a user and a fashion item category (e.g., tops, t-shirts, and shoes), the proposed system generates new images ---i.e., clothing items--- that are consistent with the user's preferences. The contributions of this work are two-fold: first, it builds and end-to-end learning framework based on the Siamese-CNN framework. Instead of using the features extracted in advance, it constructs an end-to-end system that turns out to improve the visual representation of images. Second, it uses a GAN-based framework to generate images that are consistent with the user's taste. Iteratively, $\\mathcal{G}$ learns to generate a product image integrating a \\textit{user preference maximization objective}, while $\\mathcal{D}$ tries to distinguish crafted images from real ones. Generated images are quantitatively compared with real images using the preference score (mean objective value), inception score~, and opposite SSIM~. This comparison shows an improvement in preference prediction in comparison with non-GAN based images. At the same time, the qualitative comparison demonstrates that the generated images are realistic and plausible, yet they are quite different from any images in the original dataset ---they have standard shape and color profiles, but quite different styles.\n\\setlength{\\parindent}{15pt} [\\textbf{MrCGAN}]  propose a compatibility learning framework that allows the user to visually explore candidate \\textit{compatible prototypes} (e.g., a white T-shirt and a pair of blue-jeans). The system uses metric-regularized conditional GAN (MrCGAN) to pursue the item generation task. It takes as the input a projected prototype (i.e., the transformation of a query image in the latent \"Compatibility Space\"). It produces as the output a synthesized image of a compatible item (the authors consider a compatibility notion based on the complementary of the query item across different catalog categories). Similar to the evaluation protocol in~, the authors conduct online user surveys to evaluate whether their model could produce images that are perceived as compatible. The results show that MrCGAN can generate compatible and realistic images under compatibility learning setting compared to baselines.\n\\setlength{\\parindent}{15pt} [\\textbf{Yang et al.} \\& \\textbf{$c^+$GAN}]  address the same problem settings of MrCGAN~ by proposing a fashion clothing framework composed of two parts: a clothing recommendation model based on BPR combined with visual features and a clothing complementary item generation based GAN. Notably, the generation component takes in input a piece of clothing recommended in the recommendation model and generates clothing images of other categories (i.e., top, bottom, or shoes) to build up a set of complementary items. The authors follow a similar qualitative and quantitative evaluation procedure as DVBPR~ and further propose a \\textit{compatibility index} to measure the compatibility of the generated set of complementary items. A similar approach has also been proposed in $c^+$GAN~, to generate bottom fashion item paired with a given top item.", "cites": [1001, 1209, 1211, 117, 1213, 1212, 1210], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple GAN-based approaches for fashion recommendation, connecting CRAFT, DVBPR, MrCGAN, and c+GAN to highlight their shared goal of leveraging adversarial learning for complementary item generation and personalization. It provides a coherent narrative by emphasizing differences in input/output formats and learning objectives. While it includes some critical analysis (e.g., noting the distinct feature-based approach in CRAFT), the critique is limited, and the section primarily describes the contributions of each work rather than deeply evaluating their limitations or comparing their effectiveness."}}
{"id": "aef46f5a-0d3e-4fef-b9e3-7d5fc876059b", "title": "Summary and Future Directions", "level": "section", "subsections": [], "parent_id": "68e5db2c-bcee-48f2-8fe1-51da0b40c965", "prefix_titles": [["title", "A Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Generative Adversarial Networks"], ["section", "Summary and Future Directions"]], "content": "\\label{sec:conclusion}\nIn this paper, we have surveyed a wide variety of tasks in which adversarial machine learning (AML) is important to attack/defense a recommendation model as well as improve the generalization performance of the model itself. This broad range of applications can be categorized into two ---objective-wise distinct--- technologies: (i) AML for improving security (cf. Section~\\ref{sec:security}) and, (ii) AML used in generative adversarial networks (GANs) exploited for numerous tasks such as better CF recommendation, context-aware recommendation, cross-domain system, or visually-aware fashion item/outfit recommendation (cf. Section~\\ref{sec:GAN}). The common point of both technologies is the joint min-max optimization used for training models, in which two competing players play a zero-sum differential game until they reach an equilibrium. To the best of our knowledge, this is the first work that sums up the advances of AML application in recommendation settings and proposes a clear taxonomy to classify such applications.\nWe put forward what is better to invest in AML-RS research and introduce the following open research directions:\n\\noindent\\emph{\\underline{Bridging the gap between attack/defense models in the ML/CV and RS domain.}} \nAs the prior literature of AML for security emerged in the field of machine learning (ML) and computer vision (CV), there remains a large gap between advances made in those fields and that in RS. Consider the questions: \\dquotes{Attacks for images are designed to be human-imperceptible or inconspicuous (i.e., may be visible but not suspicious).  How can we capture these notions for designing attacks in RS?}; furthermore, \\dquotes{Images are continuous-valued data while a user profile is a discrete data. Modifying users' profiles completely changes the semantic of their behaviors. What is the best approach to treat these nuances in RS attack designs?} \\\\\n\\emph{\\underline{Choice of recommendation models.}} Modern recommendation models exploit a\nwealth of side-information beyond the user-item matrix such as social-connections, multimedia content, semantic data, among others. However, most of the attacks against recommendation systems are designed and validated against CF systems. Investigating the impact of adversarial attacks against these ---heterogeneous in nature--- data types remains as an open highly interesting challenge, e.g, consider adversarial attacks against music, image, and video recommendation models leveraging multimedia content. In this regard, we also recognize attack against state-of-the-art deep and graph-based models, another highly-valued research direction.\\\\\n\\emph{\\underline{Definition of attack threat model.}} The research in RS community misses a common evaluation approach for attacking/defending scenarios such as the one introduced by Carlini at el.~. For instance, it is important to define a common attacker threat model to establish in advance the attacker knowledge and capabilities to make the attack (or defense) reproducible and comparable with novel proposals.\\\\\n\\emph{\\underline{Move the attention towards beyond accuracy goal in recommendation.}}\nAccording to our survey, most of the identified research works focus on accuracy metrics such as HR and nDCG. Consider the question: \\dquotes{What is the impact of adversarial attacks and defenses in other evaluation objectives of RS, for instance, diversity, novelty, and fairness of recommendations}. The impact on these metrics could be, in principle, the main objective of a new breed of attack strategies aiming at compromise the diversity/novelty of results.\\\\\n\\emph{\\underline{Scalability and stability of learning.}} We identify that there exists the need to further explore the stability learning problems in the discrete item sampling strategy to train the generator. This has been already identified as a big problem when GAN-based RS are applied in real scenarios with huge catalogues. A point of study may be that of novel GAN models proposed in computer vision (e.g., WGAN~, LSGAN~, and BEGAN~).\\\\\n\\emph{\\underline{Users preferences learning with GANs.}} An interesting and already established application of AML-RS is to exploit the generative power of GANs to produce more plausible user-rating profiles that can be used to improve recommendations in the cold-user scenario or improve the prediction performance in warm-start settings. We consider such applications extremely interesting, and we motivate further research in this direction to resolve the well-known cold-start obstacles in recommendation settings.\n\\input{Tables/Recommender_Models.tex}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{refs_final}\n\\end{document}\n\\endinput", "cites": [7305, 64, 63], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from the cited papers, particularly linking AML concepts from ML/CV to RS and suggesting how GANs can address cold-start and generalization issues. It identifies critical gaps in the RS domain, such as the lack of standardized threat models and the overemphasis on accuracy metrics. The discussion abstracts beyond individual works, proposing broader research directions and highlighting the need for cross-domain integration."}}
