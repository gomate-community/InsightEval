{"id": "f9d1ed5f-ebd8-41c8-b85b-c14ee4a51e33", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4e01ebc7-074c-4060-b09d-956a012ff8ea", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nData is the fuel of machine learning applications and therefore has been steadily increasing in value.\nIn many settings an abundant amount of unlabeled data is produced,\nbut in order to use such data in supervised machine learning, \none has no choice but to provide labels.\nThis usually entails a manual labeling process, which is often non-trivial and can even require a domain expert, \ne.g., in patent classification , or clinical text classification .\nMoreover, this is time-consuming and rapidly increases monetary costs, thereby quickly rendering this approach infeasible.\nEven if an expert is available, it is often impossible to label each datum due to the vast size of modern datasets.\nThis especially impedes the field of Natural Language Processing (NLP), \nin which both the dataset and the amount of text within each document can be huge,\nresulting in unbearable amounts of annotation efforts for human experts.\nActive Learning (AL) aims to reduce the amount of data annotated by the human expert.\n\\revised{It is an iterative cyclic process between an {\\em oracle} (usually the human annotator)\nand an {\\em active learner}.}\nIn contrast to passive learning, in which the data is simply fed to the algorithm, \nthe active learner chooses which samples are to be labeled next.  \nThe labeling itself, however, is done by a human expert, the so-called human in the loop.\nHaving received new labels, the active learner trains a new model and the process starts from the beginning.\n\\revised{Using the term active learner, we refer to the composition of a {\\em model}, a {\\em query strategy}, and a {\\em stopping criterion}.}\nIn this work the model is w.l.o.g. a text classification model, \nthe query strategy decides which instances should be labeled next, and the stopping criterion defines when to stop the AL loop.\nAccording to \\textcite{settles2010active} there are three main scenarios for AL:\n(1) Pool-based, in which the learner has access to the closed set of unlabeled instances, called the pool;\n(2) stream-based, where the learner receives one instance at a time and has the options to keep it, or to discard;\n(3) membership query synthesis, \\revised{in which the learner creates new artificial instances to be labeled}.\nIf the pool-based scenario operates not on a single instance, but on a batch of instances, this is called {\\em batch-mode} AL .\nThroughout this work we assume a pool-based batch-mode scenario because in a text classification setting the dataset is usually a closed set, and the batch-wise operation reduces the number of retraining operations, which cause waiting periods for the user.\\\\\n\\pindent \\revised{The underlying idea of AL is that few representative instances can be used as surrogate for the full dataset.}\nNot only does a smaller subset of the data reduce the computational costs, \nbut also it has been shown that AL can even increase the quality of the resulting model compared to learning on the full dataset .\nAs a consequence, AL has been used in many NLP tasks, e.g. text classification , named entity recognition , \nor machine translation  and is still an active area of research.\nIn recent years, deep learning (DL) approaches have dominated most NLP tasks' state-of-the-art results.\nThis can be attributed to advances in neural networks (NNs), above all Convolutional Neural Networks (CNN; ) and (Bidirectional-)Long Short-Term Memory (LSTM; ),\nwhich were eventually adopted into the NLP domain,\nand to the advances of using word embeddings  and contextualized word embeddings .\nBoth NN architectures and text representations have raised the state-of-the-art results in the field of text classification considerably (e.g., ).\nIf these improvements were transferrable to AL, this would result in a huge increase in efficiency.\nFor the AL practitioner, this either means achieving the same performance using fewer samples, or having an increase in performance using the same amount of data.\nAnother favorable development is that transfer learning, especially the paradigm of fine-tuning pre-trained language models (LMs), has become popular in NLP.\nIn the context of AL this helps especially in the small data scenario, in which a pre-trained model can be leveraged to train a model by fine-tuning using only little data, which would otherwise be infeasible.\nFinally, by operating on sub-word units LMs also handle out-of-vocabulary tokens, which is an advantage over many traditional methods.\nResulting from these advances, existing AL surveys have become both incomplete in some parts and outdated in others:\nThey lack comparison against the current state of the art models, do not provide results for more recent large-scale datasets, \nand most importantly, they are lacking the aforementioned advances in NNs and text representations.\nSurprisingly, despite the current popularity of NNs, there is only little research about NN-based active learning in the context of NLP, and even less thereof in the context of text classification (see Section \\ref{subsect:nn_al} and Section \\ref{subsect:active_tc} for a detailed summary).\nWe suspect this is due to the following reasons: \n(1) Many DL models are known to require large amounts of data ,\nwhich is in strong contrast to AL aiming at requiring as little data as possible\n(2) there is a whole AL scenario based on artificial data generation, which unfortunately is a lot more challenging for text in contrast to for example images, for which data augmentation is commonly used in classification tasks ;\n(3) NNs are lacking uncertainty information regarding their predictions \\revised{(as explained in Section \\ref{subsect:nn_al})}, which complicates the use of a whole prominent class of query strategies.\n\\hiddennotes{\n  \\begin{itemize}\n    \\item No clear evaluation protocol / huge differences in the evaluation approaches\n  \\end{itemize}\n}\nThis survey aims at summarizing the existing approaches of (D)NN-based AL for text classification.\nOur main contributions are as follows:\n\\begin{enumerate}\n  \\item We provide a taxonomy of query strategies and classify strategies relevant for AL for text classification.\n  \\item We survey existing work at the intersection of AL, text classification, and (D)NNs.\n  \\item Recent advances in text classification are summarized and related to the AL process. It is then investigated, if and to what degree they have been adopted for AL.\n  \\item The experimental setup of previous research is collectively analyzed regarding datasets, models, and query strategies in order to identify recent trends, commonalities, and shortcomings in the experiments.\n  \\item \\revised{We identify research gaps and outline future research directions.}\n\\end{enumerate}\n\\noindent Thereby we provide a comprehensive survey of recent advances in NN-based active text classification. \nHaving reviewed these recent advances, we illuminate areas that either need re-evaluation, or have not yet been evaluated in a more recent context.\nAs a final result, we develop research questions outlining the scope of future research.", "cites": [7764, 1159, 7165, 1096, 4022, 8385, 1684, 11], "cite_extract_rate": 0.32, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The introduction synthesizes key ideas from multiple papers, such as the role of word embeddings (Papers 3, 6, 7), the success of CNNs and LSTMs in text classification (Paper 2), and challenges in data augmentation for text (Paper 1). It identifies critical limitations, such as the lack of uncertainty estimates in DNNs and the difficulty of applying AL to text data. The section abstracts these points into broader themes like the tension between data requirements in deep learning and AL goals, and the relevance of pre-trained models. However, the synthesis remains somewhat surface-level, and deeper comparative or evaluative analysis is deferred to later sections."}}
{"id": "10ab6316-68f8-4bea-9503-5f9388a60998", "title": "Related Work", "level": "section", "subsections": [], "parent_id": "4e01ebc7-074c-4060-b09d-956a012ff8ea", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Related Work"]], "content": "\\textcite{settles2010active} provides a general active learning survey, \nsummarizing the prevalent AL scenario types and query strategies.\nThey present variations of the basic AL setup like \n variable labeling costs or alternative query types, \nand most notably, they discuss empirical and theoretical research investigating the effectiveness of AL: \nThey mention research suggesting that AL is effective in practice \nand has increasingly gained adoption in real world applications. However, it is pointed out that empirical research also reported cases in which AL performed worse than passive learning and that the theoretical analysis of AL is incomplete.\nFinally, relations to related research areas are illustrated, thereby connecting AL among others to reinforcement learning and semi-supervised learning.\n\\hiddennotes{\n  settles2009/settles2010active \n  \\begin{itemize}\n    \\item scenarios: membership query synthesis, stream-based, pool-based\n    \\item query strategies\n    \\begin{itemize}\n      \\item keine Klassifikation vorgenommen\n    \\end{itemize}\n    \\item most important (at that time): analysis\n    \\begin{itemize}\n      \\item does it work?\n      \\item both empirical and theoretical evidence\n    \\end{itemize}\n    \\item TODO: problem setting variants, practical considerations (only in 2010 version)\n    \\item related areas: semi-supervised, reinforcement, submodular optimization, equivalence query learning, model parroting and compression\n  \\end{itemize} \n}\nThe survey of \\textcite{fu2013survey} is focused around a thorough analysis of uncertainty-based query strategies, which are categorized into a taxonomy.\nThis taxonomy differentiates at the topmost level between the uncertainty of i.i.d. instances and instance correlation. The latter is a superset of the former \nand intends to reduce redundancy among instances by considering feature, label, and structure correlation when querying.\nMoreover, they perform an algorithmic analysis for each query strategy \nand order the strategies by their respective time complexity, highlighting the increased complexity for correlation-based strategies.\n\\hiddennotes{\n  \\begin{itemize}\n    \\item survey on instance selection\n  \\end{itemize} \n}\nAnother general survey covering a wide range of topics was conducted by \\textcite{aggarwal2014active}. \nThey provide a flat categorization of query strategies, which is quite different from the taxonomy of \\textcite{fu2013survey} and divides them into the following three categories: (1) ``heterogenity-based'', which sample instances by their prediction uncertainty or dissimilarity compared to existing labeled instances, (2) ``performance-based''', which select instances based on a predicted change of the model loss, and (3) ``representativeness-based'', which select data points to reflect a larger set in terms of their properties, usually achieved by the means of distribution density .\nSimilarly to , they present and discuss many non-standard variations of the active learning scenario.\n\\revised{\n  An NLP-focused active learning survey was performed by \\textcite{olsson2009literature}.\n  This work's main contribution is a survey of disagreement-based query strategies, which use the disagreement among multiple classifiers to select instances.\n  Moreover, Olsson reviews practical considerations, e.g., selecting an initial seed set,\n  deciding between stream-based and pool-based scenario, and deciding when to terminate the learning process.\n  Although some NN-based applications are mentioned, none of the above surveys covers NN-based AL in depth.\n  Besides, none is recent enough to cover NN-architectures, which have only recently been adapted successfully to text classification problems like e.g., KimCNN .\n  The same holds true for recent advances in NLP such as word embeddings, contextualized language models (explained in Section \\ref{subsect:tc_recent_adv}), or resulting advances in text classification (discussed in Section \\ref{subsect:nn_text_classification} and Section \\ref{subsect:active_tc}).\n  We intend to fill these gaps in the remainder of this survey.\n}\n\\hiddennotes{\n  \\begin{itemize}\n    \\item NLP-centric Active Learning Survey\n    \\item describes multiple active learning modes: Query by uncertainty, query by committee, redundant views\n    \\item extensively reviews strategies to measure disagrement\n  \\end{itemize}\n}", "cites": [1159], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple active learning surveys, highlighting their focus areas and categorization styles. It provides critical analysis by identifying the lack of depth in NN-based AL within prior surveys and pointing out their inappropriateness for recent NLP advances. The abstraction level is high as it generalizes trends and limitations in the field, setting up a broader context for the survey's contributions."}}
{"id": "3effa5dc-9905-4ec7-8c3d-464cfd4bd1d9", "title": "Query Strategies", "level": "subsection", "subsections": ["78f9fdb7-e132-4e81-b968-042f74647827"], "parent_id": "3e095066-9c33-4519-a7ec-ec4550b8cb2d", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Query Strategies"]], "content": "\\label{subsec:query_strategies}\nIn Figure \\ref{fig:query_strategies} we classify the most common AL query strategies based on a strategy's {\\em input information}, which denotes the numeric value(s) a strategy operates on.\nIn our taxonomy the input information can be either random or one of data, model, and prediction.\n\\phrasing{These categories are ordered by increasing complexity and are not mutually exclusive.}\n\\phrasing{Obviously, the model is a function of the data, as well as the prediction is a function of model and data, and moreover, in many cases a strategy use multiple of these criteria.}\nIn such cases we assign the query strategy to the most specific category (i.e. prediction-based precedes model-based, which in turn precedes data-based).\n\\begin{figure}[!h]\n  {\n    \\scriptsize\n    \\centering\n    \\begin{forest} for tree={\n        grow'=0, draw\n      },\n      forked edges,\n      [\\scshape query strategies, no edge, root\n          [\\scshape random, onode_dashed\n              [,phantom]\n          ]\n          [\\scshape data-based, onode\n              [\\scshape data uncertainty, onode\n                [discriminative\\\\ , tier=leaf, tnode]\n              ]\n              [\\scshape representativeness, onode\n                [\\scshape clustering, onode\n                    [flat\\\\ , tier=leaf, tnode]\n                    [hierarchical\\\\ , tier=leaf, tnode]\n                ]\n                [\\scshape set construction, onode\n                  [core-set\\\\ , tier=leaf, tnode]\n                ]\n              ]\n          ]\n          [\\scshape model-based, onode\n              [\\scshape model uncertainty, onode\n                  [UNC-IE\\\\ , tier=leaf, tnode]\n              ]\n              [\\scshape expected parameter\\\\change, onode\n                  [expected gradient length\\\\ , tier=leaf, tnode]\n                  [expected weight change\\\\ , tier=leaf, tnode]\n              ]\n              [\\scshape adversarial, tnode\n                  [DFAL\\\\ , tier=leaf, tnode]\n              ]  \n          ]\n          [\\scshape prediction-based, name=prediction_based, onode\n              [\\scshape prediction\\newline uncertainty, name=prediction_uncertainty, tnode\n                  [\\scshape probabilistic, onode\n                      [uncertainty sampling , tier=leaf,tnode]\n                  ]\n                  [\\scshape margin-based, onode\n                      [version space\\\\ , tier=leaf,tnode]\n                      [closest to hyperplane\\\\ , tier=leaf,tnode]\n                  ]\n                  [\\scshape entropy, name=entropy, onode\n                      [BALD\\\\ , tier=leaf,tnode]\n                  ]\n              ]\n              [\\scshape discriminative, onode\n                  [DAL\\\\ , tier=leaf , tnode]\n              ]           \n              [\\scshape expected prediction change\\\\, onode\n                  [expected error reduction\\\\ , name=lastnode, tier=leaf, tnode]\n              ]\n              [\\scshape disagreement, name=disagreement, onode_dashed\n                  [,phantom, tier=leaf]\n              ]\n          ] \n      ]\n      \\draw[zlevel,-] (disagreement.east) -| (entropy.south);\n      \\draw[zlevel,-] (entropy.north) |- (prediction_uncertainty.east);\n      \\path (current bounding box.south) coordinate (s1);\n      \\draw[decorate,decoration={brace,amplitude=1em,mirror}] ([yshift=-3pt]prediction_based.south west|-s1) -- node[below=1em] {class} ([yshift=-3pt,xshift=-1pt]prediction_based.south east|-s1);\n      \\draw[decorate,decoration={brace,amplitude=1em,mirror}] ([yshift=-3pt,xshift=1pt]prediction_based.south east|-s1) -- node[below=1em] {subclass(es)} ([yshift=-3pt,xshift=-1pt]lastnode.south west|-s1);\n      \\draw[decorate,decoration={brace,amplitude=1em,mirror}] ([yshift=-3pt,xshift=1pt]lastnode.south west|-s1) -- node[below=1em] {example} ([yshift=-3pt,xshift=-1pt]lastnode.south east|-s1);\n    \\end{forest}\n    \\caption{\\revised{A taxonomy of query strategies for AL.\n      The key distinction is at the first level, where the query strategies are categorized by their access to different kinds of input information. \n      From the second to the penultimate level we form coherent subclasses,\n      and the final level shows examples for the respective class.\n      This taxonomy is not exhaustive due to the abundance of existing query strategies, and it is biased towards query strategies in NLP.}\n      }\n    \\label{fig:query_strategies}\n  }\n\\end{figure}\n\\hiddennotes{\n  \\begin{itemize}\n    \\item Table in appendix: For each strategy here: Applicability: B / MC / ML\\\\\n  \\end{itemize}\n}", "cites": [1042, 4024, 4023, 11, 8716, 8717], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 2.8, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section provides a structured taxonomy of query strategies for active learning in deep text classification, integrating key ideas from multiple cited works. It synthesizes diverse methods into a coherent framework, showing insight by ordering them by complexity and explaining relationships between data, model, and prediction-based strategies. While the abstraction level is high, the critical analysis is moderate, with limited explicit evaluation or critique of the cited approaches beyond categorization."}}
{"id": "78f9fdb7-e132-4e81-b968-042f74647827", "title": "Random", "level": "paragraph", "subsections": ["734bf4e0-5809-4e56-9af4-79cebf71ecdb", "d53e7115-2a38-4cfc-bd3f-72de86cf317d", "c472f10c-b421-401e-aea9-953609e5c153", "ff2fa7ea-f6cb-4eae-aca9-41c04d989749"], "parent_id": "3effa5dc-9905-4ec7-8c3d-464cfd4bd1d9", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Query Strategies"], ["paragraph", "Random"]], "content": "Randomness has traditionally been used as a baseline for many tasks.\nIn this case, random sampling selects instances at random and is a strong baseline for AL instance selection .\n\\revised{It often performs competitive to more sophisticated strategies, especially when the labeled pool has grown larger .}", "cites": [1042, 4024, 8716], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes random sampling as a baseline for active learning, citing related works but without synthesizing or contrasting their contributions. It lacks critical evaluation or abstraction to broader principles, focusing instead on a general statement about its performance relative to more complex methods."}}
{"id": "c472f10c-b421-401e-aea9-953609e5c153", "title": "Prediction-based", "level": "paragraph", "subsections": [], "parent_id": "78f9fdb7-e132-4e81-b968-042f74647827", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Query Strategies"], ["paragraph", "Random"], ["paragraph", "Prediction-based"]], "content": "Prediction-based strategies select instances by scoring their prediction output.\nThe most prominent members of this class are prediction-uncertainty-based and disagreement-based approaches.\n\\textcite{sharma2017evidence-based} denote prediction-based uncertainty by {\\em conflicting-evidence uncertainty}, which they, contrary to this work, count as another form of model-based uncertainty.\nThere is sometimes only a thin line between the concepts of model-based und prediction-based uncertainty.\n\\revised{Roughly speaking, prediction-based uncertainty corresponds in a classification setting to inter-class uncertainty, as opposed to model-based uncertainty, which corresponds to intra-class uncertainty.}\nIn literature, uncertainty sampling  usually refers to prediction-based uncertainty, unless otherwise specified.", "cites": [8716], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides some synthesis by integrating the concept of prediction-based uncertainty and relating it to other uncertainty types, such as model-based. It also introduces a distinction between inter-class and intra-class uncertainty, showing abstraction. However, the critical analysis is limited, as it only briefly notes differing categorizations without deeper evaluation. The section contributes analytical insight by framing the terminology and relationships in active learning."}}
{"id": "fcf92379-9f52-4b46-8476-f3cf95e27e6c", "title": "Previous Work", "level": "paragraph", "subsections": ["72864fc8-a21e-4cfd-8e06-6087c4b614bf", "17995907-7b29-4bbf-84e0-f3a0b81b8627"], "parent_id": "aff7fdcf-a8ec-4ce1-9895-2dad247f6097", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Neural-Network-Based Active Learning"], ["paragraph", "Previous Work"]], "content": "\\hiddennotes{\n  \\begin{itemize}\n    \\item Ãœberleitung fehlt\n    \\item Argumentationsstruktur unklar?\n    \\begin{itemize}\n      \\item Classic -> RNN -> CNN -> LSTM -> GANs (-> Meta?) \n    \\end{itemize}\n  \\end{itemize}\n}\nEarly research in NN-based AL can be divided into uncertainty-based ,\nand ensemble-based  strategies.\nThe former often use prediction entropy  as measure of uncertainty,\nwhile the latter utilize the disagreement among the single classifiers.\n\\textcite{settles2008multiple-instance} proposed the expected gradient length (EGL) query strategy, which selects instances by the  expected change in the model's weights. \\textcite{zhang2017active} were first to use a CNN for AL.\nThey proposed a variant of the expected gradient length strategy , in which they select instances that are expected to result in the largest change in embedding space, thereby training highly discriminative representations.\n\\textcite{sener2017active} observed uncertainty-based query strategies not to be effective for CNN-based batch-mode AL, and proposed core-set selection, which samples a small subset to represent the full dataset. \\textcite{ash2019deep} proposed BADGE, a query strategy for DNNs, which uses k-means++ seeding  on the gradients of the final layer, in order to query by uncertainty and diversity.\nFinally, Generative Adversarial Networks (GANs; ) have also been applied successfully for AL tasks: \n\\textcite{zhu2017generative} use GANs for query synthesis of images within an active learner using an SVM model.\nThe instances are synthesized so that they would be classified with high uncertainty. \nThe authors report this approach to outperform random sampling, pool-based uncertainty sampling using an SVM , and in some cases passive learning, while having the weakness to generate too similar instances.\nThe approach itself is neither pure NN-based, nor does it belong to the pool-based scenario, however, it is the first reported use of GANs for AL.\n\\textcite{ducoffe2018adversarial} use adversarial attacks to find instances that cross the decision boundary with the aim to increase the model robustness.\nThey train two CNN architectures and report results superior to the core-set  strategy on image classification tasks.\nIt is obvious that GANs inherently belong to the membership query synthesis scenario.\n\\revised{Therefore their performance correlates with the quality of artificial data synthesis, i.e. they are usually not that effective for NLP tasks. This has already been recognized and first improvements towards a better text generation have been made }.", "cites": [1042, 3986], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes several key papers in NN-based active learning, categorizing them into uncertainty-based and ensemble-based strategies and introducing specific methods such as EGL, core-set selection, and BADGE. It also identifies limitations of GAN-based approaches for NLP tasks, showing some critical evaluation. However, the narrative lacks deeper integration and a more systematic comparison, and the abstraction level remains limited to surface-level observations of trends."}}
{"id": "72864fc8-a21e-4cfd-8e06-6087c4b614bf", "title": "Uncertainty in Neural Networks", "level": "paragraph", "subsections": [], "parent_id": "fcf92379-9f52-4b46-8476-f3cf95e27e6c", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Neural-Network-Based Active Learning"], ["paragraph", "Previous Work"], ["paragraph", "Uncertainty in Neural Networks"]], "content": "One of the earliest and in many variations adopted class of strategies is uncertainty sampling .\n\\label{sect:nn_uncertainty}\nUnfortunately, this widely-used concept is not straightforward to apply for NNs, as they do not provide an inherent indicator of uncertainty.\nIn the past, this has been tackled among others by ensembling , or by learning error estimates .\nMore recent approaches furthermore use Bayesian extensions , \nobtain uncertainty estimations using dropout ,\nor use probabilistic NNs to estimate predictive uncertainty .\nHowever, ensemble and Bayesian approaches quickly become infeasible on larger datasets,\nand NN architectures are generally known to be overconfident in their predictions .\nConsequently, uncertainty in NNs is only insufficiently solved and therefore still remains a highly relevant research area.\n\\hiddennotes{\n  To circumvent this, various approaches have been suggested:\n  (1) Bayes by Backprop  uses Bayesian variational inference in order to learn approximated distributions over the weights of a NN.\n  (2)  extend dropout  to represent uncertainties \n  by interpreting dropout-enhanced NNs as a Bayesian approximation to deep Gaussian Processes.\n  However, this has been reported to not scale well for larger datasets .\n  (3) Traditionally this has been tackled by ensembling , i.e. producing $k$-many different models and outputting a prediction by interpreting their disagreement (see \\textcite{olsson2009literature}).\n  The disadvantage of this approach is the obvious additional cost in runtime and model size.\n  \\textcite{sener2017active} report uncertainty-based approaches to be not effective for CNNs, and \\textcite{ducoffe2018adversarial} observed worse performance compared to random sampling.\\\\\n}", "cites": [4025, 759, 1042, 3288], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches to uncertainty estimation in neural networks, connecting Bayesian methods, ensembling, and dropout-based techniques across cited papers. It also critically points out the limitations of these methods, such as scalability and overconfidence. While it identifies broader patterns (e.g., inefficiency of Bayesian methods on large datasets), it does not offer a novel overarching framework, keeping the abstraction at a moderate level."}}
{"id": "17995907-7b29-4bbf-84e0-f3a0b81b8627", "title": "Contrasting Paradigms", "level": "paragraph", "subsections": [], "parent_id": "fcf92379-9f52-4b46-8476-f3cf95e27e6c", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning"], ["subsection", "Neural-Network-Based Active Learning"], ["paragraph", "Previous Work"], ["paragraph", "Contrasting Paradigms"]], "content": "DNNs are known to excel in particularly at large-scale datasets,\nbut often having large amounts of data available is a strict requirement to perform well at all (e.g., ).\nAL on the other hand tries to minimize the labeled data.\nThe small labeled datasets can be a problem for DNNs, since they are known to overfit on small datasets (e.g., ), \nwhich results in bad generalization performance on the test set.\nMoreover, DNNs often offer little advantage over shallow models when they are trained using small datasets , thereby lacking justification for their higher computational costs.\nOn the other hand we clearly cannot require AL to label more data, since this would defeat its purpose.\nTherefore there has been research on dealing with (D)NNs using small datasets, however, it is only a scarce amount, especially in relation to the large volume of NN literature in general.\nHandling small datasets is mostly circumvented by using pre-training  or other transfer learning approaches .\nFinally, the search for optimal hyperparameters is often neglected and instead the hyperparameters of related work are used, which are optimized for large datasets, if at all.", "cites": [7764, 1096], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by highlighting the tension between the data-hungry nature of DNNs and the goal of AL to minimize labeled data. It connects the cited papers to broader challenges in AL for DNNs, such as overfitting and the limited benefit over simpler models with small data. However, the synthesis is limited, as the connection between the cited papers and the general discussion is not deeply elaborated, and the abstraction remains at a moderate level without proposing overarching theoretical frameworks."}}
{"id": "ad081844-efd8-44ae-a153-552a2838fa22", "title": "Representations", "level": "paragraph", "subsections": ["e093196b-af7c-4543-b2db-908cbfcfea1d"], "parent_id": "54497d60-bdb0-4315-828f-3d8f53e9c84c", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning for Text Classification"], ["subsection", "Recent Advances in Text Classification"], ["paragraph", "Representations"]], "content": "Traditional methods use the bag-of-words (BoW) representation, which are sparse and high-dimensional.\nHowever, with the introduction of word embeddings like word2vec , \nGloVe , or fastText ,\nword embeddings have replaced BoW representations in many cases.\nThis is due to several reasons:\n(1) They represent semantic relations in vectors space and avoid the problem of mismatching features as for example due to synonymy;\n(2) incorporating word embeddings resulted in superior performance for many downstream tasks ;\n(3) unlike bag-of-words, word vectors are dense, low-dimensional representations, \nwhich makes them applicable to a wider range of algorithms --\nespecially in the context of NNs which favor fixed-size inputs.\nVarious approaches have been presented in order to obtain similar fixed size representations for word sequences, i.e. sentences, paragraphs or documents .\\\\\n\\pindent Word embeddings are representations, which provide exactly one vector per word and in consequence one meaning as well.\nThis makes them also unaware of the current word's context and therefore makes them unable to detect and handle ambiguities.\nUnlike word embeddings, language models (LMs) compute the word vector using the word and the surrounding context .\nThis results in a contextualized representation, which inherits the advantages of word embeddings, and at the same time allows for context-specific representation (in contrast to static embeddings) . \nELMo was the first LM to gain wide adoption and surpassed state of the art models on several NLP tasks .\nShortly thereafter, BERT  was introduced and provided bidirectional pre-training-based language modelling.\nThe process to create a BERT-based model consists of a pre-training and a fine-tuning step as opposed to ELMO's direct feature-based approach in which contextualized vectors are obtained from the pre-trained model and used directly as features .\nBy masking, i.e. randomly removing a fraction of tokens during training, the training was adapted to predict the masked words.\nThis made the bidrectional training possible, which would otherwise be obstructed because a word could \"see itself\" when computing its probability of occurrence given a context .\nFollowing this, XLNet  introduced a similar approach of pre-training and fine-tuning using an autoregressive language model, however, it overcame BERT's limitation as it does not rely on masking data during pre-training , and moreover, successfully manages to integrate the recent TransformerXL architecture .\nSince then, a variety of LMs have been published, which further optimize the pre-training of previous LM architectures (e.g., RoBERTa  and ELECTRA ), or distill the knowledge into a smaller model (e.g., DistilBERT ).\nSimilarly to word embeddings, there are approaches to use LMs in order to obtain sentence representations from LMs .\\\\\n\\pindent All mentioned representations offer a richer expressiveness than traditional BoW representations and therefore are well-suited for active learning purposes.", "cites": [7265, 7165, 856, 1684, 7370, 11, 826, 1557, 8385, 673], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of the evolution of text representations, integrating key concepts from multiple papers such as word embeddings, ELMo, BERT, and XLNet. It connects these works to form a narrative of moving from static to contextualized representations. However, it lacks deeper critical evaluation or comparison of the cited methods, and while it hints at broader trends (e.g., shift from BoW to embeddings and contextual models), it does not abstract these into high-level principles or frameworks."}}
{"id": "e093196b-af7c-4543-b2db-908cbfcfea1d", "title": "Neural-Network-Based Text Classification", "level": "paragraph", "subsections": [], "parent_id": "ad081844-efd8-44ae-a153-552a2838fa22", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning for Text Classification"], ["subsection", "Recent Advances in Text Classification"], ["paragraph", "Representations"], ["paragraph", "Neural-Network-Based Text Classification"]], "content": "\\label{subsect:nn_text_classification}\nA well-known CNN architecture presented by \\textcite{kim2014convolutional} (KimCNN) operates on pre-trained word vectors and achieved state of the art results at the time using only a simple but elegant architecture.\nThe investigated CNN setups did not require much hyperparameter tuning and confirmed the effectiveness of dropout  as a regularizer for CNN-based text classification.\\\\\n\\pindent The word embeddings of fastText  differ from other word embeddings in the sense that the approach is (1) supervised and (2) specifically designed for text classification.\nBeing a shallow neural network, it is still very efficient, while still obtaining performances comparable to deep learning approaches at that time.\\\\\n\\pindent\\textcite{howard2018universal} developed Universal Language Model Fine-tuning (ULMFiT), a LM transfer learning method using the AWD-LSTM architecture ,\nwhich outperformed the state of the art on several text classification datasets when trained on only $100$ labeled examples, and thereby achieved results significantly superior to more sophisticated architectures of previous work.\nContext-specific LMs like BERT  and XLNet  yield a context-dependent vector for each token, thereby strongly improving NN-based text classification .\nState of the art in NN-based text classification is LM-based fine-tuning with XLNet, which has a slight edge over BERT in terms of test error rate .\nULMFiT follows closely thereafter, and KimCNN is still a strong contender.\nNotably, ULMFiT, BERT and XLNet all perform {\\em transfer learning}, which aims to transfer knowledge from one model to another ,\nthereby massively reducing the required amounts of data.", "cites": [7595, 673, 11, 4026], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of several neural network-based text classification models, including KimCNN, fastText, ULMFiT, BERT, and XLNet, mentioning their architectures, performance, and efficiency. While it connects the concept of transfer learning across the models, it lacks deeper critical analysis or evaluation of their limitations. The synthesis is basic, and the abstraction remains limited to surface-level generalization about transfer learning."}}
{"id": "5247a74b-cf2b-493b-a901-1dbfdda01e22", "title": "Text Classification for Active Learning", "level": "subsection", "subsections": [], "parent_id": "909d9662-92b7-4a13-b530-6c9553233a2e", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning for Text Classification"], ["subsection", "Text Classification for Active Learning"]], "content": "\\label{subsect:active_tc} \nTraditional AL for text classification heavily relied on query strategies based on prediction-uncertainty  and ensembling . \nCommon model choices included support vector machines (SVMs; ), naive bayes , logistic regression  and neural networks .\nTo the best of our knowledge, no previous survey covered traditional AL for text classification, however,  ensembling-based AL for NLP has been covered in depth by \\textcite{olsson2009literature}.\\\\\n\\pindent Regarding modern NN-based AL for text classification, the relevant models are primarily CNN- and LSTM-based deep architectures: \\textcite{zhang2017active} claim to be the first to consider AL for text classification using DNNs.\nThey use CNNs and contribute a query strategy, which selects the instances \\phrasing{based on the expected change of the word embeddings and the model's uncertainty given the instance,} thereby learning discriminative embeddings for text classification.\n\\textcite{an2018deep} evaluated SVM, LSTM and gated recurrent unit (GRU; \\textcite{cho2014properties}) models, and reported that the latter two significantly outperformed the SVM baseline on the Chinese news dataset ThucNews.\n\\textcite{lu2020investigating} investigated the performance of different text representations in a pool-based AL scenario.\nThey compared frequency-based text representations, word embeddings and transformer-based representations used as input features for a SVM-based AL and different query strategies,\nin which transformer-based representations yielded consistently higher scores.\n\\textcite{prabhu2019sampling} investigate sampling bias and apply active text classification on the large scale text corpora of \\textcite{zhang2016character-level}.\nThey demonstrate FastText.zip  with (entropy-based) uncertainty sampling to be a strong baseline, which is competitive compared to recent approaches in active text classification.\nMoreover, they use this strategy to obtain a surrogate dataset (comprising from 5\\% to 40\\% of the total data) on which a LSTM-based LM is trained using ULMFiT , reaching accuracy levels close to a training on the full dataset.\nUnlike past publications, they report this uncertainty-based strategy to be effective, robust, and at the same time computationally cheap.\nThis is the most relevant work in terms of \\revised{the intersection between text classification, NNs and DL}.\n\\hiddennotes{\n    \\begin{itemize}\n        \\item Previous Research nach Query Strategy Class / Embeddings untersuchen\n        \\item Reinforcement Learning using NNs\n    \\end{itemize}\n}\n\\begin{table}[h!]\n  \\begin{center}\n  \\begin{tabular}{l P{3cm} P{2.5cm} P{4.5cm}}\n    \\hline\n    \\textbf{Publication} & \\textbf{Datasets} & \\textbf{Model(s)} & \\textbf{Query Strategy Class(es)}\\\\\n    \\hline\n    \\mbox{} & 20N, R21, RV2, SPM & NB, SVM, kNN & 1. Prediction uncertainty (LC)\\newline 2. Prediction uncertainty (CTH)\\newline 3. Prediction uncertainty (disagreement) \\\\\n    \\mbox{} & CR, MR, SJ, MRL, MUR, DR & CNN & 1. Model uncertainty (EGL)\\newline 2. Prediction Uncertainty (entropy)\\\\\n    \\mbox{} & RMA & SVM & 1. Prediction uncertainty (CTH)\\newline 2. Prediction uncertainty (disagreement)\\\\\n    \\mbox{} & TQA, MR & SVM, CNN, BiLSTM & Prediction uncertainty (disagreement)\\\\\n    \\mbox{} & MR, SJ, TQA, CR & SVM, CNN, BiLSTM & 1. Prediction uncertainty (entropy)\\newline 2. Prediction uncertainty (disagreement)\\\\\n    \\mbox{} & SGN, DBP, YHA, YRP, YRF, AGN, ARP, ARF & FTZ, ULMFiT & Prediction uncertainty (entropy)\\\\\n    \\mbox{} & MRL, MDS, BAG, G13, ACR, SJ, AGN, DBP &  SVM & 1. Prediction uncertainty (CTH) \\newline 2. Prediction uncertainty (disagreement) \\newline 3. Data-based (EGAL) \\newline 4. Data-based (density)\\\\\n    \\hline \n  \\end{tabular}\n  \\captionsetup{aboveskip=10pt}\n  \\caption{An overview of recent work on AL for text classification.\n    We referred to the datasets using short keys, which can be looked up in Table \\ref{tab:datasets} in the Appendix. Models: Naive Bayes (NB), Support Vector Machine (SVM), k-Nearest Neighbours (kNN), Convolutional Neural Network (CNN), [Bidirectional] Long Short-Term Memory ([Bi]LSTM), FastText.zip (FTZ), Univeral Language Model Fine-Tuning (ULMFiT). Query strategies: Least confidence (LC), Closest-to-hyperplane (CTH), expected gradient length (EGL).\n    Random selection baselines were omitted.}\n  \\label{table:recent_work}\n\\end{center}\n\\end{table}", "cites": [7765, 4023, 4022, 8716, 11], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of active learning for text classification, mentioning key papers and their models, query strategies, and datasets. While it includes a table to organize cited works and references different approaches, it lacks deeper synthesis, critical evaluation, and abstraction. It mostly describes individual studies without connecting their contributions into a broader narrative or framework."}}
{"id": "c22085b6-c5bc-4141-87dc-c353d7a13732", "title": "Commonalities and Limitations of Previous Experiments", "level": "subsection", "subsections": [], "parent_id": "909d9662-92b7-4a13-b530-6c9553233a2e", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Active Learning for Text Classification"], ["subsection", "Commonalities and Limitations of Previous Experiments"]], "content": "\\label{subsec:experiments}\nTable \\ref{table:recent_work} shows the most recent AL for text classification experiments, all of them more recent than the surveys of \\textcite{settles2010active} and \\textcite{olsson2009literature}. \nFor each publication we list the utilized datasets, models, and classes of query strategies (with respect to the taxonomy in Section \\ref{subsec:query_strategies}).\nWe present this table in order to get insights about the recently preferred classification models and query strategy classes.\\\\\n\\pindent We can draw multiple conclusions from Table \\ref{table:recent_work}:\nIt is obvious that a significant majority of these query strategies belong to the class of prediction-based query strategies, more specifically to the prediction-uncertainty and disagreement-based sub-classes. \nIn addition to that, we can identify several shortcomings:\nFirst, in many experiments two or more standard datasets are evaluated, but very often there is little to no intersection between the experiments in terms of their datasets. \nAs a result we lose comparability against previous research.\nFor recent research, this can seen in Table \\ref{table:recent_work}, where the only larger intersections is between the works of \\textcite{zhang2017active} and \\textcite{lowell2019practical}.\n\\textcite{siddhant2018deep} provide at least some comparability against \\textcite{zhang2017active} and \\textcite{lowell2019practical} through one dataset each.\nAdditionally, RMA  is a subset of R21 , which are used by \\textcite{bloodgood2018support} and \\textcite{hu2016active}, so they might be comparable to some degree.\n are the only ones to evaluate on the more recent large-scale text classification datasets , and although these datasets are more realistic in terms of their size, the authors omitted the classic datasets, so it is difficult to relate their contributions to  previous work.\nMoreover, as a result of this, we do not know if and to what degree past experiments generalize to DNNs .\n\\noindent Finally, it is not clear if recent (D)NNs benefit from the same query strategies, i.e. past findings may not apply to modern NN architectures:\n\\textcite{prabhu2019sampling} identified contradicting statements in recent literature about the effectiveness of using prediction uncertainty in combination with NNs.\nThey achieved competitive results using a FastText.zip (FTZ) model and a prediction uncertainty query strategy, which proved to be very effective while requiring only a small amount of data, despite all reported weaknesses concering NNs and uncertainty estimates.", "cites": [11, 1096], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes recent work by organizing experiments into a taxonomy of query strategies, showing how different studies approach AL for text classification. It critically evaluates the lack of dataset overlap and the resulting challenges in comparability, and highlights contradictions in the literature regarding prediction uncertainty and DNNs. The analysis abstracts beyond individual papers to identify broader trends and limitations in the field."}}
{"id": "00aa4769-dd36-4d2f-ba39-18878b0e6be3", "title": "Representations", "level": "paragraph", "subsections": ["6e1cfbc6-56a2-4243-bbb5-f779c6fa69b3", "fd5f0c0a-5428-46a3-b6af-76f50107da5a", "1b466353-341c-4aec-900d-656fddb7ff9e"], "parent_id": "b759602b-ca2d-4e2f-9b70-cf51e332ff01", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Open Research Questions"], ["paragraph", "Uncertainty Estimates in Neural Networks"], ["paragraph", "Representations"]], "content": "As outlined in Section \\ref{subsect:tc_recent_adv}, the use of text representations in NLP has shifted from bag-of-words to static and contextualized word embeddings.\nThese representations evidentially provide many advantages like disambiguation capabilities, non-sparse vectors, and an increase in performance for many tasks.\nAlthough there have been some applications , there is no AL-specific systematic evaluation to compare word embeddings and LMs using NNs.\nMoreover, they are currently only scarcely used,\nwhich hints at either a slow adoption, or some non-investigated practical issues.", "cites": [11, 4023], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a brief analytical perspective by discussing the shift in text representations and noting the lack of AL-specific evaluations for word embeddings and language models. It connects general trends in NLP with active learning challenges, but does not deeply synthesize multiple sources or abstract beyond the immediate context. The critique is limited, pointing out a gap in research without a comprehensive evaluation of existing approaches."}}
{"id": "fd5f0c0a-5428-46a3-b6af-76f50107da5a", "title": "Comparable Evaluations", "level": "paragraph", "subsections": [], "parent_id": "00aa4769-dd36-4d2f-ba39-18878b0e6be3", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Open Research Questions"], ["paragraph", "Uncertainty Estimates in Neural Networks"], ["paragraph", "Representations"], ["paragraph", "Comparable Evaluations"]], "content": "In Section \\ref{subsec:experiments} we provided an overview of the most common AL strategies for text classification.\nUnfortunately, the combinations of datasets used in the experiments are often completely disjoint, e.g. \\textcite{siddhant2018deep}, \\textcite{lowell2019practical}, and \\textcite{prabhu2019sampling}.\nAs a consequence, comparability is decreased or even lost, especially between more recent and past work.\nComparibility is, however, crucial to verify if past insights regarding shallow NN-based AL still apply in context of DNN-based AL .", "cites": [11], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section highlights a key issue in the fieldâ€”lack of comparable evaluations due to disjoint dataset usage across studiesâ€”but does not deeply synthesize the cited papers or connect their specific contributions. It identifies a limitation (i.e., reduced comparability) and its implications for validating past insights in the DNN context, showing some critical perspective. However, the abstraction is limited as it does not generalize this issue to broader principles or propose a meta-level framework for addressing it."}}
{"id": "1b466353-341c-4aec-900d-656fddb7ff9e", "title": "Learning to Learn", "level": "paragraph", "subsections": [], "parent_id": "00aa4769-dd36-4d2f-ba39-18878b0e6be3", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Open Research Questions"], ["paragraph", "Uncertainty Estimates in Neural Networks"], ["paragraph", "Representations"], ["paragraph", "Learning to Learn"]], "content": "There is an abundance of query strategies to choose from, which we have (non-exhaustively) categorized in Section \\ref{subsec:query_strategies}.\nThis introduces the problem of choosing the optimal strategy.\nThe right choice depends on many factors like data, model, or task, and can even vary between different iterations during the AL process.\nAs a result, {\\em learning to learn} (or {\\em meta-learning}) has become popular and can be used to learn the optimal selection , or even learn query strategies as a whole .", "cites": [8718, 4027], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section briefly introduces the concept of meta-learning for active learning and references two papers that propose data-driven approaches to learning query strategies. It synthesizes these ideas by highlighting the shift from heuristic-based to learned strategies. While it offers some abstraction by framing the issue as one of adaptability and learning across tasks, it lacks deeper critical evaluation of the approaches or limitations. The discussion remains at a general level without detailed comparative insights."}}
{"id": "3ba159ca-0eee-4f69-aedd-91724bce7349", "title": "Conclusions", "level": "section", "subsections": [], "parent_id": "4e01ebc7-074c-4060-b09d-956a012ff8ea", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Conclusions"]], "content": "In this survey, we investigated (D)NN-based AL for text classification and inspected factors obstructing its adoption.\nWe created a taxonomy, distinguishing query strategies by their reliance on data-based, model-based, and prediction-based input information.\nWe analyzed query strategies used in AL for text classification and categorized them into the respective taxonomy classes.\nWe presented the intersection between AL, text classification and DNNs, which is to the best of our knowledge the first survey of this topic.\nFurthermore, we reviewed (D)NN-based AL, identified current challenges and state of the art, and pointed out that it is both underresearched and often lacks comparability.\nIn addition to that, we presented relevant recent advances in NLP, related them to AL, and showed gaps and limitations for their application.\nOne of our main findings is that uncertainty-based query strategies are still the most widely used class, regardless of whether the analysis is restricted to NNs.\nLM-based representations offer finer-grained context-specific representations while also handling out-of-vocabulary words. \nMoreover, we find fine-tuning-based transfer learning alleviates the small data problem to some degree but lacks adoption.\nMost important DNNs are known for their strong performance on many tasks and first adoptions in AL have shown promising results .\nAll these gains would be highly desirable for AL.\nTherefore improving the adoption of DNNs in AL is crucial, especially since the expected increases in performance could be either used to improve the classification results while using the same amount of data or to increase the efficiency of the labeling process by reducing the data and therefore the labeling efforts.\nBased on these findings we identify research directions for future work in order to further advance (D)NN-based AL.\n\\section*{Acknowledgements}\nWe thank Gerhard Heyer for his valuable feedback on the manuscript, Lydia MÃ¼ller for fruitful discussions about the taxonomy and advice thereon, and Janos Borst for sharing his thoughts on recent advances in language models.\nThis research was partially funded by the Development Bank of Saxony\n(SAB) under project number 100335729.\n\\printbibliography\n\\appendix", "cites": [4023], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides analytical insights by synthesizing findings from the survey, integrating the taxonomy of query strategies, and connecting them to broader trends in NLP and DNNs. It abstracts from individual papers to identify overarching patterns, such as the dominance of uncertainty-based strategies and the underutilization of transfer learning. While it offers some critical evaluation of current limitations, the critique could be more detailed and nuanced."}}
{"id": "01e0ed63-a97a-4079-8877-7a6544e5b88a", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "ca5cf519-fe65-47a3-9e14-707edbf02174", "prefix_titles": [["title", "A Survey of Active Learning for Text Classification using Deep Neural Networks"], ["section", "Appendix"], ["subsection", "Datasets"]], "content": "The following table provides additional information about the datasets which were referred to in Section \\ref{subsect:active_tc}. \n{\n\\setlength{\\LTcapwidth}{\\textwidth}\n\\begin{longtable}{l p{3.8cm} l p{4.1cm} r r}\n  \\hline\n  \\textbf{Id} & \\textbf{Name} & \\textbf{Type} & \\textbf{Publication} & \\textbf{\\#Train} & \\textbf{\\#Test} \\\\\n  \\hline\n  TQA & TREC QA & MC &  & 5,500 & 500 \\\\\n  CR & Customer Reviews & MC &  & \\textsuperscript{*}{315} & - \\\\\n  ACR & Additional Customer\\newline Reviews & MC &  & \\textsuperscript{*}{325} & - \\\\\n  MDS & Multi-Domain Sentiment & B &  & \\textsuperscript{**}{8,000} & - \\\\\n  BAG & Blog Author Gender & B &  & 3,100 & - \\\\\n  G13 & Guardian 2013 & MC &  & 6,520 & -\\\\\n  MR & Movie Reviews & B &  & 10,662 & - \\\\\n  MRL & Movie Reviews Long & B &   & 2,000 & - \\\\\n  MUR & Music Review & B &  & 2,000 & - \\\\\n  DR & Doctor Reviews & MC &  & 58,110 & - \\\\\n  SJ & Subjectivity & B &  & 10,000 & -\\\\\n  20N & 20newsgroups & MC &  & \\textsuperscript{***}18,846 & - \\\\\n  R21 & Reuters-21578 & ML &  & 21578 & - \\\\\n  RMA & Reuters ModAptÃ© & ML &  & 9,603 & 3,299\\\\\n  RV2 & RCV1-V2 & ML &  & 23,149 & 781,265 \\\\\n  SPM & Spam & B &  & 1,000 & - \\\\\n  AGN & AG News & MC & \\newline & 120,000 & 7,600 \\\\\n  SGN & Sogou News & MC &  & 450,000 & 60,000\\\\\n  DBP & DBPedia & MC &  & 560,000 & 70,000\\\\\n  YRP & Yelp Review Polarity & B &  & 560,000 & 38,000\\\\\n  YRF & Yelp Review Full & MC &  & 650,000 & 50,000\\\\\n  YAH & Yahoo! Answers & MC &  & 1,400,000 & 60,000\\\\\n  ARP & Amazon Review Polarity & B &  & 3,600,000 & 40,000\\\\\n  ARF & Amazon Review Full & MC &  & 3,000,000 & 650,000\\\\\n  \\hline\n  \\caption{A collection of widely-used text classification datasets.\n  The column \"Type\" denotes the classification setting (B = binary, MC = multi-class, ML = multi-class multi-label). \n  The columns \"\\#Train\" and \"\\#Test\" show the size of the train and of the test set.\n  In the case that no predefined splits were available \"\\#Train\" represents the full dataset's size. \n  Each dataset was assigned a short id (first column), which we use in the paper for reference.\\\\\n  \\newline\n  (*): documents, (**) labels reduced to positive/negative, (***) 20news-bydate with duplicates removed}\n  \\label{tab:datasets}\n\\end{longtable}\n}\n\\end{document}", "cites": [1096, 4028, 8420], "cite_extract_rate": 0.17647058823529413, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.3}, "insight_level": "low", "analysis": "The section provides a descriptive list of datasets used in active learning for text classification but does not synthesize or integrate insights from the cited papers. It lacks critical evaluation or comparison of the papers' approaches and contributes little in terms of abstraction or meta-level understanding of the field."}}
