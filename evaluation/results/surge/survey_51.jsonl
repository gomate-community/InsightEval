{"id": "c569bf52-61b4-43ce-991b-c83ebdd2905a", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "1352d6e5-225d-41b6-be40-dfba979ded66", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=.89\\columnwidth]{Figs/tesser3.pdf}\n  \\caption{A general framework for acoustic sensing.}\n  \\label{fig:system overview}\n\\end{figure}\nInternet of Things (IoT)~ technologies enable everyday objects to connect and communicate with each other by augmenting them with sensing, processing, and computation units. \nWith the ever increasing computation power and rich built-in sensors available in IoT devices, novel applications emerge by repurposing sensors beyond their primary use. For instance, cameras are intended for taking photos but have been utilized in visible light communication~. Gyroscope and accelerometer sensors are designed for attitude estimation but have been used extensively in activity recognition~. WiFi signals, originally used for communication, have been widely applied in many context-aware computing applications including localization~ and gesture recognition~. \nIn this paper, we target innovative sensing mechanisms that exploit acoustic front-ends on commodity IoT devices. \nAcoustic front-ends, namely microphones and speakers, are among of the most commonly used transducers in IoT devices. They are generally designed for playing back and recording audio signals, but also play a pivotal role in passive sensing applications such as speech recognition~ and acoustic source localization~.\nNovel active sensing mechanisms that treat acoustic front-ends as transceivers to emit and capture wireless signals have gained a lot interests in the research community. \nFor instance, acoustic signals have been used to establish aerial acoustic communication channels to transmit a small amount of information~. \nAlso, the reflective property of acoustic signals have enabled the development of acoustic short-range radars for floor map reconstruction~ and gesture recognition~. \nMoreover, the relatively slow propagation speed of acoustic waves (compared to, e.g., Radio Frequency or RF) in common media allows to achieve comparable performance using a relatively low bandwidth than those with RF technologies. Consequently, it is possible to achieve accurate Time-of-Flight estimations that further support many context-aware applications~.\nLast but not least, \nactive acoustic sensing can enable deformity detection and estimation of non-acoustic emitting objects by transmitting purposefully modulated acoustic signals and make inference based on the reflected waveforms captured by microphones~. \n\\begin{figure*}[b]\n  \\centering\n    \\subfigure[Sound recording system.]\n    {\n        \\label{fig:a5}\n        \\includegraphics[width=1.3\\columnwidth]{Figs/sound_recording.pdf}\n    }\n    \\\\\n    \\vspace{0.002\\textwidth}\n    \\subfigure[Sound playback system.]\n    {\n        \\label{fig:b5}\n        \\includegraphics[width=1.3\\columnwidth]{Figs/sound_playback.pdf}\n    }\n  \\caption{Diagrams for typical acoustic hardware.}\n  \\label{fig:microphone and speaker}\n\\end{figure*}\nDespite tremendous efforts in developing acoustic sensing applications in the past decade, a systematic treatment of fundamental principles, key design considerations, and innovative methodologies is still missing. As a result, when developing applications based on acoustic sensing, researchers and developers often have to start from scratch and reinvent the wheel. \nIn this paper, we provide the first systematic survey on recent advances in acoustic sensing with emphasis on novel sensing approaches on commodity hardware (with a bandwidth below ${24}$ kHz), as opposed to those that require special-purposed hardware such as underwater acoustic communication or ultrasonic sensing. We review the relevant research in a bottom-up manner, from the physical layer, core technique layer, and application layer, as shown in Fig.~\\ref{fig:system overview}. The physical layer includes basic hardware components, acoustic platforms as well as the air-borne and structure-borne channel characteristics.\nThe core technique layer encompasses key mechanisms to generate acoustic signals (waveforms) and to extract useful temporal, spatial and spectral information from received signals. \nThe application layer builds upon the functions offered by the core techniques to realize different acoustic sensing applications. We group acoustic sensing applications into  aerial acoustic communication, applications leveraging temporal features such as ranging, acoustic radar, acoustic localization and tracking, applications enabled by estimating acoustic channel characteristics such as gesture recognition, speaker liveliness detection and interactive controls. \nWe highlight unique challenges due to the limitations of physical devices and acoustic channels and how they are mitigated or overcame by core processing techniques and application-specific solutions.\nAlong each category of applications, we discuss research opportunities for further investigation. Finally, we summarize under-investigated areas and emerging applications in acoustic sensing as a whole. \nThe remainder of this paper is organized as follows. In Section~\\ref{sec:device or hardware background}, we introduce typical hardware components and system supports offered by commodity devices and the properties of acoustic channels. In Section~\\ref{sec:core acoustic mechanisms}, we present the core techniques to generate acoustic signals and to extract temporal and channel features from received signals.\nIn Section~\\ref{sec:solution or application},  a variety of applications are discussed in details according to the core techniques on which they are based;\nwe discuss research opportunities for each category separately and present future directions in Section~\\ref{sec:future direction}.\nFinally, we conclude the paper in Section~\\ref{sec:conclusion}.", "cites": [1695], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The introduction section provides a general overview of acoustic sensing on IoT devices, mentions several application areas, and introduces a three-layer framework. However, it does not effectively synthesize or integrate the cited papers, nor does it engage in critical evaluation or abstraction. The section is primarily descriptive in nature, outlining concepts and categorizing applications without deeper analysis or synthesis of the literature."}}
{"id": "2c67dbe8-fd43-495f-9419-b78c614400ca", "title": "Pure Tone Signals", "level": "paragraph", "subsections": [], "parent_id": "81396aea-2f93-4430-8227-b6862ecb550e", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Core Acoustic Sensing Techniques"], ["subsection", "Waveform Design"], ["subsubsection", "Waveforms for Active Sensing"], ["paragraph", "Pure Tone Signals"]], "content": "Pure tone signals are commonly used in acoustic sensing due to their low complexity and high resolutions in tracking Doppler shifts. Their  waveform is represented as ${s\\left(t\\right) = \\cos\\left( 2\\pi ft + \\phi \\right)}$ where ${f}$ and ${\\phi}$ are the frequency and the initial phase. The waveform and its Short Time Fourier Transform (STFT) spectrogram of a single tone signal are shown in Fig.~\\ref{fig:pure tone waveform} and \\ref{fig:pure tone stft}, respectively. \nConsider that a moving target transmits a pure tone signal with frequency ${f}$, and the detected Doppler frequency is ${f_\\mathrm{shifted}}$ at the receiver side. The relative moving speed can thus be estimated as ${v = \\frac{f_\\mathrm{shifted} - f}{f}c}$, where ${c}$ is the sound speed. Due to the low propagation speed of sound, it is feasible to achieve a cm/s-level estimation accuracy.  \nMoreover, \nif phase components across multiple pure tones are available, the phase diversity yields multiple constraints to obtain more accurate phase and frequency estimations~. \nAs phase and frequency shifts are correlated with spatial quantities such as range and speed, pure tone signals have been extensively used in localization and tracking applications ~, as well as gesture recognition~. However, they are not suitable for extracting precise timing information due to the periodicity and shallow peaks of auto-correlation functions.", "cites": [1695], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of pure tone signals and their properties, such as Doppler shift estimation and phase diversity. It does not synthesize or integrate the cited paper meaningfully, nor does it offer critical evaluation or comparison of approaches. The content remains at a descriptive level with limited abstraction beyond specific examples."}}
{"id": "da40022d-71f7-47b3-921c-09fdcca5adf5", "title": "Channel Model Construction", "level": "subsubsection", "subsections": [], "parent_id": "5cd952a0-5695-4e0a-bba9-229ae4e0ed56", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Core Acoustic Sensing Techniques"], ["subsection", "Channel Profiling"], ["subsubsection", "Channel Model Construction"]], "content": "\\label{sec:channel model construction}\nConstructing an appropriate channel model is the cornerstone for acoustic sensing application development. A canonical data flow of common channel modeling methods are shown in Fig.~\\ref{fig:channel modeling}. \nThe basic idea behind channel model construction is to map respective states, say hand moving directions, to certain CSI representations, say Doppler frequency. A naive approach, also called \\textit{Data-Driven} (DD), to construct this model is through direct mapping, which often involves massive data and intensive training. Another method, known as \\textit{Model-Driven} (MD), formulates this mapping with a close-formed expression, which are often more efficient but non-trivial to achieve. The respective advantages, disadvantages, and suitable applications for these two methods are displayed in TABLE~\\ref{tab:processing technique comparison}. In this section, we highlight the basic steps for aforementioned two common channel model construction methods. \n\\begin{table}[b]\n\\caption{Comparison of MD and DD approaches.}\n\\label{tab:processing technique comparison}\n\\centering\n\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n & \\textbf{Problem} & \\textbf{Advantages} & \\textbf{Disadvantages} & \\textbf{\\makecell{Suitable\\\\applications}}\\\\ \\hline\nMD & \\makecell{Regression\\\\ problem} & \\makecell{Effective\\\\and\\\\efficiency} & \\makecell{Require domain\\\\knowledge,\\\\time-consuming} &\\makecell{Temporal\\\\feature\\\\/channel\\\\characteristics}\\\\ \\hline\nDD & \\makecell{Categorical\\\\ problem} & \\makecell{Simple\\\\in\\\\model\\\\designs} & \\makecell{Massive data,\\\\computation-\\\\intensive,\\\\heterogeniety\\\\problem}& \\makecell{Channel\\\\characteristics} \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\begin{table*}[t]\n\\centering\n\\caption{Categorizing acoustic sensing enabled applications.}\n\\label{table:application comparison}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n\\textbf{Category}   &  \\textbf{Key physical layer }   & \\makecell{\\textbf{Core processing techniques}}  & \\textbf{Applications} \\\\ \\hline\n\\makecell{Aerial acoustic \\\\ communication} & \\makecell{Acoustic hardware,\\\\acoustic channel property} & Waveform design & Communication~ \\\\ \\hline\n\\multirow{4}{*}{\\makecell{Temporal feature\\\\based applications}} & \\multirow{4}{*}{\\makecell{Acoustic hardware,\\\\acoustic channel property}} &     \n    \\makecell{One-way sensing/\\\\ Phase-enabled accurate timing} & Ranging~ \\\\ \\cline{3-4} \n    &  & \n    \\makecell{Signal onset point detection,\\\\Phase-enabled accurate timing} & Acoustic radar~ \\\\ \\cline{3-4} &  & \n    \\makecell{Signal onset point detection,\\\\one-way/two-way sensing} & Localization~ \\\\ \\cline{3-4} \n    &  & \n    \\multirow{3}{*}{Phase-enabled accurate timing} & \n        Device-based tracking~ \\\\ \\cline{4-4} \n        &  &  &  \n        Device-free gesture tracking~ \\\\ \\cline{4-4} \n        &  &  & \n        \\makecell{Biometric sensing~ \\\\ } \\\\ \\hline\n\\multirow{2}{*}{\\makecell{Channel characteristics\\\\enabled applications}} \n& \\multirow{2}{*}{\\makecell{Acoustic hardware,\\\\acoustic channel property,\\\\platform diversity}} & \n    \\multirow{3}{*}{Over-the-air channel profiling} & \n        Gesture recognition~ \\\\ \\cline{4-4} \n        & & & \n        \\makecell{Speaker authentication~} \\\\ \\cline{4-4}\n        & & & \n        \\makecell{Novel interactive control~} \\\\ \\cline{3-4} \n    &    &   \n    \\multirow{3}{*}{\\makecell{Structure-borne channel profiling}} & \n        Keystroke detection~ \\\\ \\cline{4-4} \n        & & & \n        \\makecell{Force detection~} \\\\ \\cline{4-4}\n        & & & \n        \\makecell{Touch recognition~} \\\\ \\cline{4-4}\n    \\hline\n\\end{tabular}\n\\end{table*}\nModel-driven approach, formulating acoustic sensing problem as regression, can be effective and efficient, which however, require sophisticated signal processing designs and specific domain knowledge hence is often remarkable challenging. The key insight behind MD approaches is to mathematically quantify the relationship between CSI and certain application-specific states through regression, say mapping hand moving direction to specific Doppler frequency shift in a close-form equation. Actually, the hard part of this approach is to discovery the respective CSI and make it notable via signal processing techniques. Additionally, this approach may only be suitable when CSI is a scalar variable. Since these signal processing techniques are application-specific, we hence postpone their discussions in the next section. \nSince a close-form or explicit parametric model is often hard to craft, one often resort to DD approach. The DD approach, taking acoustic sensing as categorical problem, implicitly builds the inference model by directly mapping CSIs with respective application-specific states. \nThis approach is more suitable to handle the cases when CSI is in the format of a vector, say frequency response, or matrix, say MFCC. \nThe basic idea behind DD approach is fingerprinting. \nThis approach first collects sufficient data (CSIs) under respective states in an offline manner and then utilize these data to train a model using machine learning techniques. The model is then used online to predict the corresponding state with respect to its input data. Such a processing pipeline may require less sophisticated domain knowledge hence is comparatively easier than MD approaches.  \nNevertheless, this approach requires massive data or constant calibrations due to device heterogeneity problem hence is labor intensive. Meanwhile, high computational cost and storage requirement are other drawbacks for this method.", "cites": [1695], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 22, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparative overview of model-driven (MD) and data-driven (DD) approaches for channel model construction, highlighting their problem types, advantages, and disadvantages. It synthesizes the key ideas from the cited paper by linking the concept of model-agnostic meta-learning to the DD approach, though the connection is not deeply elaborated. The section offers some critical evaluation by discussing limitations like data intensity and domain knowledge requirements but stops short of deeper analysis or identifying broader research trends."}}
{"id": "a242203e-70c4-4f1d-87e0-5060de1aa6fb", "title": "Localization", "level": "subsubsection", "subsections": [], "parent_id": "98b8d49f-bd0a-4043-938a-589f8b510dfa", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Acoustic Sensing Applications"], ["subsection", "Applications Leveraging Temporal Features"], ["subsubsection", "Localization"]], "content": "\\label{sec:localization}\nLocalization is a key enabler for Location Based Service (LBS). Despite tremendous research efforts on indoor localization, many existing solutions either require expensive dedicated infrastructures~ or rely on cumbersome device-dependent kernel hacking~, prohibiting their practical deployment. \nAmong existing cutting-edge indoor localization approaches, acoustic-based systems attract much interests since they can achieve sub-meter level localization accuracy with relatively low infrastructure costs and deployment efforts. The underlying techniques for these localization systems are the timing measurement methods discussed in Section~\\ref{sec:basic timing measurements} and the major challenge is \\textit{\\textbf{Challenge}}-\\ref{clg:robust onset detection}. The evaluation metric for localization systems is RMS error. \nIn this section, we present existing works on acoustic-enabled localization solutions in two categories, namely, \\emph{infrastructure-based} and \\emph{infrastructure-free}. A comparison of related work is summarized in TABLE~\\ref{table:localization comparison}.\n\\textbf{Infrastructure-based} schemes typically deploy low-cost and power-efficient distributed acoustic anchors in target areas. The locations of these anchors are determined in advance. Apart from acoustic transceivers, each anchor may be equipped with wireless modules to communicate among themselves or with a remote server. The remote server can coordinate the transmission schedule among the anchors either in synchronous or asynchronous manner. When the transmitted acoustic signals are detected by either a target or other anchors, the associated timestamps (ToA or TDoA) are obtained. Finally, the location of a target is determined using trialeration or more sophisticated optimization methods. \nIn the subsequent discussion, we first review synchronous schemes, and then asynchronous approaches. \nIn ~, Liu et al. developed a centimeter-level localization system named Guoguo. The anchors in this system are synchronized by Zigbee and are scheduled to transmit orthogonal codes, which are used by targets to perform ToA estimation via one-way sensing. Multilateration is then used to locate the targets.\nA speaker-only localization system was proposed by Lazik and Rowe in~. In their approach, distributed speakers are connected to different synchronized channels of an advanced audio device that transmits chirp signals for localization. A target locates itself locally by performing one-way TDoA estimation. According to , its ${95}$-percentile localization accuracy is within ${10}$~\\!cm. ALPS~ improves upon the work~ in deployment efforts. In ALPS, anchors are synchronized via Bluetooth, and each anchor is equipped with one microphone and one speaker. The locations of the anchors are efficiently obtained through acoustic-assisted simultaneously localization and mapping. ALPS reports average errors of ${30}$~\\!cm and ${16.1}$~\\!cm in locating targets and anchors. Recently, a ultrasonic localization system called UPS+ is presented in~. UPS+ leverages the non-linearity of receiver microphones (details are presented in Section~\\ref{sec:future direction for channel characteristics}) to enable ultrasonic beacons to locate smart devices without ultra-sonic sensors. Consequently, the audibility problem, induced either by \\textit{\\textbf{Challenge}}-\\ref{clg:audibility} or \\textit{\\textbf{Challenge}}-\\ref{clg:timing estimation trade-off}, is eliminated. UPS+ achieves centimeter-level accuracy in localization. \n\\begin{table*}[b]\n\\caption{Comparison of device-free gesture tracking systems.}\n\\label{table:device-free gesture tracking comparison}\n\\begin{center}\n    \\begin{tabular}{ | c | c | c | c | c | c |}\n    \\hline\n    \\textbf{System} & \\textbf{Waveform Design} & \\textbf{\\makecell{Occupied bandwidth\\\\(kHz)}} & \\textbf{\\makecell{Tracking latency (ms)}} & \\textbf{\\makecell{Operation range\\\\(m)}}& \\textbf{\\makecell{Performance}} \\\\ \\hline\n    FingerIO~ & OFDM modulated signal  & ${18 - 20}$ & ${5.92}$  &  ${< 0.5}$ & \\makecell{${8}$~\\!mm (2D) average\\\\tracking error}\\\\ \\hline\n    LLAP~ & Multiple pure tones & ${17 - 23}$  & ${\\le 15}$   & ${0.5}$ & \\makecell{${3.5}$~\\!mm (${1}$D) and ${4.57}$~\\!mm (${2}$D)\\\\average tracking error} \\\\ \\hline\n    Strata~ & GSM sequence   & ${18 - 22}$  & ${12.5}$  & ${0.5}$  & ${3}$~\\!mm tracking error \\\\ \\hline\n     & Chirp signal   & ${18 - 20}$  & ${40}$  & ${4.5}$ & ${1.2}$ to ${3.7}$~\\!cm within 4.5~\\!m\\\\\n    \\hline\n    CovertBand~ & OFDM   & ${18 - 20}$  & ${4.2}$  & ${6}$ & \\makecell{A median of ${18}$~\\!cm\\\\tracking error} \\\\\n    \\hline\n    \\end{tabular}\n\\end{center}\n\\end{table*}\nThe localization accuracy of the aforementioned work is highly dependent on clock synchronization accuracy, which depends on network latency, non-negligible in a large-scale network. In contrast, asynchronous approaches can overcome such shortcomings.\nARABIS  is an asynchronous acoustic localization system that utilizes two-way ranging~ to avoid the need for synchronization. In ARABIS, anchors transmit acoustic beacons periodically following a coarse time-division-multiple-access schedule. Targets, as well as anchors, overhear the transmissions and record the corresponding timestamps. These timestamps can be used to estimate TDoA information in locating a target. ARABIS reports a ${95}$-percentile localization error of ${7.4}$~\\!cm. AALTS~ improves upon ARABIS by a more robust onset detection approach to handle the near-far problem, hardware heterogeneity, and multipath effects. To handle these challenges (from \\textit{\\textbf{Challenge}}-\\ref{clg:robust onset detection}), it normalizes the current correlation value by the mean of a number of its preceding samples. Additionally, a pseudo orthogonal chirp spread spectrum modulation technique is proposed, which effectively doubles the transmission rate. AALTS achieves $90$-percentile tracking errors of $0.49$~\\!m for mobile targets and a median of $0.12$~\\!m for stationary ones with only four anchor nodes. \n\\textbf{Infrastructure-free} localization systems do not require the deployment of custom-built infrastructure devices in target areas. However, they tend to achieve less competitive localization accuracy compared with infrastructure-based solutions.\nIn~, Liu et al. built a localization system utilizing acoustic and WiFi signals. It first estimates pair-wise distances within a device group via acoustic ranging~, forming a spatial constraints. \nEach device in the group also uses WiFi fingerprints to impose another location constraints. By combining the two, target locations can be determined.\nThis scheme achieves an ${80}$-percentile localization error of ${1}$~\\!m. Since the computation of  spatial constraints requires multiple pair-wise acoustic ranging  measurements and is time consuming, the application of such an approach is limited to  static target localization. \nCentaur~, similar to , is a joint optimization framework utilizing acoustic and WiFi signals, and reports meter-level localization accuracy. The authors propose a novel multipath mitigation algorithm to address \\textit{\\textbf{Challenge}}-\\ref{clg:robust onset detection}, and achieve robust onset detection. The key idea is to inspect signal changes in cross-correlation as opposed to absolute magnitudes considered in existing methods. \nEchoTag~ is an acoustic  fingerprinting localization system that can detect minor location changes. It associates different acoustic profiles with different positions, known as tags, to train a classification model. This model is then used for online tag detection, enabling context-aware applications. \nEchoTag reports an accuracy of ${98\\%}$ in distinguishing ${11}$ tags at ${1}$~\\!cm resolution. However, EchoTag is sensitive to environmental dynamics and will suffer from degraded performance in absence of new data collections. As a matter of fact, applications that are based on a fingerprinting strategy are subject to problems aroused from \\textit{\\textbf{Challenge}}-\\ref{clg:calibration} and \\textit{\\textbf{Challenge}}-\\ref{clg:cross-platform models}, limiting their practical adoption. \nAlthough infrastructure-free solutions incur less hardware costs, they require labor-intensive site survey to obtain location-dependent signal profiles, making them sensitive to environment changes. Infrastructure-based approaches deliver a satisfactory localization accuracy at the cost of extra hardware. But the complexity in deploying multiple acoustic anchors and the synchronization requirements are still not economical and lightweight enough for practical deployment. To this end, the authors of~ propose a single beacon-enabled passive localization system that can also identify a target. They discover that a footstep contains separable structure-borne and air-borne components. The former contains range information and the latter provides angle-of-arrival (AoA) along with identity signatures. Consequently, by placing a single acoustic array in the place of interest, a target can be simultaneously tracked and identified. Additionally, the domain adversarial training technique is employed in this proposal so as to enhance the generalizability of the system, easing the efforts in calibration and thus addressing \\textit{\\textbf{Challenge}}-\\ref{clg:cross-platform models}. The reported median localization accuracy can reach 30~\\!cm, which is highly enough given that a foot has a similar size. Another single acoustic anchors based proposal that leverage the geometry constraints shaped by LoS and NLoS acoustics can be found in~ that reports 0.44~\\!m localization accuracy across different environments. It is worth to mention that the aforementioned microphone array enabled localization techniques can obtain specific coordinates of a target rather than conventional source localization that can only obtain AoA information. \nCompared to localization solutions utilizing RF signals~, visible light~, or IMU data~, acoustic-enabled localization techniques strike good trade-offs between costs and accuracy. The acoustic diffraction property makes it feasible to locate targets in presence of small-scale random blockages. This puts less restriction on deployment. However, due to limited transmission ranges, more anchor nodes are needed in infrastructure-based localization systems.   \n\\begin{table*}[b]\n\\centering\n\\caption{Comparison between different biometric sensing systems.}\n\\label{tab:biometric sensing comparison}\n\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n\\textbf{Work} & \\textbf{Waveform design}  & \\textbf{Bandwidth} & \\textbf{Key techniques} & \\textbf{Performance}\\\\ \\hline\n & FMCW & $18-20$~\\!kHz & FFT &  \\makecell{Less than 0.11~\\!bpm\\\\within 1m} \\\\ \\hline\nBreathJunior~ & FMCW \\& white noise & 24~\\!kHz & \\makecell{Phase-based accurate timing,\\\\beamforming} & \\makecell{0.4~\\!bpm at 40~\\!cm,\\\\3~\\!bpm at 60~\\!cm}\\\\ \\hline\nRespTracker~ & ZC & 2~\\!kHz & \\makecell{Phase-based accurate timing} & \\makecell{Less than 1~\\!bpm at 3~\\!m,\\\\0.8~\\!bpm for moving targets}\\\\ \\hline\n & NA & NA & \\makecell{Envelop detection} & \\makecell{Less than 0.05~\\!bpm\\\\(device close to user)}\\\\ \\hline\nBreathListener~ & tone at 20~\\!kHz  & NA & \\makecell{Energy spectrum density,\\\\ensenmble empirical mode decomposition,\\\\generative adversarial network} & \\makecell{0.11~\\!bpm\\\\in driving environment}\\\\ \\hline\nSpiroSonic~ & Multiple tones  & $17-24$~\\!kHz & \\makecell{Phase-based accurate timing,\\\\neural network regression} & \\makecell{5\\%-10\\% error in\\\\lung function monitoring}\\\\ \\hline\n\\end{tabular}\n\\end{table*}", "cites": [1695], "cite_extract_rate": 0.2608695652173913, "origin_cites_number": 23, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively compares infrastructure-based and infrastructure-free acoustic localization systems, integrating information about techniques and performance from multiple works. It references key challenges and improvements, but lacks deeper critical evaluation of methodologies or broader theoretical insights. The narrative is structured and informative, yet remains largely descriptive in nature with limited abstraction beyond the cited systems."}}
{"id": "25f70ee6-6676-4265-9de9-e6288503ee44", "title": "Tracking", "level": "subsubsection", "subsections": [], "parent_id": "98b8d49f-bd0a-4043-938a-589f8b510dfa", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Acoustic Sensing Applications"], ["subsection", "Applications Leveraging Temporal Features"], ["subsubsection", "Tracking"]], "content": "\\label{sec:tracking}\nHigh-accuracy object tracking is important in many applications such as automated surveillance, traffic monitoring, and Augmented/Mixed Reality~.\nTracking is a well-investigated topic in computer vision (CV)~. However, CV techniques impose substantial computation costs and do not work well under poor light conditions. Acoustic tracking systems can overcome these limitations. Depending on whether the targeted object can emit acoustic signals or not, existing solutions can be divided into device-based tracking and device-free tracking. \n{\\bf Device-based acoustic tracking} aims to track acoustic emitting devices in motion. \nAAMouse~ utilizes multiple carriers to estimate the Doppler speed of a target and integrates the speed over time for tracking. Though the reported tracking performance is at centimeter-level, tracking errors can accumulate over time, making it unsuitable for long-term tracking. CAT~ improves upon AAMouse by adopting chirp mixing and boosts the tracking accuracy to a sub-centimeter level. However, due to the use of one-way sensing, it is sensitive to irregular SFOs~ that cannot be easily compensated. Under the assumption of linear drift, CAT performs re-calibration when integration errors become intolerable hence allow the system to work sufficiently long. \nAnother application of chirp mixing for high-accuracy tracking is presented in~ where a drone follows a person with a safe range in challenging indoor environments. In this work, the authors introduce several advanced signal processing modules, in particular, MUlitple SIgnal Classification algorithm (MUSIC) to resolve multipath effects (aroused from \\textit{\\textbf{Challenge}}-\\ref{clg:robust onset detection}) and enhance system robustness. Furthermore, a reciprocal filter is introduced to address \\textit{\\textbf{Challenge}}-\\ref{clg:freq_sel}, compensating the frequency selectivity problem and thereby further enhancing the system stability.\nIn Backdoor~, a more precise compensation technique for frequency selectivity is proposed. First adopted in wireless communication, it equalizes channel effects by measuring channel state information  using probe signals. Backdoor transmits acoustic signal in 40~\\!kHz and takes advantages of non-linear diaphragm of power-amplifier so that sounds in the range of 20~\\!kHz can be recorded. Thus, it does not suffer from the audibility problem ({\\it{\\bf Challenge}}-\\ref{clg:audibility} or \\textit{\\textbf{Challenge}}-\\ref{clg:bw_tradeoff}). However, this approach requires customized hardware devices, making its adoption more difficult. \nMilliSonic~ achieves sub-millimeter 1D tracking accuracy in the presence of multipath using a single beacon with a small 4-microphone array. The high precision is achieved by leveraging phase information after chirp mixing. \n{\\bf Device-free acoustic tracking} tracks moving objects in an environment by reflected acoustic signals. Due to significant attenuation of reflected signals, high precision device-free acoustic tracking is often limited to short ranges. Next, we use finger, body posture tracking and respiration sensing as driving applications to discuss techniques in this category. \nFingerIO turns a mobile phone or a smartwatch into an active sonar that is capable of tracking moving fingers for Around Device Interaction (ADI). Such a technology can extend the physical interaction boundaries hence is particular useful for small wearables. FingerIO achieves a median accuracy of ${8}$ mm~ in 5.92~\\!ms. \nIt utilizes OFDM modulated signals to estimate the CSI between a hand and a smartphone periodically. In each estimation cycle, the CSI is acquired through cross-correlation. Since only moving fingers can dynamically affect the channel between the speaker and microphone, their movements can be tracked by comparing consecutive channel frames. Static multipath reverberations remain the same across frames and can thus be removed. The proposed technique successfully addresses \\textit{\\textbf{Challenge}}-\\ref{clg:robust onset detection} by extracting finger-movement-only CSI profiles, and inspired several follow-up work.\nA multi-tone device-free gesture tracking system, LLAP, was proposed in~. It leverages coherent detection to extract the phases of acoustic echoes for finger localization and tracking. In LLAP, a mobile device actively transmits multiple tone carriers and decomposes finger-generated echoes via Empirical Mode Decomposition (EMD) for later processing. It further uses the phase divergence of multiple carriers to coarsely locate the start position of a finger and track its displacement via phase shifts.\nLLAP reports a tracking accuracy of ${3.5}$ mm for ${1}$D hand movement and ${4.57}$ mm for ${2}$D drawing with less than ${15}$ ms latency. However, both FingerIO and LLAP are sensitive to nearby interference. To address nearby interference, another work named Strata was proposed in~. \nIt also uses a coherent detector but applies a GSM training sequence modulated by Binary Phase Shift Keying (BPSK). \nEvaluation results demonstrate that it outperforms FingerIO and LLAP in all cases with an average tracking accuracy at 3~\\!mm. \nThe aforementioned work considers micro-finger gesture tracking. For macro-body parts such as hand or the whole body, one often utilizes more powerful speakers to increase SNR so that the sensing range is larger. We hereby present tracking technologies on macro-body posture. \nThe work in~ shows that it is feasible to achieve room-level hand motion tracking with a customized platform. It employs an acoustic radar along with many advanced processing techniques including MIMO beamforming and deep learning for signal quality enhancement. The proposed system achieves $1.2$-$3.7$~\\!cm tracking errors within $4.5$~\\!m range and supports multi-user tracking. CovertBand~ is an active sensing system for passive multiple object tracking. It builds on an active sonar with an enhanced speaker and uses the same parametric models as FingerIO~ to track human body posture. CovertBand reports a median of $18$~cm in tracking mobile targets. For static objects, it can achieve an accuracy of $8$~cm with a distance up to $8$~m in LOS conditions. \nA comparison of these tracking schemes is given in TABLE~\\ref{table:device-free gesture tracking comparison}. \nThe last category of applications concern continuous monitoring of human's respiration rates and breathing patterns. With device-free acoustic tracking, one can estimate chest displacements caused by respiration over time of target subjects. A comparison of these techniques is shown in TABLE~\\ref{tab:biometric sensing comparison}.\nIn~, a portable life sign detection system based on commodity smartphones was presented. It uses a smartphone as an active sonar to detect chest movements for breathing rate estimation and sleep apnea detection. The proposed system can achieve an error of fewer than ${0.11}$ breaths per minute (bpm) even at a distance of up to ${1}$~\\!m. Though the work of~ utilizes an inaudible frequency range (above $18$~\\!kHz), it can still be perceived by animals and infants whose hearing systems are more sensitive to high frequency sounds. \nTo deal with this audibility issue (\\textit{\\textbf{Challenge}}-\\ref{clg:audibility}) and handle \\textit{\\textbf{Challenge}}-\\ref{clg:timing estimation trade-off}, BreathJunior improves upon~ by using white noise for respiration estimation~. Before transmitting chirp signals for sensing, it randomizes signal phases in frequency domain and recovers it at a receiver end making the generated sounds less obtrusive. The reported respiration rate error can be as low as 0.4~\\!bpm at a distance of 40~\\!cm. The work in~ overcomes audibility issue (\\textit{\\textbf{Challenge}}-\\ref{clg:audibility}) by using Zadoff-Chu (ZC) sequence. As we outlined in Section~\\ref{sec:waveform for sensing}, the FHSS modulated signal enables fine-grained multipath decomposition and  allows multi-person respiration monitoring simultaneously. The reported error is within 0.6~\\!bpm under various test environments in presence of multiple targets at a minimal distance of 10~\\!cm. \nRen et al.~ developed a passive sensing system that can detect breathing rates and sleep-related events from breathing signals. This approach employs high-quality sensors, and reports less than ${0.5}$~\\!bpm detection error rates. \nThe performance of previous solutions degrades significantly in noisy environments. To combat noise, BreathListener~ extracts breath patterns from energy spectrum density and regenerates clean breath signal via a generative adversarial network. It achieves an average error of 0.11~\\!bpm for breathing rate estimation in driving conditions. SpiroSonic~ went one step further toward conducting spirometry tests in a regular home setting under various environment noises. It measures a target's chest wall motion via acoustic radar and maps the obtained waveforms to lung function indices. SpiroSonic achieves $5\\%$ to $10\\%$ monitoring error in clinical studies, allowing reliable out of clinic disease tracking and evaluation.", "cites": [1695], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of various acoustic tracking systems, grouping them under device-based and device-free categories while connecting them to overarching technical challenges. It critically evaluates the performance, limitations, and innovations of each approach (e.g., error accumulation, hardware customization, interference handling). While it identifies patterns (e.g., use of chirp mixing and signal processing techniques), the abstraction level could be higher by more explicitly formulating generalized principles or frameworks."}}
{"id": "a7b0997f-a3d7-43ec-bb59-a41485ec8abc", "title": "Research Opportunities", "level": "subsubsection", "subsections": [], "parent_id": "ca05f4b8-afe7-4436-b55e-aef15e251d42", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Acoustic Sensing Applications"], ["subsection", "Solutions Enabled by Channel Characteristics"], ["subsubsection", "Research Opportunities"]], "content": "\\label{sec:future direction for channel characteristics}\nAs mentioned in both \\textit{\\textbf{Challenge}}-\\ref{clg:calibration} and \\textit{\\textbf{Challenge}}-\\ref{clg:cross-platform models}, platform diversity can be particularly detrimental to system performance as it requires constant calibration. To ease the pain, building calibration agnostic parametric models is important. However, as acknowledged in \\textit{\\textbf{Challenge}}-\\ref{clg:appropriate CIR representation}, it requires sophisticated domain knowledge and is remarkably challenging. Alternatively, one can leverage deep learning techniques that have shown promising results in  generalizability. Techniques such as adversarial training~  allow a model to generalize well on even unseen data, while few-shot learning  techniques~ only require limited labels to quickly adapt on target environments. Some early attempts in this direction has been made. For example, the authors in~ employ meta learning~, a kind of shot learning technique, to facilitate cross-device mobile sensing with only one or two data instances. We envision that more sophisticated deep learning approaches will be incorporated in acoustic sensing based on channel characteristics to address device, environment or subject diversity.", "cites": [1695], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper on meta-learning to connect it with broader challenges in acoustic sensing, such as cross-platform generalization. It offers some critical perspective by highlighting the limitations of calibration and the potential of deep learning to overcome them, but the critique remains somewhat high-level without deeper evaluation of specific methods. The section abstracts the concept of few-shot and meta-learning into a potential research direction, suggesting a move toward calibration-agnostic models."}}
{"id": "f907a89d-21a9-4a38-a425-e0a004c3a9ef", "title": "Repurposing Other Sensors for Acoustic Sensing", "level": "subsection", "subsections": [], "parent_id": "76bc7108-cd5a-455f-b0a0-9d7148712e27", "prefix_titles": [["title", "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey"], ["section", "Future Directions"], ["subsection", "Repurposing Other Sensors for Acoustic Sensing"]], "content": "In addition to use acoustic sensors for non-conventional purposes such as touch sensing and gesture tracking, it is possible to use other sensors to capture acoustic signals. \nFor instance, the authors from~ and  exploit RF-radar and Lidar sensor to hack audio content, respectively. The key observation is that acoustic signals originate from  vibrations, which can be detected through sensors that measure displacement. A main benefit of cross-technology sensing is that these non-dedicated acoustic sensors are immune to background acoustic noise and thus provide high SNR signals as long as the sampling rate is adequate. \nTherefore, an interesting research direction is to incorporate data from multiple sensing modalities to enable sophisticated sensing tasks or novel applications in adverse environments.", "cites": [1695], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general analytical perspective by discussing the broader idea of repurposing non-acoustic sensors for acoustic sensing. It synthesizes the cited examples (RF-radar and Lidar) to highlight a common principle (vibration-based detection), but does not deeply connect or integrate multiple sources into a novel framework. The critical analysis is limited, with no detailed evaluation of the cited works' strengths or weaknesses. The abstraction level is moderate, as it identifies a general principle and suggests a research direction."}}
