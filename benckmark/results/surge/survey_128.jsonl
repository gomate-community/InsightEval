{"id": "5879c1a0-fabe-41bb-916e-d4bf29da6713", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "88978fb3-2cb3-44da-9096-2cd49c8ec456", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Introduction"]], "content": "Information Retrieval (IR) is the process of finding information that is relevant to the need of a user. The last two decades have completely changed how humans consume and interact with information. This change has been driven by the advances in web search engines, the ease of access to the Internet and the explosion of information available online. Information pertaining to a variety of needs is available - from lecture slides to news articles to descriptions and reviews of items, and so on. It becomes imperative that the IR systems continually improve to accommodate such information needs, which have been growing both qualitatively (in terms of complexity) and quantitatively. Essentially, the task of IR systems can be reduced to two aspects. One is how to efficiently and effectively represent and rank the variety of unstructured information being created at each instant. This involves tasks like indexing and improved understanding of the content through advanced representation methods, as well as ranking of the information items based on the representation. For example, representation of textual information can be improved with better understanding of natural language. The second is how to make an IR system better understand user's complex information need and information seeking behaviour. This involves understanding user's search context, search task and intent, and ability to measure task completion and user satisfaction through user interactions. \nIR researchers have been investigating different approaches to improve IR systems from both the system point of view (representation and ranking) and the user point of view. Various areas in IR, for example, Neural IR~, Interactive IR~, Cognitive IR~, and Dynamic IR~, have been developed. Quantum-inspired IR (QIR) is one such area, where the mathematical framework of Quantum Theory (QT) is utilized to develop representation and user models in IR that are expected to better align with human cognitive information processing. It is different from the field of Quantum Computing in that it does not involve computations based on physical quantum states. \nThe benefits of using QT in IR are many-folds. It offers a new way of representing events and computing probabilities of events. Instead of the set-theoretic method of representing events as subsets of a larger sample space, QT represents events as subspaces of an abstract, complex vector space (called Hilbert space)~. Moreover, the same event can have multiple representations in multiple basis of the Hilbert space. This method of representation can help in the abstraction and contextualization of information objects like documents and queries~. For example, if a set of basis vectors correspond to a set of documents, another set of basis vectors in the same Hilbert space can represent the same set of documents in a different context. Hence a query (as an event) will be represented by these different basis depending upon the context of retrieval. The Hilbert space representation of events also leads to a generalized method of calculating probabilities (Born rule)~, by taking into account interference between events. This can model a user's decisions under ambiguity better than traditional probability models~. Such a representation method can inherently model incompatible variables - those where measurement on one variable affects the outcome of the other. For two such incompatible variables $A$ and $B$, measuring $A$ would alter the state of the system, so that the subsequent measurement of $B$ would be different than if it was measured alone or before $A$. Thus these two variables cannot be measured simultaneously or jointly, and different orders of measurement would lead to different outcomes. Traditional probability theory assumes that for any pair of events, $p(A,B) = p(B,A)$, which would be incorrect for incompatible variables. The cognitive phenomenon of order effect is generally considered to be a consequence of incompatibility in measuring human decisions~. There has been a lot of research in recent years, which shows the presence of Order Effects in relevance judgment of documents (detailed in Section 3).\nCorrespondingly, the application of QT to IR can be broadly divided into two subareas: (1) Representation and Ranking, and (2) User Interaction. Figure \\ref{quantum_ir_brief} shows a sketch of the overlap between traditional IR and Quantum-inspired IR, and their underlying components. We show traditional IR in terms of the two sub-areas, which overlap because user interactions like relevance feedback are often used in re-ranking tasks. In this sense, QIR overlaps with traditional IR as it is also divided into these two sub-areas. The difference comes in the tools used by QIR. It utilises the mathematical framework of QT including complex Hilbert space models for representation learning and quantum probability rules to model cognitive interference in document ranking. \nQIR borrows heavily from  concepts, models and techniques developed in the field of Quantum Cognition, especially in the modelling and incorporating the user interactions in IR. QT has been successfully applied to model and predict irrational human decision making and explain cognitive biases in human judgments~. The emerging field of Quantum Cognition~ studies such quantum-like phenomena in cognitive and decision sciences. There is already a growing community of researchers, under the umbrella of Quantum Interaction (see http://www.quantuminteraction.org/home), who are applying QT to various disciplines such as Biology, Cognition, Economics, Natural Language Processing and Information Retrieval. In 2017, a major project which seeks to investigate a Quantum Theoretical approach to IR (QUARTZ - see http://www.quartz-itn.eu/) has started, under the Marie Skłodowska Curie Actions scheme of the European Union's Horizon 2020 programme, with 7 participating universities all over Europe and several external partners around the world. \nQIR is a growing multi-disciplinary area and has been attracting an increasing attention of researchers in IR. Especially, the recent several years have witnessed a large number of models and applications of QIR which have shown good results and a great potential. However, the field lacks a comprehensive review of the literature, and the individual works are largely segmented. This is why a survey paper is urgently needed. It is important and timely to review the literature systematically to provide a clear picture of the landscape and a road-map for the future. Although this is not the first work to accumulate findings in QIR. Around ten years ago, a position paper~, organised QIR into three themes: frameworks, spaces, and interference. However, it was more on a conceptual level and the field of QIR was in its infancy. After a decade of development since then, the landscape of QIR research has significantly changed. A large number of more comprehensive and larger scale QIR approaches have been developed covering different aspects of IR, and have achieved remarkable experimental results. \nThe next section introduces the basic concepts and notations of QT, to enable readers to understand their usage in IR. Section 3 reviews the literature of Quantum-inspired IR, followed by a discussion on its shortcomings and benefits in Section 4. Section 5 concludes the paper by discussing future work directions in Quantum-inspired IR. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/qir-brief-overlap.png}\\vspace{-4mm}\n    \\caption{Brief overview of quantum-inspired IR}\n    \\label{quantum_ir_brief}\n    \\vspace{-4mm}\n\\end{figure}", "cites": [1], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction section provides a clear analytical overview of Quantum-inspired IR (QIR), integrating concepts from QT and connecting them to existing IR subfields like Dynamic IR. It highlights the unique aspects of QIR, such as Hilbert space representation and modeling cognitive interference, and places it within the broader context of Quantum Cognition. While it offers a strong abstraction of QT principles and their relevance to IR, it lacks deeper critical evaluation of cited works and primarily sets up the field rather than synthesizing multiple sources into a novel framework."}}
{"id": "078b9b09-b64a-4438-925d-27864257de30", "title": "Quantum inspired Language Models and Applications", "level": "subsubsection", "subsections": [], "parent_id": "1839907c-42c5-401c-a690-6855aad66e75", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "Representation and Ranking"], ["subsubsection", "Quantum inspired Language Models and Applications"]], "content": "The Quantum Language Model (QLM) proposed by  combines the Vector Space Model and Probabilistic Language Model of classical IR via the Hilbert space formalism. The Quantum generalization of probabilities comes in the form of representing compound terms in queries and documents as superposition events, which have no classical analogue. This generalized Quantum probability model reduces to classical in case of using single terms only. More recently, a number of extensions of QLM have been made. \n\\\\\n\\\\\n\\noindent\\textit{Basic QLM}\n\\\\\nIn QLM proposed in~, a document or query is represented as a sequence of projectors. A projector represents a single term or compound term from the document/query. A document $d$ containing words from a vocabulary of size $N$ is represented as:\n\\vspace{-4mm}\n\\begin{align}\nP_d = \\{\\pi_i : i = 1, ..., M\\} \\hspace{0.2cm} where \\hspace{0.2cm} M \\leq N\n\\end{align}\nThe Hilbert space is a term space of dimensionality $N$, where each vector $\\ket{v_s}$ represents a term from the vocabulary. Thus the projector for a single term is $\\pi_w = \\ket{v_s}\\bra{v_s}$. The vector for a compound term $\\ket{v_{s_1..s_k}}$ is the superposition of all the vectors corresponding to the single terms:\n\\vspace{-4mm}\n\\begin{align}\n\\ket{v_{s_1..s_k}} = \\sum_{i=1}^k{\\sigma_i \\ket{v_{s_i}}}\n\\end{align}\nwhere $\\sigma_i$ quantifies how much the compound term represents the single term $s_i$, and $\\sum_{i=1}^k{|\\sigma_i|^2}=1$. Thus in the same subspace, the representation of new term is created. This is not possible in traditional Vector Space Models because for every new term, single or compound, one has to add a new dimension to the vector space. Representing compound terms as superposition events solves that problem. Also, the compound term and the single terms in it are not disjoint and are related by the $\\sigma_is$. \nPractically, in order to construct the projectors for a document, the terms co-occurring in the document within a fixed window of size $L$ are considered as compound terms. A language model is essentially a density matrix $\\rho$, and for a document it is represented by projectors $P_d = \\{\\pi_1, \\pi_2, ..., \\pi_M\\}$. It is obtained by maximizing the following function:\n\\vspace{-2mm}\n\\begin{equation}\nL_{P_d}(\\rho) = \\prod_{i=1}^{M}{tr(\\rho \\pi_i})\n\\end{equation}\nThe language model is estimated using a generalization of an Expectation-Maximization based algorithm, called the $R\\rho R$ algorithm~. \nThe language model for a query $\\rho_q$ can be estimated in a similar way. The relevance of a document for a query can be calculated using a generalization of the KL divergence method called \\textit{quantum relative entropy} or \\textit{Von-Neumann(VN) divergence}~. Given two language models $\\rho_q$ and $\\rho_d$, the scoring function is:\n\\vspace{-2mm}\n\\begin{align}\n\\Delta_{VN}(\\rho_q || \\rho_d) = -tr(\\rho_q \\log \\rho_d)\n\\end{align}\nwhere $tr(x)$ denotes the trace of the matrix $x$. Experimentally, the QLM has been shown to outperform a baseline language model and a Markov random fields (MRF) based model~ (which was state-of-the-art at the time of publication of ~) for document ranking.\n\\\\\n\\\\\n\\noindent\\textit{Extended QLMs}\n\\\\\nSeveral extensions have been made to the basic QLM described above.  propose to augment the QLM by making use of ``entangled'' terms. Based on the relation between Unconditional Pure Dependence (UPD) and Quantum entanglement~, the UPD patterns are extracted from queries and documents, and the corresponding projectors are constructed. Instead of considering arbitrary compound terms, these UPD patterns are used as they show a statistical relationship similar to entangled systems.\nMoreover, the $R\\rho R$ algorithm used in QLM has a disadvantage in that it does not always converge. Hence a new global convergence algorithm is used in  for a Global Quantum Language Model (GQLM) but applied on twitter sentiment analysis tasks. Two dictionaries of positive and negative sentiment words are constructed. The global convergence algorithm constructs density matrices for the dictionaries and documents. Then using the quantum relative entropy, a document is projected onto each dictionary to determine its sentiment class. \nA Quantum language model based Query expansion approach is presented in . Using the GQLM described earlier, the language models for documents and query are constructed. The initial ranking is achieved using the Quantum relative entropy. Then a density matrix is constructed for the top $k$ retrieved documents. The top $n$ non-query terms, corresponding to the top $n$ diagonal elements of the density matrix, are selected as expanded terms. The top $n$ diagonal elements are in the order of the Quantum probabilities of the terms. Hence the advantage of using Quantum probabilities can be intuitively understood from here - the quantum probability reflects both the single term occurrence and the co-occurrence between terms. Hence, ``a term with a high frequency but a low co-occurrence with other terms may as well have a lower quantum probability than a terms with lower frequency but higher co-occurrence''~. After having formed the expanded query, a new GQLM is constructed for the expanded query and the documents are re-ranked accordingly. Experiments on the TREC 2013 and 2014 session track datasets show a better performance than the original QLM and another quantum model proposed in~, which is based on user interactions. Indeed, there have been various extensions of QLM that adapt to user interactions~, which will be discussed in Section 3.2.\nThe QLM is also extended within a neural network structure in  for Question Answering (QA), while the authors mention that the model can also be applied to other IR tasks, such as ad-hoc retrieval. Using word embeddings as vectors, a density matrix for each sentence is constructed, for both questions (as queries) and answers (as documents). The density matrix represents a mixture of semantic spaces. A joint representation of queries and documents is constructed by multiplying the density matrices for queries and documents. Then a convolution layer is applied over this joint representation followed by pooling, a fully connected layer and a softmax layer. The binary output of the softmax layer represents probabilities of relevance and non-relevance of the answer with respect to the question. This process is repeated for each question and answer pair and a ranking based on their relevance probabilities is produced. This model achieved MAP and MRR scores of 0.7589 and 0.8254 on the TREC-QA dataset, which was 2.46\\% and 3.24\\% improvement over a neural model TANDA , which was state-of-art at the time when the paper was published\\footnote{Note that the current state-of-the-art BERT-based neural model for TREC-QA has achieved MAP and MRR of 0.943 and 0.974 respectively~.}.\nA Quantum many body wave function based language model is presented in~. The aim is to create a language model which addresses the challenges in word combinations, where each of the individual words can possess multiple meanings. Different meanings of a word are represented as different basis vectors of a Hilbert space. The state vector for a word is a superposition of different base vectors corresponding to different meanings of the word. The state vector of sentence is represented as a tensor product of the individual word's state vectors. This is termed as a local representation, and a similar global representation of the language model is constructed using another corpora, to account for unseen words and unseen compound words. The global representation is projected onto the local representation akin to the smoothing process in classical language models. As the global representation is a higher rank tensor, it is decomposed using tensor decomposition techniques. The projection of the reduced tensor onto the local representation tensor is realized in the form of a convolutional neural network. While it is unable to outperform the above mentioned model  on the TREC-QA dataset, it performs significantly better on the WikiQA dataset\\footnote{The TANDA model mentioned above currently gives the best performance on WikiQA dataset (MAP and MRR of 0.92 and 0.933 respectively, as compared to 0.695 and 0.71 by~).}. \n\\\\\n\\\\\n\\noindent\\textit{Other Quantum-inspired language models}\nIn , the Quantum theoretic framework is used to construct a syntax-aware semantic model. It also takes word order into account, unlike the QLM. Firstly, for each sentence, dependency parsing is performed and a set of dependency relations are extracted. This set is partitioned into clusters of syntactically similar relations, and each cluster is assigned a Hilbert space. Each Hilbert space has the word vectors as the basis vectors. The state vector in each Hilbert space is a superposition of the word vectors, which are dependencies of the same word. The state vector for a given word is written as a tensor product of the state vectors in all the Hilbert spaces. A complex phrase is ascribed to the state vector. A word occurring in different senses will have different state vectors, which are then superposed to get the final vector for each word.  It is then converted into a density matrix, and the density matrices of the occurrences of the word in different documents are added up. The similarity between two words can be measured using the trace rule, which essentially takes the pair-wise inner products of the state vectors. This allows the words to \"select\" each other's context and should lead to more accurate similarity values. Experiments done on word similarity and word association tasks reveal a better performance than some classical models. This method is extended in  to construct density matrices for sentences. To create a density matrix for a sentence, first the dependency parsing tree is constructed. For each node in the tree, its dependencies are projected onto it and the post projection states are summed up together with the density matrix of the node. This procedure is performed recursively until the whole sentence is covered. The method is tested on the paraphrase detection task with the Microsoft Paraphrase Detection dataset, and shows better accuracy and F1 scores than a recent neural network model reported in .\nIn~, an n-gram language model inspired from QT is introduced, with application to speech recognition. Unlike the quantum inspired language models presented earlier, this paper makes use of the unitary evolution of a quantum state in time, e.g. $\\rho_{t+1} = U\\rho_t U^{\\dagger} $, where $U$ is a unitary operator which changes the state of a system $\\rho_t$. To measure the probability of a word $w$, the system state is projected onto the state of the word $w$ using the projector $\\Pi_w = \\ket{w}\\bra{w}$. Probabilistic information about a sequence of words $w_1, w_2, ..., w_n$ is encoded in a density matrix built using the following process:\n\\vspace{-3.5mm}\n\\begin{align}\n    p(w_1; \\rho_0, U) &= tr(\\rho_0, \\Pi_{w_1}) \\nonumber \\\\\n    \\rho_1^{'} &= \\frac{\\Pi_{w_1}\\rho_0\\Pi_{w_1}}{tr(\\Pi_{w_1}\\rho_0\\Pi_{w_1})} \\\\ \\nonumber\n    \\rho_1 &= U\\rho_1^{'}U^{\\dagger}\n\\end{align}\nwhere $\\rho_0$ is the initial state of the system, and $\\rho_1$ is state of the system after processing the first word in the sequence. The unitary matrix $U$ is responsible for the time evolution of the system and $tr(x)$ stands for the trace operation. The final probability of the whole sequence becomes:\n\\vspace{-2mm}\n\\begin{equation}\n    p(w_i|w_1, ..., w_{i-1}) = tr(\\rho_{i-1}\\Pi_{w_i})\n\\end{equation}\nOne possible issue arises here. Continuously projecting and collapsing the system state to individual words may remove any quantum effects from the system, i.e. the system reduces to a classical markov model like system. To address the issue, the system is coupled with an ancillary system to avoid the complete collapse. A D-dimensional Hilbert space represents the ancillary system and thus the composite system has the Hilbert space $H_2 = H_{ancilla} \\otimes H$. The new projectors for the composite space are given by $\\Pi_w^{(2)} = I_D \\otimes \\Pi_w$. The advantage of doing this is that the time evolution of the composite system can give rise to non-trivial correlations between them (analogous to non-separability and entanglement) so that even when the state of the word sequence is collapsed, some information is retained in the ancillary part (owing to their non-trivial correlations). The words are represented in low-dimensional vectors and for each dimension, a unitary matrix is assigned for the composite system. The parameters are learned by minimizing the perplexity of the corpus of sentences. The perplexity is given by:\n\\vspace{-1mm}\n\\begin{equation}\n    \\Gamma(\\rho_0, U) = exp(-\\frac{1}{C}\\sum_{w \\in S}{\\log p(w|\\rho_0, U)} \n\\end{equation}\nExperiments on the TIMIT dataset show that this n-gram quantum language model has a lower perplexity than the state-of-the-art deep neural network architectures like RNN and RNN-LSTM. Although the paper reports an application of the proposed language model in speech recognition, it would be interesting to use it to construct document and query language models.", "cites": [5, 2, 3, 4, 8307], "cite_extract_rate": 0.2631578947368421, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section synthesizes multiple quantum-inspired language models, connecting their mathematical foundations and applications in IR. It provides critical evaluations, such as the convergence issues of the RρR algorithm and the limitations of certain QLM approaches. The section abstracts the core principles of using superposition and entanglement for term representation and ranking, highlighting broader patterns like how quantum probabilities capture both term frequency and co-occurrence."}}
{"id": "2233c6f9-3b28-4603-b965-83d14c302cae", "title": "Quantum-inspired Ranking", "level": "subsubsection", "subsections": [], "parent_id": "1839907c-42c5-401c-a690-6855aad66e75", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "Representation and Ranking"], ["subsubsection", "Quantum-inspired Ranking"]], "content": "The research on quantum-inspired ranking has been done from two perspectives: quantum probability ranking principle and quantum-like measurement.\n\\\\\n\\\\\n\\textit{Quantum Probability Ranking Principle}\n\\\\\nAccording to the Probability Ranking Principle ~, an IR system should rank the documents for a user IN in a decreasing order of their probability of relevance. It makes the assumption that ``the relevance of a document to an information need does not depend on other documents''~. However, in real world situations, judgment of documents by a user is influenced by its previously judged documents~. ``The utility of a document may become void if the user has already obtained the same information''~. This 'interference' between documents can be due to information overlap between documents or a change in the IN, and is accounted for in a Quantum Probability Ranking Principle (QPRP)~. QPRP draws an analogy~ with the Double Slit Experiment by assuming the two slits to be two documents $A$ and $B$ which the user judges for a query. The position $x$ on the screen corresponds to the event that the user is satisfied by the documents $A$ and $B$, and decides to stop the search. If $A$ is first document presented to the user, we have $p_{AB}(x)$ as the probability that the user stops the search at document $B$. In the Double slit experiment, if slit $A$ is fixed and slit $B$ is varied in dimensions, which is analogous to having different documents listed after document $A$, we get $p_{AB_i}(x)$ as ``the probability of stopping the search process after seeing documents $A$ and $B_i$''~. The problem then boils down to finding which configuration of slits (set of documents) $AB_i$ exhibits maximum value of $p_{AB_i}(x)$. \nFor the classical case, if there is no interference, i.e. only one of the $B_i$ slit is opened at a time, we have \"$p_{AB_i}(x) = p_A(x) + p_{B_i}(x)$\"~:\n\\vspace{-2mm}\n\\begin{align}\n\\arg\\max_x(p_{AB_i}(x)) = \\arg\\max_x(p_A(x) + p_{B_i}(x)) = \\arg\\max_x(p_{B_i}(x))\n\\end{align}\nHowever, in the quantum case, with all slits open, or all documents considered by the user till $B_i$, $p_{AB_i}(x) = p_A(x) + p_{B_i}(x) + I_{AB_i}(x)$, where $I_{AB_i}(x)$ is the interference term. Thus:\n\\vspace{-2mm}\n\\begin{align}\n\\arg\\max_x(p_{AB_i}(x)) &= \\arg\\max_x(p_A(x)  + p_{B_i}(x) + I_{AB_i}(x)) \\\\ \\nonumber \n&= \\arg\\max_x(p_B(x) + I_{AB_i}(x))\n\\end{align}\nHence the best choice of document to rank after $A$ is not the one whose relevance probability is maximum, but rather the one whose sum of individual relevance probability and the interference term with $A$ is maximum. Hence, between two documents $B$ and $C$, $B$ is ranked before $C$ if and only if:\n\\vspace{-3mm}\n\\begin{equation}\np_B(x) + I_{AB} \\geq p_C(x) + I_{AC}(x)\n\\end{equation}\nRecall from Equation \\ref{interference-term} that the interference term depends upon the phase difference of the probability amplitudes of two quantum systems. Thus:\n\\vspace{-4mm}\n\\begin{align}\np_{AB}(x) = p_A(x) + p_B(x) + 2\\sqrt{p_A(x)}\\sqrt{p_B(x)}\\cos(\\theta_{AB})\n\\end{align}\nIn~, the authors did not give details of how to estimate the interference term. This estimation is done in an application of the QPRP to subtopic retrieval~. Subtropic retrieval is a task of providing a list of documents which covers all possible topics (IN aspects) relevant to the user IN. It advocates a more diverse ranking of documents, to achieve a minimal redundancy. Thus redundant documents can be assumed to be destructively interfering (negative interference term) and the documents having exclusive information be positively interfering. The $\\cos(\\theta)$ part of the interference term is estimated as the Pearson's correlation between the term vectors of two documents. The term vectors are constructed using the BM25 scheme. Experiments show that the QPRP based ranking for subtopic retrieval performs better than a model based on Portfolio Theory (then state-of-the-art) for subtopic retrieval on the ClueWeb09-B collection.\n\\\\\n\\\\\n\\noindent\\textit{Quantum Measurement inspired Ranking }\n\\\\\nAnother method for document ranking using Quantum probabilities is discussed in , called Quantum Measurement inspired Ranking (QMR). Document retrieval process is considered to be similar to a photon polarization process. A photon has a Horizontal or Vertical polarization, which can be measured by a polarizer. There also exist superposition states of both vertical and horizontal polarizations, which is detected by a horizontal or vertical polarizer rotated at a $45$ degree angle. \n\\vspace{-6mm}\n\\begin{align}\n\\ket{\\nwarrow} = \\frac{1}{\\sqrt{2}}(\\ket{\\uparrow} + \\ket{\\downarrow})\n\\end{align}\nSuperposition states can be generated by passing a horizontal or vertically polarized photon through the rotated polarizer. Mathematically, the vertical and horizontal polarizers form an orthonormal basis of a two dimensional Hilbert space. The rotated polarization state forms another orthonormal basis in the same Hilbert space. In the analogy, the first round of document retrieval for a query is analogous to the measurement along the vertical or horizontal basis. Then, a second round retrieval is performed to re-rank the documents by comparing all retrieved documents with the top $k$ documents. This is analogous to passing the photons coming from a horizontal or vertical polarizer through the rotated polarizer.  Mathematically this is formulated as projecting a vector represented in one basis onto the subspace generated by another rotated basis. \nIn the first round of retrieval, let $\\ket{\\uparrow}$ and $\\ket{\\downarrow}$ denote relevance and non-relevance of document respectively for a query. Then a document $d$ with relevance probability $|\\alpha_d|^2$ is represented in the first round as:\n\\vspace{-2mm}\n\\begin{align}\n\\ket{d} = \\alpha_d\\ket{\\uparrow} + \\beta_d\\ket{\\downarrow}\n\\end{align}\nTaking the simple case of $k=1$, let the topmost document in the first round of retrieval be represented as: \n\\vspace{-2mm}\n\\begin{align}\n\\ket{t} = \\alpha_t\\ket{\\uparrow} + \\beta_t\\ket{\\downarrow}\n\\end{align}\nThen, re-ranking is done by representing the document $d$ in terms of $t$:\n\\vspace{-2mm}\n\\begin{align}\n\\ket{d} = \\lambda\\ket{t} + \\mu\\ket{\\widetilde{t}}\n\\end{align}\nwhere $\\lambda = \\alpha_d\\alpha_t + \\beta_d\\beta_t$ (see appendix in ~ for a proof). The probability of relevance of the document $d$ when re-ranking is performed using the top-ranked document of first round is the square of the projection of $d$ onto $t$, which is $|\\lambda|^2$, multiplied by the probability of relevance of $t$, which is $|\\alpha_t|^2$:\n\\vspace{-2mm}\n\\begin{align}\np(d|t) =  |\\lambda\\alpha_t|^2\n\\end{align}\nWhen $d = t$, then $\\lambda = 1$ and the probability becomes $|\\alpha_t|^2$, the original probability of relevance of the top-ranked document. The QMR approach performs significantly better than the QPRP on four TREC collections - WSJ9872, AP8889, ROBUST04 and WT10G on MAP ranking metric.\nQuantum-inspired ranking has also been used to solve the query drift problem, which is defined as the inferiority of results obtained on query expansion, than the original query. This is because the underlying intent of the query might change upon expansion. Several solutions have been proposed for the query drift problem using pseudo relevance feedback~, focusing on the combination of document scores in the ranked lists of documents based on the original query and the expanded query. For example:  \n\\vspace{-1.5mm}\n\\begin{itemize}\n\\item CombMNZ rewards documents that are ranked higher in both original retrieval list and second retrieval list by adding the relative score of a document in each of the two lists.\n\\end{itemize}\n\\begin{itemize}\n\\item Interpolation technique makes a weighted addition of relative scores in the two lists.\n\\end{itemize}\nIn , a document $d$ is represented in terms of relevance and non-relevance for a query $q$:\n\\vspace{-2mm}\n\\begin{align}\n\\ket{d} =  a_d\\ket{q} + b_d\\ket{\\widetilde{q}}\n\\end{align}\nwhere $\\ket{q}$ and $\\ket{\\widetilde{q}}$ represent the vectors for relevance and non-relevance of $d$ with respect to $q$, respectively. In terms of the expanded query $q^e$, the document is represented as:\n\\vspace{-2mm}\n\\begin{align}\n\\ket{d^e} =  a_d^e\\ket{q^e} + b_d^e\\ket{\\widetilde{q^e}}\n\\end{align}\n``To prevent query drift, the existing fusion models in  directly combine the two probabilities $|a_d|^2$ and $|a_d^e|^2$''~. The CombMNZ reduces to: \n\\vspace{-2mm}\n\\begin{align}\n(\\delta_q(d) + \\delta_q^e(d)).(\\delta_q(d)|a_d|^2 +  \\delta_q^e(d)|a_d^e|^2)\n\\end{align}\nwhere $\\delta_q(d) = 1$ if $d$ is relevant to query $q$. Similarly, the interpolation method becomes:\n\\vspace{-2mm}\n\\begin{align}\n\\lambda\\delta_q(d)|a_d|^2 + (1 - \\lambda)\\delta_q^e(d)|a_d^e|^2 \\hspace{0.3cm} 0\\leq \\lambda\\leq 1\n\\end{align}\nHowever, the two probabilities $|a_d|^2$ and $|a_d^e|^2$ are under different basis and we need to write one in terms of the other. The Quantum Fusion Model (QFM) proposed in  does that and the final outcome combines the probabilities in the following way:\n\\vspace{-4mm}\n\\begin{align}\n(\\delta_q(d)|a_d|^2).(\\delta_q^e(d)|a_d^e|^2)\n\\end{align}\nThus the Quantum based model is a multiplicative model, while the classical models are additive. Another slightly modified version is:\n\\vspace{-4mm}\n\\begin{align}\n(\\delta_q(d)|a_d|^2).(\\delta_q^e(d)|a_d^e|^2)^{1/\\eta}\n\\end{align}\nwhere ``a small $\\eta$ can make scores of different documents retrieved for $q^e$ more separated from each other, leading to more distinctive scores''~. The QFM achieves a better performance than the CombMNZ and interpolation methods in terms of Mean Average Precision (MAP) of retrieved documents.", "cites": [6], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple quantum-inspired ranking concepts by connecting them to foundational IR principles and experimental results. It provides critical analysis by highlighting limitations, such as the lack of detail on estimating the interference term in early work. The abstraction is strong, as it frames quantum probability and measurement as generalized models for IR tasks like subtopic retrieval and query drift, offering a meta-level understanding of relevance dynamics."}}
{"id": "a2438ba6-a800-4321-bfc3-ecefc36d9780", "title": "Multimodal Information Retrieval", "level": "subsubsection", "subsections": [], "parent_id": "1839907c-42c5-401c-a690-6855aad66e75", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "Representation and Ranking"], ["subsubsection", "Multimodal Information Retrieval"]], "content": "Despite the wide application of QT in text-based IR, limited attention has been paid to multimodal IR, which is of increasing significance in many applications. The work in this area can be divided into feature level fusion and decision level modality fusion.\n\\\\\n\\\\\n\\noindent\\textit{Feature Level Fusion}\n\\\\ \n Initially, Wang et al.  exploited tensor product of Hilbert spaces to fuse textual and image features for circumventing the heuristic combination of uni-modal feature spaces. In particular, textual and visual features are combined in a similar way as non-separable states of a Quantum system. The authors claim that the proposed modality fusion approach is able to capture cross-modal dynamics, i.e., interactions across different modalities (e.g., text and image modalities). In each single modality feature space, documents are formulated as a superposition of terms. These terms are words from a vocabulary for the text representation and visual words for the image representation. For instance, in the\ntextual feature Hilbert space denoted as $H_T$: \n\\vspace{-2mm}\n\\begin{align}\n\\ket{d_T} = \\sum_{i}{w_{t_i} \\ket{t_i}},\n\\end{align}\nwhere the squared amplitude $w_{t_i}^2$ equals the probability of a document to be about the term $t_i$ with $ \\sum_{i}{w^2_{t_i}}=1$. Similarly, for the image modality the formulation is as follows:\n\\vspace{-2mm}\n\\begin{align}\n\\ket{d_V} = \\sum_{i}{w_{v_i} \\ket{v_i}},\n\\end{align}\nwhere $v_i$ represents visual words in the image Hilbert space $H_V$. Each pure state is modelled through a  density matrix. In mathematical language, each density matrix is defined as the outer product of a superposition state. For example, for the textual representation, the density matrix is:\n\\vspace{-2mm}\n\\begin{align}\n\\rho_{d_T} = \\sum_{i} {p_i \\ket{d_{T_i}}\\ \\bra{d_{T_i}} },\n\\end{align}\nwhere  $p_i$ is the probability of the state being in the basis state $\\ket{d_{T_i}}$.\nThen, the text and image modalities are fused by taking the tensor product of the text and image Hilbert spaces as follows:\n\\vspace{-2mm}\n\\begin{align}\n\\rho_{d_{TV}} = \\rho_{d_T} \\otimes \\rho_{d_V} + \\rho_{correlation},\n\\end{align}\nwhere $\\rho_{d_T} and \\rho_{d_V}$ are the textual and visual density matrices respectively, and $\\rho_{correlation}$ is the density matrix capturing cross-modal interactions between the text and image features. The resultant product is still a valid density matrix. Finally, for measuring the similarity between a multimodal document and query, the trace rule is used as follows: \n\\vspace{-2mm}\n\\begin{align}\n   sim(d,q) = trace(\\rho_{d_{TV}} \\cdot \\rho_q),\n\\end{align}\nwhere $\\rho_{d_{TV}}$ and $\\rho_q$ are multimodal document and query density matrix representations respectively. \nFor capturing cross-modal interactions across the two modalities, two statistical approaches were explored: (a) the maximum feature likelihood that associates text with the maximal likely image features; and (b) the mutual information matrix that measures the mutual dependence between the two modalities. Experiments show that even without considering the correlation between text and image features, the pure tensor product approach outperforms other methods such as the use of image features or text features individually, or the concatenation of text and image features. However, such a method suffers from exponentially increasing computational complexity, as the outer product over multiple modalities results in high dimensional tensor representations. The experiments also show that the two proposed statistical methods are trivial to capture cross-modal interactions. For example, simple visual features were used, such as colour histograms, which can hardly be associated with high-level semantics.  A more robust statistical approach, such as the cross-modal factor analysis , might be more effective. Another issue was that images with limited or no annotation were lowly ranked or not retrieved at all. This implies that tensor product cannot manipulate missing values, which becomes a common problem in a real-world scenario. An automatic annotation task might circumvent the above problem. Also, assuming orthogonality of dimensions disregards any semantic overlap. This was an issue for textual space as the dimensions representing words need to represent language attributes such as polysemy and synonymy. Nowadays, we can address such issue by exploiting neural network language technologies  for constructing text vector spaces with compact semantic information.\nKaliciak et al.  followed up with the previous model, aiming to solve the problem of missing modalities, e.g., when images are not annotated. They proposed two approaches to alleviate this problem, which can be easily integrated with the tensor-based fusion method. The first approach projects an un-annotated image onto a subspace generated by subsets of annotated images. In particular, by exploiting the Born rule, the square projection on the basis states results in a probability distribution of terms for each un-annotated image. The second approach alternatively utilizes the trace rule to calculate the similarity between an annotated and un-annotated image. Images are formulated as density matrices, entailing a probability distribution of terms.  The results showed that such approaches under-performed the standard clustering techniques. The result might be related to the assumption that ``the correlation at the image-level (i.e., images referring to the same topic) are stronger than the correlations based on the proximity between image terms (i.e., instances of image words)''~.\nLater on, Kaliciak et al.  proposed a quantum-inspired framework for a cross-modal retrieval task. That is, given a text query, to retrieve the most relevant images. They first constructed a common Hilbert space by taking the tensor product of image and text density matrices. Both text queries and image documents are represented in the joint Hilbert space. In this joint space, they also utilized a mechanism of trans-media pseudo-relevance for re-ranking retrieved images. Then, a projection measurement measures the relevance between the text query and each image document represented on the same space.\n\\\\\n\\\\\n\\noindent\\textit{Decision-level Fusion}\n\\\\\nDecision level fusion combines uni-modal classification results to reach a final decision. Despite being a common approach for fusing different modalities, only preliminary studies have investigated quantum-inspired decision level approaches for IR tasks. Gkoumas et al.  investigated non-classical correlations between mono-modal decisions on a pair of text-image documents for a multi-modal retrieval task. In principle, non-classical correlations or quantum correlations are stronger than classical correlations due to latent contextual influences. In that study, the authors investigated the existence of this kind of non-classical correlation through the Bell inequality (CHSH inequality) violation in a small-scale experiment on the ImageCLEF dataset. Although they did not find a violation of the CHSH inequality, the experiment design provides useful insights for future investigations into such non-classical contextual correlations.\nQuantum-inspired modality fusion models have also been developed for multimodal sentiment analysis. Sentiment is one of the factors considered by users in judging certain types of documents (e.g. news articles, blogs). Sentiment label can be considered as a feature in predicting relevance~.\nZhang et al.~ proposed a quantum-inspired decision level modality fusion approach for image-text sentiment analysis. Even this task is far from IR tasks, the approach could be fruitful for IR tasks as well. In particular, both text and image information is associated with density matrices which use the same globally convergent algorithm mentioned earlier in the case of extended QLMs to estimate the density matrices. In this way, density matrices capture the cross-modal interactions. Additionally, the human cognitive interference phenomenon caused when a user is exposed to conflicting text and image information channels, is also considered as analogous to quantum interference. Though, the interference term is treated as a single parameter and adjusted experimentally. The results suggest that, when the Cosine of the interference term equals 0.3, the model achieves the best performance. Moreover, the accuracy is the highest when a user pays more attention to the text instead of image modality, assigning weights 0.7 and 0.3 for the text and visual representation respectively. When the weights are reversed, the model performs the lowest. This is an interesting outcome since it helps us understand under which conditions the quantum-like interference works at the decision level and enhances explainability over the modality fusion process. Overall, large-scale experiments show that the proposed approach outperforms a wide range of state-of-the-art baselines. \nA combination of Long Short Term Memory (LSTM) and a quantum-inspired framework for conversational sentiment analysis was proposed in . In particular, words are represented as pure states in a real-valued Hilbert space. Then, a sentence is formulated as a mixture density matrix of pure states, i.e., unit vectors, which further is processed by a CNN, resulting in a dense representation. Next, the output of CNN is fed into an LSTM cell to make a  decision. Considering conversation sentiment analysis contains time-series and thus requiring fusing time-varying signals, the authors exploited a sequence of LSTM cells and the concept of quantum-inspired measurement, namely weak measurement, to model inter-speaker sentiment influences over a dialogue.\n is a follow up of  by extending the framework with two modalities, namely, text and visual modalities. Specifically, each modality is represented in an individual real-valued Hilbert space. The exact pipeline with  is followed to predict unimodal sentiment judgments. Then, the concept of quantum interference has been exploited to fuse text and visual sentiment judgements. Comprehensive experiments on two benchmarking datasets for conversational human language analysis showed that the proposed quantum-inspired framework beats the state-of-the-art performance for the video emotion recognition task. It is to be noted that conversational sentiment analysis is an important feature in conversational IR tasks.", "cites": [8, 7], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple studies, particularly from Wang et al. and Kaliciak et al., to present a coherent narrative on quantum-inspired multimodal IR. It offers critical evaluation by discussing limitations such as computational complexity, effectiveness of statistical methods, and handling missing modalities. The section also abstracts some principles like the use of Hilbert spaces and density matrices for cross-modal fusion, though broader theoretical or conceptual generalizations are somewhat limited."}}
{"id": "2df2201c-2984-4df8-8784-7b468e4b37aa", "title": "Quantum inspired Representation Learning", "level": "subsubsection", "subsections": [], "parent_id": "1839907c-42c5-401c-a690-6855aad66e75", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "Representation and Ranking"], ["subsubsection", "Quantum inspired Representation Learning"]], "content": "The quantum-inspired representation and ranking models depend on the construction and learning of Hilbert spaces. These are developed or applied in areas like lexical semantic spaces, topic modelling, word embeddings, and text classification.\n\\\\ \n\\\\\n\\\\\n\\noindent\\textit{Lexical Semantic Spaces}\n\\\\\nThe first connections between QT and semantic spaces were established in . In , one such connection is presented using the Hyperspace Analogue to Language (HAL) model~. For a vocabulary of $N$ words, the HAL algorithm constructs an $N \\times N$ matrix by sliding a window of length $l$ over a text corpus, thus capturing word co-occurrences within the window. Each element of the matrix measures word co-occurrence and in one way, word similarity.  Each window is considered as a semantic space and approximates the context or the sense associated with the word. The semantic space for a word is computed in terms of the sum of semantic spaces. If there are $y$ windows around the word $w$ and $x$ of them deal with a particular context $i$, then the semantic space $S_i$ occurs with probability $p_i = \\frac{x}{y}$ and the semantic space for word $w$ can be written in terms of context semantic spaces as:\n\\vspace{-3mm}\n\\begin{equation}\nS_w = \\sum_{i=1}^{m}{p_iS_i}\n\\end{equation}\nThis formula is the same as that of a mixed density matrix written as a mixture of density matrices of pure states. Thus the context of words can be considered as pure states. HAL is also used in  to model word correlations like Quantum correlations of non-separable states.\nThe explicit term occurrence based approaches are insufficient to capture hidden semantics. The advent of machine learning techniques have opened up a door to learning semantic spaces based on topic modelling and word embedding. \n\\\\ \n\\\\\n\\noindent\\textit{Interference Topic Model}\n\\\\\nThe analogy to Quantum interference is used in  for modeling interactions between topics. Topic modeling is used to discover hidden themes in text collections. A topic is defined as a probability distribution over a vocabulary and a document is a mixture of one or more such topics. Finally, every word in a document is supposed to come from one of the topics. The probability of a term $w$ in a document model $\\theta_d$ with $k$ topics is given as~:\n\\vspace{-4mm}\n\\begin{align} \\label{classical-topic}\np(w|\\theta_d) = \\sum_{k}{p(w|z=k,\\phi)}p(z=k|\\theta_d) = \\sum_{k}{\\theta_{dk} \\phi_{kw}}\n\\end{align}\nwhere $w \\in \\{1,...,N\\}$ denotes a word from the vocabulary. $z \\in \\{1,...,K\\}$ is the index for a topic. $\\theta_{d}$ denotes a document where $\\theta_{d} = (\\theta_{d1},...,\\theta_{dk})$ , $\\theta_{di}$ being the ``topic proportions for the document''~. $\\phi$ is a $N \\times K$ matrix representing the distribution of topics over terms. \nConsider the case of two topics: `war' and `oil'. The term `Iraq' is present in both topics. Now if a document contains both topics, still the probability of term 'Iraq' in the document is less than the maximum of its probability in either of the topics. Mathematically speaking~,\n\\vspace{-2mm}\n\\begin{align}\np(w=Iraq|\\theta_d) &= p(Iraq|war)*p(war|\\theta_d) + p(Iraq|oil)p*(oil|\\theta_d) \\\\ \\nonumber\np(Iraq|\\theta_d) &\\leq \\max(p(Iraq|war), p(Iraq|oil))\n\\end{align}\nHowever, the probability of the term `Iraq' occurring in the document should be significantly more given it contains topic `war' and `oil'. Current topic models do not consider the interference or relation between two topics when generating a word. They assume the topics to be independent. To capture topic dependence via Quantum probabilities,  assumes a Hilbert space where each dimension corresponds to a word from the vocabulary. Then, each topic is a vector in this Hilbert space $z_k$, which is a superposition of vectors corresponding to the terms. Thus we have:\n\\vspace{-2mm}\n\\begin{equation}\n\\ket{z_k} = \\sum_{w}z_{kw}\\ket{e_w} = \\sum_{w}{\\sqrt{\\phi_{kw}}}e^{i\\varphi_{kw}}\\ket{e_w}\n\\end{equation}\nwhere $\\sqrt{\\phi_{kw}}e^{i\\varphi_{kw}}$ is the complex amplitude for the topic $\\ket{z_k}$ in state $\\ket{e_w}$ and $|\\sqrt{\\phi_{kw}}e^{i\\varphi_{kw}}|^2 = p(w|z=k,\\phi)$. A document can be represented as a superposition of topic states, with the coefficients being the proportion of topic in the document. \n\\vspace{-2mm}\n\\begin{equation}\n\\ket{d} = \\frac{1}{Z_d}(\\sum_{k}{\\sqrt{\\theta_{dk}}\\ket{z_k}})\n\\end{equation}\nwhere $Z_d$ is a normalization constant. The projection of a document vector onto a word vector is given as: \n\\vspace{-2mm}\n\\begin{align}\nd_w = \\bra{e_w}\\ket{d} \\propto \\sum_{k}{\\sqrt{\\theta_{dk}\\phi_{kw}}e^{i\\varphi_{kw}}}\n\\end{align}\nThe probability of a term in the document is given by:\n\\vspace{-2mm}\n\\begin{align}\np(e_w^+e_w) &= |\\bra{e_w}\\ket{d}|^2 \\propto |\\sum_{k}{\\sqrt{\\theta_{dk}\\phi_{kw}}e^{i\\varphi_{kw}}}|^2  \\\\ \\nonumber\n&= \\sum_{k}{\\theta_{dk}\\phi_{kw}} + 2\\sum_{i<j}{\\sqrt{\\theta_{di}\\theta_{dj}}\\sqrt{\\phi_{iw}\\phi_{jw}}\\cos(\\varphi_{iw} -  \\varphi_{jw})}\n\\end{align}\nThis equation represents the proposed interference-topic model. The first part of the expression on the right hand side corresponds to the classical topic model given in Equation~\\ref{classical-topic}, and the second is ``the interference term which boosts or penalizes the probability for term $w$ in the final document model depending on the phase differences $\\varphi_{iw} -  \\varphi_{jw}$''~. For a particular word, if a pair of topics are in the same phase then, $\\varphi_{iw} -  \\varphi_{jw} = 0$ and $cos(\\varphi_{iw} -  \\varphi_{jw}) = 1$. This increases the probability of seeing the word $w$ in the document. For the phase difference of $\\frac{\\pi}{2}$, the interference term vanishes and the classical topic model is recovered. In their experiment,  estimated the interference term using a similarity measure between the topic distributions, such as the Cosine similarity. The topic model helps in relevance ranking in IR by providing a better match for queries and documents, beyond the term level. This Quantum-,inspired topic model is applied to retrieval tasks like the TREC newswire corpora and performs better than the classical topic model.\n\\\\\n\\\\\n\\noindent\\textit{Complex Numbers}\n\\\\\nQT in its most general formulation uses complex numbers in its representations and computations. Without the use of complex amplitudes, for example, the interference effects will be restricted to only positive and negative interference values ($+1$ and $-1$), while not utilizing the full range of possibilities in between. Therefore, it is imperative that Quantum models outside of Physics which are directly or indirectly making use of the superposition and interference phenomena use complex numbers in representation of state vectors, in order to maximally exploit the power of quantum probabilities. However, it is difficult to get an intuition as to how to represent concepts, objects, terms, decisions, etc. using complex numbers.\n proposed using inverse document frequency (idf) of a term as the imaginary part and the term frequency (tf) as the real part of a complex number. In , this proposal was investigated and found to be performing poorly than the baseline Vector Space models. In , different types of word semantics are combined using a complex Hilbert Space. The main idea is to represent distributional semantics, like the word co-occurrence information as real part and represent ontological information about words as the imaginary part of a complex valued vector. The real vectors are constructed using the technique of Random Indexing, where a word vector is constructed using the vectors that represent the contexts of the word. A document is then represented as a sum of its word vectors. Besides, using the technique of concept indexing, a document is also represented as a Bag of Concepts vector. Each word is mapped to one or more concepts from a medical ontology. These two representations are merged into a single complex vector. Thus, similarity between two documents can be calculated as the inner product of the two complex vectors which will reflect both the distributional and ontological similarity. This model is used in the IR task of TREC Medical Records Track and the retrieval effectiveness is found to be better than either the term-based only or concept-based only approaches in terms of the Mean Average Precision (MAP) and Precision@10 metrics.\n\\\\\n\\\\\n\\noindent\\textit{Quantum-inspired Neural Representation Models}\n\\\\\nIn , the challenge of emergent meaning and sentiment of a combination of words is addressed. They hypothesize that humans do not associate a single meaning or sentiment to a word. A word contributes to the meaning or the sentiment of a sentence depending upon the other words it is combined with. For example, the words `Penguin' and `Flies' (Verb) might be neutral in polarity individually, but the phrase `Penguin Flies' is of negative polarity. Similar examples can be constructed for sentiment of sentences. This is compared to the Quantum interference phenomenon where two superposed quantum states interfere and the final outcome depends upon their relative phase. As such a word embedding model using complex numbers is introduced. Each word is represented by a complex vector. It comprises a real part that holds word co-occurrence information, and a complex phase that captures abstract combinatory information like the sentiment factor. A sentence is considered as a superposition of words and thus a sentence vector can be constructed as a density matrix. This density matrix is learnable from labelled data, along with a projection matrix, which is used to calculate the probabilities assigned by the sentence vector. For a sentiment classification task, the projection matrix is used to classify the sentiment of the sentence according to the trace rule of calculating probabilities. The proposed model outperformed some word-embedding based models.\nWang et al.  proposed an end-to-end quantum-inspired neural framework for text classification. In particular, words are represented as pure states in a complex-valued Hilbert space, while sentences as mixture of pure states (i.e., words). Hence a sentence is represented in a mixture density matrix.  In this work, phases are not defined explicitly but learnt through a backpropagation algorithm. Having said that, the exploitation of complex values is pivotal to the formulation of quantum concepts. Then, a set of measures is applied to the mixture density matrix, resulting in a sequence of scalar values. Practically, the measurements are related to high semantic concepts. Finally, a softmax function normalises the output of measurements into a probability distribution before classifying the sentence. Comprehensive experiments on six different datasets demonstrated the effectiveness of the proposed method against some neural models.\nThis framework was extended in  for a question-answering task. The words are formulated as pure states in a complex-valued Hilbert space. Though, in contrast to , each word is represented in a pure density matrix while a sliding window is applied to the sentence, generating a local mixture density matrix for each local window. Both question and answer are represented by a sequence of mixture density matrices. Then, the same set of common measurements is applied to those density matrices in the sense that they share common semantic concepts. A max-pooling function is performed over the measurement output components, resulting in a dense representation before the matching process through the Cosine similarity measurement. The proposed complex-valued network for matching achieved comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.\nIn~, a quantum many-body wave function is used to model the semantic meaning of words within a local context, i.e., sentences, and a global context, i.e., corpus. In particular, each word is represented by different base states in the sense that each basis corresponds to a different word meaning. First, they model the word meaning within a sentence and corpus by the tensor product of basis states and then project the global tensor representation onto the local one. This results in a high dimensional tensor, capturing interactions of words within a sentence and corpus.  The high dimension resulted tensor representation is further decomposed in subspace base states, which finally processed by a CNN component for constructing the final representation.\n\\emph{Remark:} The above works outperformed various existing neural models at the time when the papers were published. However, more recent neural models, such as BERT based models~ have achieved a largely improved performance on the same tasks. It is worth exploring the integration of quantum models with the new BERT architecture~ in the future.\nReaders interested in tensor networks for representation learning, with or without quantum-inspirations, can also refer to  which introduces a Tensor Space Language model (TSLM) by building higher dimensional tensors using word vectors, leading to a generalisation of n-gram models. \nIn , authors integrate quantum interference phenomena in neural networks with application to ad-hoc retrieval. Existing neural IR models are formulated in terms of classical probability. For example, if $q_i$ represent sub-units (e.q. words) of a query $Q$, then neural models first perform sub-unit level matching and then aggregate the scores obtained to calculate a final relevance score. Formulated in terms of probabilities, such aggregation follows the law of total probability:\n\\vspace{-2mm}\n\\begin{equation}\n    P(R_D|Q) = P(q_1)P(R_D|q_1) + P(q_2)P(R_D|q_2)\n\\end{equation}\nThe authors hypothesise that a quantum-like cognitive interference can occur such that the aggregation of relevance scores can happen non-linearly, due to negative contribution of certain query or document sub-units. Thus the above equation becomes similar to Equation~\\ref{interference-term}:\n\\vspace{-2mm}\n\\begin{equation}\n        P(R_D|Q) = P(q_1)P(R_D|q_1) + P(q_2)P(R_D|q_2) + Int(R_D,Q,q_1,q_2)\n\\end{equation}\nThey proceed to model this interference term and incorporate into a neural architecture. Query and document states are represented as a superposition of their respective sub-unit states with the coefficients of document sub-unit states being the tf-idf values, and those of query sub-units being trainable parameters. A composite system is constructed by taking the tensor product of the query and document state vectors. Then, calculating relevance probability using the trace rule breaks down the probability into two parts -  similarity matching as used in neural IR models and the interference term, which is determined by the interaction between different document features. The similarity matching is achieved using a query attention mechanism, and the interference is modelled as a n-gram window convolution network. The new model is tested on Robust04 and Clueweb09-cat-B collections. While it performs better than various existing neural ranking models and even a vanilla-BERT~ (on P@20 metric on Robust04 dataset), it under-performs the current-state-of-the-art model~ on both NDCG@20 and P@20 on the Robust04. On the other hand, on the Clueweb-09-cat-B dataset it beats existing neural IR models and also the state-of-the-art XLNet model~ in NDCG@20 and ERR@20 metrics.\n\\\\\n\\\\\n\\textit{Quantum-inspired Classification}\n\\\\\nClassification algorithms inspired by Quantum detection theory~ are discussed in . Binary classification is formulated as a signal detection problem.  Projectors are defined to detect a signal, i.e. whether a document belongs to a topic or not. The average cost of incorrect detection (false alarm - detecting a signal when it is absent, miss - failing to detect a signal) is expressed in terms of projection operators and is minimized over a training set. Experiments performed over Reuters Newswire dataset show comparable results with Naive Bayes and SVM algorithms. \nIn , which used dimensionality reduction techniques for word embeddings, the computation times for the complex embeddings in  is reduced, with additional application to the TREC-10 Question Classification task.", "cites": [10, 8308, 5, 7, 8309, 8310, 9, 13, 12, 3, 11], "cite_extract_rate": 0.52, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to discuss quantum-inspired representation learning in IR, connecting lexical semantic spaces, topic models, and complex number representations into a coherent narrative. It critically evaluates the use of complex numbers and mentions limitations of certain approaches. While it identifies some broader patterns in how quantum concepts are applied to semantic modeling, the abstraction remains somewhat limited to the specific IR tasks discussed."}}
{"id": "0e8325c4-ebc7-47e2-8651-6a37eff5b0f3", "title": "Projection Models", "level": "subsubsection", "subsections": [], "parent_id": "95d763de-761a-4cd3-a708-0b1f9c931b03", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "User Interactions"], ["subsubsection", "Projection Models"]], "content": "The area of user interactions in IR has many sub-aspects, primarily - the cognitive level of interaction, understanding the user IN by reformulation, expansion of queries, and building a user profile based on historical interactions. Earlier, we mentioned about the work in , which use multiple basis of a Hilbert space to model different user contexts. This work is further extended in  to combine different user interaction and contextual features for Implicit Relevance Feedback (IRF). Their model uses interaction features like document display time, document saving, document bookmarking, webpage scrolling, webpage depth and document access frequency to construct a user interest profile. A basis vector represents each of these interaction features. The matching of documents against a user profile is done by projecting a document vector onto the subspace spanned by the basis vectors for the user profile. A large projection signifies high relevance of the document to the profile. The features described above are calculated for each document that the user has interacted with and a document-features correlation matrix is formed. Singular Value Decomposition (SVD) is performed to get the eigenvectors, which form the basis for a user profile. \nIn , a general framework for query reformulation using Quantum probabilities is described. The queries are represented as density matrices in a term space and query reformulation updates the query density matrix, which can be used to detect change in user IN in a search session. \nA Hilbert Space for user's perception of document relevance is constructed in~. It deals with the challenge of modeling multidimensional relevance of documents. In an extended Multidimensional User Relevance Model (MURM)~, seven factors or dimensions of relevance are identified, which influence user's judgment of a document. They are Topicality, Interest, Novelty, Understandability, Scope, Habit and Reliability. The features defined for each of these seven dimensions in  are extracted from the retrieved documents of a query and fed into the LambdaMART~ Learning to Rank algorithm. Thus for each relevance dimension, a document has a relevance score. In other words, for a query one gets seven different ranking lists, one for each dimension. The scores obtained are converted into probabilities using max-min normalization. Representing user's relevance perception of a document with respect to each relevance dimension as a vector, and perception of non-relevance as its orthogonal vector, a document can be represented in a two dimensional vector space~. The two relevance and non-relevance vectors for a relevance dimension form an orthonormal basis of the vector space. Figure \\ref{hilbert-space-a} denotes one such vector for a user's perception of document relevance $\\ket{U}_d$ with respect to Topicality dimension of relevance. The projection of the document perception vector on the Topicality vector is proportional to the probability of relevance of the document with respect to the Topicality dimension. Relevance and non-relevance of the document with respect to another dimension is represented as another set of orthogonal vectors, which, in general form another basis in the same Hilbert Space. In Figure \\ref{hilbert-space-b}, we see that the projection of the document perception vector is different in the Reliability basis, suggesting that, while the document has a high relevance when considering the Topicality dimension, it is not so much relevant when considering the Reliability dimension.\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[t]{0.33\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/hilbert-space-document-perception-single-basis.png}\n        \\caption{Topicality dimension in a single basis}\n        \\label{hilbert-space-a}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.42\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/hilbert-space-document-perception-two-basis.png}\n         \\caption{Topicality and Reliability in multiple basis}\n         \\label{hilbert-space-b}\n  \\end{subfigure}\n  \\vspace{-2mm}\n\\caption{Modelling user's perception of relevance dimensions in Hilbert space}\n\\label{hilbert-space-user-perception}\n\\vspace{-5mm}\n\\end{figure} \nThe most popular interpretation of QT is that the state vector collapses upon measurement from a superposition to a definite state. When drawing an analogy, the user's cognitive state is generally considered as in a superposition of various Information Needs (IN) and on judging a document as relevant, it collapses to one particular IN. However, in practice, this may not always be the case. Even after judging documents, a user may still be in an ambiguous or superposed state and there may not have been an apparent change in the IN. The standard interpretation of state collapse may not accurately capture the evolution of IN. This challenge is investigated in  using a Quantum Weak Measurement (QWM) model. It is a generalization of the standard collapse model where the variance of measurement results is much larger. To test the weak measurement phenomena in user judgments, a study is carried out. Users are asked to judge documents on a -4 to 4 scale of relevance. For some query-document pairs, the users are asked to judge the same document for a second time. According to the standard collapse of the IN, after the judging a document as relevant, subsequent judgments will produce the same relevance result. However, it is found that in many cases, users change the relevance decisions. This will happen only when after the judging the document for the first time, the users were still uncertain about the document and their IN. The user's cognitive state was still superposed. This is especially the case where judgments on some documents are not trivial and difficult to make. The weak measurement model involves the Two State Vector Formalism (TSVF) of QT. In TSVF, the state of a system is not represented by a single vector $\\ket{\\psi}$ but rather by two vectors $\\ket{\\phi}$ and $\\ket{\\psi}$, where one vector represents the state of the system in the past (relative to a time $t$), and the other represents the state after time $t$. A weak measurement of an observable $W$ on the system is given by: \n\\vspace{-2mm}\n\\begin{equation} \\label{weak_measurement}\n    w = \\frac{\\bra{\\phi}W\\ket{\\psi}}{\\braket{\\phi}{\\psi}}\n\\end{equation}\nThis type of Quantum measurement is applied in case of session search. A user's IN is represented by two vectors. One contains the historical session information in terms of Implicit relevance feedback, and the other represents the current query, in terms of Pseudo Relevance Feedback. The document vectors, $\\ket{d_i}$, are calculated using word embedding techniques, and then the corresponding projectors $P_{D_i} = \\ket{d_i}\\bra{d_i}$ are constructed. The relevance probability of document $d_i$ using weak measurement is calculated as:\n\\vspace{-2mm}\n\\begin{equation}\n    p = \\frac{\\bra{\\phi_{past}}\\ket{d_i}\\bra{d_i}\\ket{\\phi_{curr}}}{\\braket{\\phi_{past}}{\\phi_{curr}}}\n\\end{equation}\nThe experiment is conducted on the TREC Clueweb12 document collection and the QWM method gives a better performance than the Quantum Language Model and its variations, as well as some state-of-the-art classical IR models.\nThe TSVF is also used in  to modify the query density matrix in QLM. A quantum state is denoted as $\\bra{\\psi_{post}} \\ket{\\psi_{pre}}$. Here $\\ket{\\psi_{pre}}$ is a state evolving from the past to the present and $\\bra{\\psi_{post}}$ is a state devolving from future to the present. The previous user query in the session is considered for the past state and the current query for which the documents are to be retrieved is considered as the future query. Then, separate projectors are constructed for the past and future queries and the density matrix $\\rho_d$ for the document is estimated in the following manner:\n\\vspace{-2mm}\n\\begin{equation}\n    \\rho_d = \\arg\\max(\\sum_{i=1}^{M_{past}}\\log tr(\\rho_d\\Pi_i) + \\sum_{j=1}^{M_{future}}\\log tr(\\rho_d\\Pi_j)) \n\\end{equation}\nwhere $M_{past}$ and $M_{future}$ are the number of projectors (made up of single terms or compound terms) in the past query and the future query respectively.", "cites": [14], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple quantum-inspired approaches to modeling user interactions in IR, connecting them under a common QT framework. It provides critical evaluation by identifying the limitations of the standard quantum state collapse model and discussing scenarios where user IN remains ambiguous. Additionally, it abstracts the concepts into broader patterns, such as the use of Hilbert spaces and multidimensional relevance modeling, and explains their implications for IR systems."}}
{"id": "76577694-8664-4a41-bfca-2ed0cd028b55", "title": "Context Effects", "level": "subsubsection", "subsections": [], "parent_id": "95d763de-761a-4cd3-a708-0b1f9c931b03", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Quantum Theory inspired Information Retrieval"], ["subsection", "User Interactions"], ["subsubsection", "Context Effects"]], "content": "A series of research has been carried out from the user cognitive aspect of IR, drawing parallels from QT and using the Quantum framework to model and explain some of the aspects. An early work~ investigated the interference in relevance judgment of a topic caused by another topic. Consider the topics ``Brave Heart'' (William Wallace's nickname and the name for his film biography) and ``William Wallace'', and a biographical article about William Wallace. Both topics are relevant to the article. Consider another topic about ``William Wallace's wife''. In a user study, it was found out that when the topics ``Brave Heart'' and ``William Wallace'' were displayed together for the article, $93\\%$ users chose to judge the article as relevant to ``William Wallace'' and only $14\\%$ chose it as being relevant to the topic ``Brave Heart''. However, when ``Brave Heart'' was displayed together with ``William Wallace's wife'', $89\\%$ of the users judged ``Brave Heart'' as relevant to the article and $5\\%$ judged ``William Wallace's wife'' to be relevant. There were experiments conducted with different topics and articles and such type of context effects were found, where the presence of one topic or document influences the relevance judgment of another topic or document. In the first case, ``William Wallace'' is highly relevant to the article. It sets a high comparison baseline which affects the judgment for the topic ``Brave Heart'' and results in a low probability of relevance. However, it appears more relevant in comparison with ``William Wallace's wife''. For a Quantum probabilistic explanation of this result, we regard ``William Wallace'' and ``William Wallace's Wife'' as two different contexts for the topic ``Brave Heart''. Each context is described by a basis. So a document or topic $d$ can be represented in the context basis as:\n\\vspace{-2mm}\n\\begin{equation}\n\\ket{d} = a_1\\ket{q_1} + a_2\\ket{\\bar{q_1}}\n\\end{equation}\nwhere $\\ket{\\bar{q_1}}$ represents the absence of context $q_1$. Representing a query $q$ in the same basis as $\\ket{q} = b_1\\ket{q_1} + b_2\\ket{\\bar{q_1}}$, we can calculate the relevance of the document $d$ for query $q$ as:\n\\vspace{-2mm}\n\\begin{align}\np(d|q) &= |\\bra{q}\\ket{d}|^2 \\\\ \\nonumber\n&= (a_1b_1+a_2b_2)*(a_1b_1+a_2b_2)^{\\dagger} \\\\ \\nonumber\n&= a_1^2b_1^2 + a_2^2b_2^2 + 2a_1b_1a_2b_2\\cos\\theta\n\\end{align}\nwhere the probability amplitudes are complex quantities, and $\\theta$ represents the phase difference term. The third term is the interference term, which can be positive or negative depending upon the phase differences. For some contexts, the interference term is negative and the relevance of the same document for the query can be low, which explains why ``Brave Heart'' is judged less relevant when seen in the context of ``William Wallace''. There is a negative interference term that lowers the probability of relevance for the given query/article.\n\\begin{figure}\n\\centering\n    \\begin{subfigure}[t]{0.4\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/order-effect-einstein-1.png}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.4\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/order-effect-einstein-2.png}\n  \\end{subfigure}\n  \\vspace{-7mm}\n\\caption{On viewing document about Theory of Relativity, the judgment of topic Newton is lower for the query Einstein}\n\\label{order-effects-judgment}\n\\vspace{-5mm}\n\\end{figure} \nA follow-on work~ further explored the influence of context in document relevance judgment. It specifically investigates the presence of Order Effects in relevance judgment of documents. In the experiment, users are shown a pair of documents for a query and the relevance judgment by the user for a document is affected by the order, in which the document is presented. For example, for the query ``Albert Einstein'' users are shown documents about ``Issac Newton'' and ``Theory of Relativity''. The relevance probability of ``Issac Newton'' is lower when it is shown after ``Theory of Relativity'' (called a comparative context) than when it is shown first (non-comparative context). In simple terms, having seen a more relevant document first, user's perception of relevance for a particular document may be lower. This can be explained as an Order Effect due to incompatibility between the topics, as shown in Figure \\ref{order-effects-judgment}. The paper also tested the Quantum Question Order inequality~, which is an inequality for testing incompatibility in decision making systems. \nOne of the earliest works to investigate order effects in the different relevance dimensions is . A user study was conducted, in which participants were asked questions about different pairs of relevance dimensions for a document, e.g. Credibility and Understandability, etc. It was found that the judgement of credibility, novelty, etc. was different depending upon the order in which they were asked to judge. \nSimilar order effects using query log data have been investigated in . The method of constructing a Hilbert space for multidimensional document relevance perception from  is used (discussed earlier in this subsection). It is assumed that a user may consider multiple relevance dimensions while judging a document, for example, topicality and novelty. The relevance perception vectors corresponding to different relevance dimensions are in general incompatible in the Hilbert space representation. Thus different orders of consideration of the relevance dimensions may lead to different final judgment of the document. To investigate such behaviour in query log data, a subset of queries are found, where the top two retrieved documents have similar scores of relevance in all the seven dimensions. Yet the first document in the ranked list is not judged relevant, but the second one is. A small set of such queries are indeed found and order effects arising out of different order of consideration of relevance dimensions is offered as a possible explanation for such queries. Figure \\ref{order-effects-logs} explains the order effect for two documents $d_1$ and $d_2$ (ranking order for a query), which have the exact same Hilbert space, yet only $d_2$ is clicked. For $d_1$, if the user first considers Topicality and then Reliability to judge document $d_1$, then the final probability of judgment obtained is $0.0399$ (Figure \\ref{order-effects-logs-a}). Whereas, for $d_2$, if the order is reversed, the probability of final judgment obtained will be $0.3014$ (Figure~\\ref{order-effects-logs-b}), much larger in this case. However, an important question is why the order is reversed in the user's mind for the next document. The authors argue that it could be due to a memory bias - the user can use the last relevance dimension considered for the previous document as the first dimension while judging the current document. Also, there is a possibility that such behaviour can be due to a variety of different reasons, or just random errors in the log data. Nonetheless, a quantum cognitive explanation based on order effects is a possibility. \n\\begin{figure}\n\\centering\n    \\begin{subfigure}[t]{0.35\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/hilbert-space-order-effect-1.png}\n        \\caption{User considers the sequence Topicality,   \\\\ Reliability while judging first document}\n        \\label{order-effects-logs-a}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.35\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/hilbert-space-order-effect-2.png}\n         \\caption{User considers the sequence Reliability, Topicality while judging second document}\n        \\label{order-effects-logs-b}\n  \\end{subfigure}\n  \\vspace{-3mm}\n  \\caption{Different order of consideration of dimensions leads to different final probability}\n  \\label{order-effects-logs}\n  \\vspace{-6mm}\n\\end{figure}\nAs we see that there is some evidence of incompatibility between different relevance dimensions,  investigated the violation of Bell-type inequalities for multidimensional relevance judgment data. A violation of Bell-type inequalities would confirm the quantum nature of data. However, no such violation is found due to lacking probabilities of relevance available for joint judgment of a pair of documents in their dataset.\nOrder effect in risk and ambiguity is also investigated and observed in Information Foraging Theory in . This paper identifies a theoretical limit to simultaneous consideration of risk and ambiguity in decision making using eye tracking data, analogous to the uncertainty principle.\nIn , the phenomena of incompatibility and order effects between relevance dimensions has been studied through a novel protocol design inspired from the Stern-Gerlach experiment of Physics. For a query-document pair, two groups of users were asked three questions relating to Topicality (T), Understandability (U) and Reliability (R) of a document, in orders TUR and TRU respectively. A complex-valued Hilbert space for the user cognitive state is constructed using the data obtained from the experiment, which is used to construct operators corresponding to the T, U and R measurements/judgements. Interference and incompatibility is discussed using these operators. This is the first work where complex numbers are used to capture interactions between relevance dimensions such as incompatibility and interference. It is extended in  to test the violation of a Kolmogorovian probability axiom:\n\\vspace{-2mm}\n\\begin{equation}\\label{eq-kolmogorov}\n     0=\\delta=P(A \\lor B)-P(A)-P(B)+P(A \\land B)\n        \\vspace{0mm}\n\\end{equation}\nwhere the events $A$ and $B$ are the questions regarding Understandability and Reliability of a document. The conjunction and disjunction questions are asked through a specific experiment design and a violation of the above equality is observed in the data. Quantum model predicts a violation for all queries. This paper also\ncompares quantum and Bayesian models for predicting  multidimensional relevance probabilities. Quantum predictions are consistently closer to the experimental data, while predictions from the Bayesian model deviate significantly in some cases. \n\\vspace{-2mm}", "cites": [14, 15, 6], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key findings on context effects in quantum-inspired IR, integrating multiple works into a coherent explanation involving interference and incompatibility. It offers a critical perspective by acknowledging possible alternative explanations (e.g., memory bias, random errors) and the need for further investigation. The abstraction is strong, as it generalizes the quantum cognitive model to explain broader phenomena like order effects and relevance dimension incompatibility."}}
{"id": "81e9b566-0be8-47f4-b42b-70035e5c31e5", "title": "Summary and Limitations", "level": "section", "subsections": [], "parent_id": "88978fb3-2cb3-44da-9096-2cd49c8ec456", "prefix_titles": [["title", "A Survey of Quantum Theory Inspired Approaches to Information Retrieval"], ["section", "Summary and Limitations"]], "content": "\\vspace{-1mm}\n\\begin{table}\n\\begin{tabular}{|p{2.25cm}|p{2.3cm}|p{11.5cm}|}\n\\hline\nArea                                                                                   & Sub-area                                                                           & References \\\\ \\hline\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}l@{}}Representation \\\\ and Ranking\\end{tabular}} & Projection Models                                                                  &  ,,, ,,,,, ,           \\\\ \\cline{2-3} \n                                                                                       & \\begin{tabular}[c]{@{}l@{}}Quantum \\\\ Language Models\\end{tabular}                 &  , , , , , ,  , ,, ,          \\\\ \\cline{2-3} \n                                                                                       & \\begin{tabular}[c]{@{}l@{}}Quantum-inspired\\\\  Ranking\\end{tabular}                &  , , ,           \\\\ \\cline{2-3} \n                                                                                       & Multimodal IR                                                                      & , ,, , ,            \\\\ \\cline{2-3} \n                                                                                       & \\begin{tabular}[c]{@{}l@{}}Quantum-inspired\\\\ Representation \\\\ Learning\\end{tabular} &\n                                                                                       , , , , , , , , , , \\\\ \\hline\n\\multirow{3}{*}{User Interactions}                                                     & Projection Models                                                                  &  , , , , , ,           \\\\ \\cline{2-3} \n                                                                                       & Feedback                                                                           &   \n                                                             , , \n                             \\\\\\cline{2-3} \n                                                                                       & Context Effects                                                                    &   , , , , , ,          \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Quantum-inspired\\\\Neural Networks \\end{tabular}  & - & , , , ,\n , , , , \\\\\n\\hline\n\\end{tabular} \\caption{Review Summary}\n\\label{table-summary}\n\\vspace{-11mm}\n\\end{table}\nvan Rijsbergen's seminal work introduces us to similarities between the mathematical representation of microscopic particles in QT and information objects in IR. It does not delve into much depth over the distinct advantages of the quantum framework over traditional IR frameworks.\nEarly research inspired by van Rijsbergen's ideas implement QT-based ad-hoc IR models by considering information need space as Hilbert space and introducing ideas of superposition for ambiguous queries. These representations provide a good starting point in QR, but they generally fail to outperform the state-of-art methods in IR.\nThe Quantum Language Model (QLM) is a promising application and intends to solve a crucial problem in NLP and IR - of representing compound terms in relation to the individual terms. Superposition principle is made use of and a quantum algorithm to build a language model is applied. It performs better than baseline models like tf-idf and BM25. The later quantum-inspired language models show marked improvement over the QLM but need to be applied on a wide variety of IR and NLP tasks and compared with the state-of-the-art baselines. The complex word embedding is another promising approach, however there is a lack of clarity as to why this methods performs better than some classical methods and what is the intuition behind the interference terms and complex phases. \nThe Quantum Probability Ranking Principle is an important milestone in quantum-inspired IR as it approaches and combines QT and IR from an axiomatic point of view. However, the problem of quantifying the interference term remains and document similarity approaches applied do not take the quantum advantage. One needs to devise a way to subscribe complex phases to documents and then calculate the interference terms. \nThe query fusion and query expansion approaches make use of superposition and interference phenomena, however it is difficult to get an intuitive explanation of how these two are coming into effect and providing the advantage over classical methods. The Contextual QLM (CQLM) and Adaptive-CQLM are promising applications of the QLM to incorporate user interactions, however they are outperformed by the state-of-art machine learning based methods.\nThe integration of the quantum framework to neural networks is promising and combines the representational complexity of neural networks with the probabilistic generalization provided by the quantum framework, especially when complex numbers are included. However, as we see in the results reported in this survey, the state-of-the-art neural networks outperform quantum-inspired models. A reason could be that the datasets used are mostly static, devoid of context, the human factor and its complexities, but it is not the case in real applications. \nThe cognitive experiments on order effects in document judgment provide a good insight into why quantum probability is useful in modeling human decision making. However, most of these experiments are only performed on small user collected samples and need to be conducted on real world search data. Also, they do not yet provide a way to make use of the order effect information to improve the effectiveness of IR systems. \nWe summarise the survey in Table \\ref{table-summary} with the papers categorised into the sub-areas of IR mentioned in Figure \\ref{quantum_ir_brief}. We also list papers which use quantum-inspired neural networks, encompassing all the other sub-areas.\n\\vspace{-2.5mm}", "cites": [10, 8308, 8310, 8, 14, 15, 13, 12, 3, 6], "cite_extract_rate": 0.2033898305084746, "origin_cites_number": 59, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers by organizing them into thematic sub-areas and discussing their shared use of quantum concepts like superposition and interference. It critically evaluates their strengths and limitations, such as underperformance compared to state-of-the-art models and lack of intuitive explanations. The section also abstracts to broader themes, such as the potential of quantum-inspired models to capture context and human cognition, though it stops short of proposing a unified theoretical framework."}}
