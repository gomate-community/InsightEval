{"id": "92692f3f-5d0b-4b5b-92dc-708a1b94dbd5", "title": "Introduction", "level": "section", "subsections": ["73693802-7c4a-4924-a44d-b558714f64f9", "ec4ad3eb-4813-4807-93c2-dcc827dc64d7"], "parent_id": "160c6790-e92e-4d72-a546-5d2f900ac104", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Introduction"]], "content": "\\label{Section 1}\nPretrained Foundation Models (PFMs) are regarded as essential and significant components of Artificial Intelligence (AI) in the era of big data. The foundation model is first named in~, which means a broader class of models and their functions. PFMs are extensively studied in the three major AI fields: natural language processing (NLP)~, computer vision (CV)~ and graph learning (GL)~. PFMs are powerful general models that are effective in various fields or across fields. They have demonstrated great potential in learning feature representations in various learning tasks, such as text classification~, text generation~, image classification~, object detection~, and graph classification~. \nPFMs show superior performance for training on multiple tasks with large-scale corpus and fine-tuning it to similar small-scale tasks, making it possible to initiate rapid data processing.", "cites": [1458, 1550, 2464, 1445], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction section provides a basic overview of Pretrained Foundation Models (PFMs) and mentions their applications in NLP, CV, and GL, citing relevant surveys. However, it lacks synthesis by not connecting the ideas or themes across the cited papers to form a broader narrative. There is minimal critical analysis or evaluation of the cited works, and abstraction is limited to general statements about PFMs without deeper conceptual generalization."}}
{"id": "73693802-7c4a-4924-a44d-b558714f64f9", "title": "PFMs and Pretraining", "level": "subsection", "subsections": [], "parent_id": "92692f3f-5d0b-4b5b-92dc-708a1b94dbd5", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Introduction"], ["subsection", "PFMs and Pretraining"]], "content": "PFMs are built upon the pretraining technique, which aims to train a general model using large amounts of data and tasks that can be fine-tuned easily in different downstream applications.\nThe idea of pretraining originates from transfer learning~ in CV tasks. Recognizing the effectiveness of pretraining in the field of CV, people have begun to use pretraining technology to enhance model performance in other areas. When pretraining techniques are applied to the NLP domain, well-trained language models (LMs) can capture rich knowledge beneficial for downstream tasks, such as long-term dependencies, hierarchical relationships, etc. In addition, the significant advantage of pretraining in the NLP field is that training data can be derived from any unlabeled text corpus, that is, there is an unlimited amount of training data in the pretraining process.\nEarly pretraining is a static technique, such as NNLM~ and Word2vec~, but static methods were difficult to adapt to different semantic environments. Therefore, dynamic pretraining techniques are proposed, such as BERT~, XLNet~, etc.\nFig.~\\ref{history_evolution} depicts the history and evolution of PFMs in the NLP, CV, and GL domains. The PFMs based on the pretraining technique use large corpora to learn generic semantic representations. With the introduction of these pioneering works, various PFMs have emerged and been applied to downstream tasks and applications.\nA great example of PFM application is ChatGPT\\footnote{\\url{https://openai.com/blog/chatgpt/}}. ChatGPT is fine-tuned from the generative pretrained transformer GPT-3.5, which was trained on a blend of text and code~. ChatGPT applies reinforcement learning from human feedback (RLHF) , which has become a promising way to align large language models (LLMs) with a human's intent~. The surprisingly superior performance of ChatGPT may lead to a tipping point for a shift of training paradigm for each type of PFMs -- applying \\textit{instruction aligning} techniques, e.g., reinforcement learning (RL), prompt tuning , and chain-of-thought (COT) , to move towards artificial general intelligence. \nWe focus on reviewing PFMs for text, image, and graph, which is a relatively mature research taxonomy.\nFor text, it is a multi-purpose LM to predict the next word or character in a sequence. For example, PFMs can be used for machine translation, question-answering systems, topic modeling, sentiment analysis, etc.\nFor image, it is similar to PFMs on text, which uses huge datasets to train a big model suitable for many CV tasks. For graphs, a similar pretraining idea is also applied to obtain PFMs, which are used for many downstream tasks. Apart from the PFMs for a specific data domain, we also review and state some other advanced PFMs, such as the PFMs for speech, video, and cross-domain data, and multimodal PFMs. An exemplary illustration is the GPT-4 model, as described by OpenAI~, which is a massive multimodal language model that can process both text and image inputs and generate text outputs. GPT-4 has demonstrated human-level performance on various professional and academic evaluation tasks.  \nMoreover, there is a growing trend in PFMs that deals with multimodal data, known as unified PFMs. This term refers to models that can handle different types of data such as text, images, and audio. In this regard, we provide a definition of unified PFMs and a review of the current state-of-the-art models in recent research. Notable examples include OFA~, UNIFIED-IO~, FLAVA~, BEiT-3~, and others.\nAccording to the features of existing PFMs, we conclude that the PFMs have the following two major advantages. First, minor fine-tuning is required to enhance the model performance on downstream tasks. Second, the PFMs have already been vetted on the quality aspect. Instead of building a model from scratch to solve a similar problem, we can apply PFMs to task-related datasets. \nThe great promise of PFMs has inspired a wealth of related work to focus on the model efficiency~, security~ and compression~.\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{pictures/times.pdf}\n    \\caption{The history and evolution of PFMs.}\n    \\label{history_evolution}\n\\end{figure*}", "cites": [679, 1354, 2465, 1150, 2472, 2466, 9115, 364, 1578, 2470, 7165, 8466, 2467, 2469, 7465, 8536, 2471, 11, 2468, 7579, 7, 1582, 1557], "cite_extract_rate": 0.8518518518518519, "origin_cites_number": 27, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a general overview of pretraining and PFMs, integrating some key ideas from the cited works (e.g., BERT, GPT, RLHF). However, the synthesis is largely chronological and lacks deeper thematic integration. Critical analysis is limited, with no substantial evaluation of the limitations or trade-offs of the cited methods. Abstraction is minimal, with only a brief mention of broader advantages like multimodal pretraining and efficiency."}}
{"id": "ec4ad3eb-4813-4807-93c2-dcc827dc64d7", "title": "Contribution and Organization", "level": "subsection", "subsections": [], "parent_id": "92692f3f-5d0b-4b5b-92dc-708a1b94dbd5", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Introduction"], ["subsection", "Contribution and Organization"]], "content": "There are several survey studies~ that have reviewed the pretrained models for some specific areas such as text generation~, visual transformer~, objection detection~.\nBommasani et.al.~ summarize the opportunities and risks of the foundation model. However, existing works did not achieve a comprehensive review of PFMs in different areas (e.g., CV, NLP, GL, Speech, Video) and different aspects such as pretraining tasks, efficiency, efficacy, and privacy.\nIn this survey, we specifically track the evolution of PFMs in the NLP domain, as well as how pretraining  is transferred to and adopted by CV and GL.\nCompared with other surveys, there is no comprehensive introduction and analysis of existing PFMs from all three fields.\nUnlike reviews of previous pretrained models, we summarize existing models ranging from traditional models to PFMs with recent works in the three domains.\nTraditional models emphasize static feature learning. \nDynamic PFMs give an introduction to structures, which is the mainstream research.\nWe further present some other research for PFMs, including other advanced and unified PFMs, model efficiency and compression, security, and privacy. Finally, we summarize future research challenges and open problems in different domains. We also comprehensively present the related evaluation metrics and datasets \\textbf{in Appendix~\\ref{Evaluation_Metrics} and~\\ref{datasets}}.\nIn summary, the main contributions are as follows:\n\\begin{itemize}\n    \\item We present a solid and up-to-date review of the development of PFM in NLP, CV, and GL. Over the review, we discuss and provide insights about the generalized PFM design and pretraining methodology among the three major application domains.\n    \\item We summarize the development of PFMs in other multimedia areas such as speech and video. Besides, we discuss advanced topics about PFMs, including unified PFMs, model efficiency and compression, and security and privacy.\n    \\item Through the review of PFMs in various modalities for different tasks, we discuss the main challenges and opportunities for future research of very large models in the big data era, which guides a new generation of collaborative and interactive intelligence based on PFMs.\n\\end{itemize}\nThe rest of the survey is organized as follows. \nSection~\\ref{Section 2} introduces the basic components.\nSections~\\ref{Section 3},~\\ref{Section 4} and~\\ref{Section 5} summarize the existing PFMs in NLP, CV and GL, respectively.\nSections~\\ref{Section 6},~\\ref{Section 7} introduce other advanced research for PFMs, including advanced and unified PFMs, model efficiency and compression, as well as security and privacy, respectively. \nFurthermore, we summarize the main challenges for PFMs in Section~\\ref{Section 11} before concluding the survey in Section~\\ref{Section 12}.", "cites": [1458, 7518, 1550, 2464, 1445], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of existing survey works and positions this survey as filling gaps in the literature. It synthesizes information by contrasting traditional models with dynamic PFMs and mentions how pretraining is adapted across domains. While it identifies some limitations of prior surveys and outlines broader research themes, the critical evaluation remains somewhat high-level and does not deeply analyze the cited works' strengths or weaknesses."}}
{"id": "a96c9b0e-9d71-4bee-bf0c-0428d07e4d1b", "title": "Transformer for PFMs", "level": "subsection", "subsections": [], "parent_id": "5b0bdffe-fb02-409c-9a32-2a9ca46fae77", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Basic Components"], ["subsection", "Transformer for PFMs"]], "content": "The Transformer~ is an innovative architecture that facilitates the transfer of weighted representation knowledge between various neural units. It relies solely on attention mechanisms and doesn't use recurrent or convolutional architectures. The attention mechanism is a crucial component of the Transformer as it assigns weights to all the encoded input representations and learns the most important part of the input data. The output of the attention is obtained by taking the weighted sum of the values, and the weights are calculated using the compatibility function of the query with the corresponding key~. Numerous attention mechanisms~ have been developed in large models. For instance, in natural language processing, self-attention is created to connect various positions in a single sequence for generating a representation of the same sequence. Transformer leverages a mask matrix to provide an attention mechanism based on self-attention, in which the mask matrix specifies which words can ``see''  each other.\nTransformer is an important structure for PFMs in NLP, CV, and GL areas. For NLP, the Transformer can help solve the long-range dependency issues when processing sequential input data. For example, the GPT-3~ is a generative model based on the transformer. For CV, the Vision Transformer (ViT)~ is proposed to represent an image to a series of image patches, which is similar to a series of word embeddings. For GL, the Graph Transformer Networks (GTN)~ are employed to learn new graph structures and powerful node representations without domain knowledge. Transformers become scalable enough to drive ground-breaking capabilities for PFMs thanks to the transformer structures to achieve higher parallelization. The ViT-22B model~, for instance, has about 22B parameters, and the largest language models can have upwards of 100B parameters (e.g., GPT-3 has 175B and PaLM~ has 540B parameters).", "cites": [679, 1183, 8558, 732, 1554, 7095, 38], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the Transformer architecture and its application in various domains like NLP, CV, and GL. It integrates some concepts (e.g., self-attention in NLP, image patches in ViT) from cited papers but lacks deeper synthesis or comparative analysis. There is minimal critical evaluation or abstraction beyond specific examples, resulting in a low insight level."}}
{"id": "956e2ed7-ba51-44b2-9c84-7c69ea7faa9f", "title": "Pretraining Tasks for NLP", "level": "subsubsection", "subsections": [], "parent_id": "235dfd2e-07af-44e0-9efe-f1d9c2ab1fa0", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Basic Components"], ["subsection", "Pretraining Tasks for PFMs"], ["subsubsection", "Pretraining Tasks for NLP"]], "content": "The pretraining tasks can be divided into five categories according to the learning methods: Mask Language Modeling (MLM), Denoising AutoEncoder (DAE), Replaced Token Detection (RTD), Next Sentence Prediction (NSP), Sentence Order Prediction (SOP). \nRTD, NSP, and SOP are contrastive learning methods, which assume that the observed samples are more semantically similar than the random samples. \n\\textbf{Mask Language Modeling (MLM).} MLM erases some words randomly in the input sequence and then predicts these erased words during pretraining. Typical examples include BERT~ and SpanBERT~.\n\\textbf{Denoising AutoEncoder (DAE).} DAE is used to add noise to the original corpus and reconstruct the original input using the corpus containing noise. BART~ is a representative example.\n\\textbf{Replaced Token Detection (RTD).} RTD is a discriminant task that determines whether the LM has replaced the current token. This task is introduced in ELECTRA~. By training the model to distinguish whether a token has been replaced or not, the model can acquire language knowledge.\n\\textbf{Next Sentence Prediction (NSP).} In order to make the model understand the correlation between the two sentences and capture sentence-level representations, a NSP task is introduced. The PFM inputs two sentences from different documents and checks whether the order of the sentences is correct. A typical example is BERT.\n\\textbf{Sentence Order Prediction (SOP).} Different from NSP, SOP uses two contiguous fragments from a document as positive samples and the exchange order of the two fragments as negative samples. The PFMs can better model the correlation between sentences, such as ALBERT~.", "cites": [2473, 1150, 1557, 7], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear, factual description of pretraining tasks for NLP models, categorizing them and briefly explaining each. It integrates some information from the cited papers (e.g., BERT, SpanBERT, ELECTRA, ALBERT) to define the tasks, but the synthesis remains minimal and does not present a novel framework. There is little critical analysis or evaluation of trade-offs between methods, and abstraction is limited to basic task-level distinctions without identifying broader trends or principles."}}
{"id": "8b882e93-190e-4264-b9b4-fa6f2a75f940", "title": "Pretraining Tasks for CV", "level": "subsubsection", "subsections": [], "parent_id": "235dfd2e-07af-44e0-9efe-f1d9c2ab1fa0", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Basic Components"], ["subsection", "Pretraining Tasks for PFMs"], ["subsubsection", "Pretraining Tasks for CV"]], "content": "There are many pretraining tasks created for CV to learn the feature space, which is based on SSL. It utilizes pretext tasks that contain human-designed labels, like jigsaw puzzles or the comparison of various patches from images. This enables the generalization of learned representations to a range of downstream tasks.\n\\textbf{Specific Pretext Task.}\nA pretext task also referred to as a predefined task, is created for the encoder networks to perform during the pretraining phase. The network is trained by predicting the answer to a special pretext task. Based on particular features of the data, pseudo labels are generated for the fictitious task. Then, using guided learning techniques, the encoder networks are trained to solve the pretext task.  For example, inpainting aims to pretrain models by predicting the missed center part.\n\\textbf{Frame Order Learning Task.} Learning frame order from videos involves frame processing through time steps, which can serve as the pretraining task for CV. This issue usually relates to completing pretextual exercises that can aid in the acquisition of visual temporal representations.\n\\textbf{Data Generation Task.} The representational capabilities within the generative adversarial networks (GANs) can also be used in the pretraining tasks. Projecting data back into the latent space, as demonstrated by BiGANs~, is helpful for auxiliary supervised discrimination tasks by acting as feature representations.\n\\textbf{Data Reconstruction Task.} Since the images can be divided into patches inspired by the natural language, some pretraining tasks for NLP can also be used in CV, e.g., the autoencoder-based masked prediction. The original image is first divided into a few patches and discrete visual tokens are used to encode each patch. The visual tokens from the masked patches are outputted in the second stage to match the corresponding visual tokens from the fixed tokenizer.\n\\textbf{Miscellaneous.} To train the PFMs in CV, additional pretraining tasks are suggested. For instance, based on contrastive learning, encoder networks are used for pretraining on various data augmentation. The parameters are trained by maximizing the distance between negative pairs (e.g., pairs with different labels) and minimizing the distance between positive pairs (e.g., pairs with the same labels). To pretrain the parameters of the backbone network, the DeepClustering~ method divides the representations into various clusters and labels these clusters as supervised signals.", "cites": [9095, 9094], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various pretraining tasks in computer vision, mentioning SSL and specific methods like inpainting, frame order learning, GANs, and contrastive learning. It references two papers (BiGANs and DeepClustering) but does so in a superficial manner without connecting their ideas cohesively or offering a critical evaluation. The content lacks depth in analysis and abstraction, focusing mainly on listing tasks and their high-level objectives."}}
{"id": "52fdf3e9-1a6e-40a4-a385-dab0848a2c75", "title": "Autoregressive Language Model", "level": "paragraph", "subsections": ["4e905948-4656-4583-bcd3-ea4d62f36d7d", "6f4b628d-0f75-4948-a123-10026b7e2d15"], "parent_id": "95e3c15e-351a-4fa1-a960-f999e1a0eb56", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Word Representations Methods"], ["paragraph", "Autoregressive Language Model"]], "content": "The autoregressive LM predicts the next possible word based on the preceding word or the last possible word based on the succeeding word. \nIt is selected as a feature extractor and text representations are extracted from the former words.\nThus, it has better performance in NLG tasks such as text summarization and machine translation. \nFor a sequence, $T=[w_{1}, w_{2}, \\ldots, w_{N}]$, the probability of a given word calculated as follows:\n\\begin{equation}\np\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)=\\prod_{i=1}^{N} p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{i-1}\\right),\n\\end{equation}\nwhere $i>1$ and $N$ is the length of the input sequence.\nThe GPT~ adopts a two-stage method of self-supervised pretraining and supervised fine-tuning and uses stacked Transformer~ as its decoder.\nAs a follow-up, the OpenAI team continues to expand GPT, proposes the GPT-2~ and increases the number of stacked Transformer layers to 48 layers. The total number of parameters reached 1.5 billion. GPT-2 also introduces multi-task learning~.\nThe GPT-2 has a considerable model capacity and can be adjusted for different task models rather than fine-tuning them. However, GPT-2 also uses an autoregressive LM. Therefore, it improves the performance of the model without increasing the cost dramatically. Due to the lack of contextual modeling ability with a one-way Transformer, the main performance improvement of GPT-2 comes from the combined effect of multi-task pretraining, super-large datasets, and super-large models.\nTask-based datasets for fine-tuning are still needed for specific downstream tasks.\nIncreasing the training scale of the LM can lead to a significant enhancement in task-independent performance. Hence, GPT-3~ was developed, which features a model size of 175 billion parameters and is trained with 45 Terabytes of data. This enables it to exhibit good performance without the need for fine-tuning for specific downstream tasks.", "cites": [679, 38], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of autoregressive language models and their application in GPT variants. It integrates some information from the cited papers (e.g., the architecture from Attention Is All You Need), but the synthesis remains basic and limited to a linear progression of model developments. There is minimal critical evaluation or abstraction beyond the specific models discussed."}}
{"id": "4e905948-4656-4583-bcd3-ea4d62f36d7d", "title": "Contextual Language Model", "level": "paragraph", "subsections": [], "parent_id": "52fdf3e9-1a6e-40a4-a385-dab0848a2c75", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Word Representations Methods"], ["paragraph", "Autoregressive Language Model"], ["paragraph", "Contextual Language Model"]], "content": "The autoregressive LM only uses the information above or below and cannot use the information above and below at the same time. ELMO~ only uses bi-directional Long Short-Term Memory (LSTM), which is a concatenation of two unidirectional LSTMs in backward and forward.\nThe contextual LM predictions are based on contextual words.\nIt uses a Transformer encoder, and the upper and lower layers of the model are all directly connected to each other due to the self-attention mechanism.\nFor a sequence of words $T$, the probability of a given word calculates as follows\n\\begin{equation}\np\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)=\\prod_{i=1}^{N} p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{N}\\right).\n\\end{equation}\nBERT~ uses a stacked multi-layer bi-directional Transformer as the basic structure, and WordPiece~ as a word segmentation method. The model input consists of three parts: word embedding, segment embedding, and position embedding.\nIt uses a bi-directional Transformer as a feature extractor, which offsets the defect of ELMO and GPT. \nHowever, the shortcomings of BERT are also not to be ignored. The bidirectional Transformer structure does not eliminate the constraints of the self-encoding model. Its vast number of model parameters are very unfriendly to devices with low computing resources and are challenging to deploy and apply.\nFurthermore, the hidden language modeling in pretraining will lead to inconsistencies with the input of the model in the fine-tuning stage.\nMost PFMs need more training tasks and a larger corpus.\nAiming at the problem of insufficient training, Liu et al.~ propose the RoBERTa. It uses a larger batch size and unlabeled data. Furthermore, it trains the model for a longer time, removes the NSP task, and adds long sequence training. In processing text input, different from BERT, Byte Pair Encoding (BPE)~ is adopted for word segmentation. BPE uses a different mask mode for each input sequence, even if the input sequence is the same.", "cites": [207, 826, 8385, 8559, 7], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key elements from BERT, ELMO, and RoBERTa, showing how they relate through the lens of contextual language modeling. It provides some critical analysis by pointing out BERT's limitations in deployment and pretraining-finetuning consistency. However, the critique remains somewhat surface-level and the abstraction is limited to general observations rather than deeper principles or trends."}}
{"id": "6f4b628d-0f75-4948-a123-10026b7e2d15", "title": "Permuted Language Model", "level": "paragraph", "subsections": [], "parent_id": "52fdf3e9-1a6e-40a4-a385-dab0848a2c75", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Word Representations Methods"], ["paragraph", "Autoregressive Language Model"], ["paragraph", "Permuted Language Model"]], "content": "The modeling method with a contextual LM can be regarded as the autoencoding model. However, due to the inconsistency in the training stage and fine-tuning stage, the performance of the autoencoding model is poor in the Natural Language Generation (NLG) task.\nPermuted LM aims to combine the advantages of the autoregressive LM and the autoencoder LM.\nIt improves the defects of the two models to a great extent and can be used as a basic idea for the construction of future pretraining target tasks.\nFor a given input sequence $T=[w_{1},w_{2}...\n,w_{N}]$, the formal representation of the target function of the permuted LM is as follows\n\\begin{equation}\n\\max _{\\theta} \\mathbb{E}_{z \\sim Z_{N}}\\left[\\sum_{t=1}^{N} \\log p_{\\theta}\\left(x_{z_{T=t}} \\mid x_{z_{T<t}}\\right)\\right],\n\\end{equation}\nwhere $\\theta$ is the shared parameter in all permutations, $Z_{N}$ represents the set of all possible permutations of the input sequence $T$, and $z_{T=t}$ and $z_{T<t}$ represents the $t$-th element and the $[1, 2, \\ldots, t-1]$ elements of a permutation $z \\in Z_{N}$.\nMLM represented by BERT can implement bi-directional coding well. However, MLM uses the mask marking during pretraining but not during fine-tuning, which resulted in inconsistent data during pretraining and fine-tuning. To achieve bi-directional coding and avoid the problems of MLM, the permuted LM is proposed. permuted LM is based on the autoregressive LM, which avoids the influence of inconsistent data. However, unlike traditional autoregressive models, permuted LM no longer models sequences in order. It gives all possible permutations of sequences to maximize the expected logarithmic likelihood of the sequence. In this way, any position can take advantage of contextual information from all positions, making permuted LM implement bidirectional encoding.\nThe most common permuted LM models are XLNET~ and MPNet~. XLNET is a PFM based on a permuted language modeling approach, which incorporates two crucial techniques from Transformer-XL: relative positional encoding and the segment recurrence mechanism. In contrast, MPNet combines Masked Language Modeling (MLM) and permuted language modeling to predict token dependencies, using auxiliary position information as input to enable the model to view a complete sentence and reduce position differences. These two models represent significant advancements in the field of PFMs.", "cites": [11, 2474], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from both XLNet and MPNet, connecting their motivations and technical approaches to broader issues in pretraining (e.g., pretrain-finetune discrepancy, position information). It provides critical analysis by identifying limitations in BERT’s masked approach and XLNet’s partial position handling, and highlights how permuted LM addresses these. The abstraction level is strong as it generalizes the concept of permuted LM as a foundational idea for future PFM development."}}
{"id": "c73ddc0a-1477-4bcd-be5e-b0d9f7ea2690", "title": "Masking Designing Methods", "level": "subsection", "subsections": [], "parent_id": "c043d8a4-ac81-470e-a27d-0a13ffaae1ac", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Masking Designing Methods"]], "content": "The attention mechanism first aggregates essential words into sentence vectors, and vital sentence vectors into text vectors, which allows the model to pay different attention to different inputs~. \nFor BERT, as a bidirectional encoding LM, any two words in an input sentence can see each other.\nHowever, \nit hinders the ability of BERT model to learn NLG tasks. \nJoshi et al.~ propose \nSpanBERT based on RoBERTa, which adopts the idea of dynamic masking and single segment pretraining, as shown in Fig.~\\ref{SpanBERT} from ~.\nThe span mask and the Span Boundary Objective (SBO) are also proposed to mask words of a certain length.\nThe target task of the span-boundary is to restore all the masked span (tokens) by the observed tokens at both ends.\nThe training stage uses the dynamic mask strategy proposed in the RoBERTa, instead of the mask during the data preprocessing.\nUnlike BERT, SpanBERT randomly covers up a continuous text and adds the SBO training target. It predicts the span using the token closest to the span boundary and eliminates the NSP pretraining task.\nThe BERT and GPT can only separate the training encoder and decoder without joint training in the NLG task. Song et al.~ propose the masked seq2seq pretraining model MASS.\nIn the training stage, the input sequence of the encoder is randomly masked as a continuous segment of length $k$. The masked segment will be recovered through the MASS decoder.\nUniLM~ completes the learning of the NLG model by designing a different mask for two sentences in the input data.\nFor the first sentence, UniLM uses the same structure as the Transformer encoder making each word notice its preceding and following words.\nFor the second sentence, each word can only notice all the words in the first sentence and the preceding words in the current sentence.\nThus, the first and second sentences of the model input form the classic seq2seq pattern.\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{pictures/picture66.pdf}\n    \\caption{The architecture of SpanBERT~.}\n    \\label{SpanBERT}\n\\end{figure*}", "cites": [7096, 2473, 50], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key masking strategies from SpanBERT, MASS, and UniLM, connecting them to BERT and GPT as foundational models. It provides an analytical overview of how each method addresses limitations in bidirectional or unidirectional masking for natural language generation. While it identifies differences in masking approaches, the analysis remains at a moderate level without deeper evaluation or highlighting broader theoretical implications."}}
{"id": "d5594cf1-ec15-4e99-82d4-c0aff6c92c8b", "title": "Boosting on Model Performance", "level": "paragraph", "subsections": ["0411fac8-d211-46f3-867e-f60b611650a8", "9edfc3e7-eda8-42a3-b875-c17fbee5e7bf", "3c5d0aed-29fb-46fe-b5ff-9e1e66b4e827"], "parent_id": "0a4121f5-6ae9-4717-8c66-33026e52d366", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Boosting Methods"], ["paragraph", "Boosting on Model Performance"]], "content": "Most of the popular pretraining models need lots of pretraining data, which imposes huge requirements on the hardware, making it challenging to retrain, and only fine-tuning can be done to the model.\nTo solve these problems, some models appear. For example, ERNIE Tiny released by Baidu is a miniaturized ERNIE~, that reduces the number of layers and increases the prediction speed by 4.3 times with a slight decrease in accuracy.\nLan et al. propose the ALBERT~ to reduce memory consumption and training speed.\nHowever, it is undeniable that no matter what kind of compression is done for these large-scale models, the performance of the models in these tasks will deteriorate sharply. It requires paying attention to the efficient representation of high-level semantic and grammatical information and lossless compression in future works.\nBy using word-embedded parameter factorization and hidden parameter sharing between layers, ALBERT significantly reduces the number of parameters of the model without performance loss. It proposes the training task of SOP, which predicts the order of the two sentences to improve the performance.", "cites": [1150, 2475], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers (ERNIE Tiny and ALBERT) to explain how model compression techniques address the challenges of training large PFMs. It includes some critical remarks about the trade-off between compression and performance but lacks deeper comparative or evaluative analysis. The abstraction is limited to a general observation about the need for future improvements in compression methods, without elevating the discussion to broader principles."}}
{"id": "0411fac8-d211-46f3-867e-f60b611650a8", "title": "Boosting for Multi-task Learning", "level": "paragraph", "subsections": [], "parent_id": "d5594cf1-ec15-4e99-82d4-c0aff6c92c8b", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Boosting Methods"], ["paragraph", "Boosting on Model Performance"], ["paragraph", "Boosting for Multi-task Learning"]], "content": "ERNIE(Baidu)~ is mainly composed of two parts, the Transformer encoder and task embedding. In the Transformer encoder, the self-attention mechanism is used to capture the context information of each token and generate context representation embedding. \nTask embedding is a technique that applies different characteristics to a task.\nERNIE 2.0~ introduces multi-task learning to realize the pretraining of lexical, grammar, and semantics.\nERNIE 2.0 uses seven different pretraining tasks, covering three aspects: word level, sentence level, and semantic level. It uses continual learning, making the knowledge in the previous training task retained and enabling the model to acquire long-distance memory.\nIt uses a Transformer encoder and introduces task embedding, enabling the model to distinguish different tasks in the continual learning process.\nUniLM~ uses three pretraining tasks: unidirectional LM, bidirectional LM, and encoder-decoder LM. \nIt can simultaneously complete three kinds of target tasks in the pretraining stage through the self-attention layer mask mechanism.\nIn the training stage, UniLM adopts the small-segment mask strategy proposed by SpanBERT, and the loss function is composed of the loss functions of the above three pretraining tasks. To maintain the contribution consistency on all loss functions, the three pretraining tasks are trained simultaneously.\nModeling and parameter sharing of multiple tasks make LMs achieve good generalization ability in Natural Language Understanding (NLU) and NLG tasks.", "cites": [2476, 50, 2475], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the methods and architectures of ERNIE and UniLM without deeply connecting them to broader trends or each other. While it touches on multi-task learning and continual training, it lacks critical evaluation or comparison of the approaches. There is minimal abstraction beyond the specific techniques outlined in the cited papers."}}
{"id": "9edfc3e7-eda8-42a3-b875-c17fbee5e7bf", "title": "Boosting for Different Downstream Tasks", "level": "paragraph", "subsections": [], "parent_id": "d5594cf1-ec15-4e99-82d4-c0aff6c92c8b", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Boosting Methods"], ["paragraph", "Boosting on Model Performance"], ["paragraph", "Boosting for Different Downstream Tasks"]], "content": "The pretraining models tend to be large-sized, so how to match different downstream tasks is equally important.\nSome pretraining models that are trained on specialized corpora have appeared~.\nCui et al.~ propose the BERT-whole word masking model (BERT-WWM). They directly use BERT in Chinese to be masked randomly according to the original MLM training, resulting in the loss of semantic information. Since there is no explicit language boundary in Chinese, it is easy to lose significant meaning. ZEN~ is a text encoder based on BERT, which adopts N-gram to enhance performance and effectively integrates considerable granular text information with fast convergence speed and good performance.\nTsai et al.~ propose an oriented multilingual sequence labeling model for sequence labeling tasks. The knowledge distillation method is adopted to achieve better performance in the two tasks: part of speech labeling and morphological attribute prediction for multiple low-resource languages. The inference time is shortened by 27 times.", "cites": [8560], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of methods like BERT-WWM, ZEN, and a multilingual sequence labeling model, referencing their approaches and outcomes. It shows minimal synthesis by grouping them under the theme of boosting for tasks, but lacks deeper analysis or comparison. There is little abstraction or critique, resulting in a medium insight level."}}
{"id": "3c5d0aed-29fb-46fe-b5ff-9e1e66b4e827", "title": "Examples: ChatGPT and Bard", "level": "paragraph", "subsections": [], "parent_id": "d5594cf1-ec15-4e99-82d4-c0aff6c92c8b", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Boosting Methods"], ["paragraph", "Boosting on Model Performance"], ["paragraph", "Examples: ChatGPT and Bard"]], "content": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/chatgpt_RL.png}\n    \\caption{Boosting GPT-3.5 to ChatGPT using Reinforcement Learning from Human Feedback.}\n    \\label{fig:chatgpt_RL}\n\\end{figure*}\nAs shown in Fig.~\\ref{fig:chatgpt_RL}, ChatGPT is fine-tuned based on the PFM GPT-3.5 using RLHF. ChatGPT uses a different data collection setup compared to InstructGPT. First, a large dataset with prompts and the desired output behaviors is collected. The dataset is used to fine-tune GPT-3.5 with supervised learning. Second, given the fine-tuned model and a prompt, the model will generate several model outputs. A labeler gives the desired score and ranks the output to compose a comparison dataset, which is used to train the reward model. Finally, the fine-tuned model (ChatGPT) is optimized against the reward model using the Proximal Policy Optimization (PPO) RL algorithm.\nAnother experimental conversational PFM, the Bard~\\footnote{\\url{https://blog.google/technology/ai/bard-google-ai-search-updates/}}, is developed by Google. Bard is based on the LM for Dialogue Applications (LaMDA). LaMDA~ is built upon the Transformer, which is pretrained on 1.56T words of dialog data and web text. Safety and factual grounding are two main challenges for conversational AI, LaMDA applies the approaches that fine-tuning with high-quality annotated data and external knowledge sources to improve model performance.  \n\\begin{table*}[!htbp]\n\\centering\n\\caption{Summary of PFMs in NLP. \nThe pretraining task includes language model (LM), masked LM (MLM), permuted LM (PLM), denoising autoencoder (DAE), knowledge graphs (KG), and knowledge embedding (KE).\n}\n\\label{text}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{llllllll}\n\\hline\n\\textbf{Year} & \\textbf{Conference}       & \\textbf{Model}    & \\textbf{Architecture}          & \\textbf{Embedding}   & \\textbf{Training method} & \\textbf{Code}                                                                                                                                                                     \\\\ \\hline\n2013 & NeurIPS          & Skip-Gram~             & Word2Vec               & Probabilistic      & -               & \\href{https://github.com/tensorflow/models}{https://github.com/.../models}                                                                                     \\\\ \\hline\n2014 & EMNLP            & GloVe~         & Word2Vec               & Probabilistic      & -               & -                                                                                                                                                                                 \\\\ \\hline\n2015 & NeurIPS          & LM-LSTM~                          & LSTM                   & Probabilistic      & LM              & \\href{https://github.com/stanfordnlp/GloVe}{https://github.com/.../GloVe}                                                                                      \\\\ \\hline\n2016 & IJCAI            & Shared LSTM~                 & LSTM                   & Probabilistic      & LM              & \\href{https://github.com/tensorflow/models/tree/master/research/adversarial\\_text}{https://github.com/.../adversarial\\_text}                                   \\\\ \\hline\n2017 & TACL             & FastText~             & Word2Vec               & Probabilistic      & -               & \\href{https://github.com/facebookresearch/fastText}{https://github.com/.../fastText}                                                                           \\\\ \\hline\n2017 & NeurIPS          & CoVe~                       & LSTM+Seq2Seq           & Probabilistic      & -               & \\href{https://github.com/salesforce/cove}{https://github.com/.../cove}                                                                                         \\\\ \\hline\n2018 & NAACL-HLT        & ELMO~                          & LSTM                   & Contextual         & LM              & \\href{https://allennlp.org/elmo}{https://allennlp.org/elmo}                                                                                                    \\\\ \\hline\n2018 & NAACL-HLT        & BERT~             & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/google-research/bert}{https://github.com/.../bert}                                                                                    \\\\ \\hline\n2018 &                  & OpenAI GPT~              & Transformer Decoder    & Autoregressive     & LM              & \\href{https://github.com/openai/finetune-transformer-lm}{https://github.com/...transformer-lm}                                                                 \\\\ \\hline\n2019 & ACL              & ERNIE(THU)                                                           & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/PaddlePaddle/ERNIE}{https://github.com/.../ERNIE}                                                                                     \\\\ \\hline\n2019 & ACL              & Transformer-XL~            & Transformer-XL         & Contextual         & -               & \\href{https://github.com/kimiyoung/transformer-xl}{https://github.com/.../transformer-xl}                                                                      \\\\ \\hline\n2019 & ICLR             & InfoWord~                      & Transformer Encoder    & Contextual         & MLM             & -                                                                                                                                                                                 \\\\ \\hline\n2019 & ICLR             & StructBERT~                & Transformer Encoder    & Contextual         & MLM             & -                                                                                                                                                                                 \\\\ \\hline\n2019 & ICLR             & ALBERT ~                        & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/google-research/ALBERT}{https://github.com/.../ALBERT}                                                                                \\\\ \\hline\n2019 & ICLR             & WKLM~                     & Transformer Encoder    & Contextual         & MLM             & -                                                                                                                                                                                 \\\\ \\hline\n2019 & ICML             & MASS~                            & Transformer            & Contextual         & MLM(Seq2Seq)    & \\href{https://github.com/microsoft/MASS}{https://github.com/.../MASS}                                                                                          \\\\ \\hline\n2019 & EMNLP-IJCNLP     & KnowBERT~                 & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/allenai/kb}{https://github.com/.../kb}                                                                                                \\\\ \\hline\n2019 & EMNLP-IJCNLP     & Unicoder~                   & Transformer Encoder    & Contextual         & MLM+TLM         & -                                                                                                                                                                                 \\\\ \\hline\n2019 & EMNLP-IJCNLP     & MultiFit~             & QRNN                   & Probabilistic      & LM              & \\href{https://github.com/n-waves/multifit}{https://github.com/.../multifit}                                                                                    \\\\ \\hline\n2019 & EMNLP-IJCNLP     & SciBERT~                   & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/allenai/scibert}{https://github.com/.../scibert}                                                                                      \\\\ \\hline\n2019 & EMNLP-IJCNLP     & BERT-PKD~                      & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/intersun/PKD-for-BERT-Model-Compression}{https://github.com/...Compression}                                                           \\\\ \\hline\n2019 & NeurIPS          & Xlnet~             & Transformer-XL Encoder & Permutation        & PLM             & \\href{https://github.com/zihangdai/xlnet}{https://github.com/.../xlnet}                                                                                        \\\\ \\hline\n2019 & NeurIPS          & UNILM~                        & LSTM + Transformer     & Contextual         & LM + MLM        & \\href{https://github.com/microsoft/unilm}{https://github.com/.../unilm}                                                                                        \\\\ \\hline\n2019 & NeurIPS          & XLM~                          & Transformer Encoder    & Contextual         & MLM+CLM+TLM     & \\href{https://github.com/facebookresearch/XLM}{https://github.com/.../XLM}                                                                                     \\\\ \\hline\n2019 & OpenAI Blog      & GPT-2~                    & Transformer Decoder    & Autoregressive     & LM              & \\href{https://github.com/openai/gpt-2}{https://github.com/.../gpt-2}                                                                                           \\\\ \\hline\n2019 & arXiv            & RoBERTa~                       & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/pytorch/fairseq}{https://github.com/.../fairseq}                                                                                      \\\\ \\hline\n2019 & arXiv            & ERNIE(Baidu)~                    & Transformer Encoder    & Contextual         & MLM+DLM         & \\href{https://github.com/PaddlePaddle/ERNIE}{https://github.com/.../ERNIE}                                                                                     \\\\ \\hline\n2019 & EMC2@NeurIPS            & Q8BERT~                      & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/IntelLabs/nlp-architect/blob/master/nlp\\_architect/models/transformers/quantized\\_bert.py}{https://github.com/.../quantized\\_bert.py} \\\\ \\hline\n2019 & arXiv            & DistilBERT~                & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/huggingface/transformers/tree/master/examples/research\\_projects/distillation}{https://github.com/.../distillation}                   \\\\ \\hline\n2020 & ACL              & fastBERT~                     & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/autoliuweijie/FastBERT}{https://github.com/.../FastBERT}                                                                              \\\\ \\hline\n2020 & ACL              & SpanBERT~     & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/facebookresearch/SpanBERT}{https://github.com/.../SpanBERT}                                                                           \\\\ \\hline\n2020 & ACL              & BART~            & Transformer            & En: Contextual     & DAE             & \\href{https://github.com/huggingface/transformers}{https://github.com/.../transformers}                                                                        \\\\\n     &                  &                                                                      &                        & De: Autoregressive &                 &                                                                                                                                                                                   \\\\ \\hline\n2020 & ACL              & CamemBERT~      & Transformer Encoder    & Contextual         & MLM(WWM)        & \\href{https://camembert-model.fr}{https://camembert-model.fr}                                                                                                  \\\\ \\hline\n2020 & ACL              & XLM-R~        & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/facebookresearch/XLM}{https://github.com/.../XLM}                                                                                     \\\\ \\hline\n2020 & ICLR             & Reformer~           & Reformer               & Permutation        & -               & \\href{https://github.com/google/trax/tree/master/trax/models/reformer}{https://github.com/.../reformer}                                                        \\\\ \\hline\n2020 & ICLR             & ELECTRA~                     & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/google-research/electra}{https://github.com/.../electra}                                                                              \\\\ \\hline\n2020 & AAAI             & Q-BERT~                             & Transformer Encoder    & Contextual         & MLM             & -                                                                                                                                                                                 \\\\ \\hline\n2020 & AAAI             & XNLG~                            & Transformer            & Contextual         & MLM+DAE         & \\href{https://github.com/CZWin32768/xnlg}{https://github.com/.../xnlg}                                                                                         \\\\ \\hline\n2020 & AAAI             & K-BERT~                              & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/autoliuweijie/K-BERT}{https://github.com/.../K-BERT}                                                                                  \\\\ \\hline\n2020 & AAAI             & ERNIE 2.0~                       & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/PaddlePaddle/ERNIE}{https://github.com/.../ERNIE}                                                                                     \\\\ \\hline\n2020 & NeurIPS          & GPT-3~                      & Transformer Decoder    & Autoregressive     & LM              & \\href{https://github.com/openai/gpt-3}{https://github.com/.../gpt-3}                                                                                           \\\\ \\hline\n2020 & NeurIPS          & MPNet~              & Transformer Encoder    & Permutation        & MLM+PLM         & \\href{https://github.com/microsoft/MPNet}{https://github.com/.../MPNet}                                                                                        \\\\ \\hline\n2020 & NeurIPS          & ConvBERT~         & Mixed Attention        & Contextual         & -               & \\href{https://github.com/yitu-opensource/ConvBert}{https://github.com/.../ConvBert}                                                                            \\\\ \\hline\n2020 & NeurIPS          & MiniLM~            & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/microsoft/unilm/tree/master/minilm}{https://github.com/.../minilm}                                                                    \\\\ \\hline\n2020 & TACL             & mBART~        & Transformer            & Contextual         & DAE             & \\href{https://github.com/pytorch/fairseq/tree/master/examples/mbart}{https://github.com/.../mbart}                                                             \\\\ \\hline\n2020 & COLING           & CoLAKE~          & Transformer Encoder    & Contextual         & MLM+KE          & \\href{https://github.com/txsun1997/CoLAKE}{https://github.com/.../CoLAKE}                                                                                      \\\\ \\hline\n2020 & LREC             & FlauBERT~        & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/getalp/Flaubert}{https://github.com/.../Flaubert}                                                                                     \\\\ \\hline\n2020 & EMNLP            & GLM~              & Transformer Encoder    & Contextual         & MLM+KG          & \\href{https://github.com/THUDM/GLM}{https://github.com/.../GLM}                                                                                                \\\\ \\hline\n2020 & EMNLP (Findings) & TinyBERT~       & Transformer            & Contextual         & MLM             & \\href{https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT}{https://github.com/.../TinyBERT}                                          \\\\ \\hline\n2020 & EMNLP (Findings) & RobBERT~        & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/iPieter/RobBERT}{https://github.com/.../RobBERT}                                                                                      \\\\ \\hline\n2020 & EMNLP (Findings) & ZEN~               & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/sinovation/ZEN}{https://github.com/.../ZEN}                                                                                           \\\\ \\hline\n2020 & EMNLP (Findings) & BERT-MK~           & KG-Transformer Encoder & Contextual         & MLM             & -                                                                                                                                                                                 \\\\ \\hline\n2020 & RepL4NLP@ACL     & CompressingBERT~ & Transformer Encoder    & Contextual         & MLM(Pruning)    & \\href{https://github.com/mitchellgordon95/bert-prune}{https://github.com/.../bert-prune}                                                                       \\\\ \\hline\n2020 & JMLR             & T5~       & Transformer            & Contextual         & MLM(Seq2Seq)    & \\href{https://github.com/google-research/text-to-text-transfer-transformer}{https://github.com/...transformer}                                                 \\\\ \\hline\n2021 & T-ASL            & BERT-wwm-Chinese~                  & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/ymcui/Chinese-BERT-wwm}{https://github.com/...BERT-wwm}                                                                      \\\\ \\hline\n2021 & EACL             & PET~                 & Transformer Encoder    & Contextual         & MLM             & \\href{https://github.com/timoschick/pet}{https://github.com/.../pet}                                                                                           \\\\ \\hline\n2021 & TACL             & KEPLER~       & Transformer Encoder    & Contextual         & MLM+KE          & \\href{https://github.com/THU-KEG/KEPLER}{https://github.com/.../KEPLER}                                                                                        \\\\ \\hline\n2021 & EMNLP   & SimCSE~     & Transformer Encoder    & Contextual         & MLM+KE          & \\href{https://github.com/princeton-nlp/SimCSE}{https://github.com/.../SimCSE}                                                                                  \\\\ \\hline\n2021 & ICML   & GLaM~     & Transformer    & Autoregressive         & LM          & -                                                                                  \\\\ \\hline\n2021 & arXiv   & XLM-E~     & Transformer    & Contextual         & MLM          &                                                                                   \\\\ \\hline\n2021 & arXiv   & T0~     & Transformer    & Contextual         & MLM          & \\href{https://github.com/bigscience-workshop/t-zero}{https://github.com/.../T0}                                                                                  \\\\ \\hline\n2021 & arXiv   & Gopher~     & Transformer    & Autoregressive         & LM          & -                                                                                 \\\\ \\hline\n2022 & arXiv   & MT-NLG~     & Transformer   & Contextual         & MLM          & -                                                                                 \\\\ \\hline\n2022 & arXiv   & LaMDA~     & Transformer Decoder   & Autoregressive         & LM          & \\href{https://github.com/conceptofmind/LaMDA-rlhf-pytorch}{https://github.com/.../LaMDA}                                                                                  \\\\ \\hline\n2022 & arXiv   & Chinchilla~     & Transformer   & Autoregressive         & LM          & -                                                                                  \\\\ \\hline\n2022 & arXiv   & PaLM~     & Transformer   & Autoregressive         & LM         & \\href{https://github.com/lucidrains/PaLM-pytorch}{https://github.com/.../PaLM}                                                                                  \\\\ \\hline\n2022 & arXiv   & OPT~     & Transformer Decoder  & Autoregressive         & LM          & \\href{https://github.com/facebookresearch/metaseq}{https://github.com/.../MetaSeq}                                                                                  \\\\ \\hline\n\\end{tabular}\n}\n\\end{table*}", "cites": [1554, 679, 50, 8561, 7580, 2482, 2466, 2478, 1150, 1185, 2493, 2491, 8469, 8369, 2484, 1684, 1551, 2487, 2474, 8385, 856, 1557, 8462, 2480, 9, 826, 2490, 2477, 2483, 2473, 794, 2219, 2475, 11, 7096, 2488, 7271, 2492, 4511, 2479, 8424, 7097, 7098, 2476, 2485, 2489, 7461, 2481, 7370, 1156, 7581, 7463, 7579, 7460, 1553, 2486, 7], "cite_extract_rate": 0.8507462686567164, "origin_cites_number": 67, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of ChatGPT and Bard, referencing how they are built and trained, but lacks deeper synthesis or integration of the cited papers. It summarizes individual techniques (e.g., RLHF for ChatGPT, fine-tuning for LaMDA) without connecting broader themes or comparing them critically. The abstraction is minimal, focusing on specific models and methods rather than identifying general trends or principles."}}
{"id": "1d6edf07-e612-4411-b731-77aa6ec24303", "title": "Instruction-Aligning Methods", "level": "subsection", "subsections": ["99bd7f7c-a4dc-45f2-b182-b792c3d76c56"], "parent_id": "c043d8a4-ac81-470e-a27d-0a13ffaae1ac", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Instruction-Aligning Methods"]], "content": "Instruction-aligning methods aim to let the LM follow human intents and generate meaningful outputs. The general approach is fine-tuning the pretrained LM with high-quality corpus in a supervised manner. To further improve the usefulness and harmlessness of LMs, some works introduce RL into the fine-tuning procedure so that LMs could revise their responses according to human or AI feedback. Both supervised and RL approaches can leverage chain-of-thought  style reasoning to improve the human-judged performance and transparency of AI decision-making.", "cites": [1578], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces instruction-aligning methods and mentions chain-of-thought prompting as a technique used in fine-tuning, but it lacks deeper synthesis or comparison with other methods. It does not critically evaluate the cited work or identify broader trends or principles beyond the surface-level description."}}
{"id": "99bd7f7c-a4dc-45f2-b182-b792c3d76c56", "title": "Supervised Fine-Tuning (SFT)", "level": "paragraph", "subsections": ["0b6ce232-e4d0-4aa1-8097-e70973f4ea40", "f0b30ce0-79ed-4853-b7a2-3527355f4a4c"], "parent_id": "1d6edf07-e612-4411-b731-77aa6ec24303", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Instruction-Aligning Methods"], ["paragraph", "Supervised Fine-Tuning (SFT)"]], "content": "SFT is a well-established technique to unlock knowledge and apply it to specific real-world, even unseen tasks. The template for SFT is composed of input-output pairs and an instruction . For example, given the instruction ``Translate this sentence to Spanish:'' and an input ``The new office building was built in less than three months.'', we want the LM to generate the target ``El nuevo edificio de oficinas se construyó en tres meses.''. The template is commonly humanmade including unnatural instructions  and natural instructions , or bootstrap based on a seed corpus . Ethical and social risks of harm from LMs are significant concerns in SFT . LaMDA, the largest LM to date, thus relies on crowdworker annotated data for providing a safety assessment of any generated LaMDA response in three conversation categories: natural, sensitive, and adversarial. The list of rules serves further safety fine-tuning and evaluation purposes.", "cites": [2208, 2494, 1587, 8534, 2198], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of Supervised Fine-Tuning (SFT) and mentions several related papers, but it does not effectively synthesize or integrate their contributions into a cohesive narrative. It lacks critical analysis of the methods or limitations and only briefly touches on general patterns like the use of natural and unnatural instructions. The content is primarily factual and descriptive in nature."}}
{"id": "0b6ce232-e4d0-4aa1-8097-e70973f4ea40", "title": "Reinforcement Learning from Feedback", "level": "paragraph", "subsections": [], "parent_id": "99bd7f7c-a4dc-45f2-b182-b792c3d76c56", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Instruction-Aligning Methods"], ["paragraph", "Supervised Fine-Tuning (SFT)"], ["paragraph", "Reinforcement Learning from Feedback"]], "content": "RL has been applied to enhance various models in NLP tasks such as machine translation , summarization , dialogue generation , image captioning , question generation , text-games , and more . RL is a helpful method for optimizing non-differentiable objectives in language generation tasks by treating them as sequential decision-making problems. However, there is a risk of overfitting to metrics that use neural networks, leading to nonsensical samples that score well on the metrics . RL is also used to align LMs with human preferences . \nInstructGPT proposes to fine-tune large models with PPO against a trained reward model to align LMs with human preference , which is the same method applied by ChatGPT \nnamed RLHF. Specifically, the reward model is trained with comparison data of human labelers' manual rankings\nof outputs. For each of them, the reward model or machine labeler calculates a reward, which is used to update the LM using PPO. More details are illustrated in Fig. \\ref{fig:chatgpt_RL}. \nOne of the recent breakthroughs in PFM technology is GPT-4~, which follows a pretraining approach to predict the subsequent token in a document and then undergoes RLHF fine-tuning. \nAs the task complexity increases, GPT-4 outperforms GPT-3.5 in terms of reliability, creativity, and capability to handle more nuanced instructions.\nSparrow , developed by DeepMind, also utilizes RLHF that reduces the risk of unsafe and inappropriate answers. Despite some promising results using RLHF by incorporating fluency, progress in this field is impeded by a lack of publicly available benchmarks and implementation resources, resulting in a perception that RL is a difficult approach for NLP. Therefore, an open-source library named RL4LMs~ is introduced recently, which consists of building blocks for fine-tuning and evaluating RL algorithms on LM-based generation. \nBesides human feedback, one of the latest dialogue agents --  Claude favors Constitutional AI  where the reward model is learned via RL from AI Feedback (RLAIF). Both the critiques and the AI feedback are steered by a small set of principles drawn from a ‘constitution’, the specification of a short list of principles or instructions, which is the only thing provided by humans in Claude. The AI feedback focuses on controlling the outputs to be less harmful by explaining its objections to dangerous queries.", "cites": [432, 8472, 364, 2499, 2497, 7099, 2495, 2501, 2498, 1928, 2399, 2500, 2496, 9115], "cite_extract_rate": 0.875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear synthesis of reinforcement learning techniques for aligning large language models with human preferences, connecting RLHF in InstructGPT/ChatGPT, Constitutional AI in Claude, and other methods like Quark and ILQL. It critically addresses limitations such as the difficulty of benchmarking and implementation challenges. However, while it touches on broader patterns in alignment strategies, it stops short of offering a deep, meta-level abstraction or a novel unifying framework."}}
{"id": "f0b30ce0-79ed-4853-b7a2-3527355f4a4c", "title": "Chain-of-Thoughts", "level": "paragraph", "subsections": [], "parent_id": "99bd7f7c-a4dc-45f2-b182-b792c3d76c56", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Natural Language Processing"], ["subsection", "Instruction-Aligning Methods"], ["paragraph", "Supervised Fine-Tuning (SFT)"], ["paragraph", "Chain-of-Thoughts"]], "content": "Chain-of-thought (CoT) prompting is a technique for improving the reasoning ability of LLMs by prompting them to generate a series of intermediate steps that lead to the final answer of a multi-step problem. The CoT is a series of intermediate reasoning steps, which can significantly improve the ability of LLMs to perform complex reasoning . Besides, fine-tuning with CoT shows slightly more harmless compared to without CoT . CoT prompting is an emergent property of model scale, meaning it works better with larger and more powerful language models. It is also possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.\nIn a CoT prompting experiment, a prompt is provided to the model that outlines a multi-step problem. The prompt might pose a question such as ``After selling 30 out of his 100 chickens and 10 out of his 20 pigs, how many animals does a farmer have left?'' The model then generates a sequence of intermediate reasoning steps, for example, ``The farmer has 100-30=70 chickens remaining'' and ``The farmer has 20-10=10 pigs remaining,'' before generating the final answer, such as ``The farmer has 70+10=80 animals remaining.'' CoT prompting has demonstrated its efficacy in improving the performance of LLMs on various reasoning tasks, such as arithmetic, symbolic reasoning, and common sense. It is a promising technique that can enhance the ability of language models to reason about complicated problems.", "cites": [7468, 8472, 1578], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a clear description of the chain-of-thought prompting technique and references relevant papers, but it lacks deeper synthesis and integration of ideas beyond what is explicitly stated in the cited works. While it mentions that CoT is an emergent property of model scale and improves reasoning, it does not critically evaluate the limitations or trade-offs of the method. Some abstraction is present, but it is limited to general observations rather than meta-level insights."}}
{"id": "bbf1343d-17e2-4b47-9679-bde2c480d2f0", "title": "Learning by Specific Pretext Task", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Specific Pretext Task"]], "content": "In the early stage of unsupervised learning, the network is trained by designing a special pretext task and predicting the answer to this task. Dosovitskiy et al.~ pretrain the Exemplar CNN to discriminate the different patches from the unlabelled data. The experiments prove the designs can learn useful representations transferred to the standard recognition assignments. In the method based on context prediction~, a handcrafted supervised signal about the position information serves as the label for the pair classification. \nInpainting~ aims to pretrain models by predicting the missed center part. \nBecause inpainting is a semantic-based prediction, another decoder is linked to the context encoder in this manner. Furthermore, the standard pixel-by-pixel reconstruction process of the decoder can be transferred to any other downstream inpainting tasks.\nSpecifically, Colorization~ is a method that evaluates how colorization as a pretext task can help to learn semantic representation for downstream tasks. It is also known as the \\emph{cross-channel encoding} since different image channels serve as input and the output is discriminated. Similarly, Split-Brain\tAutoencoder~ also learns representations in a self-supervised way by forcing the network to solve cross-channel prediction tasks.\nJigsaw~ is proposed to pretrain the designed Context-Free Network (CFN) in a self-supervised manner by first designing the Jigsaw puzzle as a pretext task.  Completing Damaged Jigsaw Puzzles (CDJP)~ learns image representation by complicating pretext tasks furthermore, in which puzzles miss one piece and the other pieces contain incomplete color. Following the idea of designing efficient and effective pretext tasks, Noroozi et al.~ use counting visual primitives as a special pretext task and outperform previous SOTA models on regular benchmarks.\nNAT~ learns representation by aligning the output of backbone CNN to low-dimensional noise.\nRotNet~ is designed to predict different rotations of images.\n\\begin{figure*}[!htp]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/CPC_zip.pdf}\n    \\caption{Contrastive Predictive Coding~. The input sequence can represent both images and videos.}\n    \\label{fig:cpc}\n\\end{figure*}", "cites": [134, 2504, 126, 2507, 2502, 2506, 131, 2505, 1254, 2503], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of various pretext tasks used in unsupervised representation learning for computer vision, listing methods and their objectives. While it integrates some terminology and categorizes techniques, it lacks deeper synthesis of overarching themes or connections between the approaches. There is minimal critical evaluation or abstraction beyond individual methods."}}
{"id": "6505e098-bf5b-4afb-a19e-a38f0c9a03f2", "title": "Learning by Frame Order", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Frame Order"]], "content": "The learning of sequence data such as videos always involves frame processing through time steps. This problem often connects with solving pretext tasks that can help to learn visual temporal representations. Contrastive Predictive Coding (CPC)~ is the first model to learn data representations by predicting the future in latent space. This model can be fed with data in any modalities, like speech, images, text, etc.\nThe components of CPC are shown in Fig. \\ref{fig:cpc} from~, where the $x_t$ represents the input sequence of observations, $z_t$ is a sequence of latent representations after the encoder $g_{enc}$, and $c_t$ is a context latent representation that summarizes all the latent sequence $z_{\\le t}$ after an autoregressive model $g_{ar}$. Unlike the traditional model predicts future frames $x_{t+k}$ by a generative model $p_k(x_{t+k}|c_t)$, CPC models a \"density ratio\" $f_k$ to represent the mutual information between the context latent representation $c_t$ and future frame $x_{t+k}$:\n\\begin{equation}\n    f_k(x_{t+k},c_t)\\propto p(x_{t+k}|c_t) / x_{t+k}.\n\\end{equation}\nAfter the encoding of recurrent neural networks, $z_t$ and $c_t$ can both be chosen\nfor the downstream tasks as needed. The encoder and autoregressive model are trained by InfoNCE~ as follows\n\\begin{equation}\n    \\mathcal{L}=-\\mathbbm{E}_X[\\log f_k(x_{t+k},c_t) / \\sum\\nolimits_{x_j\\in X}f_k(x_j,c_t)],\n\\end{equation}\nwhere $X$ denotes the training dataset containing both positive and negative samples. The density ratio $f_k$ can be estimated by optimizing $\\mathcal{L}$.\nCPC v2 revisits and improves CPC~ by pretraining on unsupervised representations, and its representation generality can be transferred to data-efficient downstream tasks.", "cites": [134, 2508], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key concepts of CPC and its improvement (CPC v2) into a coherent explanation of how frame order is used for learning temporal representations. It abstracts the core idea of predicting the future in latent space and explains the InfoNCE objective. However, the critical analysis is limited, as the section does not evaluate the strengths or weaknesses of the approaches or compare them to others in depth."}}
{"id": "3bd63707-4357-48a2-914a-922ccb66761b", "title": "Learning by Generation", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Generation"]], "content": "\\label{sec:Learning by Generation}\nAlthough many existing applications are popular after the development of the GAN-based approach, the representation abilities inside the GANs are not entirely exploited due to the absence of a feature encoder. \nThus, Bidirectional Generative Adversarial Networks (BiGANs)~ is proposed to project data back into the latent space, which is useful for auxiliary supervised discrimination tasks via serving as feature representations. \nBased on BiGANs, BigBiGAN~ first achieves the SOTA in unsupervised representation learning on ImageNet by adding an encoder and modifying the discriminator. As shown in Fig. \\ref{fig:bigbigan} from~, the traditional components of GANs (encoder $\\mathcal{E}$ and generator $\\mathcal{G}$) are used to produce data-latent pairs, denoted as $(\\textbf{x}\\sim P_{\\textbf{x}},\\hat{\\textbf{z}}\\sim\\mathcal{E}(\\textbf{x}))$ and $(\\hat{\\textbf{x}}\\sim\\mathcal{G}(\\textbf{z}),\\textbf{z}\\sim P_{\\textbf{z}})$. The final loss $\\ell$ is defined as the sum of data-specific term $s_{\\textbf{x}},s_{\\textbf{z}}$ and data-joint term $s_{\\textbf{xz}}$. The introduced discriminator $\\mathcal{D}$ (Adversarially Learned Inference (ALI)~, or BiGAN~) learns to discriminate between pairs from the raw data, latent distribution and encoded vector. \n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/BigBiGAN_zip.pdf}\n    \\caption{The structure of the BigBiGAN framework~.}\n    \\label{fig:bigbigan}\n\\end{figure*}", "cites": [7100, 9095, 2509], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates the key concepts from the three cited papers by explaining the evolution from GANs to BiGANs and finally to BigBiGAN, highlighting their role in unsupervised representation learning. However, the synthesis remains limited and descriptive, focusing on technical components rather than connecting broader ideas across the papers. There is some critical analysis in noting that GAN-based approaches were initially underutilized for representation learning, but deeper evaluation or comparison of their strengths and weaknesses is missing. The abstraction level is moderate, as it generalizes the idea of using adversarial learning for generation and inference but does not extract overarching principles or trends."}}
{"id": "5285bdeb-057a-4fb0-96bd-eb8d91f710bc", "title": "Learning by Reconstruction", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Reconstruction"]], "content": "The iGPT  and ViT  models have demonstrated the feasibility of adapting the pretext task of masked prediction using auto-encoder from language to image data. BEiT  is the first to demonstrate that autoencoder-based masked prediction can outperform DINO , a conventional SOTA method without pretraining techniques. Specifically, BEiT consists of two stages: token embedding with discrete variational autoencoder (dVAE) , and tokenizer training with masked image prediction. In the first stage, the original image is split into some patches and encoded using discrete tokens, which is different from BERT since image patches don't have off-the-shelf tokens as words in NLP. In the second stage, the BEiT encoder takes a corrupted image containing unmasked and masked patches, and then the visual tokens of the masked patches are outputted to match the corresponding visual tokens from the fixed tokenizer. Despite its success, the separation between masked prediction and autoencoder training induces that the whole framework is not end-to-end and hinders learning effectiveness and efficiency. \n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/memorybank_zip.pdf}\n    \\caption{The general pipeline for the Memory Bank Method~.}\n    \\label{fig:memorybank}\n\\end{figure*}\nTo migrate this issue, MAE  proposes an end-to-end simple solution by predicting the masked patches directly from the unmasked ones with the Mean Squared Error (MSE) loss. It's worth noting that MAE uses a masking ratio of 75\\%, which is significantly higher than that of BERT (typically 15\\%). Ablation study suggests that higher masking ratios are beneficial for both fine-tuning and linear probing. Concurrently, SimMIM  proposes a similar autoencoder-based solution as MAE, in which they also confirm that a higher marking ratio and leveraging random masking strategy helps improve performance. The major difference is how they partition the responsibility of representation encoding and pretext prediction in the autoencoder. Since the decoder of SimMIM is simple, the encoder of SimMIM synchronously conducts both of them. On the contrary, the encoder in MAE solely undertakes the role of representation encoding, and the decoder is responsible for pretext prediction. Recently, Meta AI announces the Segment Anything Model (SAM)~ which prompts users to specify what to segment in an image, allowing for a wide range of segmentation tasks without the need for additional training. SAM employs an MAE pretrained ViT-H~ image encoder that runs once per image and produces an image embedding, as well as a prompt encoder that embeds input prompts such as clicks or boxes. Following that, a lightweight transformer-based mask decoder predicts object masks from image and prompt embeddings. The results show that SAM can generate high-quality masks from a single foreground point that are typically just modestly inferior to the manually annotated ground truth. It routinely achieves strong quantitative and qualitative outcomes on a wide range of downstream tasks using a zero-shot transfer approach and prompt engineering.\nLeveraging ViT in MAE poses a serious inefficiency issue, where decreasing the patch size results in a quadratic increase in computing resources. To address the problem, there are two important solutions: (1) hierarchical ViT and (2) local attention. In the first direction, hierarchical ViT (hViT) was introduced, which utilizes a shrinking pyramid structure and techniques like shifted windows  to reduce computational demands. Unfortunately, hViT cannot be directly applied to enable MAE pretraining because the local window attention used in hViT makes it difficult to handle randomly masked patches as in MAE. Recently, Uniform Masking MAE (UM-MAE)  is proposed to empower MAE with hViTs, which introduces a two-stage pipeline: sampling and masking. It starts by randomly sampling a portion of patches (25\\% reported in the paper) from each block, and then follows by masking additional patches on top of the sampled ones. \nThe first step helps to maintain common elements across different local windows, while the second step prevents shortcuts for pixel reconstruction from nearby low-level features, making the task more difficult. Another direction to improve efficiency focuses on reducing the input size by putting the attention of the network into some local small windows of the image. Motivated by the observation that local knowledge is sufficient for reconstructing masked patches, Local masked reconstruction (LoMaR)  was proposed. Rather than using the entire image for mask reconstruction, LoMaR samples a number of small windows and focuses attention on local regions, which outperforms MAE on downstream tasks in terms of learning efficiency.", "cites": [9149, 2512, 732, 2513, 2514, 2515, 1501, 2511, 7339, 2510], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes several works on vision foundation models by tracing the evolution of masked image modeling techniques. It critically evaluates limitations, such as the inefficiency of ViT in MAE, and presents how different papers (e.g., UM-MAE and LoMaR) attempt to resolve them. The abstraction level is moderate, as it identifies broader trends in efficiency improvements and design choices without offering a completely meta-level conceptual framework."}}
{"id": "50ed8c6b-4ab0-4ae5-a530-8e92f1907e7b", "title": "Learning by Memory Bank", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Memory Bank"]], "content": "Non-Parametric Instance Discrimination (NPID)~ is the first method that utilizes the instances to learn representations for downstream tasks. The detailed pipeline is shown in Fig. \\ref{fig:memorybank}. The feature representations are stored in the memory bank for the convenience of computation because the instance-level classification objective needs all images in the training dataset. For any image $x$ with feature representation $\\textbf{v}=f_\\theta(x)$, its probability of being recognized as $i$-th example is:\n\\begin{equation}\n    P(i|\\textbf{v}) = exp(\\textbf{v}_i^{\\mathrm{T}}\\textbf{v}/\\tau) / \\sum\\nolimits_{j=1}^n exp(\\textbf{v}_j^{\\mathrm{T}}\\textbf{v}/\\tau),\n\\end{equation}\nwhere $\\textbf{v}_i$ or $\\textbf{v}_j$ is the representation of $i$-th or $j$-th sample, which serves as a substitute for the parametric class prototype (i.e., weights of a classifier). Addtionally, $\\tau$ is the temperature parameter borrowed from the knowledege distillation~. \nLocal Aggregation (LA)~ is another method that trains a CNN encoder to embed raw images into a lower dimension space -- embedding space.\nWhen a metric of local aggregation is maximized, similar data instances move together in the embedding space while dissimilar instances move apart.\nBased on NPID, Pretext Invariant Representation Learning (PIRL, pronounced as ``pearl'')~~ is proposed to argue that semantic representations are invariant under pretext transformation tasks. Suppose the original view and transformed view of images are denoted as $I$ and $I^{t}$, respectively. These sample views are fed into a CNN encoder, and the total empirical loss on the training dataset $\\mathcal{D}$ can be defined as:\n\\begin{equation}\n    \\mathcal{L}_{total}(\\theta;\\mathcal{D})=\\mathbbm{E}_{t\\sim\\mathcal{T}}\\left[\\frac{1}{|\\mathcal{D}|}\\sum\\nolimits_{\\boldsymbol{I}\\in\\mathcal{D}}\\mathcal{L}(\\boldsymbol{V_I},\\boldsymbol{V}_{\\boldsymbol{I}^{t}})\\right],\n\\end{equation}\nwhere $\\mathcal{T}$ denotes the different transformations of images. \nThe loss encourages the representation of image $\\boldsymbol{I}$ to be similar to that of $\\boldsymbol{I}^t$, and the representation of $\\boldsymbol{I}^t$ to be dissimilar to that of different images $\\boldsymbol{I}'$, as shown in the dotted box of Fig.~\\ref{fig:two-stream}. \nTherefore, more negative sample pairs contribute to  improving the scalability of the gradient and lead to the final learned encoder with stronger representation ability.\nThat is the reason why the memory bank is introduced to store more previous representations for subsequent comparison. \n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{figures/sum_cropped.pdf}\n    \\caption{Summary of all two-stream models, including contrastive learning and memory-bank-based methods.}\n    \\label{fig:two-stream}\n\\end{figure*}", "cites": [7018, 681, 2516], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three different papers to present a connected narrative around memory bank-based methods in PFMs for vision. It explains how NPID, LA, and PIRL build on similar concepts, such as embedding spaces and invariance of representation. However, the critical analysis is limited to pointing out benefits like scalability and representation ability without deeper evaluation of limitations or trade-offs. Some abstraction is achieved through the discussion of general principles in contrastive learning and memory banks, but it does not fully rise to a meta-level insight."}}
{"id": "f232b9b3-0708-4ccd-84aa-6a5b5b5d66e4", "title": "Soft Sharing.", "level": "paragraph", "subsections": ["cd9e37cc-d2be-4335-9b89-7c6b8fe84b1b"], "parent_id": "d93ecf7c-0026-4dc5-bb46-2d93ada041fa", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Sharing"], ["paragraph", "Soft Sharing."]], "content": "Facebook AI Research (FAIR) presents Momentum Contrast (MoCo)~ \nby using momentum to control the slight difference between two encoders. As shown in Fig. \\ref{fig:moco}, one of the encoders is served as a dictionary look-up task that generates a queue of encoded data samples $\\{k_0, k_1, \\cdots\\}$.\nAnother encoder generates encoded query $\\{q_0, q_1, \\cdots\\}$ with the training batch updated. \nThe similarity is measured by the dot product of the new coming encoded query $q$ and the encoded keys stored in the dictionary queue. Suppose there are $K$ keys stored in the queue before the new key comes. The $K$ keys are treated as negative samples to the query of the new key. To combine the contrastive loss on both negative and positive samples, InfoNCE Loss ~ is used for the pretraining in MoCo.\nThe key design in MoCo for soft parameter sharing is called momentum update. He et al.~ suggest that the direct parameter change of key encoder (i.e., momentum encoder) to query encoder loses the necessary consistency and yields poor results. The momentum encoder parameter $\\theta_k$ is updated as:\n\\begin{equation}\\label{momentum-update}\n    \\theta_k = m\\theta_k + (1-m)\\theta_q,\n\\end{equation}\nwhere the query encoder parameter $\\theta_q$ is learned directly from the gradients of new coming instance, and $m\\in[0, 1)$ is a hyper-parameter that controls the consistency ($\\theta_k$ is more consistent if $m$ is closer to $1$).\nInspired by the design of SimCLR~, in MoCo v2~, the FAIR team introduces an MLP projection head after encoders and utilizes more data augmentation techniques to improve the performance. The further improvements are from that: 1) embedded linear classifier bridges the gap between unsupervised and supervised pretraining representations; 2) more contrastive samples are feasible from both the larger training batch and stronger data augmentation.\nDeepMind proposed Bootstrap Your Own Latent (BYOL)~ that contains representation, projection, and discrimination stages to achieve a new SOTA without using negative samples. They understand the discrimination between different views of raw images as necessary prevention from collapse during the pretraining. However, they argue that many negative samples\nare not indispensable to prevent this collapse. As shown in the left part of Fig. \\ref{fig:two-stream}, there are two streams in BYOL with different parameters. The online network (top green) updates parameters by comparing the prediction generated itself and the regression target provided by the target network. Then the parameters of the target model (bottom red) are updated the same as Eq.~(\\ref{momentum-update}), \\textit{i.e.}, $\\xi\\gets\\tau\\xi+(1-\\tau)\\theta$, where $\\tau$ is the target decay rate to control the degree of parameter changing in the target network. Therefore, the target network can also be understood as a momentum encoder. Here,\n$\\xi$ in the target model is the parameter $\\theta_k$ in momentum encoder, and $\\theta$ in the online network denotes the parameter $\\theta_q$ in the query encoder.\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/MoCo_zip.pdf}\n    \\caption{The general pipeline of MoCo~, which is also a two-stream framework with different parameters.}\n    \\label{fig:moco}\n\\end{figure*}", "cites": [134, 122, 133, 2517], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from MoCo, SimCLR, and BYOL, connecting the concept of momentum updates across different models to highlight a common mechanism in soft parameter sharing. It provides some critical analysis, such as BYOL's argument that negative samples are not indispensable, but does not extensively compare limitations or evaluate performance trade-offs. The section identifies broader patterns in the use of momentum-based encoders and contrastive learning, though it remains grounded in specific techniques without reaching a highly abstract or meta-level perspective."}}
{"id": "cd9e37cc-d2be-4335-9b89-7c6b8fe84b1b", "title": "Hard Sharing.", "level": "paragraph", "subsections": [], "parent_id": "f232b9b3-0708-4ccd-84aa-6a5b5b5d66e4", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Sharing"], ["paragraph", "Soft Sharing."], ["paragraph", "Hard Sharing."]], "content": "SimCLR~ is proposed by Brain Team in Google Research which utilizes the hard parameter-sharing architecture. This simple framework can also be concluded in Fig. \\ref{fig:two-stream}, in which\nwe can see that representations of different views of the same image are learned in \nthe network $f(\\cdot)$. This base encoder shares the parameters with each other.\nThus, memory bank and momentum setting to learn key and query encoders are not necessary, which contributes to a simpler backbone architecture and easier learning strategy.\nThe loss function to maximize the similarity between different views of the same image (positive pairs) is defined as\n\\begin{equation}    \n\\label{nce-loss}\n    \\ell_{i,j}=-\\log exp(sim(z_i,z_j)/\\tau) \\\\ / \\sum\\nolimits_{k=1}^{2N}\\mathbbm{1}_{[k\\neq i]}exp(sim(z_i,z_k)/\\tau),\n\\end{equation}\nwhere $(i,j)$ is a pair of positive samples, $\\tau$ is an introduced hyper-parameter called temperature parameter~, and $\\mathbbm{1}_{[k\\neq i]}\\in\\{0,1\\}$ is an indicator function to control the denominator containing only negative pairs. \nTo avoid the dependence on a large number of explicit pairwise feature comparisons, Swapping Assignments between multiple Views of the same image (SwAV)~ is proposed as an online algorithm by Inria and FAIR. SwAV introduces clustering to substitute the previous comparison between pairs, which gains more memory with the help of non-queue architecture. In this method, the clustering prototype joins the computation of the defined loss function. This prototype is encoded as the concatenation of vectors learned through the backpropagation in CNNs. Thus, there is no need for SwAV to compare the encoded representations between different views.\nBased on the existing SwAV, a novel model called SElf-supERvised (SEER)~ aims to learn a pretrained encoder from any random image and unbounded dataset in the wild. The base network is RegNetY architectures~ trained with the SwAV SSL method~. This method proves that the SSL is not specific to a curated dataset such as ImageNet, and the scalability of recent RegNet releases the limitation of traditional backbones such as ResNet. In addition, this method encourages the research community to explore more backbones suitable for universal SSL.\nAttracting the attention in the recent SSL, \nFAIR conducts empirical experiments on the SSL by utilizing the structure of Simple Siamese (SimSiam) networks. This method~ can avoid the design of negative sample pairs, large batches (or memory banks), and momentum encoders in traditional contrastive learning. The two encoders in Fig. \\ref{fig:two-stream} with identical parameters that process two different views $t$ and $t^{\\prime}$ of image $x$ are substituted by the only siamese network. MLP predictor $g$ is used for one of the view representations, and then the stop-gradient operation is applied to another view representation.\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{figures/clustering_zip.pdf}\n    \\caption{The key pipeline for the DeepCluster model~.}\n    \\label{fig:clustering}\n\\end{figure*}", "cites": [9094, 2518, 2521, 2519, 2520, 7000], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several key self-supervised learning (SSL) methods for vision, including SimCLR, SwAV, SimSiam, and SEER, and connects their design choices (e.g., hard vs. soft sharing, clustering, and architecture flexibility). It shows some analytical depth by highlighting how these methods simplify traditional contrastive learning frameworks and reduce dependencies on components like memory banks or negative samples. However, the critical analysis remains limited to stating empirical successes without deeper evaluation of trade-offs or limitations."}}
{"id": "53fe15b6-6791-4528-8285-03f6b110d6b2", "title": "Learning by Clustering", "level": "subsection", "subsections": [], "parent_id": "81242bcb-042c-4bf0-9ebe-f6cb44a94843", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Computer Vision"], ["subsection", "Learning by Clustering"]], "content": "DeepCluster~ is the first model that adopts the clustering algorithm for large-scale dataset learning. This method groups the representations into different clusters and labels these clusters as supervised signals to pretrain the parameters of the backbone network. It demonstrates SOTA performance on a wide range of standard transferred tasks used in unsupervised learning.\nWhen it comes to the connection between contrastive learning and clustering, SwAV~ has utilized prototypes that serve as a clustering center to help classify the sample pairs during pretraining, while Prototypical Contrastive Learning (PCL)~ first targets bridging contrastive learning with clustering. Compared to instance discrimination as pretext tasks learning low-level representations, clustering can help to encode more semantic information. Then more semantic-based downstream tasks will benefit from it. As shown in Fig. \\ref{fig:clustering}, prototypical contrastive learning uses prototypes to substitute one of the views of generated samples in NCE loss (Eq. (\\ref{nce-loss})), which is the proposed ProtoNCE loss in PCL. In addition, PCL is also a method based on soft parameter sharing, in which the momentum encoder is updated as Eq.(\\ref{momentum-update}).\n\\begin{table*}[t]\n\t\\tiny\n\t\\centering\n\t\\caption{Summary of the PFMs in CV.}\n\t\\label{tab:pretraining model for image}\n\t\\resizebox{\\textwidth}{!}{\n\t\\begin{threeparttable}\n    \\begin{tabular}{llllllll}\n\t\t\\hline \n\t\t\\textbf{Year} & \\textbf{Conference} & \\textbf{Method} & \\textbf{Pretext Task} & \\textbf{Architecture} & \\textbf{Downstream Task}\\tnote{1} & \\textbf{Code} \\\\\n\t\t\\hline \n\t\t2014 & NeurIPS & Exemplar-CNN~ & discrimination & CNN & cla, rec & \\href{https://lmb.informatik.uni-freiburg.de/resources/binaries/nips2014\\_ExemplarCNN.zip}{https://lmb.informatik.uni-freiburg.de/...} \\\\\n\t\t\\hline\n\t\t2015 & ICCV & Context~ & context prediction & CNN & cla, det, clu & \\href{https://github.com/cdoersch/deepcontext}{https://github.com/.../deepcontext} \\\\\n\t\t\\hline\n\t\t2016 & CVPR & Inpainting~ & inpainting & GAN, CNN &  cla, det, seg, inp & \\href{https://github.com/pathak22/context-encoder}{https://github.com/.../context-encoder} \\\\\n\t\t\\hline\n\t\t2016 & ECCV & Colorization~ & colorization & CNN & cla, det, seg & \\href{https://github.com/richzhang/colorization}{https://github.com/.../colorization}  \\\\\n\t\t\\hline\n\t\t2016 & ECCV & Jigsaw~ & Jigsaw puzzles & CNN & cla, det, seg, ret & \\href{https://github.com/MehdiNoroozi/JigsawPuzzleSolver}{https://github.com/.../JigsawPuzzleSolver}  \\\\\n\t\t\\hline\n\t\t2017 & CVPR & Split-Brain~ & channel prediction & CNN & cla, det, seg & \\href{https://richzhang.github.io/splitbrainauto}{https://richzhang.github.io/splitbrainauto} \\\\\n\t\t\\hline\n\t\t2017 & ICCV & Counting~ & counting & CNN & cla, det, seg, ret & \\href{https://github.com/clvrai/Representation-Learning-by-Learning-to-Count}{https://github.com/clvrai/...} \\\\\n\t\t\\hline\n\t\t2017 & ICML & NAT~ & noise & CNN & cla, det & - \\\\\n\t\t\\hline\n\t\t2017 & ICLR & BiGAN~ & generation & GAN, CNN & cla, det, seg & \\href{https://github.com/jeffdonahue/bigan}{https://github.com/.../bigan} \\\\\n\t    \\hline\n\t    2018 & WACV & CDJP~ & Jigsaw puzzles &  CNN & cla, det, seg & - \\\\\n\t\t\\hline\n\t\t2018 & ICLR & RotNet~ & rotation &  NIN, CNN & cla, det, seg & \\href{https://github.com/gidariss/FeatureLearningRotNet}{https://github.com/gidariss/...} \\\\\n\t\t\\hline\n\t\t2018 & arXiv & CPC~ & patch overlapping & CNN, GRU & cla & - \\\\\n\t\t\\hline \n\t\t2018 & CVPR & NPID~ & instance discrimination & CNN & cla & \\href{https://github.com/zhirongw/lemniscate.pytorch}{https://github.com/.../lemniscate.pytorch} \\\\\n\t\t\\hline \n\t\t2018 & ECCV & DeepCluster~ & clustering & CNN & cla, det, seg & \\href{https://github.com/facebookresearch/deepcluster}{https://github.com/.../deepcluster} \\\\\n\t\t\\hline\n\t\t2019 & ICCV & LA~ & local aggregation & CNN & rec, det & \\href{https://github.com/neuroailab/LocalAggregation}{https://github.com/.../LocalAggregation} \\\\\n\t\t\\hline\n\t\t2019 & NeurIPS & BigBiGAN~ & generation & GAN, CNN & gen, cla & \\href{https://tfhub.dev/s?publisher=deepmind\\&q=bigbigan}{https://tfhub.dev/...bigbigan}  \\\\\n\t\t\\hline\n\t\t2019 & CVPR & AET~ & transformation & CNN & cla & \\href{https://github.com/maple-research-lab/AET}{https://github.com/.../AET} \\\\\n\t\t\\hline\n\t\t2019 & NeurIPS & AMDIM~ & discrimination &  CNN & cla & \\href{https://github.com/Philip-Bachman/amdim-public}{https://github.com/.../amdim-public} \\\\\n\t\t\\hline\n\t\t2020 & CVPR & ClusterFit~ & clustering & CNN & cla, seg & - \\\\\n\t\t\\hline \n\t\t2020 & ICML & CPC v2~ & patch overlapping & CNN & cla, det & - \\\\\n\t\t\\hline \n\t\t2020 & CVPR & PIRL~ & Jigsaw puzzles & CNN & cla, rec, dec & \\href{https://github.com/facebookresearch/vissl/tree/master/projects/PIRL}{https://github.com/.../PIRL} \\\\\n\t\t\\hline\n\t\t2020 & CVPR & MoCo~ & discrimination & CNN & cla, rec, dec, pos, seg & \\href{https://github.com/facebookresearch/moco}{https://github.com/.../moco} \\\\\n\t\t\\hline\n\t\t2021 & ICLR & PCL~ & clustering & CNN & cla, det & \\href{https://github.com/salesforce/PCL}{https://github.com/.../PCL} \\\\\n\t\t\\hline\n\t\t2020 & arXiv & MoCo v2~ & discrimination & CNN & cla, dec & \\href{https://github.com/facebookresearch/moco}{https://github.com/.../moco} \\\\\n\t\t\\hline\n\t\t2020 & ICLR & SeLa~ & self-labelling & CNN & cla, det, seg & \\href{https://github.com/yukimasano/self-label}{https://github.com/.../self-label} \\\\\n\t\t\\hline\n\t\t2020 & ICML & SimCLR~ & discrimination & CNN & cla & \\href{https://github.com/google-research/simclr}{https://github.com/.../simclr} \\\\\n\t\t\\hline\n\t\t2020 & NeurIPS & SimCLR v2~ & self-distillation~ & CNN & cla & \\href{https://github.com/google-research/simclr}{https://github.com/.../simclr} \\\\\n        \\hline\n\t\t2020 & ECCV & CMC~ & view matching~ & CNN & cla, seg & \\href{https://hobbitlong.github.io/CMC/}{https://hobbitlong.github.io/CMC} \\\\\n\t\t\\hline\n\t\t2020 & NeurIPS & InfoMin~ & discrimination & CNN & cla, det, loc, seg & \\href{https://hobbitlong.github.io/InfoMin/}{https://hobbitlong.github.io/InfoMin} \\\\\n\t\t\\hline\n\t\t2020 & NeurIPS & SwAV~ & cropping & CNN, Transformer & cla, det & \\href{https://github.com/facebookresearch/swav}{https://github.com/.../swav} \\\\\n\t\t\\hline\n\t\t2020 & NeurIPS & BYOL~ & discrimination & CNN & cla, det, seg & \\href{https://github.com/deepmind/deepmind-research/tree/master/byol}{https://github.com/.../byol} \\\\\n\t\t\\hline\n\t\t2021 & arXiv & MoCo v3~ & discrimination & CNN, Transformer & cla & - \\\\\n\t\t\\hline\n\t\t2021 & ICLR & R\\textsc{e}LIC~ & discrimination & CNN & cla, rel & - \\\\\n\t\t\\hline\n\t\t2021 & ICLR & PCL v2~ & clustering & CNN & cla, det & \\href{https://github.com/salesforce/PCL}{https://github.com/.../PCL} \\\\\n\t\t\\hline\n\t\t2021 & CVPR & SimSiam~ & discrimination & CNN & cla, det, seg & \\href{https://github.com/facebookresearch/simsiam}{https://github.com/.../simsiam} \\\\\n\t\t\\hline\n\t\t2021 & ICML & DirectPred~ & discrimination & CNN & cla & \\href{https://github.com/facebookresearch/luckmatters/tree/main/ssl}{https://github.com/.../ssl} \\\\\n\t\t\\hline\n\t\t2021 & ICCV & DINO~ & discrimination & CNN, Transformer & cla, seg & \\href{https://github.com/facebookresearch/dino}{https://github.com/.../dino} \\\\\n\t\t\\hline\n\t\t2021 & arXiv & MoBY~ & discrimination & CNN, Transformer & cla, det, seg & \\href{https://github.com/SwinTransformer/Transformer-SSL}{https://github.com/.../Transformer-SSL} \\\\\n        \\hline\n        2021 & NeurIPS & MST~ & token prediction & CNN, Transformer & cla, det, seg & - \\\\\n        \\hline\n\t\t2022 & ICLR & BE\\textsc{i}T~ & token prediction & Transformer & cla, seg & \\href{https://github.com/microsoft/unilm/tree/master/beit}{https://github.com/.../beit} \\\\\n        \\hline\n        2022 & CVPR & MAE~ & reconstruction & Transformer & cla, det, seg & \\href{https://github.com/facebookresearch/mae}{https://github.com/facebookresearch/mae}\\\\ \\hline\n        2022 & CVPR & SimMIM~ & reconstruction & Transformer & cla, det, seg & \\href{https://github.com/microsoft/SimMIM}{https://github.com/microsoft/SimMIM}\\\\ \\hline\n        2022 & ArXiv & UM-MAE~ & reconstruction & Transformer & cla, det, seg & \\href{https://github.com/implus/UM-MAE}{https://github.com/implus/UM-MAE}\\\\ \\hline\n        2022 & ArXiv & LoMaR~ & reconstruction & Transformer & cla, det, seg & \\href{https://github.com/junchen14/LoMaR}{https://github.com/junchen14/LoMaR}\\\\ \\hline\n        2022 & Arxiv & CAE~ & reconstruction & Transformer & cla, det, seg & \\href{https://github.com/lxtGH/CAE}{https://github.com/lxtGH/CAE}\\\\ \\hline\n        2023 & AAAI & PeCo~ & reconstruction & Transformer & cla, det, seg & -\\\\ \\hline\n        2023 & ArXiv & SAM~ & reconstruction & Transformer & det, gen, seg & \n        \\href{https://github.com/facebookresearch/segment-anything}{https://github.com/facebookresearch/segment-anything}\\\\ \\hline\n\t\\end{tabular}\n\t\\begin{tablenotes}\n\t    \\tiny\n\t    \\item[1] Downstream task types: classification (cla), recognition (rec), detection (det), localization (loc), segmentation (seg), clustering (clu), inpainting (inp), retrieval (ret), generation (gen), pose estimation (pos), reinforcement learning (rel).\n\t\\end{tablenotes}\n\t\\end{threeparttable}\n\t}\n\\end{table*}", "cites": [9094, 2518, 2522, 2506, 7102, 131, 2514, 2505, 2523, 7101, 2528, 7000, 7018, 133, 2512, 2502, 2515, 681, 2525, 122, 2503, 2517, 2510, 8562, 2516, 2530, 134, 2504, 7100, 2521, 2508, 2507, 2527, 9095, 2526, 2529, 2524, 1254, 2511, 2513, 865], "cite_extract_rate": 0.8936170212765957, "origin_cites_number": 47, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of clustering-based PFMs in computer vision, listing various methods with their pretext tasks, architectures, and downstream applications. While it briefly connects clustering with contrastive learning (e.g., PCL and SwAV), the synthesis is minimal and lacks a deeper narrative or conceptual framework. There is little critical evaluation of the methods' strengths or limitations, and abstraction is limited to surface-level observations about pretext tasks."}}
{"id": "9c50d080-4943-4b08-8efe-735c19238add", "title": "Learning by Graph Information Completion", "level": "subsection", "subsections": [], "parent_id": "238ea228-7a56-4a7f-a4df-2e79b3065bd3", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Information Completion"]], "content": "\\label{sec:graph_gic}\nThe essential motivation of pretraining based on graph information completion (GIC) is to mask part of the information of the input graph data and recover the masked information based on the unmasked graph data, so as to pretrain the graph embedding, as shown in Fig. \\ref{fig:GIC_GPP}. \nSimilar ideas appeared earlier in the field of image and text processing.\nFor instance, in image processing, information such as image pixels and colors are recovered to pretrain the image encoder; in text processing, many methods implement pretraining of word embeddings and encoders by recovering part of the information in a sentence based on context words.\nThese methods inspire the design of graph completion tasks on graph PFMs.\n\\begin{figure*}[!t]\n \\centering\n    \\subfigure[Graph Information Completion (GIC).]{\n \\includegraphics[width=.4\\linewidth]{fig_graph/GIC.pdf}}\n \\centering\n    \\subfigure[Graph Property Prediction (GPP).]{\n \\includegraphics[width=.4\\linewidth]{fig_graph/GPP.pdf}}\n\t\\caption{Graph Information Completion (GIC) and Graph Property Prediction (GPP).}\n\t\\label{fig:GIC_GPP}\n\\end{figure*}\nAmong them, You et al.~ are inspired by image inpainting, and first propose to cover them by removing the features of the target nodes, and then recover/predict the features of the masked nodes.\nIn order to recover/predict the masked information, GraphCompetion~ is achieved by providing GCNs with unmasked node features (limited to the 2-layer GCNs of the second-order neighbors of each target node).\nThe purpose of GraphCompetion's pretraining is to help the model better perform feature representation and teach the model to extract features from the context.\nYou et al.~ propose the attribute mask task (namely, AttributeMask), which masks node attributes randomly, and then requires the self-supervising module to reconstruct the masked attributes.\nJin et al.~ think deeply about SSL on graph data, and propose the edge mask task (namely, EdgeMask), seeking to develop self-supervision in pairs based not only on a single node itself but on the connection between two nodes in the graph.\nIn particular, EdgeMask randomly masks some edges and then asks the model to reconstruct the masked edges.\nIn short, EdgeMask is expected to help GNN learn local connectivity information.\nHu et al.~ propose a PFM that masks node and edge attributes and then predicts this masked information based on the adjacent structure.", "cites": [7582, 1440, 7335], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of graph information completion and describes individual methods proposed in the cited papers (e.g., AttributeMask, EdgeMask). However, it lacks deeper synthesis of ideas, critical evaluation of the methods, or abstraction into broader trends or principles. The narrative remains descriptive and does not offer a unified framework or insightful comparison of the approaches."}}
{"id": "4ccfac6c-9d70-41dd-b30a-e80b2492db26", "title": "Context Consistency", "level": "paragraph", "subsections": ["86076f2a-243d-4e11-8ad7-99d604259a6e", "42e65643-bbf9-43d5-b4e7-3e356e1199d8"], "parent_id": "de9bff3b-ca0b-4b40-801a-4552f6809c5c", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Consistency Analysis "], ["paragraph", "Context Consistency"]], "content": "Based on the early homogeneity assumption, a mass of graph models tends to project contextual nodes to similar positions in semantic space.\nSuch consistency of the context in the graph is also applied to the pretraining graph model, which attempts to adjust the node representation by capturing the distribution characteristics of the nodes in the context, as shown in Fig. \\ref{fig:GCA} (a).\nRandom walk is an efficient method to acquire context. \nIt can capture the distribution characteristics of different perspectives in the context by designing a variety of walk strategies.\nThe DeepWalk~ adopts a truncated random walk strategy to represent the node context as the form of a sequence of nodes.\nBy introducing the idea of NLP into the network embedding model, DeepWalk regards the node sequence as a ``sentence''\nand models it based on the skip-gram model, providing an unsupervised and scalable training method for node representation.\nFurthermore, on the basis of DeepWalk, node2vec~ uses two different parameter-controlled random walk strategies to obtain deviated node sequences to fully capture the context information.\nDifferent from randomly sampling nodes from the context, some recent methods directly consider the relationship between the node's k-order neighbor distribution (as positive examples) and non-adjacent nodes (as negative examples), and use this to train the graph model.\nLINE~ respectively proposes first- and second-order proximity to describe the local similarity between pairs of nodes in the graph from different perspectives, and uses it to optimize node representation.\nMeanwhile, LINE uses negative sampling and edge sampling techniques to optimize the second-order traversal and excessive training storage overhead.\nVGAE~ introduces a variational autoencoder to encode graph structure data, and model the node first-order neighbor through a GCN encoder and a simple inner product decoder.", "cites": [1010, 218, 282, 229], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by connecting their shared goal of leveraging node context in graph pretraining, but it lacks deeper integration or a novel framework. It is primarily descriptive, outlining each method’s approach and contributions without substantial comparison or critique. Some abstraction is present in mentioning broader concepts like random walks and proximity, but the analysis remains shallow and focused on individual techniques."}}
{"id": "86076f2a-243d-4e11-8ad7-99d604259a6e", "title": "Self Consistency", "level": "paragraph", "subsections": [], "parent_id": "4ccfac6c-9d70-41dd-b30a-e80b2492db26", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Consistency Analysis "], ["paragraph", "Context Consistency"], ["paragraph", "Self Consistency"]], "content": "In the field of NLP and CV, contrastive learning as an efficient self-supervised mechanism is widely used in the pretraining of models.\nIn fact, the internal comparison mechanism of such methods is based on the mutual information estimation of the original graph data and the augmented graph data to maintain the consistency of the data itself, as shown in Fig. \\ref{fig:GCA} (b).\nInspired by contrastive learning, some studies have begun to generate augmented samples of original data samples in the graph model. \nAmong them, two augmented samples from the same original sample are regarded as positive pairs, and two augmented samples from different original samples are regarded as negative pairs.\nFor node-level tasks, GCC~ devises the pretext task as subgraph instance discrimination in and across networks.\nAnd GCC also enhances the ability of GNNs to learn the intrinsic and transferable structural representations by introducing contrastive learning.\nSpecifically, GCC samples subgraphs from the whole graph as augmentations via random walk with restart and artificially designs positional node embedding as node initial features.\nAs a novel graph representation learning model, GCA~ incorporates various priors for topological and semantic aspects of the graph to achieve adaptive contrastive augmentation. \nSpecifically, GCA devises an enhancement scheme based on node centrality measures to highlight important connection structures, while corrupting node features by adding noise to specific nodes to lead the pretraining model to recognize underlying semantic information.\nFor graph-level tasks, some studies have attempted to introduce more diverse contrastive learning strategies.\nAmong them, You et al.~ introduce four common graph augmentation tasks (i.e., node dropping, edge perturbation, attribute masking, and subgraph sampling) into the GL model based on underlying prior and propose a unified comparative learning framework: GraphCL.\nMeanwhile, GraphCL discusses in depth the role of data augmentation in comparative learning and gives experimental demonstration that joint multiple augmentation strategies can improve model performance.", "cites": [2531, 2532, 1184], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key ideas from multiple papers, integrating contrastive learning techniques used in PFMs for graph learning, such as GCC and GCA. It presents a coherent narrative on how these methods apply contrastive learning and data augmentation. However, the critical analysis is limited to stating what methods do rather than evaluating their strengths or weaknesses. The abstraction level is moderate, as it identifies the use of contrastive learning across graph-level and node-level tasks but does not fully generalize to broader principles or meta-frameworks."}}
{"id": "42e65643-bbf9-43d5-b4e7-3e356e1199d8", "title": "Cross Scale Consistency", "level": "paragraph", "subsections": [], "parent_id": "4ccfac6c-9d70-41dd-b30a-e80b2492db26", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Consistency Analysis "], ["paragraph", "Context Consistency"], ["paragraph", "Cross Scale Consistency"]], "content": "Unlike the above two methods that consider the consistency of elements in the same scale, contrasting elements in graph data of different scales can also be used to train graph models, e.g., node-subgraphs.\nMost of such methods have the idea of maximizing mutual information~.\nSpecifically, the readout function is usually used to obtain the summary of the graph/subgraph, and the MI estimator can be calculated using the Jensen-Shannon divergence.\nAs a representative method, DGI~ relies on maximizing the MI between the patch representation and the summary of the corresponding high-level graphs, which are all derived using the established graph convolutional network architecture, to learn the node representation.\nTo generate negative samples on a single graph, DGI corrupts the original graph by randomly scrambling node features while keeping the structure unchanged.\nSimilarly, Hassani and Khasahmadi propose CMVRL~, which generates an additional structural view of a sample graph based on graph diffusion.\nThe sample graph and a regular view are sub-sampled together, and the node representation and graph representation are learned based on two shared MLPs, and then contrast learning is achieved through the consistency loss provided by the discriminator.\nSUBG-CON~ samples a series of context subgraphs from the original graph and inputs them to the encoder to obtain the pooled central node and subgraph representation.\nFor the specified node, the context subgraph is expressed as a positive sample, and other randomly sampled subgraphs are expressed as a negative sample.\nThe contrast loss of the latent space will force the encoder to identify positive samples and negative samples in order to distinguish different nodes based on regional structure information.", "cites": [240, 2533, 7103], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key ideas from DGI, CMVRL, and SUBG-CON, highlighting the common theme of contrastive learning across different graph scales. It provides a basic level of critical insight by noting differences in how each method generates positive and negative samples but does not delve deeply into limitations or trade-offs. The abstraction is moderate, identifying patterns in contrastive learning strategies for graph data but not elevating the discussion to a higher-level conceptual framework."}}
{"id": "337f449c-b5d6-4dec-8767-c7610d8be018", "title": "Property Regression (PR)", "level": "paragraph", "subsections": ["de3fde25-98df-47aa-852e-6683e2e09223"], "parent_id": "9a770a85-42dc-423f-83fa-139f065e8b3e", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Property Prediction"], ["paragraph", "Property Regression (PR)"]], "content": "In the graph model, different from the GIC mentioned above, property regression primarily focuses on mining the relationship between the broader numerical structure and property attributes within the graph.\nSpecifically, this branch of methods extracts richer self-supervised signals in graph data for pretraining graph models.\nFor example, similar but different from masking node attributes, the goal of NodeProperty~ is to predict each node's auxiliary property in the graph, e.g., degree, local node importance, and local clustering coefficient.\nIn other words, NodeProperty is used to encourage GNN to capture richer local structural information while optimizing the specific downstream tasks.\nSpecifically, NodeProperty regards the node degree as a representative local node property, i.e., self-supervised signal, and takes other node properties as future work.\nMeanwhile, NodeProperty emphasizes that the intuition of devising self-supervised pretext tasks related to local node property is to ultimately guide the feature embedding of GNN (i.e., node representation) to save this information, which relies on the assumption that the node property information is relevant to the particular task.", "cites": [7582], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the Property Regression (PR) approach within graph learning, drawing on the cited paper to describe how self-supervised signals related to node properties are used. It integrates the idea with the broader context of graph pretraining but does so primarily through explanation rather than novel synthesis. There is some critical analysis regarding the assumptions behind PR, but limited comparison or evaluation of alternative methods. The section begins to abstract by highlighting the general goal of capturing local structural information, but it stops short of proposing overarching principles."}}
{"id": "de3fde25-98df-47aa-852e-6683e2e09223", "title": "Property Classification (PC)", "level": "paragraph", "subsections": [], "parent_id": "337f449c-b5d6-4dec-8767-c7610d8be018", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Graph Property Prediction"], ["paragraph", "Property Regression (PR)"], ["paragraph", "Property Classification (PC)"]], "content": "Different from the property regression task, the task of property classification is usually implemented by defining pseudo-labels based on a certain distribution in the graph data, which is a typical self-supervised method.\nAmong them, the structure density, similarity of node attributes, and difference between local and global distributions are the most commonly used. \nWe will briefly introduce the application of such methods in GL pretraining.\nAmong these methods, clustering is the most common and effective source of pseudo-labels.\nAmong them, M3S~ designs a multi-stage training strategy, using the idea of graph clustering to iteratively train the graph encoder, achieving enlarged labeled data with virtual labels in the case of very small samples.\nYou et al.~ further propose two pretraining strategies.\nAmong them, Node Clustering assigns $K$ (hyper-parameter) pseudo labels to nodes based on attribute clustering and pretrain node representation by node classification.\nIn addition, You et al. also present Graph Partitioning based on the topology density assumption.\nIn Graph Partitioning, the nodes of a graph are divided into approximately equal $K$ (hyper-parameter) subsets to minimize the number of edges connecting nodes among subsets, and then pseudo labels are provided for nodes.\nIn addition to clustering methods, some researchers generate pseudo labels based on other statistical characteristics of graph data.\nFor instance, in the molecular field, Rong et al.~ use the molecular bonds of subgraphs and related statistical information to guide GNN to learn Context-Sensitive Properties (CSP) and then apply them to prediction.\nRong et al.~ propose a Motif Prediction (MP) task, which can be expressed as a multi-label classification problem, in which each motif corresponds to a label.\nSpecifically, let's assume that $K$ motifs in molecular data are considered.\nFor a specific molecule (abstracted as graph $G$), they use RDKit to detect whether each motif appears in $G$, and then take it as the target of the motif prediction task.", "cites": [1440, 2534], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of property classification methods in graph learning, focusing on the generation of pseudo-labels through clustering and motif-based approaches. While it mentions a few specific papers and their contributions, it lacks deeper synthesis, critical evaluation, or abstraction to broader principles. It connects ideas but does not offer a novel or comprehensive analysis."}}
{"id": "ec1546aa-6d3f-4195-8c39-c3f2aca8200b", "title": "Learning by Masked Autoencoder", "level": "subsection", "subsections": [], "parent_id": "238ea228-7a56-4a7f-a4df-2e79b3065bd3", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Learning by Masked Autoencoder"]], "content": "The masked autoencoder (MAE) is first applied in MAGE~, the masked autoencoders for self-supervised learning on graphs. Following MAE , MGAE operates on a partial network structure (without masked edges) that is based on convolutions. Besides, the decoder of MGAE is designed to model the cross-correlation between the head and tail nodes of an anchor edge. Empirical results demonstrate that MGAE performs better than traditional graph autoencoders and graph SSL approaches. Furthermore, GMAE  extends this approach by using a transformer instead of convolutions and reconstructing the features of masked nodes rather than masked edges. In addition to empirical improvements, MaskGAE  further provides theoretical justifications for the potential benefits of masked graph modeling. Designing algorithms to accommodate graphs of various complex properties is a promising direction. For instance, to tackle the heterogeneous graphs scenario, HGMAE  proposes meta-path masking and adaptive attribute masking with a dynamic mask to enable effective and stable learning on complex graph structure. Moreover, several training strategies are developed, including meta-path-based edge reconstruction to incorporate complex structural information, target attribute restoration to utilize various node attributes, and positional feature prediction to encode node positional information. Besides dealing with more complex graph structures, how to improve the learning efficiency of MAE on graph data remains an open question.", "cites": [2536, 2537, 2535, 2513], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers, tracing the evolution of MAE approaches from MAGE and MGAE to more advanced variants like GMAE and HGMAE. It identifies key innovations (e.g., masking ratios, use of transformers, handling heterogeneity) and connects them to broader learning paradigms. While it provides a useful analytical overview of design choices and challenges, it does not deeply critique or evaluate the limitations of the cited methods."}}
{"id": "c72e0a76-2008-4cfb-a09b-a9aac67e07c2", "title": "Other Learning Strategies on Graph Data", "level": "subsection", "subsections": [], "parent_id": "238ea228-7a56-4a7f-a4df-2e79b3065bd3", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Other Learning Strategies on Graph Data"]], "content": "In addition to the above methods, there are lots of pretraining methods that use relatively novel or hybrid strategies. For example, CG$^3$~ generates an improved node representation by designing a semi-supervised consistency loss to maximize the consistency between different views of the same data or data from the same category.\nNext, CG$^3$ uses the graph generation loss related to the input feature to extract the potential deterministic relationship between the data feature and the input graph topology as a supplementary supervision signal for SSL.\nBased on the attention mechanism, Graph-Bert~ trains itself to reconstruct node attributes and topological structure with sampled linkless subgraphs within their local contexts.\nGMI~ extends the traditional mutual information computing idea from the vector space to the graph domain and proposes to jointly maximize feature mutual information (between the node’s embedding and raw features of its neighbors) and edge mutual information (embedding of two adjacent nodes) for graph representation learning.\nGPT-GNN~ proposes a self-supervised graph generation task to guide itself to capture the topological and semantic attributes of the graph.\nGPT-GNN roughly divides the possibility of graph generation into attribute generation and edge generation to untangle the intrinsic dependence between node attributes and graph topology.", "cites": [2538, 8563, 7357], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of various graph pretraining methods but lacks deeper integration of their ideas into a cohesive narrative. It briefly mentions the techniques used in each paper without evaluating their strengths, weaknesses, or comparing them to one another. There is little abstraction or generalization beyond the described systems, resulting in a primarily descriptive insight level."}}
{"id": "e637803f-0a59-4b5d-ad9e-e6dd380765cb", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "238ea228-7a56-4a7f-a4df-2e79b3065bd3", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Graph Learning"], ["subsection", "Summary"]], "content": "In the graph model, as traditional feature learning methods are often accompanied by information loss in the process of feature learning, and the information taken into consideration is relatively one-sided, the obtained graph representation is relatively rough and loses mass information.\nPeople began to focus on the distribution of data and attributes in the graph data as self-supervised signals to pretrain the graph model so that it can capture more valuable information.\nBy transforming the distribution of nodes, attributes, and edges in the graph into different pretext tasks, and using GNNs for modeling, the graph model can fully fit the original distribution of the input graph.\nIn lots of unsupervised or semi-supervised scenarios, such pretrained graph models have been proven to benefit downstream tasks. Besides, federated training large graph models~ can be a promising solution for building pretrained foundation models.\nCurrently, with the in-depth study of contrastive learning strategies, some work has attempted to apply contrastive learning in different forms to the pretraining of graph models.\nThrough the consistency analysis of context, self, and cross-scale, this kind of method greatly improves the performance of the pretrained graph model on different graphs.\n\\begin{table*}[t]\n\t\\tiny\n\t\\centering\n\t\\caption{Summary of PFMs in GL.}\n\t\\label{tab:pretraining model for graph}\n \\scalebox{1.1}{\n\t\\begin{tabular}{llllll}  \n\t\t\\hline \n\t\t\\textbf{Year} & \\textbf{Conference} & \\textbf{Method} & \\textbf{Pretext Task} & \\textbf{Encoder} & \\textbf{Code} \\\\\n\t\t\\hline\n\t\t2014 & KDD & DeepWalk~ & GC-C & Shallow NN & \\href{https://github.com/phanein/deepwalk}{https://github.com/phanein/deepwalk} \\\\\\hline\n\t\t2015 & WWW & LINE~ & GC-C & Shallow NN & \\href{https://github.com/tangjianpku/LINE}{https://github.com/tangjianpku/LINE} \\\\\\hline\n\t\t2016 & NeurIPS & VGAE~ & GC-C & GCN & - \\\\\\hline\n\t\t2016 & KDD & node2vec~ & GC-C & Shallow NN & \\href{https://github.com/aditya-grover/node2vec}{https://github.com/aditya-grover/node2vec} \\\\\\hline\n\t\t2017 & NeurIPS & GraphSage~ & GC-C & Shallow NN & \\href{https://github.com/williamleif/GraphSAGE}{https://github.com/williamleif/GraphSAGE} \\\\\\hline\n\t\t2018 & ICLR & DGI ~ & GC-CS & GCN/SAGE & \\href{https://github.com/PetarV-/DGI}{https://github.com/PetarV-/DGI} \\\\\\hline\n\t\t2020 & ICML & GraphCompetion~ & GIC & GCN & \\href{https://github.com/Shen-Lab/SS-GCNs}{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline\n\t\t2020 & ICLR & AttMasking~ & GIC & GCN & \\href{http://snap.stanford.edu/gnn-pretrain}{http://snap.stanford.edu/gnn-pretrain} \\\\\\hline\n\t\t2020 & ICML & AttributeMask~ & GIC & GCN & \\href{https://github.com/Shen-Lab/SS-GCNs}{https://github.com/Shen-Lab/SS-GCNs} \\\\\\hline\n\t\t2020 & arXiv & EdgeMask~ & GIC & GCN & \\href{https://github.com/ChandlerBang/SelfTask-GNN}{https://github.com/ChandlerBang/SelfTask-GN} \\\\\\hline\n\t\t2020 & arXiv & NodeProperty~ & GPP-PR & GCN & \\href{https://github.com/ChandlerBang/SelfTask-GNN}{https://github.com/ChandlerBang/SelfTask-GN} \\\\\\hline\n\t\t2020 & AAAI & M3S~ & GPP-PC & GCN & -  \\\\\\hline\n\t\t2020 & ICML & Node Clustering~ & GPP-PC & GCN & \\href{https://github.com/Shen-Lab/SS-GCNs}{https://github.com/Shen-Lab/SS-GCNs}  \\\\\\hline\n\t\t2020 & ICML & Graph Partitioning~ & GPP-PC & GCN & \\href{https://github.com/Shen-Lab/SS-GCNs}{https://github.com/Shen-Lab/SS-GCNs}  \\\\\\hline\n\t\t2020 & NeurIPS & CSP~ & GPP-PC & GCN & -  \\\\\\hline\n\t\t2020 & NeurIPS & MP~ & GPP-PC & GCN & -  \\\\\\hline\n\t\t2020 & NeurIPS & SELAR~ & GC-C & GNN &  \\href{https://github.com/mlvlab/SELAR}{https://github.com/mlvlab/SELAR} \\\\\\hline\n\t\t2020 & KDD & GCC~ & GC-S & GIN & \\href{https://github.com/THUDM/GCC}{https://github.com/THUDM/GCC} \\\\\\hline\n\t\t2020 & NeurIPS & GraphCL ~ & GC-S & GCN & \\href{https://github.com/CRIPAC-DIG/GCA}{https://github.com/CRIPAC-DIG/GCA} \\\\\\hline\n\t\t2020 & ICML & CMVRL ~ & GC-CS & GCN & - \\\\\\hline\n\t\t2020 & ICDM & SUBG-CON ~ & GC-CS & GCN & \\href{https://github.com/yzjiao/Subg-Con}{https://github.com/yzjiao/Subg-Con} \\\\\\hline\n\t\t2020 & ICLR & InfoGraph ~ & GC-CS & GCN & \\href{https://github.com/fanyun-sun/InfoGraph}{https://github.com/fanyun-sun/InfoGraph} \\\\\\hline\n\t\t2020 & AAAI & DMGI ~ & GC-CS & GCN & \\href{https://github.com/pcy1302/DMGI}{https://github.com/pcy1302/DMGI} \\\\\\hline\n\t\t2020 & arXiv & Graph-Bert ~ & Hybrid & Transformer & \\href{https://github.com/jwzhanggy/Graph-Bert}{https://github.com/jwzhanggy/Graph-Bert} \\\\\\hline\n\t\t2020 & WWW & GMI ~ & Hybrid & GCN & - \\\\\\hline\n\t\t2020 & KDD & Gpt-GNN ~ & Hybrid & GNN & \\href{https://github.com/acbull/GPT-GNN}{https://github.com/acbull/GPT-GNN} \\\\\\hline\n\t\t2021 & ICML & JOAO ~ & GC-S & GCN & \\href{https://github.com/Shen-Lab/GraphCL\\_Automated}{https://github.com/Shen-Lab/GraphCL\\_Automated} \\\\\\hline\n\t\t2021 & AAAI & CSSL ~ & GC-S & GCN & \\href{https://github.com/UCSD-AI4H/GraphSSL}{https://github.com/UCSD-AI4H/GraphSSL} \\\\\\hline\n\t\t2021 & PAKDD & GIC ~ & GC-CS & GCN & \\href{https://github.com/cmavro/Graph-InfoClust-GIC}{https://github.com/cmavro/Graph-InfoClust-GIC} \\\\\\hline\n\t\t2021 & WWW & SUGAR ~ & GC-CS & GCN & \\href{https://github.com/RingBDStack/SUGAR}{https://github.com/RingBDStack/SUGAR} \\\\\\hline\n\t\t2021 & ICML & GraphLoG ~ & GC-CS & GCN & \\href{https://github.com/DeepGraphLearning/GraphLoG}{https://github.com/DeepGraphLearning/GraphLoG} \\\\\\hline\n\t\t2021 & WWW & SLiCE ~ & GC-CS & GCN & \\href{https://github.com/pnnl/SLICE}{https://github.com/pnnl/SLICE} \\\\\\hline\n\t\t2021 & WSDM & BiGI ~ & GC-CS & GCN & \\href{https://github.com/caojiangxia/BiGI}{https://github.com/caojiangxia/BiGI} \\\\\\hline\n\t\t2021 & WWW & GCA ~ & GC-S & GCN & \\href{https://github.com/CRIPAC-DIG/GCA}{https://github.com/CRIPAC-DIG/GCA}\\\\\\hline\n\t\t2021 & KDD & HeCo ~ & GC-CS & GCN & \\href{https://github.com/liun-online/HeCo}{https://github.com/liun-online/HeCo} \\\\\\hline\n\t\t2021 & AAAI & CG$^3$ ~ & Hybrid & GCN & - \\\\\\hline\n\t\t2021 & ICLR & SuperGAT~ & GC-C & GAT & \\href{https://github.com/dongkwan-kim/SuperGAT}{https://github.com/dongkwan-kim/SuperGAT} \\\\\\hline\n\t\t2021 & KDD & MoCL ~ & Hybrid & GNN & \\href{https://github.com/illidanlab/MoCL-DK}{https://github.com/illidanlab/MoCL-DK} \\\\ \\hline\n\t\t2022 & ArXiv & MGAE ~ & Maksed Edge Reconstruction & GCN &-\\\\ \\hline\n\t   2022 & KDD & GMAE ~ & Maksed Node Reconstruction & Transformer &\\href{https://github.com/THUDM/GraphMAE}{https://github.com/THUDM/GraphMAE} \\\\ \\hline\n\t   2022 & Arxiv & MaskGAE ~ & Partial Maksed Node Reconstruction& Transformer &\\href{https://github.com/EdisonLeeeee/MaskGAE}{https://github.com/EdisonLeeeee/MaskGAE} \\\\ \\hline\n\t   2022 & Arxiv & HGMAE ~ &  Metapath Masking Reconstruction& Transformer & - \\\\ \\hline\n\t\\end{tabular}\n }\n\\end{table*}", "cites": [2538, 2543, 2537, 218, 7104, 7357, 7583, 1010, 2541, 2540, 2532, 242, 2531, 2533, 229, 8563, 7582, 2536, 2544, 7335, 2535, 282, 2534, 1184, 7103, 1440, 8564, 240, 2539, 2542], "cite_extract_rate": 0.7894736842105263, "origin_cites_number": 38, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various pretrained models for graph learning and includes a comprehensive table of methods, pretext tasks, encoders, and repositories. While it briefly mentions trends such as contrastive learning and masked autoencoders, it lacks deeper synthesis of the underlying ideas and does not critically evaluate or compare the methods in detail. It offers some abstraction by categorizing pretext tasks, but the overall narrative remains largely enumerative."}}
{"id": "4101c970-4dbe-4747-b05a-a0edf3ccecc1", "title": "PFMs for Speech", "level": "subsection", "subsections": [], "parent_id": "f9872e6b-da5b-4455-8e22-b3eac29cee8a", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "PFMs for Speech"]], "content": "In the field of speech, wav2vec~ obtains speech representation by capturing contextual information on large-scale unlabeled datasets, and fine-tuning on a few samples by noise comparison binary classification task, which greatly improves the performance of downstream tasks.\nFurthermore, vq-wav2vec~ and wav2vec 2.0~ propose a discrete unsupervised pretraining method on the basis of wav2vec, discretizing the original continuous speech signal, so that the methods in the mature NLP community can be migrated and applied.\nMeanwhile, lots of research have tried to design different mechanisms to use the representation obtained by speech pretraining as the initial input, and apply it to different tasks, e.g., automatic speech recognition~, phoneme recognition~, and speech synthesis~.\nIn particular, the extensive application of spoken language understanding has promoted the research of joint pretraining of speech and text.\nFor example, SpeechBERT~ applies MLM to speech and text pairs to perform representation learning on discrete information.\nUnlike~, which relies on a large amount of labeled data for joint pretraining, SPLAT~ uses unlabeled speech data to pretrain the speech embedding module, and proposes a label-level alignment method suitable for label-level downstream tasks based on sequence alignment.\nMusicBERT~ is a pretrained model designed for processing music data. It was developed by training on a vast symbolic music corpus consisting of over one million songs. To improve the pretraining process with symbolic music data, MusicBERT employs several mechanisms, such as OctupleMIDI encoding and a bar-level masking strategy. Huang et al.~ suggest incorporating a metrical structure in the input data, which allows Transformers to better recognize the hierarchical structure of music at the beat-bar-phrase level. AudioTransformer~ is a model that enhances the performance of Transformer architectures by implementing certain techniques, such as pooling, which were previously used in convolutional networks. Verma et al.~ demonstrate how they leverage multi-rate signal processing ideas based on wavelets to improve the Transformer embeddings and obtain better results.", "cites": [2549, 2546, 2547, 864, 2545, 2550, 7584, 2548, 2551], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section describes various speech and music-related PFMs and their mechanisms, integrating some related concepts (e.g., discrete representations, joint pretraining). However, it lacks deeper comparative or evaluative analysis of the methods and primarily presents them as a list of approaches. Some abstraction is attempted, such as linking speech pretraining to NLP methods, but broader patterns or principles are not clearly articulated."}}
{"id": "c5f12ce3-2fb4-4cee-91d0-886b0954df11", "title": "PFMs for Video", "level": "subsection", "subsections": [], "parent_id": "f9872e6b-da5b-4455-8e22-b3eac29cee8a", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "PFMs for Video"]], "content": "Video is similar to the RGB features of image and sequence information of the text. Many meaningful explorations in self-supervised video representation learning can not only perform efficiently well in video datasets but also generalize to the learning in other areas. Odd-One-Out Networks (O3N)~ is a technique that targets to predict the odd video subsequence among real subsequences sampled from a video in a training dataset. The experiments are conducted by using different video-clip encoders for O3N to prove consistent improvements of this pretraining design. Similarly, Shuffle and Learn~  aims to learn the correct temporal order from a sequence of frames in a video. However, Kim et al.~ designed a new self-supervised task called Space-Time Cubic Puzzles to train 3D CNNs. This task requires a pretrained backbone to arrange permuted 3D spatiotemporal crops. The performance of downstream tasks proves that effective video representations have been learned while solving such puzzles.\nInspired by the contrastive learning in images, many pretraining models in the video also utilize the contrastive loss to learn video presentations for downstream tasks. Inter-Intra Contrastive (IIC) framework~ can learn video representations by using positive and negative pairs generated from different videos. Specifically, different modalities in the same video are treated as positive pairs, and video clips from different videos as negative ones. Temporal Contrastive Pretraining (TCP)~ is another contrastive method based on CPC to learn video representations. Different from the existing GAN-based method that generates future frames for the video directly, TCP can predict the latent representation of future frames of the video, which is better for long-term predictions.\nSequence Contrastive Learning (SeCo)~ is a novel method considering both intra- and inter-frame instance discrimination in sequence order-based task.", "cites": [2554, 2556, 2555, 2553, 2552], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes several self-supervised learning methods for video representation learning, citing individual papers but not effectively integrating or connecting their ideas into a broader narrative. There is minimal critical evaluation or identification of limitations, and no clear abstraction to higher-level principles or trends in the field."}}
{"id": "2888b43c-cbd1-4869-920c-68b189be657f", "title": "Single-Stream Model", "level": "paragraph", "subsections": ["38bce72c-4b48-41da-8192-684816c0192e"], "parent_id": "332573be-24a1-4841-b8d3-27d3d94c91b4", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "PFMs for Multimodal"], ["paragraph", "Single-Stream Model"]], "content": "VisualBERT~ inputs text and images into the model simultaneously, which are aligned and fused using Transformer's self-attention mechanism. The input of the text is the same as BERT, and the input of the image is the image features extracted by Fasters-RCNN. VisualBERT also does pretraining and then fine-tuning the specific task. It adopts two pretraining tasks, namely MLM and sentence-image prediction, determining whether the input sentence describes the corresponding image.\nThe structure of Unicoder-VL~ is very similar to VisualBERT, except for the processing of the image. Unicoder-VL extracts the image feature through Faster-RCNN and concatenates the feature with image position-encoding mapping to the same space. It enhances the image label prediction task, which predicts the categories of images.\nThe pretraining task of VL-BERT~ is the same as Unicoder-VL.\nThe image input of VL-BERT includes four parts: the image region features extracted by Fasters-RCNN, the location of the region in the original image, location coding, fragment encoding, and [IMG] encoding.", "cites": [1272, 2015], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the structure and pretraining tasks of VisualBERT, Unicoder-VL, and VL-BERT, but lacks deeper synthesis or comparison across the models. It does not critically evaluate their strengths or limitations, nor does it abstract broader patterns or principles in multimodal PFM design. The narrative is primarily descriptive and does not offer analytical or comparative insights."}}
{"id": "38bce72c-4b48-41da-8192-684816c0192e", "title": "Cross-Stream Model", "level": "paragraph", "subsections": [], "parent_id": "2888b43c-cbd1-4869-920c-68b189be657f", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "PFMs for Multimodal"], ["paragraph", "Single-Stream Model"], ["paragraph", "Cross-Stream Model"]], "content": "In ViLBERT~, the text and image modes are first encoded separately, and their outputs go through a standard attention module. This module is based on the Transformer structure. Still, in the self-attention mechanism, each module uses its query to calculate attention with the value and key of another module to integrate the information between different modules.\nThe model is pretrained on two tasks. The first task is the mask task, which is the same as BERT. On the image side, the goal of the task is that when the region image is masked, the classification distribution of the output of the model can be as consistent as possible with the output distribution of the model used to extract the region features (such as Faster-RCNN). The second task is the language image matching task. DALL-E is a series of deep learning models developed by OpenAI to generate images from natural language prompts. The first version of DALL-E uses a transformer-based architecture, similar to the one used in the GPT LMs, to process the textual prompts and generate image-like representations. The model is trained on a dataset of images and their associated textual descriptions based on GPT-3. DALL-E 2~ is the improved version by employing contrastive language-image pretraining (CLIP)  for capturing semantic association between image-text pairs and GLIDE diffusion model  for text-conditional image synthesis. Furthermore, GPT-4 is proposed by OpenAI recently. It is a large-scale multimodal model which adopts RLHF and demonstrates human-level performance on various professional and academic benchmarks.\nBased on the multi-modal data containing more available information than previous single-modality data, thus the performance of these models gets enhanced by combining with the SSL on the benchmark dataset. Cross and Learn~ is the first method that reveals crossmodal information as an alternative source of supervision and obtains powerful feature representations from combining crossmodal loss and diversity loss in both RGB and optical flow modalities. \nDifferent from the existing methods that learn feature representations from only a single task in cross-domain datasets, Ren and Lee et al.~ propose a novel deep multi-task network to learn more generalizable visual representations to overcome the domain difference and further utilize the cross-domain information in different tasks. In that paper, the cross-domain datasets are real and synthetic datasets generated by a GAN-based network, while the multiple tasks are the predictions of the surface normal, depth, and instance contour in RGB images. This model performs better than any previous single-task-based SSL methods by learning general-purpose visual representations from cross-domain multi-task feature learning. Tian et al.~ believe that a powerful representation is one that models cross-view factors from the perspective of humans view to understand the world. They propose Contrastive Multiview Coding (CMC) to learn a video representation by maximizing mutual information between different views of the same scene.", "cites": [7585, 1639, 1278, 2558, 2557, 2559, 2525], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of cross-stream models and several related multimodal approaches, such as ViLBERT, DALL-E, and CMC. While it attempts to connect these models through the theme of cross-modal interaction and pretraining, the synthesis is limited and lacks a deeper, unifying framework. There is minimal critical analysis or identification of broader principles beyond the individual methods."}}
{"id": "ec1a273d-353b-4af3-af4b-b4673cd4d2a9", "title": "PFM for Code Generation", "level": "subsection", "subsections": [], "parent_id": "f9872e6b-da5b-4455-8e22-b3eac29cee8a", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "PFM for Code Generation"]], "content": "Code generation with LLMs involves using pretrained language models to automatically generate code based on natural language descriptions of a desired program. This approach has the potential to greatly improve the efficiency of software development by reducing the need for manual coding and allowing developers to focus on higher-level tasks.\nThe technique involves training large-scale language models on vast amounts of natural language text and then fine-tuning the models on specific programming tasks. By inputting natural language descriptions of code, the model can generate code snippets that are syntactically and semantically correct. Code generation with LLMs has been applied in various programming domains, including web development, NLP, and data analysis. The models used for code generation include GPT-4, T5, and Codex, among others. For example, Andrei et al.~ have investigated and assessed the fine-tuning of transformer models for personalized code generation. Specifically, they have evaluated the effectiveness of various personalization techniques in the domain of generating unit tests for Java methods and learning to personalize for a specific software project. Shailja et al.~ assess the capacity of LLMs to generate Verilog that is useful. To achieve this, pretrained LLMs are fine-tuned on Verilog datasets collected from GitHub and Verilog textbooks. An evaluation framework is then constructed, consisting of test benches for functional analysis and a flow for testing the syntax of Verilog code generated in response to problems of varying degrees of difficulty. An open-source CodeGen LLM that has undergone fine-tuning has been shown to outperform the current leading commercial Codex LLM. The CodeGen~ is a group of LLMs that have up to 16.1B parameters and can handle both natural language and programming language data. Additionally, they have released the training library JAX FORMER as open-source. Their work demonstrates that the model can perform as well as the previous state-of-the-art zero-shot Python code generation on HumanEval, showcasing the practical applications of the trained model. Synchromesh, introduced in the study by Poesia et al.~, adopts a novel approach called Target Similarity Tuning (TST) to retrieve a small set of examples from a training bank. Then, Synchromesh utilizes these examples to train a pretrained language model and generates programs by applying Constrained Semantic Decoding (CSD). CSD is a general framework that can restrict the output to valid programs in the target language. In this work, the authors show that the combined use of CSD and TST results in significant improvements in prediction accuracy, as well as preventing runtime errors.\nHowever, there are still some limitations to code generation with LLMs, such as the models' tendency to generate overly verbose or inefficient code and their inability to handle complex programming tasks. Nevertheless, the technology has shown significant promise and has the potential to revolutionize the software development industry.", "cites": [2561, 2563, 2560, 2562], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to present a coherent narrative about code generation with PFMs, linking methods like fine-tuning, TST, and CSD. It also provides some critical evaluation by pointing out limitations such as code verbosity and inefficiency. While it offers a clear overview and some comparative insights (e.g., CodeGen vs. Codex), it stops short of presenting a novel, meta-level framework or deeper critique of the field."}}
{"id": "13a9f290-8953-45bf-86bd-7012fe69b5de", "title": "SOTA Unified PFMs", "level": "subsection", "subsections": ["df91898d-8ab1-4769-af61-bf2d096e34b6"], "parent_id": "f9872e6b-da5b-4455-8e22-b3eac29cee8a", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "SOTA Unified PFMs"]], "content": "A big convergence of PFMs handling multiple modalities is emerging, such as backbone architecture, pretraining task, and model scaling up~. Therefore, many unified PFMs proposed by researchers arise. A unified PFM is a unified model pretrained on unimodal and multimodal\ndata with single or multiple transformers as the backbone, which has the ability to perform a large variety of downstream AI tasks, including unimodal tasks and multimodal tasks. There are currently three types of SOTA unified models based on model architectures. We defined them as the single-transformer model, multi-transformer model, and comb-transformer model. A single-transformer model refers to a PFM model which only has a large-scale transformer as its backbone, whereas a multi-transformer model refers to a PFM model having multiple transformers. A comb-transformer model is the PFM model with the combination of both single and multiple transformer structures.", "cites": [1582], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes a general trend toward unified PFMs across modalities and introduces a novel classification framework (single-, multi-, and comb-transformer models), showing moderate analytical depth. However, it lacks critical evaluation of the cited paper or comparisons between models. It abstracts the concept of unified models but does not delve into broader implications or limitations."}}
{"id": "df91898d-8ab1-4769-af61-bf2d096e34b6", "title": "Single-transformer Model", "level": "paragraph", "subsections": ["999e5823-8c94-436d-b716-3f921ea01166", "f772b5dd-3e74-483e-b2a9-a1cf58e4dcb3"], "parent_id": "13a9f290-8953-45bf-86bd-7012fe69b5de", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "SOTA Unified PFMs"], ["paragraph", "Single-transformer Model"]], "content": "UNITER~ is a large-scale PFM for joint image-text embedding, which consists of an Image Embedder, a Text Embedder, and a multi-layer Transformer. It first encodes visual features and bounding box features for image regions using Image Embedder and tokens and positions using Text Embedder. Then, a Transformer module is applied to learn generalizable contextualized embeddings for images and text through four pretraining tasks. Instead of applying random joint masking to both modalities, conditional masking on pretraining tasks is used. Six vision-language tasks are selected as the downstream tasks.\nUni-Perceiver~ is a single siamese model with shared parameters having the ability to process different modalities regarding vision\nand language tasks. Different task inputs and targets are encoded into unified token sequences with modality-specific tokenizers, which are then decoded by a modality-agnostic weight-sharing Transformer encoder into the shared representation space. Any perception task is modeled as finding the maximum likelihood target for each input through the similarity of their representations.\nUni-Perceiver is pretrained on unimodal and multimodal tasks. The evaluation results on various downstream tasks show that the performance is close to SOTA methods by conducting prompt tuning on 1\\% of downstream task data.\nGato~ builds a single large transformer sequence model that works as a multimodal, multi-task, multi-embodiment generalist policy. It can perform various tasks using a single neural network with the same set of weights. Gato is trained on 604 tasks, where different types of data, such as images, text, proprioception, joint torques, and other discrete and continuous observations and actions, are serialized into a flat sequence of tokens, batched, and processed by the transformer. During deployment, sampled tokens are assembled into different actions based on the context.\nOFA~ is a simple sequence-to-sequence learning framework with a unified instruction-based task representation that unifies various tasks. In the pretraining and finetuning stages, OFA requires no extra task-specific layers for downstream tasks to achieve Task-Agnostic. The Modality-Agnostic compute engine is a Transformer with the constraint that no learnable task- or modality-specific components are added to downstream tasks. OFA is pretrained on small-size image-text pairs to achieve crossmodal tasks while attaining highly competitive performances on unimodal tasks.\nUNIFIED-IO~ is a sequence-to-sequence model using a unified architecture that performs large and diverse tasks. UNIFIED-IO is a transformer model where both the encoder and decoder are composed of stacked transformer layers. The unified architecture does not need specific task or modality branches, which is accomplished by homogenizing the input and output of each task into a sequence of discrete vocabulary tokens. It trains a single transformer-based architecture on over 90 diverse datasets in the vision and language fields. UNIFIED-IO is the first model to perform various tasks and produce strong results across 16 diverse benchmarks without finetuning. \nBEiT-3~ is a general-purpose multimodal pretrained model on language, vision, and vision-language tasks. The big convergence of BEiT-3 can be seen from three aspects, including backbone architecture, pretraining task, and model scaling up. It introduces a shared Multiway Transformer as backbone network performing masked data modeling on both unimodal and multimodal data. To process different modalities, every Multiway Transformer block has a shared self-attention module, and a pool of feed-forward networks. \nIt is a giant-size foundation model that contains 1.9B parameters. Experimental results show that BEIT-3 can outperform SOTA models on both vision and vision-language tasks.", "cites": [1582, 2471, 2564], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of several single-transformer models for vision-language tasks, integrating basic information from the cited papers. It synthesizes the general idea that these models use unified token sequences and shared architectures to handle multiple modalities and tasks. However, it lacks critical evaluation of their approaches, limitations, or trade-offs, and does not abstract beyond specific models to highlight broader design principles or trends in the field."}}
{"id": "999e5823-8c94-436d-b716-3f921ea01166", "title": "Multi-transformer Model", "level": "paragraph", "subsections": [], "parent_id": "df91898d-8ab1-4769-af61-bf2d096e34b6", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "SOTA Unified PFMs"], ["paragraph", "Single-transformer Model"], ["paragraph", "Multi-transformer Model"]], "content": "FLAVA~ is an alignment model that targets all modalities at once and aims at solving vision and language tasks, and vision-language tasks. It utilizes a common transformer model architecture to learn strong representations from unimodal and multimodal data. An image encoder transformer is used to capture unimodal image representations. A text encoder transformer is adopted to process unimodal text information. A multimodal encoder transformer takes both encoded unimodal images and text as inputs and integrates their representations for multimodal reasoning. During pretraining, masked image modeling (MIM) and MLM losses are applied to the image and text encoders, respectively. On the other hand, masked multimodal modeling (MMM) and image-text matching (ITM) loss are used over paired image-text data. For downstream tasks, classification heads are applied to the outputs from the image, text, and multimodal encoders, respectively, for visual recognition, language understanding, and multimodal reasoning tasks. FLAVA shows good performance on 35 tasks across different domains. A noticeable advantage is that smaller datasets it used compared with other models.", "cites": [2468], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a straightforward description of the FLAVA model's architecture and pretraining methodology without synthesizing it with other works. It lacks critical evaluation of the model's design choices or limitations and only briefly notes that FLAVA uses smaller datasets, without contextualizing this in broader trends or implications. The content remains focused on specific components of the model without abstracting to overarching principles or patterns in multi-modal foundation models."}}
{"id": "f772b5dd-3e74-483e-b2a9-a1cf58e4dcb3", "title": "Comb-transformer Model", "level": "paragraph", "subsections": [], "parent_id": "df91898d-8ab1-4769-af61-bf2d096e34b6", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Other Data Modality"], ["subsection", "SOTA Unified PFMs"], ["paragraph", "Single-transformer Model"], ["paragraph", "Comb-transformer Model"]], "content": "UNIMO~ can learn both single modality and multimodalities with one model to achieve robust and generalizable representations. It employs multi-layer self-attention Transformers to learn general textual and visual representations simultaneously and unifies them into the same semantic space via cross-modal contrastive learning (CMCL). The main idea behind CMCL is to keep paired image and text representations close to the representation space while keeping non-paired representations far away. All of them are encoded by the same unified-modal Transformer in pairs or individually, and the representations of images and texts are extracted to compute the contrastive loss.", "cites": [8459], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the Comb-transformer model in UNIMO, focusing on its use of cross-modal contrastive learning and multi-layer self-attention Transformers. While it integrates information from the cited paper, it lacks deeper synthesis, critical evaluation, or abstraction to broader trends. The content is primarily descriptive without significant insight or comparative analysis."}}
{"id": "b6ec255c-bd2a-49b9-a056-b08c03e3ad37", "title": "Model Efficiency", "level": "subsection", "subsections": [], "parent_id": "4e7df0ae-6f27-4db8-94cf-15c255b077e4", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Model Efficiency"]], "content": "Model efficiency devotes to exploring more efficient pretraining methods to pretrain large-scale PFMs with a lower-cost solution. More efficient learning algorithms require more effective training methods and more efficient model architecture. Traditional pretraining tasks may be inefficient. For example, the commonly used masked token prediction task requires the model to predict masked tokens based on context. However, the masked tokens in the samples are usually a subset of the input tokens, and the model can only learn from this part of the tokens, so the training efficiency is low. To solve this problem, ELECTRA~ proposes an RTD task that predicts whether each input marker is replaced by other tokens, which enables the ELECTRA to train against all input tokens. In addition to effective training methods, more efficient architecture can also improve the efficiency of PFMS. For most PFMS based on the Transformer algorithm, a more efficient model architecture can be obtained by reducing the complexity of the Transformer algorithm.", "cites": [1557], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates the idea from the ELECTRA paper by connecting the inefficiency of traditional pretraining tasks (like MLM in BERT) to the proposed RTD task. While it provides a clear explanation of the motivation and solution, it lacks deeper comparison or evaluation of multiple methods and does not generalize broadly to other model efficiency strategies beyond the Transformer-based framework."}}
{"id": "d6377901-42cc-4855-b17d-4fd5c8c828f7", "title": "Model Compression", "level": "subsection", "subsections": [], "parent_id": "4e7df0ae-6f27-4db8-94cf-15c255b077e4", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Model Compression"]], "content": "Model compression requires less computing resources and memory. \nIt is a potential approach to reduce the model size and enhance computation efficiency. The model compression strategy can be divided into two ways: parameter compression and structure compression.\nThe methods of parameter compression include parameter pruning, parameter quantization, low-rank decomposition, and parameter sharing. Parameter pruning refers to designing evaluation criteria for model parameters to delete redundant parameters based on a sizeable PFM. For example, Compressing BERT~ prunes BERT before training while maintaining the performance equivalent to that of the original model. Parameter quantization is the quantization of model parameters from 32-bit full-precision floating-point numbers to lower-order numbers. For example, Q8BERT~ uses 8-bit quantization to compress parameters fourfold with little impact on model performance. Low-rank decomposition is to reduce the dimension of a high-dimensional parameter vector into a sparse low-dimensional vector. Parameter sharing refers to the structured matrix or clustering methods to map model parameters and reduce the number of parameters. For example, the ALBERT~ uses decomposition-embedded parameterization and cross-layer parameter sharing to reduce the parameters in the model.\nStructure compression refers to compact networks and knowledge distillation. A compact network means reducing the number of parameters and calculations by designing a new compact network structure. \nKnowledge distillation refers to the transfer of knowledge from the larger teacher model to the smaller student model through the use of a soft label, etc. DistilBERT~, for example, uses the knowledge distillation method to compress BERT, reducing the size of the BERT model by 40\\% while retaining 97\\% of its language comprehension.", "cites": [856, 7579, 1150, 7580], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of model compression techniques and references specific papers (e.g., DistilBERT, Compressing BERT, ALBERT, Q8BERT) to illustrate each method. While it makes basic connections between the cited papers and the broader topic of model compression, it lacks deeper synthesis, critical evaluation, or abstraction into overarching principles. The content primarily describes methods without comparing them or discussing their relative strengths and limitations."}}
{"id": "b323c7f8-6f4c-43df-b89c-cb635ea082af", "title": "Security and Privacy", "level": "subsection", "subsections": ["4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08"], "parent_id": "4e7df0ae-6f27-4db8-94cf-15c255b077e4", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"]], "content": "\\label{Section 8}\nThe security risks, social bias, and data privacy in PFMs become an important research topic. Qiu et al.~ recognize that deep neural networks can be attacked by adversarial samples, which mislead the model to produce false predictions. Due to the excellent portability of pretraining models, they have been widely used in NLP, CV, and GL. However, it has been found that the pretraining model is susceptible to the influence of adversarial samples. A tiny interference of the original input may mislead the pretraining model to produce specific false predictions. Meanwhile, it is possible to recover the data samples by querying the PFMs which can cause privacy leakage.", "cites": [1445], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of security and privacy issues in PFMs, citing one paper and mentioning adversarial samples and data recovery risks. However, it lacks synthesis of multiple sources, deeper critical evaluation, or abstraction to broader principles. The discussion remains surface-level with minimal integration or analysis."}}
{"id": "4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08", "title": "Generation Adversarial Samples", "level": "paragraph", "subsections": ["6e00db12-8f5e-4121-8215-7845b116cb7d", "a32d62ef-465a-457c-9f5e-a566801d437d", "f262522b-e5db-439a-a24b-091afae576f3", "2d28f3a9-be90-47f3-b05f-3ed80b7013bf"], "parent_id": "b323c7f8-6f4c-43df-b89c-cb635ea082af", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"], ["paragraph", "Generation Adversarial Samples"]], "content": "The adversarial sample originates from the image. The adversarial samples of the image are hard to recognize with an invisible change. For example, only one pixel of the image is modified. Human beings do not easily detect such disturbance, but the neural network can identify the modified image, which is the original purpose of the adversarial sample.\nSome work has found that pretrained LMs are vulnerable in some scenarios. Jin et al.~ successfully attack the three target models of BERT, CNN, and RNN by generating natural adversarial samples, which indicates that the current language processing model still has a large room for improvement in terms of security. \nHowever, it is difficult to achieve due to the distinct discreteness of languages in NLP. In particular, the generation of adversarial samples in the text must take into account linguistic characteristics to ensure that the sample's syntax and fluency are not harmed while affecting the model's output.\nFor example,~ uses adversarial samples to attack the fine-tuning stage of the BERT model for text classification and entailment successfully.~ combines the sememe-based word substitution method and search algorithm based on particle swarm optimization to generate adversarial samples.", "cites": [2565], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of adversarial sample generation in PFMs, referencing one paper and connecting it to broader challenges in NLP. It identifies the difficulty of generating adversarial samples for text due to its discrete nature and emphasizes the importance of linguistic fluency. However, synthesis and abstraction are limited, as it does not integrate multiple sources or present broader patterns or principles."}}
{"id": "6e00db12-8f5e-4121-8215-7845b116cb7d", "title": "Model Defects", "level": "paragraph", "subsections": [], "parent_id": "4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"], ["paragraph", "Generation Adversarial Samples"], ["paragraph", "Model Defects"]], "content": "Some unrelated human factors can also mislead the PFM to make wrong predictions. For example,~ discovers that the performance of BERT is limited in the reasoning task due to utilizing false statistical information in the dataset, which dramatically affects the performance by destroying this property.~ defines universal adversarial triggers. When triggers are connected to any input, it can induce the model to generate specific predictions.", "cites": [7586, 2465], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of model defects in PFMs by referencing two papers. It synthesizes the main findings of each work but does not deeply connect or contrast them in a broader context. While it touches on limitations (e.g., BERT's reliance on spurious statistical cues and the existence of universal adversarial triggers), the analysis remains at a surface level without deeper evaluation or exploration of implications."}}
{"id": "a32d62ef-465a-457c-9f5e-a566801d437d", "title": "Backdoor Attacks", "level": "paragraph", "subsections": [], "parent_id": "4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"], ["paragraph", "Generation Adversarial Samples"], ["paragraph", "Backdoor Attacks"]], "content": "There are still many methods to manipulate the predicted results of the pretraining model employing a backdoor attack.~ demonstrates that it is possible to construct a weight poisoning attack in which pretrained weights are injected. After the fine-tuning stage, the backdoor is exposed. Attackers manipulate model predictions easily by injecting arbitrary keywords.~ shows that PFMs in NLP can be manipulated by modifying the model corpus. The ``meaning'' of new words or existing words can be controlled by changing their weight parameters.", "cites": [2566, 2567], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of backdoor attacks in pretrained foundation models by paraphrasing two cited papers, but it does not synthesize or connect their findings into a broader narrative. It lacks critical evaluation or comparison of the methods, and offers no abstracted insights or principles about the nature of such attacks. The content remains largely factual and system-specific."}}
{"id": "f262522b-e5db-439a-a24b-091afae576f3", "title": "Defense Against Attacks", "level": "paragraph", "subsections": [], "parent_id": "4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"], ["paragraph", "Generation Adversarial Samples"], ["paragraph", "Defense Against Attacks"]], "content": "The human-in-the-loop method~ has been proposed and applied to generate more natural, efficient, and diversified adversarial samples. \nSome defense approaches have been proposed to defend against such attacks.  designs an auxiliary anomaly detection classifier and uses a multi-task learning procedure to defend against adversarial samples. On the other hand, some defects in the PFM may be inherited by the custom models in transfer learning, \nsuch as the adversarial vulnerabilities and backdoors mentioned above. To mitigate this issue, ~ proposes a relevant model slicing technique to reduce defect inheritance during transfer learning while retaining useful knowledge from the PFM.", "cites": [8466, 2470], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions two defense approaches, referencing the human-in-the-loop method and model slicing to address adversarial vulnerabilities and backdoors in PFMs. However, the synthesis is limited to a superficial level without connecting these approaches to broader trends or frameworks in the field. The critical analysis is minimal, and there is little abstraction or generalization of the ideas beyond the cited papers."}}
{"id": "2d28f3a9-be90-47f3-b05f-3ed80b7013bf", "title": "Data Privacy in PFMs", "level": "paragraph", "subsections": [], "parent_id": "4e4f0f17-6eee-4d4f-9c0a-f4ece492ef08", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Other Advanced Topics on PFMs"], ["subsection", "Security and Privacy"], ["paragraph", "Generation Adversarial Samples"], ["paragraph", "Data Privacy in PFMs"]], "content": "LLMs and other PFMs have been trained on private datasets~. The researchers have discovered that by querying the massive LMs, it is feasible to recover specific training samples. An adversary may, for instance, obtain IRC discussions and personally identifiable information. Even worse, because large models have so many parameters, it is simple for PFM to memorize or learn private information, making larger models more prone to attack than smaller ones.\nMany PFMs such as the LLMs have been trained on private datasets. The researchers have found that it is possible to recover individual training examples by querying the LLMs. For instance, an adversary can extract examples including personally identifiable information, and Internet Relay Chat (IRC) conversations. Even worse, because of the billion parameters of large models, it is easy for PFM to learn private information, making the larger model more vulnerable than smaller models. We must take privacy-preserving measures into account during all PFM processes, including data processing, model training, model inference, and system deployment, in order to reduce the risks of privacy leakage.", "cites": [2342], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of data privacy concerns in PFMs, primarily paraphrasing the findings from the cited paper on training data extraction. It lacks deeper synthesis of ideas and does not compare or critique multiple sources. The content remains at a concrete level without abstracting to broader principles or offering a novel framework."}}
{"id": "2c764781-8ae6-40ad-b248-c37ece960b20", "title": "Model Variety", "level": "paragraph", "subsections": ["823c2054-bfcf-4b48-9780-d9860a99d2a8", "343e1c7f-c00e-4ed2-bd06-8cdc24680627", "db63dd62-364b-4d08-bf56-8a8cb72252c8"], "parent_id": "ba3eae27-7b76-4296-9f55-60e3c677aaff", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Future Research Challenges and Open Problems"], ["subsection", "Challenges on Model Design"], ["paragraph", "Model Variety"]], "content": "There are many attempts at model design, such as generation-based models in the CV area.\nHowever, GAN-based approaches are not popular for the following two reasons: 1) the discriminator has learned meaningful feature representations, but they are forgotten during training~; 2) the mode collapse causes the generator to output samples in singular mode to cheat the discriminator. As a result, although researchers attempt to apply GAN-based approaches on SSL for pretraining, the difficulties in the convergence of discriminator and divergence of generator hinder development and progress in this area.", "cites": [55], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly mentions GAN-based approaches in the context of model variety and identifies two key issues—discriminator forgetting and mode collapse. While it draws on the cited paper to highlight these limitations, the synthesis is limited to a single paper, and the abstraction remains shallow without broader generalization. The critical perspective is present but not deeply developed."}}
{"id": "f4b581e6-41eb-48b0-a404-2de56f2bc71b", "title": "Saturation Phenomena", "level": "paragraph", "subsections": ["66cfeca1-2e5e-4ab6-a9b5-d7e7f87894e0", "486763ac-84f2-465c-a879-dc811c267bd4"], "parent_id": "8e68d626-4b06-4303-94fc-991a84acdb04", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Future Research Challenges and Open Problems"], ["subsection", "Challenges on Finetuning and Prompt"], ["paragraph", "Saturation Phenomena"]], "content": "Google Research~ observed the nonlinear relationship between the performance of upstream and downstream tasks. The higher training accuracy with more data on the upstream tasks does not always lead to better performance on the target downstream tasks. This observation challenges the most intuitive understanding of the pretraining process. Even in the most extreme case, the performance of upstream and downstream is at odds.", "cites": [7587], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly integrates the key finding from the cited paper about the nonlinear relationship between upstream and downstream performance, suggesting some synthesis. However, it lacks deeper analysis or comparison with other works, and the discussion remains at a relatively high but not insightful level. The abstraction is limited, as the section does not fully generalize the phenomenon to broader trends or principles in PFM training."}}
{"id": "787da676-2ca2-4d0e-95bb-bbe4a80772ad", "title": "Open Problems for Future PFMs", "level": "subsection", "subsections": [], "parent_id": "bff33ab9-dc91-49d5-b524-ed2d9a01f149", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Future Research Challenges and Open Problems"], ["subsection", "Open Problems for Future PFMs"]], "content": "First of all, a big convergence of text, image, graph, and multimodal pretraining is expected. Till the survey is written, no work has considered the graph in their unified PFMs. All of the SOTA unified models mainly focus on the language, vision, and language-vision tasks, while neglecting the importance of the graph in the data domain. Second, a unified backbone architecture for unified PFMs in future research will become more popular. It can be seen that a unified PFM model which only has a large-scale transformer as its backbone, i.e., a single-transformer model, is more focused by researchers than other types of unified PFMs. Third, a unified PFM is expected to achieve SOTA transfer performance for all different tasks in all data domains, including text, image, graph, and multimodalities. Most unified PFMs are only outstanding in a single data domain, whereas the performance in other domains is not competitive. BEiT-3~ shows a great example in both vision and vision-language tasks towards this research direction. Besides, in terms of RL usage in PFMs, even though ChatGPT build the milestone in NLP, CV and GL do not have significant research published yet. More work in this direction is expected in the future.", "cites": [1582], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of convergence in pretraining across modalities and connects the cited work (BEiT-3) to broader trends in unified PFMs. It critically identifies limitations such as the neglect of graph data and subpar cross-domain performance. The discussion abstracts beyond individual systems to highlight generalizable patterns and future directions in the field."}}
{"id": "f906b05c-8704-4640-bca1-f1869e1847fe", "title": "Language Model", "level": "subsubsection", "subsections": [], "parent_id": "f677f474-2e16-4eea-ac20-04add9ce0c07", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Basic Components"], ["subsection", "Basic Components on NLP"], ["subsubsection", "Language Model"]], "content": "With the rapid development of deep learning, LMs are more and more applicable to the pretraining of NLP models. The LM can estimate the probability of rationality of a paragraph of the text. \nThere are two main types of LMs: statistical LM and neural network LM.\n\\myparagraph{\\textbf{Statistical LM}}\nThe statistical LM is a mathematical model to solve the context-related characteristics of natural language from the perspective of probability and statistics. The core of statistical LMs is to determine the probability of a sentence appearing in a text.\nAs the theoretical basis of the probabilistic LM, the N-gram model profoundly influences the subsequent LM. It plays a pivotal role in the field of the LM. \nThe N-gram LM introduces the Markov hypothesis, which assumes that the probability of the occurrence of the current word only depends on the nearest $n-1$ words.\nThe maximum likelihood probability of a word $w_{i}$ can be calculated by\n\\begin{equation}\n    p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{N} \\right)= \\\\ p\\left(w_{i} \\mid w_{i-n+1}, w_{i-n+2}, \\ldots, w_{i-1} \\right) = \\\\ \\frac{C\\left(w_{i-n+1}, w_{i-n+2}, \\ldots, w_{i}\\right)}{\\sum_{N} C\\left(w_{i-n+1}, w_{i-n+2}, \\ldots, w_{i-1} \\right)},\n\\end{equation}\nwhere $T=[w_{1}, w_{2}, \\ldots, w_{N}]$ is the text sequence and $C(w_{i-n+1}, w_{i-n+2}, \\ldots, w_{i})$ is the co-occurrence frequency of $(w_{i-n+1}, w_{i-n+2}, \\ldots, w_{i})$. \nThe $p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{N} \\right)$ is calculated according to the chain rule\n\\begin{equation}\np\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)=\\prod_{i=1}^{N} p\\left(w_{i} \\mid w_{1}, w_{2}, \\ldots, w_{i-1}\\right).\n\\end{equation}\nN-gram uses the probabilities of each word in the sequence to represent the co-occurrence probability of the whole text sequence.\nWhen $N$ is large, it indicates a more vital constraint on the occurrence of the next word in the sequence and leads to more sparse frequency information. When $N$ is small, the statistical results have higher reliability and better generalization ability, but the constraint will be weaker.\n\\myparagraph{\\textbf{Neural LM}}\nThe statistical LM adopts maximum likelihood estimation, which is intuitive and easy to understand. However, there are still problems such as a lack of long-term dependence, the rapid growth of parameter space and sparse data. Therefore, the neural network is introduced to map the LM to a continuous space. Neural LMs use distributed representations of words to model natural language sequences. Unlike class-based N-gram models, neurolinguistic models are able to recognize two similar words without losing the ability to encode each word as different from the other. It can be directly used for NLP tasks. It mainly introduces Forward Feedback Neural Networks (FFNN), Recurrent Neural Networks (RNN), and pretrained LMs.\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{pictures/picture11.pdf}\n    \\caption{The model architectures of forward feedback neural network, recurrent neural network and Pretrained LMs. $H^{1,2}$, $H^{2,3}$ and $H^{1,3}$ are the weight matrices used to connect each layer.}\n    \\label{picture1}\n\\end{figure*}\nAs shown in Fig.~\\ref{picture1} (a), FFNN according to the all former words of $x=[w_{1}, \\ldots, w_{i-1}]$ calculates the probability of $w_{i}$. In order to predict the conditional probability of $w_{i}$, $x$ is sharing the projection matrix $M \\in R^{|V| \\times m}$ to a continuous feature vector space according to the projection index, $|V|$ is word library size, $m$ is the dimension of the feature vector. The output is represented as\n\\begin{equation}\ny=b_{2}+H^{1,3}_{x}+H^{2,3}_{x} \\tanh (b_{1}+H^{1,2}_{x}),\n\\end{equation}\nwhere $H^{1,2}$, $H^{2,3}$ and $H^{1,3}$ are the weight matrices used to connect each layer, and $b_{1}$ and $b_{2}$ are the bias values of the hidden layer and the output layer respectively.\nThe structure of the FFNN contains only limited information about the foregoing and has some limitations on the length of the input sequence. Therefore, the RNN LM comes into being. As shown in Fig.~\\ref{picture1} (b), RNN can accept input of any variable length. When the input window is moved, its internal state mechanism can avoid repeated calculation, and parameter sharing further reduces the number of model parameters. Therefore, compared with FFNN, RNN has a great advantage.\nThe pretraining LM is to get a set of model parameters by pretraining some tasks. It initializes the model with these parameters and then trains to improve the model performance effectively.\nThe commonly used pretraining models are fixed embedding (Word2vec~, Glove~, etc), variable embedding (Embeddings from LMs (ELMO)~, Generative Pretrained Transformer (GPT)~ and Bidirectional Encoder Representations from Transformers (BERT)~, etc). \nHere, we give an example of the GPT model, as shown in Fig.~\\ref{picture1} (c). It adopts a two-stage process. In the first stage, the Transformer decoder is used as the basic unit of the model to perform text prediction. In the second stage, the GPT is initialized differently for different downstream tasks, training the model and fine-tuning the parameters.", "cites": [7165, 8385, 7], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of language models, including statistical and neural approaches, and introduces key pretraining models like BERT and GPT. While it integrates some information from the cited papers, such as the N-gram model from Paper 1 and ELMo/GPT from Paper 2, the synthesis is limited and mostly additive. There is minimal critical evaluation or abstraction beyond specific systems, focusing instead on factual descriptions."}}
{"id": "11190425-dd22-44ac-b1da-ce058fe83643", "title": "Traditional Text Learning", "level": "subsection", "subsections": [], "parent_id": "8cba48d4-e174-4a7a-afb3-8ba526649a63", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Text Learning"]], "content": "NLP is a research field that integrates linguistics and computer science. Its main research tasks include part-of-speech tagging, named entity recognition, semantic role labeling, machine translation, question answering, sentiment analysis, text summarization, text classification, relationship extraction, event extraction, etc. \nThe LM can be considered the cornerstone of the downstream NLP tasks. \nIt experiences four processes: grammar rule LM, probabilistic LM, neural network LM, and pretraining LM.\nA PFM trains on a large benchmark dataset to obtain a model which can solve new similar tasks, which has become a new hotspot in current LM research.\nWord representations play a significant role in downstream tasks, which is the basis of NLP.\nThe N-gram model preprocesses text features and encodes adjacent $N$ words as a group, which makes it overly dependent on the richness of the training corpus. Otherwise, data-sparse is likely to occur, and the computational complexity will increase exponentially with the increase of $N$.\nNeural Network LM (NNLM)~ adopts the idea of word vector for the first time, and the low-dimensional word vector of distributed representation can solve the discrete problem caused by word embedding well. However, it is still challenging to solve the problem of high computational complexity.\nThe computational complexity of the word2vec model is independent of the selected window size but is determined by the dictionary size and the word vector dimension.\nMany downstream tasks can be significantly improved by training on a large corpus using word vector embedding after initial training.\nHowever, the problem of polysemy for the static word vector is still unsolved, and it still belongs to the shallow LM~~. Therefore, more effective models are urgently needed to deal with the dataset more flexibly.\nTo capture high-level concepts of context, such as polysemy elimination, syntactic structure, etc.\nNeelakantan et al.~ propose to learn multiple embeddings per word type. Zhou et al.~ integrate the features on both dimensions of the matrix to enrich semantics by using subword information.\nBased on the Continuous Bag Of Words (CBOW)~ in word2vec, Hui et al.~ fine-tune the generated word vectors for emotion and obtain the word vectors containing both semantic meaning and emotional tendency, which significantly improved the performance in the Weibo sentiment classification task.\nLiu et al.~ propose a model of hierarchical translation for machine translation. It uses the neural LM based on RNN as the word vector generation model. \nLiang et al.~ propose an approach based on the double-layer self-attention mechanism for machine reading comprehension, and the model is divided into three parts: single document encoder, multi-document encoder, and answer prediction.\nIn the single document encoder, the problem of the context information is represented by the Gated Recurrent Unit (GRU) model.\nZhang et al.~ propose an INDependent RNN (INDRNN) and attention mechanism for user intention classification, using word vectors generated by word2vec as input.\nThe model introduces a word-level attention mechanism to effectively quantify the contribution of domain vocabulary to the intention category.", "cites": [8565, 1684, 7165, 2568], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of traditional text learning methods and mentions several papers and their contributions, but it lacks meaningful synthesis and integration of the cited works. The critical analysis is minimal, as it primarily describes each paper’s method without evaluating limitations or comparing them in depth. Some general patterns, such as the transition from static to context-aware embeddings, are mentioned, but the section remains largely at a concrete level without offering deeper abstraction or novel insights."}}
{"id": "ab140725-6a23-46b0-9db1-6450157b1c01", "title": "Convolution-Based Networks.", "level": "subsubsection", "subsections": [], "parent_id": "aee7bd49-072e-4f3b-be42-b7fb570a094d", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Image Learning"], ["subsubsection", "Convolution-Based Networks."]], "content": "ImageNet~, as one of the most important databases in computer vision, has aroused many milestone network architectures in image classification, including AlexNet~, NIN~, VGG~, GoogLeNet~, ResNet~, DenseNet~, etc. When it comes to object detection and semantic segmentation, researchers explore R-CNNs~, FCN~, SSD~, YOLOs~, SegNet~, PSPNet~, Deeplabs~, RefineNet~, etc. on common benchmark datasets, such as PASCAL VOC~, MS COCO~, etc. \nThere are several shared features among these popular convolution-based networks: 1) \\emph{data augmentation}. Deep models require much more data to fit a complicated model, thus the data augmentation technique such as flipping, rotation, cropping, scaling, translation, and even adding noises enlarges the training dataset; 2) \\emph{convolution}. The convolutional kernel is used to extract the features of original image data, which maintains the spatial structure for the adjacent pixels; 3) \\emph{deep architecture}. The deep architecture contains more parameters, which enhance the capability of the model. These common features contribute to the SOTA performance of convolutional neural networks (CNNs) in computer vision for nearly recent 10 years.", "cites": [96, 1223, 1768, 514, 508, 8429, 7589, 305, 206, 836, 486, 2569, 2571, 802, 2570, 1759, 7588, 97, 810, 7501, 1741, 209, 520], "cite_extract_rate": 0.8214285714285714, "origin_cites_number": 28, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of convolution-based networks in traditional image learning, listing key models and datasets without deep analysis. It mentions shared features like data augmentation, convolution, and deep architecture but does not synthesize the cited papers into a broader framework or highlight specific innovations from each work. There is minimal critical evaluation or abstraction beyond individual examples."}}
{"id": "aa063366-dd27-4046-a390-172ea8a0e90a", "title": "Recurrent neural networks", "level": "subsubsection", "subsections": [], "parent_id": "aee7bd49-072e-4f3b-be42-b7fb570a094d", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Image Learning"], ["subsubsection", "Recurrent neural networks"]], "content": "Different from CNNs targeting 2D-dimensional image applications, recurrent neural networks (RNNs)~ try to use recursive cells to process pictures in sequence, i.e., video data. However, the weaknesses of gradient explosion and long-term dependencies restrict further development of this model. To handle these problems embedded inside the RNN-based models, long short-term memory (LSTM)~ was proposed by Hochreiter and Schmidhuber in 1997. In addition, the improved capability of LSTMs produces popularity and attracts attention both in NLP and CV~.", "cites": [2572, 2401], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of RNNs and LSTMs, referencing two key papers, but it lacks synthesis of their contributions and does not connect them to a broader narrative. There is minimal critical evaluation or abstraction, with the content remaining largely at the level of summarizing established facts without deeper analysis or insight."}}
{"id": "cc275360-66c7-4eeb-acdb-c935357457b1", "title": "Generation-Based Networks", "level": "subsubsection", "subsections": [], "parent_id": "aee7bd49-072e-4f3b-be42-b7fb570a094d", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Image Learning"], ["subsubsection", "Generation-Based Networks"]], "content": "Generative Adversarial Networks (GANs)~ have provided a paradigm to learn representations for unlabelled data, and spawn many GAN-based approaches on downstream tasks. In image translation, pix2pix software~ first proposes the conditional adversarial networks as a solution to the image-to-image translation problems, and achieves reasonable results on real-world datasets. \nMarkovian Generative Adversarial Networks (MGANs)~ is a method to generate texture synthesis, which can be applied to style transfer and video\nstylization. CycleGAN~ provides a learning algorithm to translate an original image from the source domain to a target domain without containing pairs of images in datasets for supervised learning. StyleGAN~ is a style-based generator to serve as an alternative architecture for traditional GANs. Pixel Recurrent Neural Networks (PixelRNN)~ aims to complete images by modeling full dependencies between the color channels. DiscoGAN~ is designed to learn relations between different domains. \nGANs have also provided a novel direction to study data synthesis because it perfectly simulates the distribution of the original data. Laplacian Pyramid of Adversarial Networks (LAPGAN)~ uses a cascade of convolutional networks to generate images in a coarse-to-fine fashion. Similarly, Stacked Generative Adversarial Networks (SGAN)~ decompose variations into multiple levels and gradually resolve uncertainties by stacking several GANs in a top-down way.", "cites": [8567, 1252, 2574, 7217, 8566, 7022, 2573, 157, 896], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various generation-based networks in the context of traditional image learning, listing several GAN variants and their applications. While it references multiple papers, it lacks deeper synthesis of their ideas and fails to connect them into a broader framework or narrative. There is minimal critical analysis or identification of overarching principles, focusing instead on surface-level summaries."}}
{"id": "194fb98a-9ae8-4a2c-a7cb-7ac0ce02b1a5", "title": "Attention-Based Networks", "level": "subsubsection", "subsections": [], "parent_id": "aee7bd49-072e-4f3b-be42-b7fb570a094d", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Image Learning"], ["subsubsection", "Attention-Based Networks"]], "content": "Based on the success of CNNs in the area of CV, the attention module is designed to equip with the popular CNNs. For example, SENet~ proposes a channel attention module, which won first place in the competition of ILSVRC2017. In addition,  CBAM~ sequentially infers attention maps along both channel and spatial dimensions. Many innovative works, such as GCNet~ and CCNet~, are inspired by this idea of soft-attention mechanism, which outperforms the traditional CNNs on major benchmarks for both recognition and segmentation tasks. In particular, the self-attention mechanism~, calculating the response at a position among all entities in a sequence by attending to all positions within the same sequence, is proposed to estimate the relevance of one position to other positions in feature maps. To control the expected entities and model more complex relations among different elements in the sequence, masked self-attention and multi-head attention~ are the key components proposed to substitute the function of convolutions in the era of transformers.", "cites": [2576, 7268, 7194, 2575, 38], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of attention-based networks in traditional image learning, integrating several papers by mentioning their contributions. However, it lacks deeper synthesis across works and does not establish a novel framework. The analysis remains surface-level, with minimal critique or comparison of the approaches, and the level of abstraction is limited to general mentions of attention mechanisms without identifying broader theoretical patterns."}}
{"id": "efabf58e-818d-451d-a426-4bfb3309251f", "title": "Transformer-Based Networks", "level": "subsubsection", "subsections": [], "parent_id": "aee7bd49-072e-4f3b-be42-b7fb570a094d", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Traditional Learning Methods"], ["subsection", "Traditional Image Learning"], ["subsubsection", "Transformer-Based Networks"]], "content": "Recently, inspired by the self-attention mechanism and subsequent success of the transformer in NLP, researchers in CV also try to use the transformer as an alternative to the convolution. Self-attention-based transformer models always operate in a two-stage training mechanism: 1) pretraining on a primitive dataset (always big but not well labeled) by defining pretext tasks; 2) transferring the pretrained weights to the downstream tasks and adjusting the parameters on the target domain dataset by finetuning. Vision Transformer (ViT)~ is applied on CV and achieves the SOTA performance on major benchmark datasets. Data-efficient image Transformers (DeiT)~was proposed by Facebook AI to train image transformers more efficiently and maintain the SOTA performance simultaneously. DEtection TRansformer (DETR)~ significantly outperforms competitive baselines in both object detection and semantic segmentation. LeViT~ outperforms existing benchmarks with respect to balancing the accuracy and training speed. Image GPT~ is inspired by a sequence transformer in NLP, which can compete with several self-supervised benchmarks on ImageNet. On the basis of this research, DeepViT~ explores a deeper architecture to improve performance consistently by making the transformer go deeper. Moreover, many researchers try to apply the transformer to more specific tasks. Pyramid Vision Transformer (PVT)~ introduces the pyramid structure to overcome the difficulties of porting the transformer to various dense prediction tasks, and achieves the SOTA performance on major benchmark datasets. M3DeTR~ is a novel research on multi-representation, multi-scale, and mutual-relation 3D object detection with transformers. Medical Transformer (MedT)~ has focused on medical image segmentation and outperforms previous CNN-based and transformer-based architecture. In conclusion, the transformer has become a novel and popular research area in CV and its performance is proved by many existing works.", "cites": [7360, 2580, 2579, 732, 2578, 7590, 2577, 2581], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a list of transformer-based vision models and their achievements, showing basic synthesis by grouping them under the common theme of replacing or complementing CNNs in CV. However, it lacks deeper critical analysis of their relative strengths, weaknesses, or trade-offs. Abstraction is limited, as it does not generalize beyond the specific models to broader principles or trends in the field."}}
{"id": "7f51ca8f-a2ef-4915-9892-afbc3e061179", "title": "Different Perspectives", "level": "subsection", "subsections": [], "parent_id": "720be46a-69c4-42e1-a410-af6829c488a7", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs Theory"], ["subsection", "Different Perspectives"]], "content": "\\myparagraph{\\textbf{Pretext Tasks}} \n posits a mechanism based on approximate conditional independence (CI) to connect pretext and downstream task data distributions, which suggests that pretext tasks can self-supervisedly learn the representations from unlabelled data that reduce the sample complexity of downstream supervised tasks. The experiments both on CV and NLP task supports this theory. Representation Learning via Invariant Causal Mechanisms (R\\textsc{e}LIC)~ also provides a theoretical understanding from the perspective that the explicit invariance constraints across augmentations can yield improved generalization guarantees.\n\\myparagraph{\\textbf{Multi-View Redundancy}}\nFrom the perspective of a multi-view setting,  understands contrastive learning as exploiting multiple views of data for representation learning.\nThis theory provides a theoretical analysis that the linear functions of these representations from pretraining are still competitive compared with the non-linear optimal predictor of the label. In other words, the linear functions of the learned representations are nearly optimal on downstream prediction tasks whenever the different views provide redundant information about the label.", "cites": [8562, 2582, 2583], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates key theoretical perspectives from three cited papers, connecting the concepts of pretext tasks, invariant causal mechanisms, and multi-view redundancy to provide a partial synthesis of ideas. While it offers a structured analytical framework, it lacks deeper comparison or critique of the approaches. The abstraction level is moderate, as it identifies general principles in self-supervised learning but stops short of proposing a meta-level theory."}}
{"id": "de3557a8-d0ff-477f-bcd8-0aea88f57ef6", "title": "Different Categories", "level": "subsection", "subsections": [], "parent_id": "720be46a-69c4-42e1-a410-af6829c488a7", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs Theory"], ["subsection", "Different Categories"]], "content": "\\myparagraph{\\textbf{Contrastive Learning}}\nAlthough experimental results show us that previous designs such as contrastive loss or momentum updating can produce impressive performance in SSL. However, one of the most important questions that remain in SSL is why these methods can maintain representation consistency during the pretraining process. A naive view is the minimization between positive pairs can boost invariance learning, while the maximization between negative pairs contributes to avoiding representational collapse.  shows that contrastive learning can achieve competitive bound via intra-class concentration, thus leading to the reduction of sample complexity on downstream tasks from the benefit of transferred representations. This research also provides a framework that can be utilized both on the guarantees of the quality of learning representations during the pretraining phase and the future assumptions added to the framework that allow tighter guarantees.\n\\myparagraph{\\textbf{Non-Contrastive Learning}}\nWhile contrastive learning shows an effect by capturing the similarity and dissimilarity among the unlabelled examples, and further converging to an average local optimum which represents the general representations, recent non-contrastive SSL methods such as BYOL and SimSiam also shows the SOTA performance without the design of comparison between negative pairs. Based on the analysis of the eigenspaces, Tian et al.~ study the behavior of non-contrastive SSL training and prove that the effects are from both the predictor and stop-gradient signal. Based on this theory, a novel and simple \\textbf{DirectPred} method is proposed as a by-product of this theoretical exploration.", "cites": [2529, 2584], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of contrastive and non-contrastive learning by integrating key ideas from both cited papers, connecting them to broader SSL principles. It offers some abstraction by highlighting theoretical implications such as representation consistency and predictor mechanisms. However, the critical analysis is limited to posing questions and describing behaviors without deeper evaluation of limitations or contrasting theoretical strengths."}}
{"id": "1224d99a-6f33-4a1f-8309-c4a5655141c1", "title": "Pretext Task Taxonomy on CV", "level": "section", "subsections": [], "parent_id": "160c6790-e92e-4d72-a546-5d2f900ac104", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Pretext Task Taxonomy on CV"]], "content": "Pretext tasks are always designed to use pseudo labels generated from the data itself to pretrain the proxy model. \nThere are five categories of pretext tasks for self-supervised: 1) generation-based methods; 2) transformation-based methods; 3) context-based methods; 4) semantic-based methods; 5) view-based methods.\n\\myparagraph{\\textbf{Generation-Based Methods}} This type of method is GAN-based in the deep learning era. For image generation, there are several applications including image colorization~, image super-resolution~, image editing~, context encoders~, image-to-image translation~, etc. On the other hand, video generation tasks contains future prediction~, video action recoginition~, video generation~, and video representaion~.\n\\myparagraph{\\textbf{Transformation-Based Methods}} Transformation is a typical technology that serves as a data augmentation method to enlarge the training dataset in traditional DL. However, if transformations of the same image are labeled as positive samples and others as negative samples, this pretext task can be used for self-supervised pretraining~. Popular transformation in self-supervised learning (SSL) contains color transformation (such as Jitter, Gaussian blur, and adjusting brightness) and geometric transformation (such as flipping, cropping, scaling, and rotation).\n\\myparagraph{\\textbf{Context-Based Methods}} Basically, the design and construction of many artificial tasks, such as solving Jigsaw puzzles~, comparing context similarity, and discriminating sequence order. Solving Jigsaw puzzles is defined as identifying the correct position of patches from an image. This task can help the model to learn an encoder for transfer learning~, and the feature representations are effective after the pretrained dataset is big enough. In addition, the design of video Jigsaw is also proposed for unsupervised learning~. Differently, context similarity tries to label the patches from the same images as positive samples and others as negative samples, then use a predefined similarity function to scale the distance between different pairs~.\n\\myparagraph{\\textbf{Semantic-Based Methods}} Semantic-based methods contain object detection, semantic segmentation, and depth prediction. These tasks also involve pretext tasks because their pixel-based labels can learn a more robust feature representation than simpler tasks. These pre-text tasks always establish on video dataset~.\n\\myparagraph{\\textbf{View-Based Methods}} This type of method contains both single-modal data and multi-modal data. For the single-modal data, the original data is treated as the anchor and different viewpoints generate its positive pair samples. Sometimes the time slices in sequence-based data are treated as negative pairs because the scene is changed as time goes~. In addition, multi-modal data is usual in view-based methods, which are also called cross-modal-based methods here. Such as audio-video cooperative learning~, RGB and optical flow cross-modal distance training~.", "cites": [9094, 2506, 2586, 7000, 2588, 2590, 8569, 7585, 2502, 7022, 2585, 2503, 312, 8568, 2589, 134, 2587, 1254], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.5, "abstraction": 2.2}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of different pretext task categories in computer vision, with limited synthesis of the cited papers. It mentions methods and applications but fails to connect or analyze the ideas in depth. There is little critical evaluation or abstraction to broader principles, resulting in a low insight level."}}
{"id": "2f7164de-3a58-4a21-ac2f-e8e2ec66800e", "title": "PFMs for Reinforcement Learning", "level": "section", "subsections": ["3df97b8f-533d-47bf-a8d7-c1e2d6830a59"], "parent_id": "160c6790-e92e-4d72-a546-5d2f900ac104", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Reinforcement Learning"]], "content": "The success of pretraining learning methods in the supervised learning domain has spurred interest in the reinforcement learning (RL) domain to study whether the same paradigms can be adapted to RL algorithms. General pretraining RL can include broad directions, such as Reward-Free RL , Goal-condition RL , and Representation Learning in RL . Here we focus the Representation Learning in RL. Specifically, this direction seeks to \\textit{improve the performance by pretraining the visual perception competent of RL agent, i.e., the state encoder, with some large-scale datasets using unsupervised/self-supervised data augmentation techniques}. The pretraining process empowers the state encoder to capture the essential structure information from the raw inputs (pixel-level input for CV). An RL policy network is built based on the pretrained state encoder to learn the specific downstream control tasks in the fine-tuning stage. Recent studies have demonstrated that  can greatly benefit both in sample efficiency and learning effectiveness from unsupervised , semi-supervised , and self-supervised  learning techniques. Specifically, this direction could be roughly classified into the following two categories: \nModel-based Pretraining RL and Contrastive-like Pretraining RL.", "cites": [7591, 7593, 2594, 2592, 2591, 2593, 1911, 8445, 7105, 1411, 7592], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple cited papers on how pretraining can enhance RL, particularly through state encoder pretraining and categorizing methods into model-based and contrastive-like approaches. It offers a coherent narrative by linking these concepts to broader goals like sample efficiency and robustness. However, it lacks deeper critical analysis of the papers' limitations or comparative strengths, and abstraction is moderate, as it identifies general patterns but does not rise to a meta-level framework."}}
{"id": "3df97b8f-533d-47bf-a8d7-c1e2d6830a59", "title": "Model-based Pretraining RL", "level": "paragraph", "subsections": ["cc367997-8d2f-45f7-8082-f6c3642cddb1"], "parent_id": "2f7164de-3a58-4a21-ac2f-e8e2ec66800e", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Reinforcement Learning"], ["paragraph", "Model-based Pretraining RL"]], "content": "Model-based Pretraining RL aims to first pretrain a generative world model to capture the underlying structure of the environment and then leverage the world model as a state encoder or simulator during fine-tuning. World Models  is the first work that proposes to learn a compressed spatial and temporal representation of the environment in an unsupervised manner using a simple Variational Autoencoder, which greatly improves the sample efficiency compared to training from scratch. However, learning the world model without being aware of the environment's dynamic might lead to ignorance of some key information in the environment. Dreamer  proposed to learn latent dynamics by approximating the representation, transition, and reward model. They then train RL agents purely by imagination in a latent space, which is more efficient since it brings a low memory footprint and enables fast predictions of thousands of imagined trajectories in parallel.\nFurthermore, DreamerPro  proposes a reconstruction-free approach based on prototypical representations to migrate the task-irrelevant visual distractions problem in the latent dynamics modeling. DreamerPro significantly outperforms previous SOTA methods when there are complex background distractions. To verify whether learning accurate world models for the real world is promising, Daydreamer  applies Dreamer to the real-world physical robots problem and empirically demonstrates significant learning efficiency gains.", "cites": [2595, 2598, 2596, 2597, 7592], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative of model-based pretraining in RL, highlighting the progression from World Models to Dreamer and its variants. It critically discusses limitations, such as the impact of task-irrelevant visual distractions and the challenges of real-world deployment, while also identifying key improvements like reconstruction-free approaches. The abstraction level is moderate, as it captures patterns in the use of latent representations but stops short of a fully meta-level analysis."}}
{"id": "cc367997-8d2f-45f7-8082-f6c3642cddb1", "title": "Contrastive-like Pretraining RL", "level": "paragraph", "subsections": [], "parent_id": "3df97b8f-533d-47bf-a8d7-c1e2d6830a59", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "PFMs for Reinforcement Learning"], ["paragraph", "Model-based Pretraining RL"], ["paragraph", "Contrastive-like Pretraining RL"]], "content": "Contrastive-like Pretraining RL techniques seek to improve the representation ability of state encoders by pretraining the state encoder with a large amount of out-of-domain data or adding some auxiliary loss using unsupervised learning or data augmentation techniques. \nCURL  combines instance contrastive learning and  by using MoCo  mechanism, which significantly improves the data efficiency of RL agents. Furthermore, RAD  proposes an implicit approach that directly trains the RL objective on multiple augmented observations views, which outperforms CURL on some of the environments in the DeepMind Control Suite. Concurrent to RAD, DrQ  introduces a simple regularization term, which applies image augmentation to compute current and target Q values. They demonstrate that data efficiency can be significantly improved after applying it to DQN. DrQ-v2  further extends this approach to solve complex humanoid locomotion tasks by inserting similar techniques into the DDPG algorithm. Orthogonal to this direction,  demonstrate that pretraining the vision part of RL agent using supervised or unsupervised methods on out-of-domain data can improve the learning efficiency of downstream RL control tasks. Besides ensuring consistency across different views of observation, SPR  additionally trains a dynamics model which enforces the representations to be temporally predictive. Based on SPR, SGI  proposes to pretrain representations using a combination of latent dynamics modeling, unsupervised goal-conditioned, and inverse dynamics modeling. Compared to previous methods, SGI can better capture the environment's dynamics and facilitate downstream RL control task training.", "cites": [2601, 2600, 7593, 2599, 2602, 2591, 2593, 2603, 122, 8570], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes several contrastive-like pretraining methods for RL, connecting them through shared themes like data augmentation and visual representation learning. It provides critical comparisons (e.g., CURL vs. RAD, DrQ vs. DrQ-v2) and highlights methodological differences, such as the use of MoCo, implicit training on augmented views, and regularization. It also abstracts the broader idea of improving data efficiency through pretraining, which unifies the cited works into a coherent conceptual framework."}}
{"id": "aa79e92f-f28d-4c8e-87b8-ea972edb271f", "title": "Downstream Tasks and Datasets on NLP", "level": "subsection", "subsections": ["d089f43d-90f2-4b75-a3ac-15d615f3e514"], "parent_id": "99f4cfea-bdc9-409b-976b-dcdca4dfb1e0", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"]], "content": "There are many available datasets in the NLP domain, divided according to different tasks. We summarize them in Table~\\ref{datasets1}.\nIt mainly comprises two categories: the task of classification of texts and the task of generating texts.\nThe text classification tasks mainly include Sentiment Analysis (SA), News Classification (NC), Topic Labelling (TL), Natural Language Inference (NLI), Named Entity Recognition (NER), Question Answering (QA), Dialogue Act Classification (DAC), etc. The generation tasks mainly include text summaries and machine translation.\n\\begin{table*}[]\n\\caption{The statistics of the datasets on NLP. For the QA task, the class represents the sum number of candidate answers and the correct answer. For dialogue, class is the number of slots. Length means the average tokens in turn.}\\label{datasets1}\n\\resizebox{0.9\\textwidth}{!}{\n\\begin{tabular}{cclllll}\n\\hline\n\\textbf{Type}     & \\textbf{Task}   & \\textbf{Datasets} & \\textbf{Class} & \\textbf{Length}  &\\textbf{ Number}     & \\textbf{Related Papers} \\\\ \\hline\nClassification & Sentiment Analysis& MR & 2     & 20 & 10662 &    \\\\ \\cline{3-7} \n   &  & SST-1    & 5 & 18 & 11,855 &   \\\\ \\cline{3-7} \n   &  & SST-2    & 2 & 19 & 9,613 &    \\\\ \\cline{3-7} \n   &  & MPQA     & 2 & 3 & 10,606 &    \\\\ \\cline{3-7} \n   &  & IMDB     & 2 & 294 & 50,000 &     \\\\ \\cline{2-7} \n   & News Classification& 20NG     &20 & 221 & 18,846 &  \\\\ \\cline{3-7} \n   &  & AG News  & 4 & 45/7 &127,600 &       \\\\ \\cline{3-7} \n   &  & R8 &8 & 66&7,674 &    \\\\ \\cline{3-7} \n   &  & R52& 52 & 70 & 9,100 &    \\\\ \\cline{2-7} \n   & Topic Labeling& DBPedia  & 14 &55 & 630,000 &    \\\\ \\cline{3-7} \n   &  & Ohsumed  & 23 &136 &7,400 &   \\\\ \\cline{3-7} \n   &  & YahooA   & 10 & 112 & 1,460,000 &    \\\\ \\cline{2-7} \n   & Natural Language Inference     & SNLI     & 3&   - &570,152 &  \\\\ \\cline{3-7} \n   &  & MNLI     & 3& -   & 433,000&   \\\\ \\cline{3-7} \n   &  & QNLI     &2 & -   & 115,667&    \\\\ \\cline{3-7} \n   &  & WNLI     & 2&  -  &852 &   \\\\ \\cline{3-7} \n   &  & RTE& 2&  - & 5,768&    \\\\ \\cline{3-7} \n   &  & SICK     &3 & -   & 10,000&    \\\\ \\cline{3-7} \n   &  & MSRP     &2 &  -  & 5,801&    \\\\ \\cline{2-7} \n   & Named Entity Recognition    &  CoNLL 2003  & 4&  -  &2,302 &     \\\\ \\cline{3-7} \n    & &OntoNotes 4.0&18&-&-&   \\\\ \\cline{3-7} \n   & &OntoNotes 5.0&18&-&2,945,000&    \\\\ \\cline{3-7} \n   & &MSRA & 3&-&- &   \\\\ \\cline{3-7} \n   & &ACE 2004 &7 &-&443 &   \\\\ \\cline{3-7}\n   & &ACE 2005 &7 &-&437 &  \n   \\\\ \\cline{3-7} \n   & &KBP2017  & -&-&- &   \\\\ \\cline{2-7} \n   & Question Answering& QQP&2 &    &799,266 &   \\\\ \\cline{3-7} \n   &  & MRPC     & 2&  -  & -&    \\\\ \\cline{3-7} \n   &  & SQuAD    & - & 5,000 &  5,570 &     \\\\ \\cline{3-7} \n   &  & RACE     &5 &  -  & 100,000&    \\\\ \\cline{3-7} \n   &  & TREC  &6&10&6,400&   \\\\ \\cline{3-7} \n   &  & WikiQA   & - &  873 &  243 &   \\\\ \\cline{2-7} \n   & Dialog Act Classification     & DSTC 4   &89 & - &30,000 &  \\\\ \\cline{3-7} \n   &  & MRDA     & 5 & - &62,000 &    \\\\ \\cline{3-7} \n   &  & SwDA     & 43 & - &1,022,000 &   \\\\ \\cline{1-7} \n Generation  & Text Summarization &  NYT&- &  -  &109,910 &    \\\\ \\cline{3-7} \n   &  & CNN&- & 760   & 92,579&   \\\\ \\cline{3-7} \n   &  & Dailymail    &- & 653   &219,506 & \\\\ \\cline{3-7} \n   &  & Gigaword    &- & -   &3,991,000 &  \\\\ \\cline{2-7} \n    & Machine Translation&   WMT14  &- &  -  & -& \\\\ \\cline{3-7} \n    &  &   WMT16  &- &  -  &- & \\\\ \\cline{3-7} \n    &  &   WMT17  &- & -   &- & \\\\ \\cline{3-7} \n    &  &   WMT18  &- & -   &- &  \\\\ \\cline{2-7} \n    &Dialogue  &   DSTC2  &- &  -  & 3,000&  \\\\ \\cline{3-7} \n    &  &   MWOZ  &35 &  15.03  & 10,438& \\\\ \\cline{3-7} \n    &  &   GSIM  &- &  -  &3,008 &  \\\\ \\cline{3-7} \n    &  &   OOS  &151 &  -  & 23,700&  \\\\ \\cline{1-7} \n\\end{tabular}\n}\n\\end{table*}", "cites": [2623, 2614, 7595, 1150, 1174, 2605, 1156, 2611, 2622, 7594, 7106, 1091, 2616, 1067, 9120, 2617, 7596, 7265, 1159, 2613, 2618, 2615, 2608, 1090, 1938, 2624, 2609, 1173, 1970, 2607, 2612, 2606, 826, 2383, 2620, 2604, 2610, 8571, 8385, 1096, 11, 2621, 2387, 8412, 8410, 2619, 8572, 9121, 943, 7], "cite_extract_rate": 0.6578947368421053, "origin_cites_number": 76, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily lists NLP tasks and associated datasets without offering a synthesized narrative or analysis of how the cited papers contribute to the field. There is minimal critical evaluation or abstraction of broader trends or principles. The content is largely descriptive and lacks depth in connecting or interpreting the cited works."}}
{"id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "title": "Sentiment Analysis (SA)", "level": "paragraph", "subsections": ["9fe52d45-e578-4512-bc0d-aa2624058bf0", "7734643e-601a-4301-afa9-e862a3a604e0", "3e442af0-8a82-4ae4-bba4-2f14acc88dee", "387c3436-5b01-4b33-bab1-b87ca6857923", "adb781f7-1acd-43e8-93f7-307e91e07aeb", "48277441-dab6-498d-990e-57aff2317d20", "928fb12c-555b-436f-b3e3-0da733843ee4", "96b1fc1a-aa58-46c9-bd04-71dfc1c6836c", "6692edf1-7b35-4a59-9b52-0c90edbdf04b"], "parent_id": "aa79e92f-f28d-4c8e-87b8-ea972edb271f", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"]], "content": "It consists of judging the emotional polarity and dividing it into several classes.\nDepending on the granularity of sentiments, the SA is divided into three categories: dichotomy (positive and negative), trichotomy (positive, negative, and neutral), and multiple categories.\nHere we introduce several datasets in detail.\n\\myparagraph{Stanford sentiment treebank (SST)~} The dataset is an extension of MR~. SST-1 is a version of SST. It is divided into five categories and the number of training texts and testing texts is 8,544 and 2,210, respectively. It also consists of 20 average tokens.\nThe SST-2~ contains 9,613 movie reviews including 6,920 training texts, 872 development texts, and 1,821 testing texts. \n\\myparagraph{Semantic textual similarity benchmark (STS-B)~} It is used in semantic textual similarity tasks organized in the SemEval context between 2012 and 2017~.\nIt consists of text from image titles, news titles and forums.\nOn a scale of 1 to 5, STS-B displays the semantic similarity of two sentences.\nIt includes 5,749 training sets, 1,379 development sets, and 1,377 testing sets.\n\\myparagraph{Multi-Perspective Question Answering (MPQA)~} This is an opinion dataset which has two categories. \nIt contains 10,606 sentences from various news sources that have been manually annotated for opinions and other private states.\nIt is worth noting that there are 3,311 positive articles and 7,293 negative articles, having no labels for each article.\n\\myparagraph{IMDB reviews~} The dataset is the world’s most authoritative source for binary sentiment classification of film reviews. The number of content in each class is the same and it can be divided into training and testing sets whose number of comments is 25,000 on average.", "cites": [3691], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of several sentiment analysis datasets but does not effectively synthesize information from the cited paper or connect the datasets to broader trends in PFM research. It lacks critical evaluation of the datasets or their relevance to pretrained models and offers minimal abstraction beyond the specific examples."}}
{"id": "9fe52d45-e578-4512-bc0d-aa2624058bf0", "title": "News Classification (NC)", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "News Classification (NC)"]], "content": "As one of the most vital information sources, news content exerts a critical effect on people.\nThe NC facilitates users to acquire essential knowledge in real time. Its applications mainly include news topic identification and recommendation of relevant news based on user interests.\nHere we introduce several datasets in detail.\n\\myparagraph{20 Newsgroups (20NG)~} \n20NG is a text dataset derived from newsgroups. There are 20 classes with the same number of articles per class, including 18846 articles in total. The average number of tokens is 221.\n\\myparagraph{AG News~} This is an academic news search engine, which is divided into four categories. It contains news headlines and introductions. It includes 120,000 training texts and 7,600 testing texts. The number of average tokens is 45/7.\n\\myparagraph{R8 and R52~} They come from Reuters~. R8 contains 8 classes  consisting of 66 average tokens and includes 2,189 and 5,485 testing and training courses.\n There are 52 classes in R52, which consists of 70 average tokens. It is divided into 6,532 and 2,568 training and testing texts.", "cites": [1096], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes specific datasets used for news classification without synthesizing the broader implications or trends related to pretrained foundation models. It does not connect the datasets to the cited paper or the overall narrative of the survey. There is no critical analysis or evaluation of the methods or limitations presented in the cited work. The content remains concrete, focusing only on dataset statistics without abstracting to overarching principles or patterns."}}
{"id": "7734643e-601a-4301-afa9-e862a3a604e0", "title": "Topic Labeling (TL)", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "Topic Labeling (TL)"]], "content": "The task mainly obtains the meaning of the file by defining complex file themes.\nIt is a critical component of topic analysis technology, which aims at simplifying topic analysis by assigning each article to one or more topics. \nHere, we introduce a few in detail.\n\\myparagraph{DBpedia~} It is a large-scale multilingual knowledge base generated by Wikipedia's most commonly used information boxes. It releases DBpedia every month, adding or removing classes and attributes in each version. The most popular version of DBpedia has 14 categories, separated into 560,000 training data and 70,000 testing data. The number of average tokens is 55.\n\\myparagraph{Ohsumed~} This is a biomedical literature database. The number of texts is 7,400. It has 23 cardiovascular disease categories and consists of 136 average tokens. \nAll texts are medical abstracts that are categorized into one or more classes.\n\\myparagraph{Yahoo answers (YahooA)~} The dataset is a topic labeling task having 10 categories. The number of average tokens is 136. There are 140,000 training data and 5,000 testing data. Each text in YahooA has question titles, question contexts, and best answers.", "cites": [1096], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily describes three datasets (DBpedia, Ohsumed, YahooA) with basic statistics and minimal contextualization. It lacks synthesis of ideas from the cited paper, critical evaluation of the datasets or methods, and abstraction to broader patterns or principles in topic labeling. The content is factual but offers little insight into the role or implications of these datasets in the context of pretrained foundation models."}}
{"id": "3e442af0-8a82-4ae4-bba4-2f14acc88dee", "title": "Natural Language Inference (NLI)", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "Natural Language Inference (NLI)"]], "content": "This task is used to forecast whether the meaning of a text can be inferred from another. Interpretation is a broad form of NLI. \nBy comparing the semantic similarity of sentence pairings, it determines whether a sentence is the interpretation of another one.\nHere we introduce several primary datasets in detail.\n\\myparagraph{The Stanford Natural Language Inference (SNLI)~} \nIt is commonly used in NLI takes. It contains 570,152 human-annotated sentence pairs, which are annotated with three sorts of relationships: neutral, derived, and conflicting. \nMulti-genre Natural Language Inference (MNLI)~ has 3 categories and consists of 430,000 sentence pairs annotated with textual information, which is usually used in textual inference tasks. \nQuestion Natural Language Inference (QNLI)~, whose task with 2 classes is to determine whether a given text pair is a question-answer.\nWinograd Natural Language Inference (WNLI)~ which consists of 2 categories is a dataset that captures the standard reference information between two paragraphs.\n\\myparagraph{Microsoft Research Paraphrase (MSRP)~} The dataset contains sentence pairs for the text-similarity task, including 1,725 training and 4,076 testing sets. A binary label annotates each pair, discriminating whether they are paraphrases.\n\\myparagraph{Sentences Involving Compositional Knowledge (SICK)~} It includes nearly 10,000 English sentence pairs, marked with similarity, and the scale range is 1-5. It has neutral, entailment, and contradictory three categories.", "cites": [1173, 1174, 439], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a factual description of several NLI datasets but does not synthesize the underlying themes or contributions of the cited papers. It lacks critical evaluation of the datasets' strengths, limitations, or comparative analyses. Abstraction is minimal, focusing primarily on listing dataset sizes, labels, and purposes without generalizing broader patterns or principles."}}
{"id": "387c3436-5b01-4b33-bab1-b87ca6857923", "title": "Named Entity Recognition (NER)", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "Named Entity Recognition (NER)"]], "content": "This is a fundamental task of NLP to identify people, places, organizations, and other entities in text.\nIt is a crucial primary tool for many NLP tasks, including information extraction, question answering, semantic parsing, machine translation, etc.\n\\myparagraph{CoNLL 2003~}\nIt consists of newswire text from the Reuters RCV1\ncorpus. It contains four different entity types (Location, Organization, Person, and Miscellaneous) and includes 1,393 English news articles, and 909 German news articles.\n\\myparagraph{OntoNotes 5.0~}\nThe dataset consists of 174,5K English, 900K Chinese, and 300K Arabic text data. It comes from telephone conversations, news agencies, radio news, radio conversations, and blogs. It has 18 entity classes containing 11 types, seven values, and 2,945,000 text data.\n\\myparagraph{MSRA~}\nThis is a Chinese dataset that is obtained from the news domain. It has three types of entities and is used as a shared task on SIGNAN back in 2006.", "cites": [7, 8385, 7596], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the NER task and datasets but does not synthesize or connect the cited papers in a meaningful way. It lacks critical analysis of the methods or datasets and does not abstract or generalize beyond the specific examples to offer broader insights into trends or principles in NER research using PFMs."}}
{"id": "adb781f7-1acd-43e8-93f7-307e91e07aeb", "title": "Question Answering (QA)", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "Question Answering (QA)"]], "content": "There are two types of QA systems: the extraction guidance system and the generation guidance system. \nThe extractive QA can be regarded as a particular case of text classification. \nHere we detail several datasets.\n\\myparagraph{Microsoft Research Paraphrase Corpus (MRPC)~} It contains 5,800 sentence pairs extracted from Internet news, and the task type is similar to the QQP dataset. Sentence pairs are derived from comments on the same news item and determine whether the two sentences are semantically the same. The assessment criteria were classification accuracy and F1 score.\n\\myparagraph{Stanford Question Answering Dataset (SQuAD)~ } This is a large-scale machine-reading comprehension dataset that contains two tasks.\nSQuAD 1.1~ provides questions and corresponding answers, and the dataset contains 100,000 samples in total, while SQuAD 2.0~ adds unanswered questions and expands the scale to 150,000.\n\\myparagraph{RACE~} The dataset has 5 categories, containing nearly 100,000 questions extracted from middle and high school English tests, with corresponding answers given by experts.\nThe average length of RACE text is more significant than 300, which is longer than other reading comprehension datasets (such as SQuAD) sequences.", "cites": [439, 8385, 1565], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of QA datasets and their characteristics, citing relevant papers for SQuAD and MRPC. However, it lacks deeper synthesis of the cited works and fails to connect them in a broader context or framework. There is minimal critical evaluation or abstraction beyond the specific datasets."}}
{"id": "928fb12c-555b-436f-b3e3-0da733843ee4", "title": "Text Summarization", "level": "paragraph", "subsections": [], "parent_id": "d089f43d-90f2-4b75-a3ac-15d615f3e514", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on NLP"], ["paragraph", "Sentiment Analysis (SA)"], ["paragraph", "Text Summarization"]], "content": "Text summarization is a summary of given single or multiple documents. It is kept as concise as possible while ensuring that it reflects the critical content of the original document.\nIt can be divided into extractive summarization and generative summarization.\nExtractive summarization is generated by extracting and splicing the critical sentences in documents. Generative summarization is generated by a model, which summarizes documents according to the required content expressed in documents.\n\\myparagraph{NYT~}\nThe dataset comes from the corpus annotated by the New York Time. The named entities are annotated using the Stanford NER tool in conjunction with the Freebase knowledge base. It contains 9,076 articles, with the remaining 100,834 divided into a training set (96,834 examples) and a validation set (4,000 samples).\n\\myparagraph{CNN/Daily Mail~}\nIt is used for the passage-based question-answering task, and it is popular in assessing ATS systems.\nThe dataset consists of CNN/Daily Mail news stories paired with multi-sentence human-generated summaries. There are 287,226 training instances, 13,368 validation instances, and 11,490 testing instances in total.\n\\myparagraph{Gigaword~}\nThis is a dataset of English news chapters consisting of nearly 950 pieces.\nHeadlines -- stories from multiple sources, including the New York Times -- include some articles with a one-sentence, short news feed.", "cites": [1067, 2611], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of text summarization and its types, along with a list of datasets and their statistics. It cites two papers but does not integrate their contributions into a broader narrative or comparative analysis. There is no critical evaluation or abstraction of key trends or principles in the field."}}
{"id": "aec0bd98-e02a-43ef-82e5-6bf469622fdc", "title": "Downstream Tasks and Datasets on CV", "level": "subsection", "subsections": ["91d791c5-9669-435d-ad7a-46c76cd06f98"], "parent_id": "99f4cfea-bdc9-409b-976b-dcdca4dfb1e0", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on CV"]], "content": "\\begin{table*}[htbp]\n\\centering\n\\caption{The statistics of the datasets used on downstream tasks.}\\label{datasets2}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{ccccccl}\n\\hline\n\\textbf{Type} & \\textbf{Name} & \\textbf{Usage} & \\textbf{Domain} & \\textbf{Class} & \\textbf{Size} & \\textbf{Related Papers} \\\\ \\hline\n    \\multirow{2}{*}{Classification} & \\multirow{2}{*}{ImageNet} & Pretrain \\& & \\multirow{2}{*}{-} & \\multirow{2}{*}{1000+} & \\multirow{2}{*}{1,200,000+} & \\multirow{2}{*}{ \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular}} \\\\\n    & & Downstream & & & &\n\\\\ \\cline{2-7} \n    & CIFAR-10 & Downstream & - & 10 & 60,000 & \n   \n\\\\ \\cline{2-7} \n    & CIFAR-100 & Downstream & - & 100 & 60,000 &   \n\n\\\\ \\cline{2-7} \n    & STL-10 & Downstream & - & 10 & 6,000 &\n\n\\\\ \\cline{2-7} \n    & Caltech-101 & Downstream & object & 101 & 9,146 & \n \n\\\\ \\cline{2-7} \n    & MNIST-10 & Downstream & digit & 10 & 60,000 & \n  \n\\\\ \\cline{2-7}\n    & SVHN & Downstream & digit & 10 & 73,257 & \n\n\\\\ \\cline{2-7} \n & Places205 & Downstream & scene & 205 & 2,448,873 &\n\n\\\\ \\cline{2-7} \n    & SUN397 & Downstream & scene & 899 & 130,519 &\n\n\\\\ \\cline{2-7} \n    & HMDB51 & Downstream & action & 51 & 7000 & \n\n\\\\ \\cline{2-7} \n    & UCF101 & Downstream & action & 101 & - & \n  \n\\\\ \\cline{2-7} \n    & Food-101 & Downstream & food & 101 & 101,000 &   \n\n\\\\ \\cline{2-7} \n    & Birdsnap & Downstream & bird & 500 & 49,829 &\n\n\\\\ \\cline{2-7} \n    & Cars & Downstream & car & 196 & 16,185 &\n\n\\\\ \\cline{2-7} \n    & Aircraft & Downstream & aircraft & 102 & 10,200 &\n\n\\\\ \\cline{2-7} \n    & Pets & Downstream & pet & 37 & 7,400 &\n\n\\\\ \\cline{2-7} \n    & Flowers & Downstream & flower & 102 & 8,189 &\n\n\\\\ \\cline{2-7} \n    & DTD & Downstream & texture & 47 & 5,640 &\n\n\\\\ \\cline{2-7} \n    & iNaturallist2018 & Downstream & species & 8,000+ & 450,000+ &\n\n\\\\ \\cline{2-7} \n    & JFT-300M & Pretrain & - & 3,000+ & 300,000,000+ &\n\n\\\\ \\cline{1-7} \nDetection & COCO & Downstream & object & 80 & 200,000 & \n  \n\\\\ \\cline{2-7}\n    & VOC07 & Downstream & object & 20 & 9,963 & \n \n\\\\ \\cline{1-7}\nSegmentation & VOC12 & Downstream & object & 20 & 2,913 & \n\n\\\\ \\cline{2-7} \n    & NYU-Depth V2 & Downstream & scene & 894 & 1,449 & \n\n\\\\ \\cline{2-7} \n    & VOC11 & Downstream & object & 20 & 3,334 & \n\n\\\\ \\cline{2-7} \n    & ADE20K & Downstream & scene & 3,688 & 27,574 & \n\n\\\\ \\cline{2-7} \n    & Cityscapes & Downstream & scene & 25 & 25,000+ & \n\n\\\\ \\cline{2-7} \n    & LVIS & Downstream & vocabulary & 1,200+ & 160,000+ & \n\n\\\\ \\cline{2-7} \n    & DAVIS & Downstream & scene & 150 & - & \n\n\\\\ \\cline{1-7} \nInpainting & Paris StreetView & Downstream & scene & - & 15,000 & \n\n\\\\ \\cline{1-7}\nSequence & Moving-MNIST & Downstream & digit & - & 10,000 & \n\n\\\\ \\cline{1-7}\n- & YFCC100M & Pretrain & multimedia & - & 100,000,0000+ & \n\n\\\\ \\cline{1-7}\n\\end{tabular}\n}\n\\end{table*}\nThe datasets in CV mainly contain three types from the perspective of tasks: classification, detection, and segmentation. The popular datasets are concluded in Table \\ref{datasets2}, and some infrequently mentioned datasets in long tails are discussed in the text.", "cites": [9094, 2518, 2506, 131, 2514, 2505, 7101, 2528, 2524, 7000, 7018, 133, 2502, 7100, 2525, 122, 2503, 2517, 8562, 2516, 2530, 134, 2504, 2527, 2521, 2508, 2507, 2513, 9095, 2526, 732, 2529, 1254, 865], "cite_extract_rate": 0.8974358974358975, "origin_cites_number": 39, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.3}, "insight_level": "low", "analysis": "The section provides a list of datasets used in computer vision (CV) for pretraining and downstream tasks but fails to synthesize or integrate the insights from the cited papers. It does not analyze how these datasets relate to the advancements in pretrained foundation models or connect them with broader trends in representation learning. The lack of critical evaluation or abstraction results in a primarily descriptive summary with minimal insight."}}
{"id": "91d791c5-9669-435d-ad7a-46c76cd06f98", "title": "Classification", "level": "paragraph", "subsections": ["78f6477b-7ba4-4dda-bd23-ec85a2ea321d", "8927ac3e-1d6a-4b82-80e0-ce748bd79e75", "f7a26ae9-9dd6-45cc-8972-5d2e2b08b70d"], "parent_id": "aec0bd98-e02a-43ef-82e5-6bf469622fdc", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on CV"], ["paragraph", "Classification"]], "content": "In this part, we first cover the popular large-scale datasets used frequently in both the pretext and downstream tasks. Then the domain datasets only used for the downstream tasks are unfolded.\n\\myparagraph{MNIST~} \nIt's a collection of handwritten digits that includes $60,000$ samples in training and $10,000$ in testing.\nThe images are fixed-size with $28\\times28$ pixels. The pixel values are from $0$ to $255.0$ in which pixel values smaller than 255.0 can be understood as background (white) and 255 means foreground (black). The labels are from 0 to 9 and only one of these digits exists in an image. Both traditional and deep learning methods are based on this most popular dataset despite advanced methods showing perfect results. Thus, Geoffrey Hinton has described it as \"the drosophila of machine learning\".\n\\myparagraph{Street View House Numbers (SVHN)~} In the domain of digit numbers, it collects real-world digit numbers from house numbers in Google Street View images. It includes $73,257$ digits for training, $26,032$ digits for testing, and $531,131$ additional. All of them are $32\\times32$ color images with both class labels and character-level bounding boxes.\n\\myparagraph{CIFAR}~ As more advanced methods show perfect results on the simple datasets, more sophisticated datasets such as CIFAR-10 and CIFAR-100 are conducted. These two datasets are closer to the real-world object. The CIFAR-10 contains $50,000$ training images and $10,000$ testing images, with $6,000$ images per class and $32\\times32$ pixels in each RGB color image. The CIFAR-100 is similar to the CIFAR-10 but with more detailed label information. There are $100$ classes containing $500$ training images and $100$ testing images in each class. In addition, these $100$ \"fine\" classes are grouped equally into $20$ \"coarse\" classes. Researchers can adapt it to suitable learning methods.\n\\myparagraph{STL-10~} Inspired by the CIFAR-10 dataset, STL-10 is another $96\\times96$ color image dataset containing similar $10$ real-world classes. Each class has $500$ training images and $800$ testing images. The biggest difference is that STL-10 has $100,000$ unlabeled images for unsupervised learning. More construction information can be seen in~.\n\\myparagraph{Caltech-101~} It collects roughly $300\\times200$ color images of objects belonging to 101 categories, with 40 to 800 images per category and 50 on average. The outlines of the objects in the pictures are annotated for the convenience of different learning methods.\n\\myparagraph{ImageNet~} This is one of the most popular and large-scale datasets on computer vision. It is built according to the hierarchical structure of WordNet~. The full ImageNet dataset contains $14,197,122$ images and $21,841$ synsets indexed, attaching on average $1,000$ images to demonstrate each synset. The most frequently-used subset of ImageNet is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset from 2010 to 2017, containing tasks of classification, localization, and detection. The number of samples in training and testing datasets and the labels of images are determined by the specific task, more details are seen in~.\n\\myparagraph{HMDB51~} In addition to the popular MNIST, there still exist many domain datasets used for the downstream tasks in the classification problem. HMDB51 is an action video database for a total of $7,000$ clips in 51 action classes. It contains five types of facial actions and body movements.\n\\myparagraph{UCF101~} It is another action video dataset designed for more realistic action recognition. It is an extension of the UCF50~ dataset containing only 50 action categories with 101 action categories, collected from YouTube. What makes it a famous recognition dataset is the workshop in ICCV13 with UCF101 as its main competition benchmark. \n\\myparagraph{Food-101~} This is a real-world food dataset of $101$ food categories, with $750$ and $250$ images per class in training and testing dataset respectively.\n\\myparagraph{Birdsnap~} \nIt is a fine-grained visual categorization of birds on a broad scale,\nwith bounding boxes and the locations/annotations of 17 parts in the object. It contains $49,829$ images of the 500 most common species in North America, with each species containing $69$ to $100$ images and most species having 100. In addition, some images are also labeled as male or female, immature or adult, and breeding or non-breeding plumage. \n\\myparagraph{SUN397} To target the scene categorization, the extensive Scene UNderstanding (SUN) database~ fills the gap of the existing dataset with the limited scope of categories. This database has $899$ categories and $130,519$ images, and only images with more than $200\\times200$ pixels were kept. SUN397 is a more well-sampled subset that maintains 397 categories with at least 100 images per category, in which other categories containing relatively few unique photographs are discarded. \n\\myparagraph{Places205} Places205~ dataset is another large scale scene dataset consists of $2,448,873$ images from 205 scene categories.\n\\myparagraph{Cars~} The dataset in the domain of cars contains $16,185$ color images of $196$ classes (at the level of Make, Model, Year) of cars. For convenience, this dataset is split into training and testing sets in roughly equal quantities. \n\\myparagraph{Aircraft~} It is another fine-grained visual classification designed for aircraft (also known as FGVC-Aircraft). A popular form of this dataset is the fine-grained recognition challenge 2013 (FGComp2013)~ ran in parallel with the ILSVRC2013. There exist four-level hierarchies: Model, Variant, Family, Manufacturer, from finer to coarser to organize this database. The more detailed information is shown in~. \n\\myparagraph{Pets~} It represents The Oxford-IIIT Pet Dataset that collects 37 pet categories with roughly 200 images per category. All images have an associated ground truth annotation of breed for classification, head ROI for detection, and pixel-level trimap for segmentation. \n\\myparagraph{Flowers~} Similarly, Flowers is another domain dataset in flowers also collected by Oxford; it contains Oxford-17 Flowers of 17 categories and Oxford-102 Flowers of 102 categories. \n\\myparagraph{Describable Textures Dataset (DTD)~} This is an evolving collection of textural images in the wild, which consists of $5,640$ images of 47 categories, with 120 images per category.\n\\myparagraph{iNaturalist2018~} It is a large-scale species classification competition conducted on the FGVC5 workshop at CVPR2018. This dataset contains over 8,000 species categories, with more than $450,000$ images in the training and validation dataset collected from iNaturalist~.\n\\myparagraph{JFT-300M~} JFT-300M is an internal Google dataset introduced by Sun et al~ and well-known from ViT Model~. It is labeled by algorithms that utilize human-computer communications and target classification tasks. This dataset finally contains 300M images with over 1000M labels, thus leading to the multiple labels attached to this large-scale dataset.", "cites": [2625, 861, 2626, 732], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a detailed description of various classification datasets used in computer vision, including their sizes, structures, and applications. However, it lacks synthesis by not connecting these datasets to the broader trends in pretrained foundation models or the cited papers. There is minimal critical analysis or abstraction, as it primarily serves as a factual summary without deeper insights or evaluation of the datasets' roles or limitations in the context of PFMs."}}
{"id": "78f6477b-7ba4-4dda-bd23-ec85a2ea321d", "title": "Detection", "level": "paragraph", "subsections": [], "parent_id": "91d791c5-9669-435d-ad7a-46c76cd06f98", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on CV"], ["paragraph", "Classification"], ["paragraph", "Detection"]], "content": "The detection is a popular task in the CV, and almost all the research is conducted on COCO and PASCAL VOC datasets.\n\\myparagraph{COCO~} This is a large-scale dataset for object detection, segmentation, and caption; it contains $330,000$ RGB images, with more than $200,000$ labeled. There are 1.5 million object instances of 80 object categories involved. Thus, it is one of the most popular benchmark dataset in detection and segmentation in parallel with the following PASCAL VOC.\n\\myparagraph{PASCAL VOC~} \nFrom 2005 through 2012, the dataset has run challenges assessing performance on object class recognition and has provided standardized image datasets for object class recognition.\nThe main datasets used in self-supervised learning are VOC07, VOC11, and VOC12. Main competitions in VOC07~ contain classification and detection tasks; both of them consist of 20 objects and contain at least one object in each image. Thus, it is common to use VOC07 to serve as the downstream task for the detection.", "cites": [486], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the COCO and PASCAL VOC datasets used in detection tasks but lacks synthesis across multiple papers or deeper analysis. It does not critically evaluate the datasets or their impact on self-supervised learning, nor does it abstract broader trends or principles in dataset usage for CV foundation models."}}
{"id": "8927ac3e-1d6a-4b82-80e0-ce748bd79e75", "title": "Segmentation", "level": "paragraph", "subsections": [], "parent_id": "91d791c5-9669-435d-ad7a-46c76cd06f98", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on CV"], ["paragraph", "Classification"], ["paragraph", "Segmentation"]], "content": "The segmentation is a semantics-based pixel-level classification. These datasets are difficult to obtain and annotate, thus they are always used as a downstream task.\n\\myparagraph{VOC11~ \\& VOC12~} Both VOC11 and VOC12 contains classification, detection, and segmentation tasks in the main competition, thus leading to the common use of downstream task for the segmentation.\n\\myparagraph{ADE20K~} It collects $27,574$ images from both the SUN and Places205 databases, in which $25,574$ for training and $2,000$ for testing. All $707,868$ objects from $3,688$ categories existing in images are annotated. Especially, this dataset contains $193,238$ annotated object parts and parts of parts, and additional attributes, annotation time, depth ordering for the benefit of the research community.\n\\myparagraph{NYU-Depth V2~} This is a dataset consisting of images and video sequences from 464 indoor scenes that are recorded by both the RGB and Depth cameras from 3 cities. It contains $1,449$ images with the ground truth of depth, and the original RGB values are also provided. In addition, there are $407,024$ new unlabeled frames and additional class labels for the objects in images.\n\\myparagraph{Cityscapes~} It is a dataset of urban street scenes from 50 cities with the ground truth of semantic segmentation. The main instances are vehicles, people, and construction. The high-quality dense pixel annotations contain a volume of $5,000$ images. In addition to the fine annotations, coarser polygonal annotations are provided for a set of $20,000$ images. Moreover, the videos consist of not consistent images with high-quality annotations, and these annotated images with consistently changing views are provided for researchers.\n\\myparagraph{LVIS~} It is a dataset for large vocabulary instance segmentation. It features that 1) a category or word in one image is related to the only segmentation object; 2) more than $1,200$ categories are extracted from roughly $160,000$ images; 3) long tails phenomenon exist in these categories; and 4) more than $2,000,000$ high-quality instance segmentation masks.\n\\myparagraph{Densely Annotated VIdeo Segmentation (DAVIS)~} It is a video dataset designed for the in-depth analysis of the SOTA in video object segmentation, in which DAVIS 2017~ contains both semi-supervised (human-guided at the testing time) and unsupervised (human non-guided at test time) video sequences with multiple annotated instances.", "cites": [1733, 1728, 7597], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the characteristics of various segmentation datasets without critically analyzing or comparing them. It integrates minimal insights from the cited papers, focusing mostly on factual data such as image counts, categories, and annotation types. There is little abstraction or synthesis of broader trends or principles in the context of pretrained foundation models."}}
{"id": "f7a26ae9-9dd6-45cc-8972-5d2e2b08b70d", "title": "Others", "level": "paragraph", "subsections": [], "parent_id": "91d791c5-9669-435d-ad7a-46c76cd06f98", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on CV"], ["paragraph", "Classification"], ["paragraph", "Others"]], "content": "There are many datasets designed for special visual tasks such as inpainting. In addition, this part covers the data collection in the wild.\n\\myparagraph{Paris StreetView~} The dataset is designed for image inpainting task, which contains $14,900$ training images and 100 testing images collected from street views of Paris. This dataset is collected from Google Street View and mainly focuses on the buildings in the city. \n\\myparagraph{Moving-MNIST~} Based on MNIST, it is a video dataset designed for evaluating sequence prediction or reconstruction, which contains $10,000$ sequences. Each video is long of 20 frames and consisted of two digits (possibly overlapped) moving inside a $64\\times64$ patch. The first benchmark is reported on~ by the method of LSTMs.\n\\myparagraph{Yahoo Flickr Creative Commons 100 Million (YFCC100M)~} The dataset is the largest public multimedia collection that is allowed to search by users for their own targets; this dataset can browse both images and videos. It is free and for researchers to explore and investigate subsets of the YFCC100M in real time. Subsets of the complete dataset can be retrieved by any keyword search and reviewed directly. In addition, the text information attached to any image or video is abundant, such as containing location information and user tags. Briefly, it is more a multimedia library than a domain dataset.\n\\myparagraph{Data in the Wild} More generalized dataset concept in the self-supervised learning era is composed of multimedia websites, APP, or search engines such as Instagram, Flickr, Google Images, etc. I think pictures in the wild will play a major role in the future study of CV because of the quantity of data, the computation source, and the learning power of PFM.", "cites": [2626, 2627, 7587], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of several datasets used in computer vision, including their contents and purposes. It mentions the use of LSTMs in Paper 1 and the scale of YFCC100M in Paper 2, but fails to synthesize insights across the cited works. There is minimal critical evaluation or abstraction, with only a brief and speculative statement about the role of data in the wild in future research."}}
{"id": "2af89c5f-d372-4215-8bba-7f55dacb0c70", "title": "Node-Level Tasks", "level": "paragraph", "subsections": ["c074e3cd-2e98-4272-b494-720548f9d202", "dc734dd2-83d6-429f-86be-0de49bfcbb52", "7dbfd67b-ebb4-4a93-8832-e0363a9fec3e"], "parent_id": "c0c5905b-5a26-4f77-8444-f904443bb416", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on Graph"], ["paragraph", "Node-Level Tasks"]], "content": "Nodes are the most basic element of the graph, so lots of downstream tasks mainly focus on the analysis of nodes.\n\\myparagraph{Node Classification}\nNode ClassiFication (NCF) is one of the most prevalent graph-based tasks, which has important analytical value in most of the different types of graph data.\nDifferent from the pseudo-labels assigned to nodes in the graph in self-supervised methods, the labels in NCF often come from external information such as manual annotation.\nBased on Definition~\\ref{def: ind_learning} and~\\ref{def: trans_learning}, NCF can be divided into two types: transductive and inductive according to the visibility during training, verification, and testing.\nIn addition, the result of NCF can be single-label or multi-label according to the mutual exclusion of labels.\nThe statistical results of common NFC datasets are shown in Table~\\ref{tab:dataset of nodeL}.\n\\begin{table*}[htbp]\n\t\\tiny\n\t\\centering\n\t\\caption{The statistics of the datasets for node-level tasks. Homogeneous:Hom, Heterogeneous:Het.}\n\t\\label{tab:dataset of nodeL}\n\t\\resizebox{\\textwidth}{!}{\n\t\\begin{tabular}{cccccccccl}  \n\t\t\\hline \n\t\\textbf{Task}\t& \\textbf{Name} & \\textbf{Usage} & \\textbf{Source} & \\textbf{Type} & \\textbf{Nodes} & \\textbf{Edges} & \\textbf{Class} & \\textbf{Features} & \\textbf{Related Paper}  \\\\\n\t\t\\hline\n\tNCF\t&Academia & pretrain & Citation & Hom & 138K & 739K & - & - &  \\\\\\cline{2-10} \n\t\t&DBLP (SNAP) & pretrain & Citation & Hom & 317K & 2M & - & - &  \\\\\\cline{2-10} \n\t&\tDBLP (NetRep) & pretrain & Citation & Hom & 540K & 30M & - & - &  \\\\\\cline{2-10}\n\t&\tIMDB & pretrain & Movie & Hom & 896K & 8M & - & - &  \\\\\\cline{2-10}\n\t&\tFacebook & pretrain & Social & Hom & 3M & 47M & - & - &  \\\\\\cline{2-10}\n\t&\tLiveJournal & pretrain & Social & Hom & 4M & 86M & - & - &  \\\\\\cline{2-10}\n\t&\t\\multirow{2}{*}{Cora} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{2,708} & \\multirow{2}{*}{5,429} & \\multirow{2}{*}{7} & \\multirow{2}{*}{1,433} &    \\\\\n    &\t &  &  &  &  &  &  &  &  \\\\\\cline{2-10}\n\t&\t\\multirow{2}{*}{CiteSeer} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{3,327} & \\multirow{2}{*}{4,732} & \\multirow{2}{*}{6} & \\multirow{2}{*}{3,703} &    \\\\\n\t&\t&  &  &  &  &  &  &  &  \\\\\\cline{2-10}\n\t&\t\\multirow{2}{*}{PubMed} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{19K} & \\multirow{2}{*}{44K} & \\multirow{2}{*}{3} & \\multirow{2}{*}{500} &    \\\\\n\t&\t&  &  &  &  &  &  &  &  \\\\\\cline{2-10}\n\t&\tACM & Downstream & Citation & Hom &  8,994 & 26K & 4 & 1,902 &  \\\\\\cline{2-10}\n\t&\tCora-Full & Downstream & Citation & Hom & 20K & 63K & 70 & 500 &  \\\\\\cline{2-10}\n\t&\tCora-ML & Downstream & Citation & Hom & 2,995 & 8,158 & 7 & 2879 &  \\\\\\cline{2-10}\n\t&\tReddit-233K & Downstream & Social & Hom & 233K & 57M & 210 & 5,414 &  \\\\\\cline{2-10}\n\t&\tBlogCatalog & Downstream & Social & Hom & 10K & 334K & 39 & - &  \\\\\\cline{2-10}\n\t&\tYouTube & Downstream & Social & Hom & 1M & 3M & 47 & - &  \\\\\\cline{2-10}\n\t&\tReddit-231K & Downstream & Social & Hom & 231K & 11M & 41 & 602 &  \\\\\\cline{2-10}\n\t&\tAmazon & Downstream & Social & Het & 130M & - & - & - &   \\\\\\cline{2-10}\n\t&\tPPI-30K & Downstream & Protein & Het & 3,890 & 77K & 50 & - &  \\\\\\cline{2-10}\n\t&\tPPI-57K & Downstream & Protein & Het & 57K & 819K & 121 & 50 &  \\\\\\cline{2-10}\n\t&\tIMDB & Downstream & Movie & Hom & 12K & 37K & 4 & 1,256 &  \\\\\\cline{2-10}\n\t&\tFour-Univ & Downstream & Movie & Hom & 4,518 & 3,426 & 6 & 2,000 &  \\\\\\cline{2-10}\n\t&\tChameleon & Downstream & Web & Hom & 2,277 & 36K & 6 & 500 &  \\\\\\cline{2-10}\n\t&\tCrocodile & Downstream & Web & Hom & 12K & 180K & 6 & 500 &  \\\\\\cline{2-10}\n\t&\tFlickr-89K & Downstream & Web & Hom & 89K & 450K & 7 & 500 &  \\\\\\cline{2-10}\n\t&\togbn-arxiv & Downstream & Web & Hom & 169K & 117K & 40 & 128 &  \\\\\\cline{2-10}\n\t&\tWiki-CS & Downstream & Web & Hom & 12K & 277K & 10 & 300 &  \\\\\\cline{2-10}\n\t&\tDBLP & Downstream & Web & Hom & 17K & 53K & 4 & 1639 &  \\\\\\cline{2-10}\n\t&\tComputers & Downstream & Co-purchase & Hom & 14K & 246K & 10 & 767 &  \\\\\\cline{2-10}\n\t&\tPhoto & Downstream & Co-purchase & Hom & 7,650 & 119K & 8 & 745 &  \\\\\\cline{2-10}\n\t&\tCS & Downstream & Co-author & Hom & 18K & 82K & 15 & 500 &  \\\\\\cline{2-10}\n\t&\tPhysics & Downstream & Co-author & Hom & 35K & 248K & 5 & 500 &  \\\\\\cline{2-10}\n\t&\tH-index & Downstream & Co-author & Hom & 5,000 & 44K & - & - &  \\\\\\cline{2-10}\n\t&\tFlickr-81K & Downstream & Photo & Hom & 81K & 6M & 195 & - &  \\\\\\cline{2-10}\n\t&\tWikipedia & Downstream & Word & Hom & 4,777 & 185K & 40 & - &  \\\\\\cline{2-10}\n\t&\tUS-Airport & Downstream & Airline & Hom & 1,190 & 13K & - & - &  \\\\\\cline{2-10}\n\t&\tOAG & Downstream & Academic & Het & 178M & 2B & - & - &  \\\\\n\t\t\\hline\nNTKS\t&\tKDD-ICDM & Downstream & Co-author & Hom  & 2,867/2,607 & 7,637/4,774 & 697 & - &  \\\\\\cline{2-10}\n&\t\tSIGIR-CIKM & Downstream & Co-author & Hom & 2,851/3,548 & 6,354/7,076 & 874 & - &  \\\\\\cline{2-10}\n\t&\tSIGMOD-ICDE & Downstream & Co-author & Hom & 2,626/2,559 & 8,304/6,668 & 898 & - &  \\\\\n\t\t\\hline\n\t\\end{tabular}\n\t}\n\\end{table*}\n\\myparagraph{Node Clustering}\nThe goal of Node ClusterIng (NCI) is to divide a graph into different classes or clusters according to a certain standard so that the correlation of nodes in the same cluster is as large as possible, and the irrelevance of nodes that are not in the same cluster is also minimized.\nAlthough in the above-mentioned pretraining tasks, NCI is used as a pretext task has appeared, NCI can still test pretraining graph models based on other pretext tasks.\n\\myparagraph{Top-K Search}\nThe goal of task Top-K Search (TKS) is to search the K nodes with the highest predefined associations for a given node in the graph.\nUsually, TKS is used for search tasks such as recommendation and alignment.\nThe detailed statistical results of the datasets are shown in Table~\\ref{tab:dataset of nodeL}.", "cites": [2538, 218, 7357, 1010, 242, 2533, 229, 7582, 2628, 1184, 7103, 1440, 7598, 1439, 240, 2539, 2542], "cite_extract_rate": 0.8095238095238095, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes node-level tasks (node classification, clustering, top-K search) and lists various datasets along with their statistics. While it references several papers related to self-supervised and contrastive learning on graphs, it does not synthesize their ideas or connect them in a meaningful narrative. There is little critical evaluation or abstraction beyond specific datasets and methods."}}
{"id": "c074e3cd-2e98-4272-b494-720548f9d202", "title": "Link-Level Tasks", "level": "paragraph", "subsections": [], "parent_id": "2af89c5f-d372-4215-8bba-7f55dacb0c70", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on Graph"], ["paragraph", "Node-Level Tasks"], ["paragraph", "Link-Level Tasks"]], "content": "The edge is also an important part of the graph structure, which associates independent nodes and is the key to distinguishing graph data from non-relational data.\nEspecially in some specific fields (e.g., molecules, proteins), edges contain real information, so there are various tasks related to edges.\n\\myparagraph{Link Classification}\nSimilar to the NCF, the Link Classification (LC) also assigns one or more labels to a given edge.\nIn fact, in LC, the nodes at both ends of the edge are still taken into consideration.\n\\myparagraph{Link Prediction}\nLink Prediction (LP) is a common graph task (e.g., knowledge graph). \nThe goal of LP is to predict edges that are removed or may exist in the graph.\nSimilar to NCI, LP is also one of the pretext tasks in self-supervised learning, and its statistic results as shown in Table~\\ref{tab:dataset of lc}.\n\\begin{table*}[htbp]\n\t\\tiny\n\t\\centering\n\t\\caption{The statistics of the datasets for LC. Homogeneous:Hom, Heterogeneous:Het.}\n\t\\label{tab:dataset of lc}\n\t\\resizebox{\\textwidth}{!}{\n\t\\begin{tabular}{ccccccccl}  \n\t\t\\hline \n\t\t\\textbf{Name} & \\textbf{Usage} & \\textbf{Source} & \\textbf{Type} & \\textbf{Nodes} & \\textbf{Edges} & \\textbf{Class} & \\textbf{Features} & \\textbf{Related Paper}  \\\\\n\t\t\\hline\n\t\t\\multirow{2}{*}{Cora} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{2,708} & \\multirow{2}{*}{5,429} & \\multirow{2}{*}{7} & \\multirow{2}{*}{1,433} &    \\\\\n\t\t&  &  &  &  &  &  &  &  \\\\\\hline\n\t\t\\multirow{2}{*}{CiteSeer} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{3,327} & \\multirow{2}{*}{4,732} & \\multirow{2}{*}{6} & \\multirow{2}{*}{3,703} &    \\\\\n\t\t&  &  &  &  &  &  &  &  \\\\\\hline\n\t\t\\multirow{2}{*}{PubMed} & \\multirow{2}{*}{Downstream} & \\multirow{2}{*}{Citation} & \\multirow{2}{*}{Hom} & \\multirow{2}{*}{19K} & \\multirow{2}{*}{44K} & \\multirow{2}{*}{3} & \\multirow{2}{*}{500} &    \\\\\n\t\t&  &  &  &  &  &  &  &  \\\\\\hline\n\t\tML-100K & Downstream & Movie & Hom & 2,625 & 100K & 5 & - &  \\\\\\hline\n\t\tML-1M & Downstream & Movie & Hom  & 9,940 & 1M & 5 & - &  \\\\\\hline\n\t\tBlogCatalog-5K & Downstream & Social & Hom & 5,196 & 172K & 6 & 8,189 &  \\\\\\hline\n\t\tAmazon & Downstream & Social & Het  & 130M & - & - & - &   \\\\\\hline\n\t\tPPI-57K & Downstream & Protein & Het & 57K & 819K & 121 & 50 &  \\\\\\hline\n\t\tFlickr-7K & Downstream & Photo & Hom & 7,575 & 240M & 9 & 12,047 &  \\\\\\hline\n\t\tLast-FM & Downstream & Music & Hom & 15K & 73K & 122 & - &  \\\\\\hline\n\t\tBook-Crossing & Downstream & Book & Hom & 111K & 443K & 52 & - &  \\\\\\hline\n\t\tOAG & Downstream & Academic & Het  & 178M & 2B & - & - &  \\\\\n\t\t\\hline\n\t\\end{tabular}\n\t}\n\\end{table*}\n\\myparagraph{Top-K Recommendation}\nTop-K Recommendation (TKR) is exactly the same as the definition of TKS, the difference lies in the sorting goal.", "cites": [2538, 7357, 8573, 242, 2533, 229, 8563, 7582, 2628, 7103, 1440, 7598, 1439, 240, 2539, 2542], "cite_extract_rate": 0.7619047619047619, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily lists and briefly describes link-level tasks such as Link Classification and Link Prediction without effectively synthesizing the insights from the cited papers. It lacks critical evaluation or comparison of the methods and only provides a surface-level overview of the datasets and tasks. There is minimal abstraction or generalization to broader principles or trends in graph-based self-supervised learning."}}
{"id": "dc734dd2-83d6-429f-86be-0de49bfcbb52", "title": "Graph-Level Tasks", "level": "paragraph", "subsections": [], "parent_id": "2af89c5f-d372-4215-8bba-7f55dacb0c70", "prefix_titles": [["title", "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"], ["section", "Datasets"], ["subsection", "Downstream Tasks and Datasets on Graph"], ["paragraph", "Node-Level Tasks"], ["paragraph", "Graph-Level Tasks"]], "content": "The graph-level task generally focuses on the distribution of nodes, edges, and attributes in a given graph, in order to infer the possible properties of the entire graph.\n\\myparagraph{Graph Classification}\nGraph Classification (GC) is commonly used in social, molecular, and protein graph data, which aims to predict the property of the given community, chemical compound, and protein.\nThe statistic results as shown in Table~\\ref{tab:dataset of gc}.\n\\begin{table*}[htbp]\n\t\\tiny\n\t\\centering\n\t\\caption{The statistics of the datasets for GC. Homogeneous:Hom, Heterogeneous:Het.}\n\t\\label{tab:dataset of gc}\n\t\\resizebox{\\textwidth}{!}{\n\t\\begin{tabular}{ccccccccl}  \n\t\t\\hline \n\t\t\\textbf{Name} & \\textbf{Usage} & \\textbf{Source} & \\textbf{Type} & \\textbf{Graphs} & \\textbf{ Nodes} & \\textbf{Edges} & \\textbf{Class} & \\textbf{Related Paper}  \\\\\n\t\t\\hline\n\t\tZINC15 & Pretraining & Molecule & Hom & 2M & - & - & - &  \\\\\\hline\n\t\tChEMBL & Pretraining & Molecule & Hom & 456K & - & - & - &  \\\\\\hline\n\t\tPPI-pre & Pretraining & Protein & Het & 395K & - & - & - &  \\\\\\hline\n\t\tMUTAG & Downstream & Molecule & Hom & 188 & - & - & 2 &  \\\\\\hline\n\t\tPTC & Downstream & Molecule & Hom & 344 & - & - & 2 &  \\\\\\hline\n\t\tBBBP & Downstream & Molecule & Hom & 2,039 & - & - & 2 &  \\\\\\hline\n\t\tTox21 & Downstream & Molecule & Hom & 7,831 & - & - & 24 &  \\\\\\hline\n\t\tToxCast & Downstream & Molecule & Hom & 8,575 & - & - & 1,234 &  \\\\\\hline\n\t\tSIDER & Downstream & Molecule & Hom & 1,427 & - & - & 54 &  \\\\\\hline\n\t\tClinTox & Downstream & Molecule & Hom & 1,478 & - & - & 4 &  \\\\\\hline\n\t\tMUV & Downstream & Molecule & Hom & 93K & - & - & 34 &  \\\\\\hline\n\t\tHIV & Downstream & Molecule & Hom & 41K & - & - & 2 &  \\\\\\hline\n\t\tBACE & Downstream & Molecule & Hom & 1,513 & - & - & 2 &  \\\\\\hline\n\t\tPPI-88K & Downstream & Protein & Het & 88K & - & - & 80 &  \\\\\\hline\n\t\tIMDB-M & Downstream & Movie & Hom & 1,500 & 19K & 99K & 3 &  \\\\\\hline\n\t\tIMDB-B & Downstream & Movie & Hom & 1,000 & 19K & 97K & 2 &  \\\\\\hline\n\t\tFreeSolv & Downstream & Molecule & Hom & 642 & - & - & - &  \\\\\\hline\n\t\tESOL & Downstream & Molecule & Hom & 1,128 & - & - & - &  \\\\\\hline\n\t\tLipophilicity & Downstream & Molecule & Hom & 4,200 & - & - & - &  \\\\\\hline\n\t\tQM7 & Downstream & Molecule & Hom & 6,830 & - & - & - &  \\\\\\hline\n\t\tQM8 & Downstream & Molecule & Hom & 22K & - & - & - &  \\\\\\hline\n\t\tCOLLAB & Downstream & Co-author & Hom & 5,000 & 373K & - & 3 &  \\\\\\hline\n\t\tRDT-B & Downstream & Co-author & Hom & 2,000 & 859K & - & 2 &  \\\\\\hline\n\t\tRDT-M & Downstream & Co-author & Hom & 5,000 & 3M & - & 5 &  \\\\\\hline\n\t\tNCI1 & Downstream & Molecule & Hom & 4,110 & 123K & 132K & 2 &  \\\\\\hline\n\t\tNCI109 & Downstream & Molecule & Hom & 4,127 & 123K & 133K & 2 &  \\\\\\hline\n\t\tPROTEINS & Downstream & Molecule & Hom & 1,113 & 44K & 81K & 2 &  \\\\\\hline\n\t\tD$\\&$D & Downstream & Molecule & Hom & 1,178 & 335K & 843K & 2 &  \\\\\\hline\n\t\tMutagenicity & Downstream & Molecule & Hom & 4,337 & 131K & 134K & 2 &  \\\\\\hline\n\t\tMETR-LA & Downstream & Traffic & Hom & 1 & 207 & - & - &  \\\\\t\n\t\t\\hline\n\t\\end{tabular}\n\t}\n\\end{table*}", "cites": [2631, 2544, 2543, 7335, 2532, 2541, 2630, 2533, 2534, 2629, 1184, 2632, 8573], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section primarily presents a list of datasets and their statistics for graph-level tasks, with minimal synthesis or discussion of the methods used in the cited papers. There is no critical evaluation of the approaches or their limitations, and the analysis remains largely at a surface level without abstracting broader trends or principles in graph-level representation learning."}}
