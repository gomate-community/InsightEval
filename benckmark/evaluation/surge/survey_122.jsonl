{"id": "db1f4cf7-1d58-49ec-b135-7e260c7fb808", "title": "Introduction", "level": "section", "subsections": ["a35757a0-9173-4c32-83b7-39d8a998db89", "100ff0d0-97f6-4d36-903a-6350d5ac0a70"], "parent_id": "81a424eb-ce19-4d20-b1e1-67e3f071c9db", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Introduction"]], "content": "Advancements in machine learning (ML) have been one of the most significant innovations of the last decade. \nAmong different ML models, Deep Neural Networks (DNNs)~ are well-known and widely used for their powerful representation learning from high-dimensional data such as images, texts, and speech. \nHowever, as ML algorithms enter sensitive real-world domains with trustworthiness, safety, and fairness prerequisites, the need for corresponding techniques and metrics for high-stake domains is more noticeable than before. \nHence, researchers in different fields propose guidelines for \\textit{Trustworthy AI}~, \\textit{Safe AI} , and \\textit{Explainable AI}~ as stepping stones for next generation Responsible AI . \nFurthermore, government reports and regulations on AI accountability~, trustworthiness , and safety  are gradually creating mandating laws to protect citizens' data privacy rights, fair data processing, and upholding safety for AI-based products.\nThe development and deployment of ML algorithms for open-world tasks come with reliability and dependability challenges rooting from model performance, robustness, and uncertainty limitations .  \nUnlike traditional code-based software, ML models have fundamental safety drawbacks, including performance limitations on their training set and run-time robustness constraints in their operational domain.  \nFor example, ML models are fragile to unprecedented domain shift  that could easily occur in open-world scenarios. \nData corruptions and natural perturbations~ are other factors affecting ML models. \nMoreover, from the security perspective, it has been shown that DNNs are susceptible to adversarial attacks that make small perturbations to the input sample (indistinguishable by the human eye) but can fool a DNN~. \nDue to the lack of verification techniques for DNNs, validation of ML models is often bounded to performance measures on standardized test sets and end-to-end simulations on the operation design domain. \nRealizing that dependable ML models are required to achieve safety, we observe the need to investigate gaps and opportunities between conventional engineering safety standards and a set of ML safety-related techniques. \n\\begin{figure*}\n\\vspace{-0.5em}\n    \\centering\n    \\includegraphics[width=0.99\\columnwidth]{figures/roadmap-3.PNG}\n    \\vspace{-0.6em}\n    \\caption{Paper Roadmap: we first identify key engineering safety requirements (first column) that are limited or not readily applicable on complex ML algorithms (second column). From there, we present a review of safety-related ML research followed by their categorization (third column) into three strategies to achieve \\textit{(1) Inherently Safe Models}, improving \\textit{(2) Enhancing Model Performance and Robustness}, and incorporate \\textit{(3) Run-time Error Detection} techniques.} \n    \\label{fig:roadmap}\n    \\vspace{-1em}\n\\end{figure*}\n\\vspace{-0.5em}", "cites": [6976, 166, 1605, 1606, 1356, 1604, 892], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers to construct a narrative around ML safety in open-world settings, integrating topics like explainability, robustness, and adversarial attacks. It provides a critical perspective by highlighting limitations of current approaches (e.g., lack of verification techniques for DNNs) and connecting them to broader safety challenges. The abstraction level is strong, as it frames the discussion around overarching safety strategies and principles rather than specific systems."}}
{"id": "a35757a0-9173-4c32-83b7-39d8a998db89", "title": "Scope, Organization, and Survey Method", "level": "subsection", "subsections": [], "parent_id": "db1f4cf7-1d58-49ec-b135-7e260c7fb808", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Introduction"], ["subsection", "Scope, Organization, and Survey Method"]], "content": "ML safety includes diverse hardware and software techniques for the safe execution of algorithms in open-world applications . \nIn this paper, we limit our scope to only ML algorithm design and not the execution of those algorithms in platforms. \nWith that being said, we also mainly focus on ``in situ'' techniques to improve run-time dependability and not on further techniques for the efficiency of the network or training. \nWe used a structured and iterative methodology to find ML safety-related papers and categorize these research as summarized in Table \\ref{tab:ref-table}. \nIn our iterative paper selection process, we started with reviewing key research papers from AI and ML safety (e.g., ) and software safety literature and standards (e.g., ) to identify mutual safety attributes between engineering safety and ML techniques. \nNext, we conducted an upward and downward literature investigation using top computer science conference proceedings, journal publications, and the \\textit{Google Scholar} search engine to maintain reasonable literature coverage and balance the number of papers on each ML safety attribute.\nFigure \\ref{fig:roadmap} presents the overall organization of this paper. \nWe first review the background on common safety terminologies and situate ML safety limitations with reference to conventional engineering safety requirements in Section \\ref{sec:background}. \nIn Section \\ref{sec:error-types} we discuss a unified ``big picture'' of different ML error types for real-world applications and common benchmark datasets to evaluate models for these errors. \nNext, we propose a ML safety taxonomy in Section \\ref{sec:categorization} to organize ML techniques into safety strategies with Table \\ref{tab:ref-table} as an illustration of the taxonomy with the summary of representative papers on each subcategory. \nSections \\ref{sec:safe-design}, \\ref{sec:robustness-performance}, and \\ref{sec:error-detection} construct the main body of the reviewed papers organized into ML solutions and techniques for each safety strategy. \nFinally, Section \\ref{sec:discussion} presents a summary of key takeaways and a discussion of open problems and research directions for ML safety.\n\\vspace{-0.5em}", "cites": [1609, 1356, 1608, 1607], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section outlines the scope, organization, and methodology of the survey in a clear and structured manner. It references the cited papers to support its position, but does so primarily to justify the selection of ML safety-related topics rather than to synthesize or critically evaluate their contributions. There is minimal analysis of how the papers relate to each other or to broader themes in ML safety."}}
{"id": "4a4802ac-4ed5-4b71-8adf-dfa52b54866a", "title": "Related Surveys", "level": "subsection", "subsections": [], "parent_id": "2dbfe4c5-7909-4b49-8773-ed3edc158c92", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Background"], ["subsection", "Related Surveys"]], "content": "\\label{sec:terminology}\nRelated survey papers dive into ML and AI safety topics to analyze problem domain, review existing solutions and make suggestions on future directions .\nSurvey papers cover diverse topics including safety-relevant characteristics in reinforcement learning~, verification of ML components~, adversarial robustness , anomaly detection , ML uncertainty , and aim to connect the relation between well-established engineering safety principals and ML safety  limitations.\n introduce four major research problems to improve ML safety namely robustness, monitoring, alignment, and external safety for ML models.\n presents high level guidelines for teams, organizations, and industries to increase the reliability, safety, and trustworthiness of next generation Human-Centered AI systems.\nMore recently, multiple surveys present holistic review of ML promises and pitfalls for safety-critical autonomous systems.\nFor instance,  demonstrate a systematic presentation of 4-stage ML lifecycle, including data management, model training, model verification, and deployment. \nAuthors present itemized safety assurance requirements for each stage and review methods that support each requirement.\nIn a later work,  add ML safety assurance scoping and the safety requirements elicitation stages to ML lifecycle to establish the fundamental link between system-level hazard and risk analysis and unit-level safety requirements. \nIn a broader context,  study challenges and limitations of existing ML system development tools and platforms (MLOps) in achieving \\textit{Responsible AI} principles such as data privacy, transparency, and safety. \nAuthors report their findings with a list of operationalized Responsible AI principles and their benefits and drawbacks.\nAlthough prior work targeted different aspects and characteristics of ML safety and dependability, in this paper, we elaborate on  ML safety concept by situating open-world safety challenges with ongoing ML research.\nParticularly, we combine ML safety concerns between engineering and research communities to uncover mutual goals and accelerate safety developments.\n\\vspace{-0.3em}", "cites": [1605, 1614, 1356, 1612, 1615, 1608, 1610, 1613, 1607, 1611], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple related surveys to construct a narrative on ML safety, highlighting evolving aspects like the ML lifecycle and alignment with engineering safety principles. It critically notes that prior work has targeted different aspects of ML safety, but lacks detailed evaluation of their effectiveness or limitations. The abstraction level is strong as it identifies broader themes such as open-world challenges, lifecycle integration, and Responsible AI, setting the stage for the proposed taxonomy."}}
{"id": "695446ed-62d6-49c2-a600-711627e36e58", "title": "Engineering Safety Limitations in ML", "level": "subsection", "subsections": ["3d87c36f-64f8-4c05-85b0-adff49363df6", "7d7b4464-79b0-4c2d-92ca-eaf1875e72a2", "8fc22a2d-6cc6-4338-8dec-b82314bc214e", "528565de-9cae-43e4-afbf-da8fc4a82f0c", "9f2878be-f51f-4687-9bfa-175b3443c0bf"], "parent_id": "2dbfe4c5-7909-4b49-8773-ed3edc158c92", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Background"], ["subsection", "Engineering Safety Limitations in ML"]], "content": "Engineering safety broadly refers to the management of operations and events in a system in order to protect its users by minimizing hazards, risks, and accidents.\nGiven the importance of dependability of the system's internal components (hardware and software), various engineering safety standards have been developed to ensure the system's functional safety based on two fundamental principles of safety life cycle and failure analysis. \nBuilt on collection of best practices, engineering safety processes discover and eliminate design errors followed by a probabilistic analysis of safety impact of possible system failures (i.e., failure analysis). \nSeveral efforts attempted to extend engineering safety standards to ML algorithms .\nFor example, European Union Aviation Safety Agency released a report on concepts of design assurance for neural networks~ that introduces safety assurance and assessment for learning algorithms in safety-critical applications. \nIn another work, Siebert et al.~ present a guideline to assess ML system quality from different aspects specific to ML algorithms including data, model, environment, system, and infrastructure in an industrial use case. \nHowever, the main body of engineering standards do not account for the statistical nature of ML algorithms and errors occurring due to the inability of the components to comprehend the environment. \nIn a recent review of automotive functional safety for ML-based software, Salay et al.~ present an analysis that shows about 40\\% of software safety methods do not apply to ML models. \nGiven the dependability limitations of ML algorithms and lack of adaptability for traditional software development standards, we identify 5 open safety challenges for ML and briefly review active research topics for closing these safety gaps in the following. \nWe extensively review the techniques for each challenge later in Sections \\ref{sec:safe-design}, \\ref{sec:robustness-performance}, and \\ref{sec:error-detection}.\n\\vspace{-0.3em}", "cites": [1609, 1616], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes two cited works to highlight the incompatibility between traditional engineering safety standards and ML algorithms, particularly in uncontrolled environments. It critically identifies the limitations of these standards and abstracts the issue into a broader challenge—40% of software safety methods do not apply to ML. This sets the stage for the proposed taxonomy and research directions, offering a meta-level view of the problem space."}}
{"id": "3d87c36f-64f8-4c05-85b0-adff49363df6", "title": "Design Specification", "level": "subsubsection", "subsections": [], "parent_id": "695446ed-62d6-49c2-a600-711627e36e58", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Background"], ["subsection", "Engineering Safety Limitations in ML"], ["subsubsection", "Design Specification"]], "content": "Documenting and reviewing the software specification is a crucial step in engineering safety; however, formal design specification of ML models is generally not feasible, as the models learn patterns from large training sets to discriminate (or generate) their distributions for new unseen input. \nTherefore, ML algorithms learn the target classes through their training data (and regularization constraints) rather than formal specification. \nThe lack of specifiability could cause a mismatch between ``designer objectives'' and ``what the model actually learned'', which could result in unintended functionality of the system. \nThe data-driven optimization of model variables in ML training makes it challenging to define and pose specific safety constraints. \nSeshia et al.~ surveyed the landscape of formal specification for DNNs to lay an initial foundation for formalizing and reasoning about properties of DNNs. \nTo fill this gap, a common practice is to achieve partial design specification through training data specification and coverage.\nAnother practical way to overcome the design specification problem is to break ML components into smaller algorithms (with smaller tasks) to work in a hierarchical structure.\nIn the case of intelligent agents, safety-enforcing regularization terms , and simulation environments  are suggested to specify and verify training goals for the agent. \n\\vspace{-0.3em}", "cites": [1346, 1356], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section analytically addresses the challenge of design specification in ML from an engineering safety perspective, integrating concepts from the cited papers with broader observations. It synthesizes the general issue of ML model specifiability and connects it with partial solutions like training data specification and hierarchical decomposition, but does not deeply weave insights from both papers into a novel narrative. Critical analysis is limited, as the section primarily describes limitations rather than evaluating specific approaches in the cited works. The abstraction level is moderate, as it identifies a general pattern in the ML safety challenge and suggests overarching strategies."}}
{"id": "7d7b4464-79b0-4c2d-92ca-eaf1875e72a2", "title": "Implementation Transparency", "level": "subsubsection", "subsections": [], "parent_id": "695446ed-62d6-49c2-a600-711627e36e58", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Background"], ["subsection", "Engineering Safety Limitations in ML"], ["subsubsection", "Implementation Transparency"]], "content": "Implementation transparency is an important requirement in engineering safety which gives the ability to trace back design requirements from the implementations. \nHowever, advanced ML models trained on high-dimensional data are not transparent. \nThe very large number of variables in the models makes them incomprehensible or a so-called black-box for design review and inspection. \nIn order to achieve traceability, significant research has been performed on interpretability methods for DNN to provide instance explanations of model prediction and DNN intermediate feature layers~.\nIn autonomous vehicles application,  propose VisualBackProp technique and show that a DNN algorithm trained to control a steering wheel would in fact learn patterns of lanes, road edges, and parked vehicles to execute the targeted task. \nHowever, the completeness of interpretability methods to grant traceability is not proven yet~, and in practice, interpretability techniques are mainly used by designers to improve network structure and training process rather than support a safety assessment. \n\\vspace{-0.3em}", "cites": [1617, 499], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates the concept of implementation transparency in engineering safety with ML interpretability techniques, referencing two papers that explore visualization and evaluation methods for DNNs. It identifies a gap between interpretability methods and the requirements for safety traceability, showing some critical analysis. However, the synthesis is limited to two papers, and the abstraction remains at a relatively basic level without fully generalizing to a broader framework of ML safety."}}
{"id": "8fc22a2d-6cc6-4338-8dec-b82314bc214e", "title": "Testing and Verification.", "level": "subsubsection", "subsections": [], "parent_id": "695446ed-62d6-49c2-a600-711627e36e58", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Background"], ["subsection", "Engineering Safety Limitations in ML"], ["subsubsection", "Testing and Verification."]], "content": "Design and implementation verification is another demanding requirement for unit testing to meet engineering safety standards. \nFor example, coding guidelines for software safety enforce the elimination of dead or unreachable functions.\nDepending upon the safety integrity level, complete statement, branch coverage, or modified condition and decision coverage are required to confirm the adequacy of the unit tests. \nComing to DNNs, formally verifying their correctness is challenging and in fact provably an NP-hard~ problem due to the high dimensionality of the data. \nTherefore, reaching complete testing and verification of the operational design domain is not feasible for domains like image and video. \nAs a result, researchers proposed new techniques such as searching for unknown-unknowns~ and predictor-verifier training~, and simulation-based toolkits  guided by formal models and specifications. \nOther techniques, including neuron coverage and fuzz testing  in neural networks incorporate these aspects. \nNote that formal verification of shallow and linear models for low dimensional sensor data does not carry verification challenges of the image domain. \n\\vspace{-0.3em}", "cites": [1614, 1618, 1619], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the cited papers by connecting traditional software verification standards with the unique challenges of DNNs, highlighting the infeasibility of full verification in high-dimensional domains. It critically notes the differences in verification difficulty between image domains and low-dimensional models. The abstraction is strong as it frames these issues within the broader context of testing and verification in ML safety, contributing to a structured understanding of the field."}}
{"id": "99c3df0a-d1fc-465c-8107-46ba7f6eadd7", "title": "Generalization Error", "level": "subsection", "subsections": ["4f251806-d885-4c8d-9398-8818922bc1b6"], "parent_id": "5ab20adb-f657-4a79-9b60-59d62e0df59b", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Generalization Error"]], "content": "\\label{sec:generalization_error}\nThe first and foremost goal of machine learning is to minimize the \\emph{generalization error}.\nGiven a hypothesis $h$ (e.g., a DNN with learned parameters), the generalization error (also know as the \\emph{true error} and denoted as $R(h)$) is defined as the expected error of $h$ on the data distribution $\\mathcal{D}$~:\n$\n    R(h) = \\text{Pr}_{(x,y) \\sim \\mathcal{D}}[h(x)\\neq y] = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\mathbbm{1}_{h(x)\\neq y}],\n$\nwhere $(x,y)$ is a pair of data and label sampled from $\\mathcal{D}$, and $\\mathbbm{1}$ is the indicator function. \nThe generalization error is not directly computable, since $\\mathcal{D}$ is usually unknown. \nThe \\textit {de facto} practical solution is to learn $h$ by empirical risk minimization (ERM) on the training set $\\mathbb{S}_{S}=\\{(x_i,y_i)\\}_{i=1}^{N_S}$ and estimate its generalization error by the \\emph{empirical error} on the holdout test set $\\mathbb{S}_{T}=\\{(x_i,y_i)\\}_{i=1}^{N_T}$. \nFormally, the empirical error $\\hat{R}(h)$ is defined as the mean error on a finite set of data points $\\mathbb{S} \\sim \\mathcal{D}^{m}$~:\n$\n    \\hat{R}(h) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbbm{1}_{h(x_i)\\neq y_i},\n$\nwhere $\\mathbb{S} \\sim \\mathcal{D}^{m}$ means $\\mathbb{S}=\\{(x_i,y_i)\\}_{i=1}^m \\overset{i.i.d.}{\\sim} \\mathcal{D}$.\nThe training and test sets are all sampled from the same distribution $\\mathcal{D}$ but are disjoint.\nRecent years have witnessed the successful application of this holdout evaluation methodology to monitoring the progress of many  ML fields, especially where large-scale labeled datasets are available. \nThe generalization error can be affected by many factors, such as training set quality (e.g., imbalanced class distribution , noisy labels ), model capacity, and the training methods (e.g., using pre-training , regularization , etc.)", "cites": [1620, 1621], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a standard definition of generalization error and mentions some techniques that can influence it, but it lacks synthesis of the cited papers into a broader narrative or framework. It also does not critically evaluate the cited works, nor does it compare or contrast their approaches. While it hints at abstraction by listing factors affecting generalization, it does not develop deeper meta-level insights."}}
{"id": "4f251806-d885-4c8d-9398-8818922bc1b6", "title": "Benchmark Datasets:", "level": "paragraph", "subsections": [], "parent_id": "99c3df0a-d1fc-465c-8107-46ba7f6eadd7", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Generalization Error"], ["paragraph", "Benchmark Datasets:"]], "content": "Model generalization is commonly evaluated on a separate test set provided for the dataset.\nHowever, recent research has found limitations of such evaluation strategy. \nFor example,  showed that the fixed ImageNet   test set is not sufficient to reliably evaluate the generalization ability of state-of-the-art image classifiers, due to the insufficiency in representing the rich visual open-world.\n observed that noisy data collection pipeline can lead to a systematic misalignment between the training sets and the real-world tasks. \n\\vspace{-0.5em}", "cites": [1623, 1622], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section analytically addresses the limitations of using fixed test sets for evaluating ML generalization, drawing on two specific papers to highlight the issues with benchmarks like ImageNet. It connects the idea of noisy data collection to the misalignment between training and real-world tasks, showing some synthesis. However, it lacks deeper comparison or a broader meta-level abstraction, and the critical analysis is limited to identifying problems rather than offering a nuanced evaluation."}}
{"id": "1fce065f-a3de-4536-a770-2283500a55da", "title": "Robustness against Distributional Shifts", "level": "subsection", "subsections": ["51c360c4-dd04-4b3d-bb90-115509be7968"], "parent_id": "5ab20adb-f657-4a79-9b60-59d62e0df59b", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Distributional Shifts"]], "content": "\\label{sec:distributional_shift}\nWhen measuring generalization error, we assume the source training set $\\mathbb{S}_S$ and the target test set $\\mathbb{S}_T$ are from the same data distribution. \nHowever, this assumption is frequently violated in real world applications. In other words, we will have $p_S(x,y) \\neq p_T(x,y)$ where $p_S(x,y)$ and $p_T(x,y)$ are the joint probability density distributions of data $x$ and label $y$ on the training and test distributions, respectively.\nSuch mismatch between training and test data distribution is known as \\textit{dataset shift}~ (also termed as \\textit{distributional shift} or \\textit{domain shift} in literature).\nIn this paragraph, we describe two most common distributional shifts and their benchmark datasets. \n\\textit{Covariate shift} means that the training and test distributions of the input covariate $x$ are different (i.e., $p_S(x) \\neq p_T(x)$), while the labeling function remains the same (i.e., $p_S(y|x) = p_T(y|x)$). \nCovariate shift may occur due to natural perturbations (e.g., weather and lighting changes), data changes over time (e.g., seasonal variations of data), and even more subtle digital corruptions on images (e.g., JPEG compression and low saturation). \n\\textit{Label distribution shift} (also know as \\textit{prior probability shift}) is the scenario when the marginal distributions of $y$ changes while the class-conditional distribution remains the same. Formally, it is defined as $p_S(y) \\neq p_T(y), p_S(x|y) = p_T(x|y)$.  \nPrior probability shift is typically concerned in applications where the label $y$ is the casual variable (e.g., pneumonia) for the observed feature $x$ (e.g., chest X-ray) . For example, if we trained a pneumonia predictor using the chest X-ray data collected in summer (when $p(y)$ is low), we may still require it to be accurate on patients visiting in winter (when $p(y)$ is high).\nA special case for label distributional shift is long-tailed recognition , where the training set is long-tail distributed (i.e., $p_S(y)$ follows a long-tailed distribution) while the test set is balanced (i.e., $p_T(y)$ roughly follows a uniform distribution).\nBeyond the above reasonably foreseeable domain shifts, a model may also encounter out-of-distribution (OOD) test samples with semantic contents unseen in the training distribution. \nFor example, for a classifier trained on hand-written digit images, a Roman character is an OOD test sample (while a printed digit is a test sample with domain gap).\n\\textit{OOD detection}  is the approach to detect such OOD samples, whose predictions will be rejected (see Section \\ref{sec:ood-detection} for details).", "cites": [1626, 1625, 1624], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of different types of distributional shifts and references relevant papers, but it lacks deeper synthesis or critical evaluation of the cited works. The connections between the papers are minimal and mostly serve to illustrate specific concepts rather than contribute to a novel or integrated understanding. There is some abstraction in distinguishing between covariate shift, label shift, and OOD detection, but the analysis remains at a surface level without identifying broader trends or limitations."}}
{"id": "51c360c4-dd04-4b3d-bb90-115509be7968", "title": "Benchmark Datasets for Covariate Shift:", "level": "paragraph", "subsections": ["c2612af4-5ff3-4213-a3f7-f69a4283f2a7", "5a48c2d9-5e39-4f7f-904d-afb8149d4bbe"], "parent_id": "1fce065f-a3de-4536-a770-2283500a55da", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Distributional Shifts"], ["paragraph", "Benchmark Datasets for Covariate Shift:"]], "content": "Several variants of the ImageNet dataset have been introduced to benchmark robustness against distributional shifts of models trained on the original ImageNet dataset.\n introduce two variants of the original ImageNet validation set: ImageNet-C benchmark for input corruption robustness and the ImageNet-P dataset for input perturbation robustness. \n present ImageNet-A which contains real-world unmodified samples that falsify state-of-the-art image classifiers. \n present a series of benchmarks for measuring model robustness to variations on image renditions (ImageNet-R benchmark), imaging time and geographic location (StreetView benchmark), and objects size, occlusion, camera viewpoint, and zoom (DeepFashion Remixed benchmark). \n collected ImageNet-V2 using the same data source and collection pipeline as the original ImageNet paper .\nThis new benchmark leads to the observation that the prediction accuracy of even the best image classifiers are still highly sensitive to minutiae of the test set distribution and extensive hyperparameter tuning. \nShifts  is a very recently published benchmark dataset for distributional shifts beyond the computer vision tasks.", "cites": [8473, 1606, 7058, 7493, 1627], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily lists various benchmark datasets for covariate shift without synthesizing their contributions or establishing a coherent narrative. It lacks critical evaluation of the papers' methodologies or limitations and offers minimal abstraction beyond the specific datasets described."}}
{"id": "c2612af4-5ff3-4213-a3f7-f69a4283f2a7", "title": "Benchmark Datasets for Label Distribution Shift:", "level": "paragraph", "subsections": [], "parent_id": "51c360c4-dd04-4b3d-bb90-115509be7968", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Distributional Shifts"], ["paragraph", "Benchmark Datasets for Covariate Shift:"], ["paragraph", "Benchmark Datasets for Label Distribution Shift:"]], "content": "Synthetic label distribution shift is commonly used for evaluation : the test samples are manually sampled according to a predefined target label distribution $p_T(y)$ which is different from the source label distribution $p_S(y)$.\nRecently,  constructed a real-world label distribution shift dataset for the paper category prediction task. Specifically, the authors collected papers from 23 categories on arXiv and extracted the tf-idf vector from the abstract as the features.", "cites": [1628, 1626], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief factual description of label distribution shift and references two specific papers. However, it lacks synthesis by not connecting their methods or goals, offers minimal critical analysis, and provides little abstraction beyond the specific examples mentioned."}}
{"id": "5a48c2d9-5e39-4f7f-904d-afb8149d4bbe", "title": "Benchmark Datasets for OOD Detection:", "level": "paragraph", "subsections": [], "parent_id": "51c360c4-dd04-4b3d-bb90-115509be7968", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Distributional Shifts"], ["paragraph", "Benchmark Datasets for Covariate Shift:"], ["paragraph", "Benchmark Datasets for OOD Detection:"]], "content": "The semantically coherent OOD (SC-OOD) benchmarks  are the latest ones on CIFAR10 and CIFAR100, which fixed some sever defects of previous designs.\nImageNet-O , containing 2000 images from 200 classes within ImageNet-22k and outside ImageNet, is a commonly used for ImageNet.\nHendrycks et al.  presented three large-scale and high-resolution OOD detection benchmarks for multi-class and multi-label image classification, object detection, and semantic segmentation, respectively. \nThe recent ``SegmentMeIfYouCan'' benchmark  has two tasks: anomalous object segmentation and  road obstacle segmentation. \n\\vspace{-0.5em}", "cites": [8474, 1629, 1630, 7058], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a list of benchmark datasets for out-of-distribution detection, mentioning the goals and characteristics of each. It integrates minimal context between the cited works, such as noting the scale or task focus (e.g., multi-class vs. segmentation), but lacks deeper synthesis or a novel framework. There is little critical analysis of the limitations or comparative strengths of the benchmarks, and abstraction is limited to basic observations about their use cases."}}
{"id": "4b004293-0e23-42b6-b373-82d9f6603af1", "title": "Robustness against Adversarial Attacks", "level": "subsection", "subsections": ["0deede3d-fca4-4ba3-9be6-c9ef51953225"], "parent_id": "5ab20adb-f657-4a79-9b60-59d62e0df59b", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Adversarial Attacks"]], "content": "\\label{sec:adversarial_attack}\nAdversarial attacks add synthetic perturbations (termed as adversarial perturbations) onto the original clean sample. \nThe perturbed sample, known as adversarial sample, is intended to cause wrong predictions on machine learning models, while keeping the identical semantic meaning of the clean sample. \nDifferent forms of adversarial perturbations have been studied on different types of data.\nOn image data, typical forms include the $\\ell_p$ constrained additive perturbation , spatial perturbation~, and semantically meaningful perturbation~. \nBeyond the image data, adversarial attacks can also be designed, such as by altering the shape of 3D surfaces , by replacing words with synonyms  or rephrasing the sentence  in natural language data, by applying adversarial printable 2D patches on real-world physical objects , etc.", "cites": [910, 1631, 892, 904, 1632], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of adversarial attacks and their variations across data types, referencing several cited papers. However, it lacks deeper synthesis of these works, does not compare or critically evaluate the approaches, and offers little abstraction beyond the specific examples mentioned. The narrative remains primarily descriptive rather than analytical or insightful."}}
{"id": "0deede3d-fca4-4ba3-9be6-c9ef51953225", "title": "Benchmark Datasets:", "level": "paragraph", "subsections": [], "parent_id": "4b004293-0e23-42b6-b373-82d9f6603af1", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Robustness against Adversarial Attacks"], ["paragraph", "Benchmark Datasets:"]], "content": "Robustness against adversarial attacks (also known as adversarial robustness) is usually evaluated as the empirical performance (e.g., accuracy in image classification tasks) on the adversarial samples. \nThe key requirement for a faithful evaluation is that the attackers should try their best to break the model. \nThere are two commonly used strategies to achieve this goal.\nFirst, an ensemble of multiple strong and diverse adversarial attacks should be simultaneously used for evaluation. \nA typical setting for such ensemble is the AutoAttack , which consists of four state-of-the-art white-box attacks and two state-of-the-art black-box attacks. \nAnother setting is used to create evaluation benchmarks of ImageNet-UA and CIFAR-10-UA , which contains both $\\ell_p$ constrained adversarial attacks and real-world adversarial attacks such as worst-case image fogging. \nSecond, the attacks should be carefully designed to prevent the ``gradient obfuscation'' effect .\nSince the success of traditional white-box attacks depends on accurate calculation of model gradient, they may fail if the model gradient is not easily accessible (e.g., the model has non-differential operations). \nAs a result, evaluating model robustness on such attacks may provide a false sense of robustness. \n proposed three enhancement for traditional white-box attacks as solutions for common causes of gradient obfuscation.\nOther solutions include designing adaptive attacks for each specific defense strategy .\n\\vspace{-0.5em}", "cites": [1633, 7059, 912, 888, 1634], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple papers to build a coherent narrative on evaluating adversarial robustness, emphasizing the need for diverse attack ensembles and addressing gradient obfuscation. It critically discusses limitations of traditional evaluation methods and proposes solutions from the cited works. The abstraction level is high as it identifies general principles for reliable benchmarking rather than just describing individual datasets or attacks."}}
{"id": "be8a4443-5ba9-4476-bf69-4db3f1947256", "title": "Generalization Error", "level": "subsection", "subsections": ["35556dc3-956e-446b-86ea-9a4aba6a9db7"], "parent_id": "7165617c-b651-4c8c-bb23-2d8f417c79d3", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Generalization Error"]], "content": "\\label{sec:generalization_error}\nThe first and foremost goal of machine learning is to minimize the \\emph{Generalization Error}.\nGiven a hypothesis $h$ (e.g., a model with learned parameters), the generalization error (also know as the \\emph{true error} and denoted as $R(h)$) is defined as the expected error of $h$ on the data distribution $\\mathcal{D}$~:\n$\n    R(h) = \\text{Pr}_{(x,y) \\sim \\mathcal{D}}[h(x)\\neq y] = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\mathbbm{1}_{h(x)\\neq y}],\n$\nwhere $(x,y)$ is a pair of data and labels sampled from $\\mathcal{D}$, and $\\mathbbm{1}$ is the indicator function. \nHowever, the generalization error is not directly computable since $\\mathcal{D}$ is usually unknown. \nThe \\textit {de facto} practical solution is to learn $h$ by empirical risk minimization (ERM) on the training set $\\mathbb{S}_{S}=\\{(x_i,y_i)\\}_{i=1}^{N_S}$ and then estimate its generalization error by the \\emph{empirical error} on the holdout test set $\\mathbb{S}_{T}=\\{(x_i,y_i)\\}_{i=1}^{N_T}$. \nFormally, the empirical error $\\hat{R}(h)$ is defined as the mean error on a finite set of data points $\\mathbb{S} \\sim \\mathcal{D}^{m}$~:\n$\n    \\hat{R}(h) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbbm{1}_{h(x_i)\\neq y_i},\n$\nwhere $\\mathbb{S} \\sim \\mathcal{D}^{m}$ means $\\mathbb{S}=\\{(x_i,y_i)\\}_{i=1}^m \\overset{i.i.d.}{\\sim} \\mathcal{D}$.\nThe training and test sets are all sampled from the same distribution $\\mathcal{D}$ but are disjoint.\nRecent years have witnessed the successful application of this holdout evaluation methodology to monitoring the progress of many ML fields, especially where large-scale labeled datasets are available. \nThe generalization error can be affected by many factors, such as training set quality (e.g., imbalanced class distribution , noisy labels ), model learning capacity, and training method (e.g., using pre-training  or regularization ).\n\\vspace{-0.5em}", "cites": [1620, 1621], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual definition of generalization error and its estimation via empirical risk minimization, but it does not meaningfully synthesize or integrate the cited papers. It mentions factors affecting generalization error and references pre-training and regularization, yet does not elaborate on how the cited works contribute to these topics. There is minimal critical analysis or abstraction to broader ML safety principles."}}
{"id": "35556dc3-956e-446b-86ea-9a4aba6a9db7", "title": "Benchmark Datasets:", "level": "paragraph", "subsections": [], "parent_id": "be8a4443-5ba9-4476-bf69-4db3f1947256", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Generalization Error"], ["paragraph", "Benchmark Datasets:"]], "content": "Model generalization is commonly evaluated on a separate i.i.d test set provided for the dataset. \nHowever, recent research has found limitations of this evaluation strategy. \nFor example,  showed that the fixed ImageNet   test set is not sufficient to reliably evaluate the generalization ability of state-of-the-art image classifiers due to the insufficiency in representing the rich visual open-world.\nIn another work,  observed that a noisy data collection pipeline could lead to a systematic misalignment between the training sets and the real-world tasks. \n\\vspace{-0.6em}", "cites": [1623, 1622], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two papers to highlight limitations in using fixed i.i.d test sets for evaluating generalization, particularly in open-world settings. It identifies a common issue—misalignment between training data and real-world tasks—but does not integrate the ideas into a novel framework. The critique is focused and points to specific limitations in data collection and evaluation practices, though deeper analysis or broader generalization is limited."}}
{"id": "d0f4826a-d2b9-4bb3-b60d-6be6a87ea56f", "title": "Label Distribution Shift", "level": "paragraph", "subsections": [], "parent_id": "afe3ecb8-ce9a-4d47-b542-819aee8be1f0", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Distributional Error"], ["paragraph", "Covariate Shift"], ["paragraph", "Label Distribution Shift"]], "content": "is the scenario when the marginal distributions of $y$ changes while the class-conditional distribution remains the same. \nLabel distribution shift is also know as \\textit{prior probability shift} and formally defined as $p_S(y) \\neq p_T(y), p_S(x|y) = p_T(x|y)$.  \nLabel distribution shift is typically concerned in applications where the label $y$ is the casual variable for the observed feature $x$ . \nFor example, a trained model to predict pneumonia (i.e., label $y$) using chest X-ray data (i.e., features $x$) that was collected during summer time (when $p(y)$ is low), should still require it to be accurate on patients (i.e., new inputs) visiting in winter time (when $p(y)$ is high) regardless of label distribution shift. \nLong-tailed distribution  is a special case for label distributional shift where the training set $p_S(y)$ follows a long-tailed distribution but the test set is balanced (i.e., $p_T(y)$ roughly follows a uniform distribution).\n\\vspace{-0.6em}", "cites": [1626, 1625], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concepts from both cited papers by defining label distribution shift and linking it to the broader idea of long-tailed distributions. It abstracts the discussion by identifying label distribution shift as a special case, which helps in understanding the general problem. However, it lacks deeper critical analysis of the approaches or limitations mentioned in the papers, instead offering a conceptual overview."}}
{"id": "d00802b4-7548-4c7e-9a67-5a411a43d078", "title": "Out-of-Distribution Samples", "level": "paragraph", "subsections": [], "parent_id": "afe3ecb8-ce9a-4d47-b542-819aee8be1f0", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Distributional Error"], ["paragraph", "Covariate Shift"], ["paragraph", "Out-of-Distribution Samples"]], "content": "are test time inputs that are outliers to the training set without any semantic content shared with the training distribution, which is considered beyond reasonably foreseeable domain shifts. \nFor example, given a model trained to recognize handwritten characters in English, a Roman character with a completely disjoint label space is an Out-of-Distribution (OOD) test sample. \nOOD detection  is a common approach to detect such outlier samples, whose predictions should be abstained (see Section \\ref{sec:ood-detection} for details).\n\\vspace{-0.6em}", "cites": [1624], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the concept of out-of-distribution samples and briefly mentions OOD detection as a method without integrating or connecting it to broader themes in ML safety. It cites one paper but does not synthesize its contributions or compare it with other approaches. There is minimal critical analysis or abstraction beyond the specific example provided."}}
{"id": "2690b4ad-96cb-41e9-ab12-fc91f8ec6b1c", "title": "Benchmark Datasets for Covariate Shift:", "level": "paragraph", "subsections": [], "parent_id": "afe3ecb8-ce9a-4d47-b542-819aee8be1f0", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Distributional Error"], ["paragraph", "Covariate Shift"], ["paragraph", "Benchmark Datasets for Covariate Shift:"]], "content": "Several variants of the ImageNet dataset have been introduced to benchmark distributional error (i.e., evaluating robustness against distributional shifts) when the model is trained on the original ImageNet dataset.\n introduce two variants of the original ImageNet validation set: ImageNet-C benchmark for input corruption robustness and the ImageNet-P dataset for input perturbation robustness. \nImageNet-A  sorts out unmodified samples from ImageNet test set that falsifies state-of-the-art image classifiers. \n present a series of benchmarks for measuring model robustness to variations on image renditions (ImageNet-R benchmark), imaging time or geographic location (StreetView benchmark), and objects size, occlusion, camera viewpoint, and zoom (DeepFashion Remixed benchmark). \n collected ImageNet-V2 using the same data source and collection pipeline as the original ImageNet paper .\nThis new benchmark leads to the observation that the prediction accuracy of even the best image classifiers are still highly sensitive to minutiae of the test set distribution and extensive hyperparameter tuning. \nShifts  is another recent benchmark dataset for distributional shifts beyond the computer vision tasks. \n\\vspace{-0.6em}", "cites": [8473, 1606, 7058, 7493, 1627], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes a range of benchmark datasets for covariate shift in image classification, listing the contributions of each paper without deeply integrating or connecting them into a broader framework. There is minimal critical evaluation or comparison of the datasets' effectiveness or limitations. While it touches on the idea that model accuracy is sensitive to test set distribution, it does not generalize this into a meta-level principle or trend."}}
{"id": "f363f76a-5f79-4bde-9da2-7cd8033e8b1e", "title": "Benchmark Datasets for Label Distribution Shift:", "level": "paragraph", "subsections": [], "parent_id": "afe3ecb8-ce9a-4d47-b542-819aee8be1f0", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Distributional Error"], ["paragraph", "Covariate Shift"], ["paragraph", "Benchmark Datasets for Label Distribution Shift:"]], "content": "Synthetic label distribution shift is a common benchmarking method in which the test set is manually sampled according to a predefined target label distribution $p_T(y)$ that is different from the source label distribution $p_S(y)$ . \n is an example of a real-world label distribution shift benchmark for text domain.\n\\vspace{-0.6em}", "cites": [1628, 1626], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes synthetic and real-world label distribution shift benchmarking, citing two relevant papers, but provides minimal synthesis or integration of their contributions. There is no comparative or critical evaluation of the cited works, nor any abstraction or generalization of their findings. The content appears to be a superficial overview rather than a deep analytical discussion."}}
{"id": "e55a5a70-2897-4aae-b74f-c1515fc973d1", "title": "Benchmark Datasets for OOD Detection:", "level": "paragraph", "subsections": [], "parent_id": "afe3ecb8-ce9a-4d47-b542-819aee8be1f0", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Distributional Error"], ["paragraph", "Covariate Shift"], ["paragraph", "Benchmark Datasets for OOD Detection:"]], "content": "Test sets of natural images with disjoint label space are typically used for benchmarking OOD detection. \nFor example, a model trained on the CIFAR10 dataset may use ImageNet (samples that do not overlap with CIFAR10 labels) as OOD test samples. \nImageNet-O , containing 2000 images from 200 classes within ImageNet-22k and outside ImageNet is an example for the ImageNet-1k dataset.\nHendrycks et al.  presented three large-scale and high-resolution OOD detection benchmarks for multi-class and multi-label image classification, object detection, and semantic segmentation, respectively. \n presents semantically coherent OOD (SC-OOD) benchmarks for CIFAR10 and CIFAR100 datasets.\nIn another work,  presents benchmarks for anomalous object segmentation and road obstacle segmentation. \n\\vspace{-0.5em}", "cites": [8474, 1629, 1630, 7058], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes benchmark datasets for OOD detection without providing a coherent synthesis or deeper analysis of their implications. It lists several papers and their contributions but does not integrate their findings or highlight differences, limitations, or broader trends in benchmarking approaches. The content remains factual and lacks critical or abstract reasoning."}}
{"id": "82c86db6-92ce-4a1a-ae20-2b136b857e5b", "title": "Adversarial Error", "level": "subsection", "subsections": ["be2bc610-355a-486f-934f-1c8e08af7bda"], "parent_id": "7165617c-b651-4c8c-bb23-2d8f417c79d3", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Adversarial Error"]], "content": "\\label{sec:adversarial_attack}\n\\emph{Adversarial Error} is the model misprediction due to synthetic perturbations (termed as adversarial perturbations) added to the original clean sample. \nThe adversarial attack is the act of generating adversarial perturbations to cause intentional model mispredictions while keeping the identical semantic meaning of the clean sample. \nDifferent forms of adversarial attacks have been studied on different types of data.\nOn image data, typical forms include the $\\ell_p$ constrained additive perturbation , spatial perturbation~, and semantically meaningful perturbation~. \nBeyond the image data, adversarial attacks can also be designed, such as by altering the shape of 3D surfaces , by replacing words with synonyms  or rephrasing the sentence  in natural language data, by applying adversarial printable 2D patches on real-world physical objects . \n\\vspace{-0.5em}", "cites": [910, 1631, 892, 904, 1632], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of adversarial error and various attack types, citing relevant papers for different domains (e.g., image, text, 3D objects). However, it lacks deeper synthesis of the cited works into a cohesive framework and does not offer critical evaluation or comparison of these methods. The generalization is limited to categorizing adversarial attacks by data type without identifying broader principles or trends."}}
{"id": "be2bc610-355a-486f-934f-1c8e08af7bda", "title": "Benchmark Datasets:", "level": "paragraph", "subsections": [], "parent_id": "82c86db6-92ce-4a1a-ae20-2b136b857e5b", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "ML Dependability"], ["subsection", "Adversarial Error"], ["paragraph", "Benchmark Datasets:"]], "content": "Evaluating adversarial error (also known as adversarial robustness) is usually done by measuring empirical performance (e.g., accuracy in image classification tasks) on a set of adversarial samples. \nHowever, the key requirement for a faithful evaluation is to use strong and diverse unseen attacks to break the model. \nThere are two commonly used strategies to achieve this goal.\nFirst, an ensemble of multiple strong and diverse adversarial attacks should be simultaneously used for evaluation. \nFor instance, AutoAttack  consists of four state-of-the-art white-box attacks and two state-of-the-art black-box attacks. \n present another setting by creating evaluation benchmarks of ImageNet-UA and CIFAR-10-UA, which contain both $\\ell_p$ constrained adversarial attacks and real-world adversarial attacks such as worst-case image fogging. \nSecond, the attacks should be carefully designed to prevent the ``gradient obfuscation'' effect .\nSince the success of traditional white-box attacks depends on the accurate calculation of model gradients, they may fail if the model gradients are not easily accessible (e.g., the model has non-differential operations). \nAs a result, evaluating model robustness on such attacks may provide a false sense of robustness. \n proposed three enhancements for traditional white-box attacks as solutions for common causes of gradient obfuscation.\nOther solutions include designing adaptive attacks for each specific defense strategy  (see Section \\ref{sec:adv-detection} for details).\n\\vspace{-0.5em}", "cites": [1633, 7059, 912, 888, 1634], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple papers to present two main strategies for reliable adversarial robustness evaluation. It critically discusses issues like gradient obfuscation and incomplete adaptive attack evaluations, providing a deeper understanding of potential pitfalls. The discussion abstracts beyond individual works to highlight broader principles for effective benchmarking in adversarial settings."}}
{"id": "1cfe93e9-6cf7-4487-b4a4-cecec977f6bc", "title": "Challenges and Opportunities", "level": "subsection", "subsections": [], "parent_id": "3df7c8ef-e02c-4fce-9d3b-80b1e2233171", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Inherently Safe Design"], ["subsection", "Challenges and Opportunities"]], "content": "The first main challenge of designing inherently safe ML models lies in the computation complexity and scalability of solutions . \nAs ML models are becoming exponentially more complex, it will become extremely difficult to impose specifications and perform verification mechanisms that are well-adapted for large ML models. \nA practical solution could be the modular approach presented by  for scaling up formal methods to large ML systems, even when some components (such as perception) do not themselves have precise formal specifications. \nOn the other hand, recent advancements in 3D rendering and simulation have introduced promising solutions for end-to-end testing and semi-formal verification in simulated environments.\nHowever, it is challenging to mitigate the gap between simulation and real-world situations, causing questionable transfer of simulated verification and testing results. \nRecent work starts exploring how simulated formal simulation aid in designing real-world tests . \nAdditionally, thorough and scenario-based simulations enable system verification in broader terms such as monitoring interactions between ML modules in a complex system.\n\\vspace{-0.5em}", "cites": [1636, 1635, 1637], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from three different papers to discuss challenges and opportunities in inherently safe ML design, particularly focusing on scalability and simulation. It identifies a key challenge—simulation-to-reality transfer—though it does not deeply evaluate or contrast the cited approaches. The abstraction level is moderate, as it generalizes to broader issues like system verification in complex environments."}}
{"id": "af841c0e-903c-48e9-bd59-90f001bbc018", "title": "Challenges and Opportunities", "level": "subsection", "subsections": [], "parent_id": "d2689171-2b34-421a-b1bf-c43fead1b2cb", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Enhancing Model Performance and Robustness"], ["subsection", "Challenges and Opportunities"]], "content": "Despite advances in defending different types of naturally occurring distributional shifts and synthetic adversarial attacks, there lacks systematic efforts to tackle robustness limitations in a unified framework to cover the ``in-between\" cases within this spectrum. \nIn fact, in many cases, techniques proposed to enhance one type of robustness do not translate to benefiting other types of robustness. \nFor example,  showed that top-performing robust training methods for one type of distribution shift may even harm the robustness on other different distribution shifts.  \nAnother less explored direction for ML robustness is to benefit from multi-domain and multi-source training data for improved representation learning. \nThe rich contexts captured from sensor sets with diverse orientations and data modality may improve prediction robustness compared to a single input source (e.g., single camera). \nFor example, a recent paper  showed that large models trained on multi-modality data, such as CLIP , can significantly improve representation learning to detect domain shift. \nBased on the above finding, a promising direction for future research is to design multi-modality training methods which explicitly encourage model robustness. \nAnother under-exploited approach for model robustness is run-time self-checking based on various temporal and semantic coherence of data. \nFaithful and effective evaluation of model robustness is another open challenge in real-world applications. \nTraditional evaluation approaches are designed based on the availability of labeled test sets on the target domain. \nHowever, in a real-world setting, the target domain may be constantly shifting and making the test data collection inefficient and inaccurate. \nTo address this issue, recent work propose more practical settings to evaluate model robustness with only unlabeled test data  or selective data labeling . \nUnlike training datasets and evaluation benchmarks commonly used in research, a safety-aware training set requires extensive data capturing, cleaning, and labeling to increase the coverage of unknown edge cases by collecting them directly from the open-world. \nTechnique like active learning , object re-sampling , and self-labeling allow for efficient and targeted dataset improvements which can directly translate to model performance improvements. \nGenerative Adversarial Networks (GAN) could be an underway trend for generating effective large-scale vision datasets. \nFor example,  propose DatasetGAN, an automatic procedure to generate realistic image datasets with semantically segmented labels. \n\\vspace{-0.5em}", "cites": [1642, 1641, 7494, 1639, 1622, 1640, 1638], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to highlight the fragmented nature of robustness techniques in ML and proposes a unifying perspective on how multi-modality and safety-aware data strategies can enhance model performance. It critically addresses limitations in existing approaches, such as the lack of transferability across robustness types and inefficiencies in evaluation. The abstraction level is strong, as it generalizes these insights into broader research directions and principles for model robustness."}}
{"id": "09cbc33b-2cc6-42f5-9853-07558520833f", "title": "Run-time Error Detection", "level": "section", "subsections": ["34569813-910e-4527-af24-a3b2ae876212"], "parent_id": "81a424eb-ce19-4d20-b1e1-67e3f071c9db", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Run-time Error Detection"]], "content": "\\label{sec:error-detection}\nThe third strategy for ML safety is to detect model errors at run-time. \nAlthough the robust training methods discussed in Section \\ref{sec:RT} can significantly improve model robustness, they cannot entirely prevent run-time errors. \nAs a result, run-time monitoring to detect any potential prediction errors is necessary from the safety standpoint. \n\\textit{Selective prediction}, also known as prediction with reject option, is the main approach for run-time error detection .\nSpecifically, it requires the model to cautiously provide predictions only when they have high confidence in the test samples.\nOtherwise, when the model detects potential anomalies, it will trigger fail-safe plans to prevent system failure.\nSelective prediction can significantly improve model robustness at the cost of test set coverage. \nIn this section, we first review methods for model calibration and uncertainty quantification (Sec. \\ref{sec:uncertainty-estimation}) and then go over technique to adopt such methods on specific application scenarios: out-of-distribution detection (Sec. \\ref{sec:ood-detection}) and adversarial attack detection (Sec. \\ref{sec:adv-detection}). \n\\input{sections/7.1.uncertainty-estimation}\n\\input{sections/7.2.ood-detection}\n\\input{sections/7.3.adversarial-attack}\n\\vspace{-0.5em}", "cites": [1644, 1643], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an analytical overview of run-time error detection, particularly selective prediction, by integrating concepts from the cited papers into a broader discussion of ML safety. It links uncertainty estimation and application-specific techniques like out-of-distribution and adversarial attack detection to form a coherent narrative. While it offers some level of critique (e.g., acknowledging trade-offs in coverage), it could benefit from deeper comparative analysis of the cited methods."}}
{"id": "34569813-910e-4527-af24-a3b2ae876212", "title": "Challenges and Opportunities", "level": "subsection", "subsections": [], "parent_id": "09cbc33b-2cc6-42f5-9853-07558520833f", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Run-time Error Detection"], ["subsection", "Challenges and Opportunities"]], "content": "An open challenge in OOD detection is to improve performance on near-OOD samples that are visually similar to ID samples but yet outliers w.r.t. semantic meanings. \nThis scenario is very common in fine-grained image classification and analysis domains where target ID samples could be highly similar to OOD samples. \nRecent papers have made attempts in this more challenging scenario ; however, the OOD detection performance on near-OOD samples is still much worse than that performance on far-OOD samples (i.e., visually more distinct samples). \nAnother open research direction is to propose techniques for efficient OOD sample selection and training.\nIn a recent work,  present ATOM as an empirically verified technique for mining informative auxiliary OOD training data. \nHowever, this direction remains under-explored, and many useful measures such as gradient norms  could be investigated for OOD training efficiency and performance. \nDetecting adversarial examples will remain open research as new attacks are introduced to challenge and defeat detection methods .\nGiven the overhead computational costs for both generating and detecting adversarial samples, an efficient way to nullify attacks could be is to benefit from multi-domain inputs, temporal data characteristics, and domain knowledge from a known clean training set. \nA related example is the work by  that studies the spatial consistency property in the semantic segmentation task by randomly selecting image patches and cross-checking model predictions among the overlap area. \n\\vspace{-0.5em}", "cites": [1645, 7494, 888], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from cited papers to discuss key challenges in OOD detection and adversarial example detection, showing some synthesis by connecting the problem of near-OOD samples with the limitations of existing methods. It also evaluates the effectiveness of current approaches and identifies unexplored directions, reflecting a critical perspective. While it provides useful analytical insights, the abstraction remains moderate, identifying general trends but not fully articulating overarching principles or a novel theoretical framework."}}
{"id": "27093b3d-f55a-4c01-aeb4-419662c36eee", "title": "Discussion and Conclusions", "level": "section", "subsections": [], "parent_id": "81a424eb-ce19-4d20-b1e1-67e3f071c9db", "prefix_titles": [["title", "Taxonomy of Machine Learning Safety: A Survey and Primer"], ["section", "Discussion and Conclusions"]], "content": "\\label{sec:discussion}\nIn our survey, we presented a review of fundamental ML limitations in engineering safety methods; followed by a taxonomy of safety-related techniques in ML. \nThe impetus of this work was to leverage from both engineering safety strategies and state-of-the-art ML techniques to enhance the dependability of ML components in autonomous systems. \nHere we summarize key takeaways from our survey and continue with recommendations on each item for future research. \n\\vspace{-0.5em}\n\\subsection*{T1: Engineering Standards Can Support ML Product Safety} \nSafety needs for design, development, and deployment of ML learning algorithms have subtle distinctions with code-based software. Our analyses are aligned with prior work and indicate that conventional engineering safety standards are not directly applicable on ML algorithms design. Consequently, relevant industrial safety standards suggest enforcing limitations on operation domain of critical ML functions to minimize potential hazards due to ML malfunction. The limitations enforced on ML functions are due to the lack of ML technology readiness and intended to minimize the risk of hazard to an acceptable level.\nAdditionally, recent regulations mandate data privacy and algorithmic transparency which in turn could encourage new principles in responsible ML development and deployment pipelines. \nOur recommendation is aligned with safety standards to perform thorough risk and hazard assessments for ML components and limit their functionalities to minimize the risk of failure.\n\\vspace{-0.5em}\n\\subsection*{T2: The Value of ML Safety Taxonomy}\nThe main contribution of this paper is to establish a meaningful \\textit{ML Safety Taxonomy} based on ML characteristics and limitations to directly benefit from engineering safety practices. \nSpecifically, our taxonomy of ML safety techniques maps key engineering safety principles into relevant ML safety strategies to understand and emphasize on the impact of each ML solution on model reliability.\nThe proposed taxonomy is supported with a comprehensive review of related literature and a hierarchical table of representative papers (Table 1) to categorize ML techniques into three major safety strategies and subsequent solutions. \nThe benefit of the ML safety taxonomy is to break down the problem space into smaller components, help to lay down a road map for safety needs in ML, and identify plausible future research directions. \nWe remark existing challenges and plausible directions as a way to gauge technology readiness on each safety strategy within the main body of literature review in Sections 5, 6, and 7. \nHowever, given the fast pace of developments in the field of ML, a thorough assessment of technology readiness may not be a one size fits all for ML systems. \nOn the other hand, the proposed ML safety taxonomy can benefits from emerging technologies concepts such as Responsible AI  to take social and legal aspects of safety into account. \n\\vspace{-0.5em}\n\\subsection*{T3: Recommendations for Choosing ML Safety Strategies} \nA practical way to improve safety of complex ML products is to benefit from diversification in ML safety strategies and hence minimizing the risk of hazards associated with ML malfunction. We recognize multiple reasons to benefit from diversification of safety strategies. To start with, as no ML solutions guarantees absolute error-less performance in open-world environments, a design based on collection of diverse solutions could learn more complete data representation and hence achieve higher performance and robustness.\nIn other words, a design based on a collection of diverse solution is more likely to maintain robustness at the time of unforeseen distribution shifts as known as edge-cases.\nAdditionally, the overlaps and interactions between ML solutions boost overall performance and reduce development costs. \nFor instance, scenario-based testing for model validation can directly impact data collection and training set quality in design and development cycles. Another example is the positive effects of transfer learning and domain generalization on uncertainty quantification and OOD detection. Lastly, diverse strategies can be applied on different stages of design, development, and deployment of ML lifecycle which benefits from continues monitoring of ML safety across all ML product teams.\n\\vspace{-0.5em}\n\\subsection*{T4: Recommendations for Safe AI Development Frameworks}\nML system development tools and platforms (MLOps) aim to automate and unify the design, development, and deployment of ML systems with a collection of best practices. \nPrior work have emphasized on MLOps tools to minimize the development and maintenance costs in large scale ML systems . \nWe propose existing and emerging MLOps to support and prioritize adaptation and monitoring of ML safety strategies across both system and software level.\nA safety-oriented ML lifecycle incorporates all aspects of ML development from constructing safety scope and requirements, to data management, model training and evaluation, and open-world deployment and monitoring . \nIndustry-oriented efforts in safety-aware MLOps can unify necessary tools, metrics, and increase accessibility for all AI development teams. \nRecent emerging concepts such as Responsible AI  and Explainable AI  aim for building safe AI systems to ensure data privacy, fairness, and human-centered values in AI development.  \nThese new emerging AI concepts can target beyond functional safety of the intelligent system and help to prevent end-users (e.g., driver in the autonomous vehicle) from unintentional misuse of the system due to over-trusting and user unawareness .", "cites": [1605, 1612, 1615, 1604, 1610], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple cited papers, integrating them into a coherent narrative around ML safety taxonomy and strategies. It critically addresses gaps, such as the limited applicability of engineering standards to ML, and evaluates the need for diversification and adaptation. The abstraction level is high, as it generalizes key principles like the importance of safety-oriented MLOps and the alignment of ML techniques with engineering safety strategies."}}
