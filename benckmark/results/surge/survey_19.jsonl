{"id": "06a764f3-402e-4ba7-b30b-7cf079cb3cbd", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "a8e7ccbb-d323-4ebf-90e9-39beeecfaf21", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Introduction"]], "content": "Large technology companies such as Amazon, Google, and Microsoft offer serverless platforms under various brand names. Although the specifics of the services may differ the essential idea behind the offered services is almost the same i.e. by rendering computation to the pay-as-you-go model, serverless computing tries to achieve auto-scaling while providing affordable computation services . \nServerless computing differs from traditional cloud computing concepts (we refer to them as serverful in this paper) in the sense that the infrastructure and the platforms in which the services are running are hidden from customers. In this approach, the customers are only concerned with the desired functionality of their application and the rest is delegated to the service provider.\nThere are successful commercial implementations of this model. Amazon introduced Lambda \\footnote{ \\texttt{https://aws.amazon.com/lambda/}} in 2014 and later Google Cloud Functions \\footnote{\\texttt{https://cloud.google.com/functions/}}, Microsoft Azure Functions \\footnote{\\texttt{https://azure.microsoft.com/}} and IBM OpenWhisk \\footnote{\\texttt{https://openwhisk.apache.org/}} were launched in 2016. Since then, many studies have focused on the challenges and open problems of this concept. Some of the previous studies are skeptical about the potentials of serverless computing due to the poor performance of their case studies . In contrast, others believe that serverless computing will become the face of cloud computing and the performance issues will be addressed eventually .\nThe aim of the serverless services is threefold: (1) relieve the users of cloud services from dealing with the infrastructures or the platforms, (2) convert the billing model to the pay-as-you-go model, (3) auto-scale the service per customers' demand. As a result in a truly serverless application, the execution infrastructure is hidden from the customer and the customer only pays for the resources they actually use. The service is designed such that it can handle request surges rapidly by scaling automatically. The basic entities in serverless computing are functions. The customer registers their functions in the service provider. Then, those functions can be invoked either by an event or per users' request. The execution results are sent back to the customer. The invocation of the functions is delegated to one of the available computation nodes inside the service provider. Usually, these nodes are cloud containers such as Docker  or an isolated runtime environment .\nThough the concept of serverless computing is relatively new it has paved its way into many real-world applications ranging from online collaboration tools to the Internet of Things (IoT). We survey the papers that introduce real-world serverless applications and categorize them into eight different domains. We summarize the objectives (as justified by the authors) for migrating to serverless services for each of the application domains. We further assess the aptness of the serverless paradigm for each application domain based on the arguments made by the authors, the obtained results, and the challenges they reported. Table \\ref{tab:the_table} lists the application domains, migration objectives, and assessments.\nThere are several challenges that serverless services are currently facing. There exist some surveys and literature reviews that discuss those challenges . Our approach is different, in that, instead of focusing on the inherent obstacles and shortcomings of the concept, we analyze the challenges reported in each of the surveyed application domains. Then, we categorize and discuss the existing solutions for those challenges. We bring out the areas that need further attention from the research community. We also discuss the limitations of those solutions and identify open problems. Some of the challenges surveyed in this paper are common between various application domains such as the topic of providing security and privacy in serverless services. And some of them are domain-specific, such as the issues of scheduling, pricing, caching, provider management, and function invocation.\nIn this paper, we also discuss the opportunities presented by serverless computing. We emphasize that serverless services are more customer-friendly as they relieve customers from the intricacies of deployment. They are also more affordable in some cloud computing scenarios that we will discuss later in this paper. We argue that new market places are emerging around these services, which implies new business opportunities.\nThe rest of this paper is organized as follows. Section 2 presents definitions and characteristics of serverless services. Section 3 focuses on the opportunities that the serverless computing model offers. Section 4 discusses the application domains. Section 5 surveys the challenges toward the vast adoption of the concept. Section 6 concludes the paper.", "cites": [1845, 1846, 1847, 1844], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates information from the cited papers to provide a coherent overview of serverless computing's opportunities, challenges, and evolution from cloud computing. It makes some connections between the works, particularly in highlighting both optimistic and skeptical views of the paradigm. However, the critical analysis is somewhat limited to brief mentions of challenges without deep evaluation of the cited works. The abstraction level is moderate, as it introduces a general framework for understanding application domains and challenges, but does not offer meta-level insights."}}
{"id": "b87f4efe-6997-4d74-95f0-308cf20126ac", "title": "New market places", "level": "subsection", "subsections": [], "parent_id": "db295d1a-f2a5-4292-8845-b08d2353a12c", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Opportunities"], ["subsection", "New market places"]], "content": "With the advent of modern operating systems for mobile devices such as Android or iOS, various market places for applications, that are specifically designed for those operating systems, have emerged such as Google Play Store  and Apple's App Store . Such a scenario is already appearing for serverless paradigm i.e. with the growth in the popularity of serverless computing, new market places for functions has emerged. In these types of markets, developers can sell their developed functions to others. Every generalized or domain-specific functionality can be bought or offered in those markets. For example, a software developer may need a geospatial function that checks whether a point resides inside a geospatial polygon. They could buy such functions from those markets. AWS Serverless Application Repository  is an example of such a capability.  presents a quantitative analysis of functions available inside AWS Application Repository.\nThe competition forced by the economics of these markets will lead to high-quality functions i.e. both from the perspective of code efficiency, cleanness, documentation, and resource usage. The function markets may present buyers with a catalog for every function which shows the resource usage of the function and prices it incurs per request. Thus, the buyer can choose from many options for a specific task. \n\\begin{table*}[t]\n    \\centering\n    \\caption{Serverless application domains}\n    \\label{tab:the_table}\n    \\begin{tabular}{|l|c|c|}\n        \\hline\n        \\textbf{Application} & \\textbf{Main reason} & \\textbf{Assessment} \\\\\\hline\\hline\n        Real-time Collaboration and Analytics & Auto-scaling feature & Promising\\\\ \\hline\n        Urban and Industrial Management Systems & Pricing model& Promising\\\\ \\hline\n        Scientific Computing & Lower deployment overhead & Fair\\\\ \\hline\n        Artificial Intelligence and Machine Learning & Pricing model& Fair\\\\ \\hline\n        Video Processing and Streaming  & Lower deployment overhead & Fair\\\\ \\hline\n        System and Software Security & Auto-scaling feature & Promising\\\\ \\hline\n        Internet of Things (IoT) & Auto-scaling feature & Promising\\\\ \\hline\n        E-commerce, Banking and Blockchains & Auto-scaling feature & Fair\\\\ \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [1848], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of function marketplaces in serverless computing and mentions AWS Serverless Application Repository and one cited paper. However, it lacks meaningful synthesis of multiple sources, deeper critical analysis of the cited work or its implications, and abstraction to broader trends or principles. It primarily describes the idea and one example without providing substantial insight."}}
{"id": "227afe5a-7f6c-48e2-ad28-1da0119b24f1", "title": "Real-time collaboration and analytics", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Real-time collaboration and analytics"]], "content": "The stateless nature of serverless services makes them an attractive platform for real-time collaboration tools such as instant messaging and chatbots. Yan et.al.,  proposed an architecture for chatbot on OpenWhisk . An XMPP-based serverless approach for instant messaging is also introduced in . Real-time tracking is another example of collaboration tools that are very suitable for serverless services as these applications are not heavily dependant on the system's state. Anand et.al.,  proposed two real-time GPS tracking methods on low-power processors.\nServerless services are also utilized for data analytics applications . In these applications, various sources stream real-time data to a serverless service. The service gathers, analyses, and then represents the data analytics. The auto-scaling feature of serverless computing makes the handling of concurrent massive data streams, possible.  MÃ¼ller et. al  proposed Lambada which is a serverless data analytics approach that is one order of magnitude faster and two orders of magnitude cheaper compared to commercial Query-as-a-Service systems.", "cites": [1849], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of serverless applications in real-time collaboration and analytics, citing a few relevant papers but without meaningful synthesis or comparison across them. It lacks critical evaluation or discussion of limitations, and offers little abstraction beyond the specific examples mentioned."}}
{"id": "a4d4a3bf-742e-432e-b87e-8515de6d7bde", "title": "Urban and industrial management systems", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Urban and industrial management systems"]], "content": "The pay-as-you-go model of serverless services paved the way for the introduction and implementation of various budget-restricted urban and industrial management systems.  Al-Masri et.al.,  presented an urban smart waste management system. Hussain et.al.,  proposed a serverless service for oil and gas field management system. An implementation of a serverless GIS platform for land valuation is presented in . \nThe distributed nature and auto-scaling feature of serverless services make it an apt choice for smart grids. Zhang et. al,   proposed event-driven serverless services to handle SCADA/EMS failure events. A distributed data aggregation and analytics approach for smart grids is proposed in . Serverless services have been also utilized for urban disaster recovery applications. Franz et.al.,  proposed a community formation method after disasters using serverless services. Another similar approach is also proposed in . \nThe migration toward serverless paradigm seems a reasonable choice for this domain of applications, especially, for public sector services or for developing countries due to its lower deployment overheads and also its pay-as-you-go pricing model.", "cites": [1850, 1851], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of serverless applications in urban and industrial management systems, listing several examples and their domains. While it attempts to connect some ideas (e.g., the suitability of serverless for public services or developing countries), the synthesis is minimal and lacks deeper integration or novel frameworks. There is no critical evaluation of the cited works, and the abstraction remains low, with no identification of broader patterns or principles."}}
{"id": "c7c701da-8462-4069-a28f-94b5e2abf613", "title": "Scientific computing", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Scientific computing"]], "content": "It has been debated in  that serverless computing is not an attractive alternative for scientific computing applications, albeit, many studies have focused their attention toward serverless services for those applications. We believe disagreement lies in the fact that the range of scientific computing and its applications are vast and there are certainly some areas in this domain for which the utilization of serverless services is feasible. \nSpillner et.al.,  argue that serverless approaches provide a more efficient platform for scientific and high-performance computing by presenting various prototypes and their respective measurements. This idea is also echoed in  where high-performance Function-as-a-Service is proposed for scientific applications. A serverless tool for linear algebra problems is proposed in  and a case for matrix multiplication is presented in . Serverless paradigm is harnessed for large-scale optimization in . A serverless case study for scientific workflows is discussed in . \nServerless approaches have been also used in DNA and RNA computing . Niu et. al,  utilized the potentials of serverless paradigm in all-against-all pairwise comparison among all unique human proteins. On-demand high-performance serverless infrastructures and approaches for biomedical computing are proposed in .\nScientific applications that require extensive fine-grained communication are difficult to support with a serverless approach, whereas those that have limited or coarse-grained communication are good candidates. Also, note that scientific computations with time-varying resource demands will benefit from migrating to a serverless paradigm.", "cites": [8504, 7524, 1853, 1852, 1844], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a coherent narrative on the viability of serverless computing in scientific computing, highlighting both supportive and critical perspectives. It provides a critical view by acknowledging limitations, such as difficulty supporting fine-grained communication, but could offer deeper comparative or evaluative analysis of the cited works. It abstracts some general principles, such as the suitability of serverless for coarse-grained or time-varying scientific workloads."}}
{"id": "c7aab344-d216-4b16-9677-4bca5d0d0abb", "title": "Artificial intelligence and machine learning ", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Artificial intelligence and machine learning "]], "content": "Machine learning in general and neural network-based learning, in particular, are currently one of the most attractive research trends. The suitability of the serverless paradigm for this domain has received mixed reactions both from research and industrial communities. For example, it has been argued that deep learning functions are tightly coupled (they require extensive communication between functions), and also these functions are usually compute and memory intensive, as such, the paradigm is not promising for these applications . Nevertheless, it has been discussed that deep neural networks can benefit from serverless paradigms as they allow users to decompose complex model training into several functions without managing virtual machines or servers . As such, various such approaches have been proposed in the literature. A case of serverless machine learning is discussed in . Ishakian et al.,  discussed various deep learning models for serverless platforms. Neural network training of serverless services is explored in . Also, a pay-per-request deployment of neural network models using serverless services is discussed in . A prototype serverless implementation for the estimation of double machine learning models is presented in . A distributed machine learning using serverless architecture also discussed in .\nChristidis et al.,  introduce a set of optimization techniques for transforming a generic artificial intelligence codebase to serverless environments. Using realistic workloads of the UK rail network, they showed that by wielding their techniques the response time remained constant, even as the database scales up to 250 million entries. A serverless framework for the life-cycle management of machine learning-based data analytics tasks is also introduced in .\nThe viability of serverless as a mainstream model serving platform for data science applications is studied in . The authors presented several practical recommendations for data scientists on how to use the serverless paradigm more efficiently on various existing serverless platforms.", "cites": [1854, 1855], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a balanced overview of the opportunities and challenges of serverless computing for AI and ML, integrating several cited works to support both viewpoints. It synthesizes the key ideas and applications from the literature, showing how different approaches leverage serverless features. However, while it mentions limitations and benefits, it lacks deeper comparative analysis or a critical evaluation of the solutions proposed. There is some abstraction in terms of recognizing the potential of serverless for model training and deployment, but it stops short of identifying overarching principles or frameworks."}}
{"id": "1896ddeb-9297-4a09-bc32-05730b2b614a", "title": "Video processing and streaming", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Video processing and streaming"]], "content": "Serverless approaches have been proposed for video processing. Sprocket  is a serverless video processing framework that exploits intra-video parallelism\nto achieve low latency and low cost. The authors claim that a video with 1,000-way concurrency using Amazon Lambda on a full-length HD movie costs about \\$3 per hour of processed video. In another interesting work, a serverless framework\nfor auto-tuning video pipelines discussed in . It achieves 7.9 times lower latency and 17.2 times cost reduction on average compared to that of serverful alternatives. In   GPU processing power is harnessed in a serverless setting for video processing. Zhang et al.,  present a measurement study to extract contributing factors such as the execution duration and monetary cost of serverless video processing approaches. They reported that the performance of video processing applications could be affected by the underlying infrastructure.\nServerless video processing and broadcasting applications have gained much traction both from industrial and research communities during the COVID-19 pandemic. A live media streaming in a serverless setting is presented in . A serverless face-mask detection approach is discussed in . Serverless paradigm also has been utilized in video surveillance applications. Elordi et al.  proposed an on-demand serverless video surveillance using deep neural networks.", "cites": [7525, 1856, 1857], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of serverless video processing and streaming applications but lacks deeper synthesis, critical evaluation, or abstraction. It lists a few systems and their claimed benefits without systematically comparing them or identifying broader trends or principles."}}
{"id": "7c038d92-d558-47aa-b90b-af2aef27a544", "title": "Internet of Things (IoT)", "level": "subsection", "subsections": [], "parent_id": "6852ee81-b52c-4495-beb7-a239393e9665", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Applications"], ["subsection", "Internet of Things (IoT)"]], "content": "The serverless computing paradigm has been exploited for various IoT domains. Benedetti et al.  conducted various experiments on IoT services to analyze the aptness of different serverless settings. Using a real-world dataset, authors of  showed that a serverless approach to manage IoT traffic is feasible and utilizes fewer resources than a typical serverful approach. Cheng et al.  propose a serverless fog computing approach to support data-centric IoT services. A smart Internet of Things (IoT) approach using the serverless and microservice architecture is proposed in . A serverless body area network for e-health IoT applications is presented in . A serverless IoT platform for smart farming is introduced in . Serverless paradigms also have been utilized for coordination control platforms for UAV swarms .\nIn another research direction, Presson et al.  introduced a flexible and intuitive serverless platform for IoT. A decentralized framework for serverless edge computing in the Internet of Things is presented in . The objective of the paper is to form a decentralized FaaS-like execution environment (using in-network executors) and to efficiently dispatch tasks to minimize the response times. In another interesting work, George et al. introduced Nanolambda  which is a framework that brings FaaS to microcontroller-based IoT devices using a Python runtime system. Amazon's Greengrass  also provides a serverless edge runtime for IoT applications.\nIt is reasonable to confer that serverless services can act as feasible back-ends for IoT applications that have infrequent and sporadic requests. For the scenarios where rapid unpredictable surges of requests emerge, serverless services can conveniently handle requests as they can auto-scale rapidly.", "cites": [1859, 1860, 1858], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various serverless IoT applications and platforms but lacks deep synthesis of ideas across the cited works. While it mentions different architectures and use cases, it does not compare or contrast them, nor does it offer critical analysis or identify overarching principles. Some generalization is attempted, but the narrative remains surface-level with minimal analytical depth."}}
{"id": "ac372c9e-1b71-4ccc-885e-b3ffb112ca0f", "title": "Programming, modeling, testing, and debugging", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Programming, modeling, testing, and debugging"]], "content": "As the topic of serverless computing is relatively new, its development tools, concepts, and models are not rich enough. This poses a great challenge for software developers. The lack of proper modeling paradigms leads to non-unified development approaches which will reduce the quality of the code and also complicate collaborations of developers in long term. To remedy this shortcoming, Perez et al.  propose a programming model and middleware for serverless computing applications. The focus of the paper is limited to file processing applications. A formal model for serverless computing using lambda calculus is presented in . Also, model-based analysis of the serverless application is discussed in . \nTo enhance the adoption of a novel paradigm either new programming frameworks should be developed or existing programs on legacy frameworks should be ported to the new paradigm. Zhang et al.  introduced Kappa which is a programming framework for serverless computing. It facilitates and simplifies parallel programming on serverless platforms i.e. the programmer can write ordinary Python code and Kappa transforms the code and executes it on a serverless platform using parallel lambda functions. Pywren  enables programmers to directly run their existing Python codes on Amazon's Lambda platform.\nDebugging and testing tools are integral parts of any software development approach. Serverless computing is not an exception. Few papers have elaborated on this. Lenarduzzi et al.  reflect expert views on the issues and challenges that need to be investigated.   proposes a combined monitoring and debugging approach for serverless functions. Integration testing of serverless functions is discussed in .\nAnother important tool to test the applicability and performance of any new idea is benchmark suites. Several benchmark tools have been developed for serverless applications and service providers during the past few years. A rich set of benchmark suites for serverless infrastructures is presented in . Yu et al.  introduced \\textit{ServerlessBench}, an open-source benchmark suite that includes many test cases to explore important metrics such as communication efficiency, startup latency, and overall performance. FaaSdom  is another benchmark suite for serverless computing platforms that also integrates a model to estimate budget costs for deployments across the supported providers. Another such benchmarking tool is PanOpticon  which also provides an array of configurable parameters and gives out performance measurements for each selected configuration and platform.\nSimulation tools are also important for rapid modeling of real-world situations or testing new ideas and concepts. Mahmoudi et al.  introduced SimFaaS which is a performance simulator for serverless platforms. It simplifies the prediction of various serverless computing key performance metrics.", "cites": [1863, 7526, 1862, 1861], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of tools and models related to programming, testing, debugging, and benchmarking in serverless computing. It integrates a few key papers but does so in a surface-level manner without establishing deeper connections or a unifying framework. There is minimal critical evaluation or abstraction beyond the specific tools and their functionalities."}}
{"id": "084fc553-31e1-4d87-ad14-5f81901ad83f", "title": "Pricing and cost prediction", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Pricing and cost prediction"]], "content": "Many big technology companies now offer serverless computing services with different specifications and prices. As the popularity of serverless services increases, the number of companies and their options for pricing will grow. Many factors affect the price offered by each company. These factors range from the company's revenue strategy to the platform it uses and the energy prices (based on the region or the time of day during which the function execution occurs). For example, a request that gets to a server at 2 a.m. in winter typically costs lower compared to that of the same request with the same resource consumption at 2 p.m. workday in the summer. Another factor is the load level that is imposed on the provider at that moment i.e. whether the provider nearing its peak power demand or not. Peak demand prices are reported to be 200-400 times that of the nominal rate . Consequently, the contribution of peak charge in the electricity bill for a service provider can be considerable, e.g., from 20\\% to 80\\% for several Google data centers . The price offered by the competitors is also a key decision factor. Various pricing models have been proposed for cloud computing in general  that are not directly applicable to serverless service. Extracting a pricing model for service providers is a challenging issue that should further be studied by the research community.\nThe pricing problem is also important for customers. As discussed above, the diversity of the prices will lead to a competitive environment between service providers. Thus, the customer can choose between various price options in an online manner to reduce the costs. In this way, customers put their functions on multiple serverless services (or ship it instantly) and then based on the online available prices, the software decides to place the request to the most affordable service provider. The ultimate goal of customers is to reduce their costs while maintaining the quality of service. Note that, one of the important factors in determining the quality of service is the response time.\nFinding an optimal or a sub-optimal pricing strategy with multiple providers and customer constraints is a challenging issue that must be addressed in research studies. A similar notion has been extensively discussed for cloud computing in general, where supply and demand are considered in extracting dynamic pricing models .\nIt is noteworthy to mention that, the nature of serverless services makes online pricing more feasible compared to that of other cloud services. In those services such as IaaS, the cost of moving and maintaining several virtual machines in various service providers is higher compared to that of function placement in several serverless providers. This enables the scenario in which function placement can be done using auctions as discussed in .\nCurrently, major serverless providers only offer static pricing. This makes the task of predicting the customer costs straightforward as the costs only depend on the resource usage rather than other somewhat harder to predict parameters (such as time of use, etc.). Thus, several research studies have focused on either predicting or modeling resource usage of serverless functions. Using Monte Carlo simulation, Eismann et al.  proposed a methodology for the cost prediction of serverless workflows. They showed that the proposed approach can predict the response time and output parameters of a function based on its input parameters with an accuracy up to 96.1\\%. Cordingly et al.  introduced a tool for accurate performance predictions with error percentage below 4\\%. An analytical model using a heuristic algorithm is presented in . The results show accuracy up to 98\\%. The latter approach is easier to implement and faster to attain as it only executes a greedy algorithm. Overall, it seems that the problem of predicting resource usage of serverless functions is tractable with high accuracy.", "cites": [1864], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent narrative around pricing and cost prediction in serverless computing by integrating general cloud pricing concepts with serverless-specific considerations and citing relevant studies. It connects ideas from the cited paper on auction-based function placement and other cost prediction techniques. However, the critical analysis is limited to stating that the problem must be addressed, without deeper evaluation of the cited methods' limitations or trade-offs. The section abstracts to a reasonable level by framing the problem in terms of broader cost prediction and multi-provider strategies."}}
{"id": "3a628616-4316-4054-8d1f-0d71cfeac7af", "title": "Scheduling", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Scheduling"]], "content": "\\label{sec:scheduling}\nInvocation requests are sent to the provider either by customers' applications or other functions. These requests often have predefined deadlines. This is especially of great importance for real-time, latency-sensitive, or safety-critical applications. The provider must schedule where (i.e. which computation node) and when to execute the functions such that it conforms with the deadlines while considering other system-related criteria such as energy consumption, or resource utilization. There are various strategies that the scheduler could adopt. In what follows, we summarize those strategies. Note that, the real-world schedulers may adopt one or more of these strategies.\n\\\\\n\\noindent \\textbf{Energy-aware scheduling:}\nThe main idea in this type of scheduling is to put inactive containers or the execution environment in a hibernate mode (or cold-state mode) to reduce energy consumption. The transition from cold-state to active mode incurs delays in the execution of invoked functions which may go beyond the deadlines defined by the customer\\footnote{By our estimates on a laboratory installation of OpenWhisk, the cold start latency can range from 2 to 6 seconds; by contrast, real-world open-source function executions usually take millisecond scale}. Thus, in these approaches, the executions are scheduled so that the number of such transitions minimizes. For example, Suresh et al.  introduced ENSURE which is a scheduler for serverless applications. To prevent cold starts, it proactively reserves a few additional containers in a warm state which can smoothly handle workload variations. Using a theoretical model they tried to minimize the amount of additional capacity while ensuring an upper bound for the request latency. Fifer  also uses a similar approach i.e. it proactively spawns containers to avoid cold-starts.\nAn energy-aware scheduler also can take advantage of delaying non-latency-sensitive tasks (such as background or maintenance tasks) to reduce overall energy consumption. We think introducing execution scheduling for a mixture of latency-sensitive and non-latency-sensitive functions can be a good direction for future research studies.\n\\\\\n\\noindent \\textbf{Resource-aware scheduling:}\nServerless applications are diverse, so are their resource consumption patterns. For example, a scientific computing function is usually CPU-intensive while an analytic application is often memory-intensive. Co-locating many CPU-intensive functions in a physical node leads to resource contention and may incur delays to the execution of those functions. This is also true for other types of computing resources such as memory, disk, and network. Resource-aware schedulers place functions to the computation nodes so that they can provide resource requirements of the functions in a timely manner. FnSched  falls into this category of serverless schedulers. it first categorizes functions based on their CPU usage. It then reduces CPU contention between co-located functions by dynamically regulating their CPU shares at runtime. In another approach, Kim et. al.  proposed a dynamic CPU capping method along with a group-aware scheduling algorithm for serverless computing which can effectively reduce CPU contention between functions inside computation nodes. HoseinyFarahabady et. al.  proposed a prediction tool to estimate the rate of event streams inside providers with the end goal of reducing CPU contention and improving QoS. A QoS-aware resource allocation scheme that dynamically scales by predicting the future rate of incoming events for serverless environments is also introduced in . In another direction, Fifer  conducts offline profiling to calculate the expected execution time of functions and balances the load adaptively. \nA framework that uses Bayesian Optimization to find the optimal configuration for serverless functions is presented in . It uses statistical learning techniques to gather samples and predict the cost and execution time of a serverless function across unseen configuration values. The framework uses the predicted cost and execution time, to select the best configuration parameters for running a single or a chain of functions while satisfying customer objectives. \n\\\\\n\\noindent \\textbf{Workflow-aware scheduling:} Serverless applications usually require the execution of several functions to handle their tasks. These stateless small discrete functions are chained together and orchestrated as serverless workflows. We describe this in Sec. \\ref{sec:workflow} comprehensively. By knowing the chain of function invocations, schedulers can speculate the next functions in the chain to forecast, schedule, and provision in advance e.g., prepare a warm container for the execution. Xanadu  uses such speculation to reduce the overheads and delays up to 10 times compared with OpenWhisk. Sequoia  and Archipelago  also adopt variations of this strategy.  uses DAG-structure\\footnote{Directed Cyclic Graphs (DAG) is usually used to show dependency between tasks in scheduling algorithms} to predict the size of worker pools and thus achieve low scheduling overheads for request execution. Sequoia allows prioritizing, scheduling, and queuing of function chains, or functions within workflow chains in order to improve Quality-of-Service (QoS). \nWe think that this area has the potential for further studies. For example, probabilistic serverless DAGs discussed in  can be leveraged to further improve workflow-aware scheduling methods.\n\\\\\n\\noindent \\textbf{Dataflow-aware scheduling:}\nAlthough in an ideal serverless setting the functions are stateless and do not depend on any external data sources, in practice, this is usually not the case. For example, in many machine learning applications, the dependency on external sources is high.  The scheduler thus should take the availability of the data or the data serving delays into account. Hunhoff et al.  introduced \\textit{freshen} which allows developers or providers to proactively fetch data along with other runtime reuse features to reduce overheads when executing serverless functions. The scheduling policy of Cloudburst  also prioritizes data locality. Rausch et al.  presented a domain-specific dataflow-aware serverless scheduler for edge computing.  \nWe further investigate the data dependency in Sec. \\ref{sec:packing}. Data caching is also of great importance to reduce the delay imposed by data dependency, we discuss this in Sec. \\ref{sec:caching}.\n\\\\\n\\noindent \\textbf{Package-aware scheduling:}\nIn some of the current serverless technologies e.g., OpenLambda, the computation node should fetch and install application libraries and dependent packages declared by function upon receiving an invocation request. This obviously takes some time and delays the execution. Amumala et al.  proposed a package-aware scheduling scheme that addresses this issue. Other serverless platforms are usually designed such that the customers themselves incorporate those package during the registration phase. \nThis area of research also has potentials for further investigations. For example, machine learning approaches could be utilized to predict future needed libraries based on currently installed libraries similar to the next basket recommendation problem .\n\\\\\n\\noindent \\textbf{Hybrid scheduling:}\nHybrid virtual-machine/serverless scheduling also has gained attention recently. The idea is to have the best of all worlds scenario; a central scheduler decides to place requests to a private virtual machine or to a serverless provider. Spock  is a hybrid scheduling mechanism that flattens request peaks using VM-based auto-scaling. Skedulix  is also a hybrid scheduler with the objective of minimizing the cost of using public cloud infrastructures while conforming with user-specified deadlines.", "cites": [1867, 1868, 1866, 7527, 1865], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers under the theme of scheduling in serverless computing, grouping them into distinct subcategories (energy-aware, resource-aware, workflow-aware, etc.) and highlighting their shared goals and differences. It offers some critical perspectives, such as the trade-off between energy efficiency and latency, and points out limitations and future research directions. The abstraction is strong, as the section identifies overarching patterns in scheduling strategies and their implications for system performance and cost."}}
{"id": "57b52f68-ccff-46a8-ab8e-f33843a56219", "title": "Networking, sharing and intra-communications", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Networking, sharing and intra-communications"]], "content": "A serverless software is typically a composition of many functions that work together to provide the desired functionality. To attain this, the functions need to somehow communicate with each other and share their data or their state. In other cloud services, this is attained through network addressing. For example in IaaS, each virtual machine can send and receive data through point-to-point networking using network addresses. \nFunctions intra-communication and network-level function addressing in serverless platforms are challenging. Functions in serverless services have characteristics that must be considered to be able to introduce some kind of addressing scheme for them:\n\\begin{itemize}\n\\item Due to the auto-scaling nature of serverless computing, at any given time there may be several running invocations of the same function inside various computation nodes around the world. This rules out the possibility of addressing based on function name or location.\n\\item The functions are often short-lived. The short life span of the functions means that any addressing scheme should be fast enough to track the rapid changes of the system's entire state.\n\\item With the growth in the usage of serverless services, the number of copies of functions that are being deployed will grow drastically. Thus, the proposed addressing space should be scalable enough to be able to handle that volume of functions. \n\\end{itemize}\nEven with a proper addressing scheme, intra-communication between functions is still challenging. There are several possible approaches:\n\\begin{enumerate}\n\\item Intermediate functions or external coordinators that serve as brokers between functions that are suggested in . The same idea is also utilized in   where proxies act as coordinators. However, it has been argued in  that this burdens extreme overhead on the infrastructure.  Performance evaluations of this approach that reveal the bottlenecks and further investigation and optimization of contributing factors, is a good direction for related research studies.\n\\item Communication through data exchange is another approach. This is achievable either through data exchange mechanisms that rely on cloud storage to pass the data such as that of  and , or using distributed message queues such as Amazon's Kinesis Data Streams . There is also the possibility of tailored VM-based resources exchange mechanisms . This type of communication imposes an order of magnitude higher latency than point-to-point communications .  Introducing new fast protocols and methods for data exchange that is specifically designed for function communication in serverless environments needs researchers' attention. \n\\item Communicate through APIs using stateless communication schemes (or protocols) such as REST or SOAP. These protocols are vastly used over the Internet and seem to be good candidates. Such an approach is introduced by  using specialized APIs and network protocols.\n\\item Direct network sockets over TCP/IP is also another approach that is introduced in  and . The idea is to enable generalized networking capabilities for functions rather than facilitating only function-to-function communications. Wawrzoniak et al.  proposed such a method over TCP/IP. Their benchmark shows a sustained throughput of 621 Mbit/s and a round-trip latency of less than 1 ms. In another related approach, Thomas et al. introduced Particle  which is an optimized network stack for serverless settings. It improves application runtime by up to 3 times over existing approaches.\n\\end{enumerate}", "cites": [1869, 1849, 1847], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.7, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple cited papers to discuss communication challenges in serverless environments and outlines different approaches. It critically evaluates each approach by pointing out limitations such as overhead and latency, and suggests directions for future research. While it provides a structured analysis of the problem, it primarily focuses on existing methods without offering a novel framework or meta-level generalization."}}
{"id": "0333899e-c5e8-4dea-8aea-551c2f991947", "title": "Predefined workflows", "level": "subsubsection", "subsections": [], "parent_id": "8eb6f65f-522a-47a9-a047-26b8a35053ef", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Serverless workflows"], ["subsubsection", "Predefined workflows"]], "content": "In this approach, the application developer submits a file that contains the workflow or defines it through an interface. The provider receives and processes the workflow for further actions. Many of the existing serverless providers offer this service. AWS Step Functions , Azure Durable Functions , Alibaba Serverless Workflow , and Google Cloud Composer  are examples of such a service.\nVarious approaches exist to define and specify workflows for serverless platforms. Amazon uses States Language  which is a language that enables users to define state machine and determine states transitions. Microsoft Azure uses standard programming language of choice (such as JavaScript, Python, C\\#, or PowerShell) code to represent workflows . Google's composer service enables users to use Python to define workflows. To enable a platform-independent language for various existing platforms, Ristov et al.  introduced AFCL which is a  generic language for serverless workflow specification that can be translated to multiple platforms e.g., AWS Lambda or IBM Cloud Functions. Triggerflow  is another interesting approach to compose event-based workflows for serverless systems.\nWen et al.  conducted an empirical study of these serverless workflow services. They thoroughly investigated the characteristics and performance of these services based on real-world workloads. In another related study, the performance of sequential workflows under various platforms is discussed in . Domain-specific serverless workflows also have been proposed in the literature to accelerate serverless application development in that domain. SWEEP  which is a utility to define, execute and evaluate workflows of scientific domains, falls into this category.", "cites": [1870, 7072, 8505], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of predefined workflows in serverless computing, listing how different platforms define and implement them. It includes some synthesis by mentioning AFCL and Triggerflow as cross-platform or event-based solutions, but lacks in-depth comparison or evaluation of their strengths and weaknesses. The critical analysis is minimal, and while it hints at broader patterns (e.g., platform independence), it does not abstract these into overarching principles or frameworks."}}
{"id": "f2258512-3c08-4012-aaec-ee3f1a5fc7dc", "title": "Packing", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Packing"]], "content": "\\label{sec:packing}\nThe main incentive for the migration toward a serverless service is its ability to auto-scale itself by executing copies of customers' functions and assign each request to those copies. Schedulers select the physical node to execute the functions based on various strategies that we discussed in Sec \\ref{sec:scheduling}. In many real-world scenarios, functions have certain dependencies on a single remote data source or even several data sources. The data must be shipped from those sources to the computation nodes. This leads to orders of magnitude slower function execution  mainly because of higher traffic inside the provider which increases the latency and reduces the overall performance of the system. Thus, it seems wise to ship the function as near as possible to the data i.e. ``pack'' functions with data. \nTo attain a robust approach for packing of functions with data, Zhang et al.  proposed Shredder which is a multi-tenant cloud store that allows serverless functions to be performed directly within storage nodes (i.e. packing functions to storage). However, this approach leads to complexity in provisioning and utilization as the storage and compute resources are co-located. To overcome this issue  propose an approach that enables users to write storage functions that are logically decoupled from storage. The storage servers then choose where to execute these functions physically using a cost model. \nIt is also possible to pack data and functions together. Shashidhara  proposed Lambda-KV, which is a framework that aggregates and transforms compute and storage resources into a single entity to enable data locality (i.e. packing functions and storage together). This approach is not suitable for the scenarios in which the data changes with high frequency e.g., IoT sensor data readings in certain scenarios. However, the method is applicable to many transaction-based applications such as that of e-commerce and banking applications or that of background applications such as video processing tasks.\nChoosing between the two approaches i.e. packing data to functions or the other way is highly dependant on the application domain. As such, Kayak  is proposed that adaptively chooses between shipping data to functions (i.e. packing storage to functions) or vice versa. It maximizes throughput while meeting application latency requirements. There are further considerations for packing of functions that must be taken into account:\n\\\\\n\\noindent \\textbf{Multiple data sources and a single function:}  There are some scenarios in which a function consumes various data sources. The basic idea would be to ship the data sources together and then pack the function with those data sources. However, this may not be feasible due to various reasons: (1) one or more of the data sources are already near other functions that consume the data, (2) the movement is not physically possible due to the lack of sufficient storage, (3) the movement is not feasible since the number of times that functions access the data is considerably low. Finding an optimal position for the function based on the distance between the physical location of the function and its data sources on the network topology is an interesting problem that needs to be addressed.\n\\\\\n\\noindent \\textbf{Multiple functions and a single data source:} This case is actually simpler. In this case, multiple functions are packed together with the data source in one machine. Fasslets  is well-suited for this scenario. It provides isolation abstraction for functions based on software-fault isolation (SFI) while enabling memory regions to be shared between functions in the same machine. Photon  tries to co-locate multiple instances of the same function within the same runtime to benefit from the application state and data. \n\\\\\n\\noindent \\textbf{Chunked data:} The packing of function and data can be done with chunks of data instead of the whole data. For example, for a function that queries customers' tables of a company's database, that specific table is important and can be packed with the function instead of the whole database.\n\\\\\n\\noindent \\textbf{Evolutionary vs revolutionary movement of data:}\nAs mentioned above, there are scenarios in which data must be moved toward the function. This can be done in evolutionary or revolutionary modes. In the former mode, the chunks of data are moved based on requests from the function, the movement is done incrementally. This may lead to inconsistency in the data which must be taken care of by the provider. In the latter, the data is moved altogether.\n\\\\\n\\noindent \\textbf{Source location:} The relative geographical location of the request to the function also may play a role. Packing data and functions together and then shipping them to the nearest possible location to the requester would reduce the delays that the service faces due to network traffic. \nThe packing can be done before, during, or after the first execution of a function. In the case in which the packing is done before the execution of the function, a careful manifestation of data dependency is needed to find the optimum placement of the function. To this end, Tang et al.  proposed LambData that enables developers to declare functionâs data intents. In the evolutionary model, the packing is done during the execution. It can also be done after the first execution. In this case, the execution logs are inspected after the first execution and by using optimization techniques and machine learning approaches the optimal packing strategy is extracted to improve the performance of the service and to minimize costs. Note that, packing may undermine the benefits of statistical multiplexing, leading to queuing delays and inefficient resource utilization which is a good direction for future research studies.", "cites": [7073, 1847], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an analytical overview of packing strategies in serverless computing by integrating and contrasting different approaches like Shredder, Lambda-KV, Fasslets, and Photon. It critically evaluates the trade-offs of each (e.g., co-location complexity, unsuitability for high-frequency data, benefits of shared memory), and abstracts broader principles such as data locality, evolutionary vs. revolutionary data movement, and cost/performance trade-offs. While some synthesis is implied, it is not deeply interwoven with all cited works."}}
{"id": "61160333-b9e4-441d-bae0-4b109d6d1b22", "title": "Data caching", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Data caching"]], "content": "\\label{sec:caching}\nTo avoid the bottlenecks and latencies of persistent storage, software systems utilize multiple levels of caches. This is a common practice among cloud-based applications . Utilizing caches in serverless environments is a challenging issue since functions are executed independently of the infrastructure. For example, consider a serverless customer management application. When a function requests the data of a user from the database, the platform usually caches the data to handle any further requests and to reduce the number of costly database accesses. This works perfectly in serverful services. However, in a serverless service, the next execution of the function may be assigned to another available computation node, which renders the caching useless. This is also true when multiple functions consecutively work on a chunk of data i.e. if the computation node changes, the cached data becomes expired. Without caching, the costs, overheads, and latency grow dramatically which makes serverless services infeasible. Thus, this is one of the important challenges toward the successful implementation of any serverless service. Few caching mechanisms have been proposed for serverless systems. Amazon offers ElastiCache  which is an in-memory cache and data store. It is at least 700x more expensive than Amazon's storage service (called S3). Pu et al.  proposed a method to attain a cost-efficient combination of slow storage and costly in-memory caches. InfiniCache  is another in-memory object caching system based on stateless cloud functions. CloudBurst  also proposes a caching mechanism in its architecture.  \nWe think this subject has the potentials for many further research studies. In designing and implementing caches, the following must be considered:\n\\\\\n\\noindent \\textbf{Effect of packing:} In one of the packing schemes i.e. packing function with data, the functions are shipped as near as possible to the data. This may lead to a scenario in which multiple invocations of the same function are executed in a computation node near the data. This actually reduces the complexity of caching. Instead of focusing on a system-wide cache solution, one can focus on efficient local caching mechanisms. The action of packing also tends to ship other functions that consume the data toward the vicinity of the data, and thus with proper local caching, the chance of cache hits is improved.\n\\\\\n\\noindent \\textbf{Effect of workflows:} The sequence upon which a batch of functions is executed also has a great impact on designing caches. In fact, in a sequential execution, the likelihood of data dependency between two or more consecutive functions is high. Thus, caching will be effective if the execution chains are considered in the local caching strategy.\n\\\\\n\\noindent \\textbf{Local caching vs distributed caching}\nIn some of the real-world scenarios of serverless computing, an efficient local caching can be feasible\\footnote{Here, by local we mean a caching mechanism shared between multiple servers in a rack or possibly a cluster of racks near each other.}. However, there are cases in which the functions cannot be shipped to the vicinity of the data. In these cases, distributed caching can be utilized. \nDistributed in-memory caches often utilize distributed hash functions (DHT) to extract the location of cached data .  Then, the data is routed to the requester. If the data does not reside in the distributed cache, the requester extracts the data and caches it. This works well when the cost of extracting data from its source is higher than that of getting it from the remote cache. CloudBurst  proposes a variation of this idea. It utilizes both DHT-like distributed storage along with local caching to improve the performance of serverless applications. \nNote that, using distributed caching may incur more costs. We are facing a scenario in which the function cannot be shipped to the vicinity of the data. In this scenario, caching the data in the server that executes the function may accelerate the future invocations of the same function. However, as most other functions are shipped near the data, the cost of routing the cached data to the functions compared to that of extracting it from the source directly may actually be higher. This must be considered in any distributed cache design for serverless services.\n\\\\", "cites": [7527, 1869], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers (Cloudburst and InfiniCache) by integrating their approaches within the broader context of data caching in serverless computing. It provides critical analysis by discussing the trade-offs between local and distributed caching, and highlights potential cost issues with distributed caching. The section also abstracts beyond the specific systems by introducing conceptual ideas like packing and workflow-based caching strategies."}}
{"id": "f29f0ad2-44e1-42ee-8da7-e28d25579a70", "title": "Provider Management", "level": "subsection", "subsections": [], "parent_id": "ac9bcc3b-8f74-4b7c-8a2f-c5d4a9f0a4b2", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Provider Management"]], "content": "The management operation inside the serverless providers is a complex and resource-demanding task. It involves many monitoring and provisioning operations for each of the infrastructures inside the provider. The controller should handle and keep track of functions' registration, invocation, and execution. Below, we discuss each operation in detail:\n\\\\\n\\noindent \\textbf{Registration:} Every user should be able to upload their function, select its required resources. The provider then sends back credentials for invocation. Other tracking and business aspects are handled by the provider during this step.\n\\\\\n\\noindent \\textbf{Invocation:} A provider receives invocation requests from applications or other functions, checks the requester's credentials, and then finds a feasible computation node and assigns the function to the node for execution. The tasks of placement, scheduling, packing, and caching are part of the responsibility of the controller which is done in collaboration with the computation nodes.\n\\\\\n\\noindent \\textbf{Initialization and Execution:}\nTo execute functions, providers often use sandbox execution environments to provide strong isolation between function instances. OpenWhisk uses containers for the execution , however, containers usually have isolation problems. As such, many recent studies have focused on providing serverless sandboxes with reliable isolation. Catalyzer  is an example of such efforts. It provides strong isolation with sub-millisecond startup time. FireCracker  introduces Virtual Machine Monitor (VMM) device model and API for managing and configuring MicroVM. It provides strong isolation with minimal overhead (less than 5MB of memory) and millisecond scale boot time. Unikernels in which the function is linked with a bare minimum library operating system is another approach to attain a sandbox execution environment for serverless computing .\n\\\\\n\\noindent \\textbf{Monitoring:} Although the execution takes place inside the computation nodes, the controller should closely monitor the execution of functions to detect errors and malfunctions. It gathers the execution logs to analyze the footprints and thus improve future invocations. Various commercial monitoring approaches exist for serverless and cloud systems such as Epsagon\\footnote{\\texttt{https://epsagon.com/}}, Datadog\\footnote{\\texttt{https://www.datadoghq.com/}}, and Dynatrace\\footnote{\\texttt{https://www.dynatrace.com/}}. These solutions usually are restricted to basic metrics such as CPU utilization. Eismann et al.  proposed a  resource consumption monitoring module specifically tailored for serverless platforms.\nThere are two approaches to attain a controller system for serverless providers; centralized or distributed. While the centralized approach is more trivial and more efficient, it may experience extreme loads and it could become the single point of failure. Distributed monitoring, on the other hand, is complex and hard to implement.\nFor the controller to be able to handle its responsibilities, manage resources and optimize the services, it should have an online view of the entire system. Various pieces of information contribute to the formation of this view, such as:\n\\begin{enumerate}\n\\item Information about the functions: their data dependency, the workflow, their owner, the origin of the requests, rate of invocations, etc.\n\\item The state of the infrastructure: the location of nodes, the communication infrastructure, their online available resources, which functions are assigned to them, the execution logs, etc.\n\\item The data sources: the format of data, the location of data sources, their infrastructure, etc. \n\\item The state of local caches: what they have in the caches, what policy for cache they use, what is the size of their cache, etc.\n\\end{enumerate}\nHaving all of the above information in an online manner incurs heavy overhead on the provider. On the other hand, having partial information may lead to imprecise decisions by the controller. This challenge deserves attention from both research and industrial communities.", "cites": [1871], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of provider management in serverless computing, discussing key operations and integrating examples from the literature. It connects ideas across different techniques for isolation (e.g., Catalyzer, FireCracker, unikernels) and mentions a relevant paper (doc_id: 1871) in the context of resource management. However, it lacks deeper comparative analysis or a novel framework, and the critical evaluation is limited to general challenges rather than specific limitations of the cited works."}}
{"id": "16d12e8c-df5e-419e-bf3a-59c225ec7c49", "title": "Runtime Security", "level": "subsubsection", "subsections": [], "parent_id": "f70d7b5b-1985-4839-9f8c-1aee3886d7a4", "prefix_titles": [["title", "Serverless Computing: A Survey of Opportunities, Challenges, and Applications"], ["section", "Challenges"], ["subsection", "Security and Privacy"], ["subsubsection", "Runtime Security"]], "content": "In the wake of Meltdown , and Spectre  attacks, the vulnerability of applications against common execution environments has become one of the main security concerns. This issue is particularly severe in serverless environments since many functions from various owners are being executed in a shared execution environment. To counter these types of attacks, a lightweight and high-performance JavaScript engine is presented in  that utilizes secure enclaves for the execution environments. The limitation of this work is that it suffers from high memory overhead and it only supports JavaScript. We think this is an area of research that deserves special attention from the research community. \nRuntime environments also can be customized to surveil the functions during execution to detect and prevent malicious activities. SecLambda  introduces a modified container runtime environment called runsec. It intercepts HTTP requests and I/O operations to check whether or not the function conforms with a set of predefined security policies. Trapeze  also adopts the same strategy i.e. it puts each serverless function in a sandbox that intercepts all interactions between the function and the rest of the world based on policy enforcement rules defined by their dynamic information flow control model. Valve   proposes a serverless platform that performs runtime tracing and enforcement of information flow control. The downside of these approaches is that they have a fairly heavy impact on the performance of serverless services. More lightweight approaches need to be investigated for delay-sensitive functions.", "cites": [1873, 1874, 1363, 1872], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates key concepts from cited works to discuss runtime security in serverless computing, showing some synthesis by connecting the use of secure enclaves and policy enforcement mechanisms. It offers critical insights by highlighting limitations such as performance impact and language-specific constraints. While it identifies a broader need for lightweight approaches, the abstraction remains limited to a general statement rather than a meta-level framework or principle."}}
