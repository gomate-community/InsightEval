{"id": "ad0bebeb-f5e1-492e-8a28-fc71ba38e183", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "f62ce4ec-ed89-4ca6-9540-c6a7826047e5", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Introduction"]], "content": "\\ml technologies solve challenging problems which often have high social impact, such as examining re-offence rates (e.g. ), automating chat and (tech) support, and screening job applications (see ). \nYet, approaches in \\ml have ``found dark skin unattractive'',\\footnote{\\url{https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people}} claimed that ``black people reoffend more'',\\footnote{\\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}} and created a Neo-Nazi sexbot.\\footnote{\\url{https://www.technologyreview.com/s/610634/microsofts-neo-nazi-sexbot-was-a-great-lesson-for-makers-of-ai-assistants/}} With the increasingly widespread use of automated decision making and \\ml approaches in general, fairness considerations in \\ml have gained significant attention in research and practice in the 2010s. However, from a historical perspective these modern approaches often build on prior definitions, concepts, and considerations that have been suggested and developed over the past five decades. Specifically, there is a rich set of fairness-related work in a variety of disciplines, often with concepts that are similar or equal to current \\ml fairness research . For example, discrimination in hiring decisions has been examined since the 1960s . \nResearch into (un)fairness, discrimination, and bias emerged after the 1964 US Civil Rights act, making it illegal to discriminate based on certain criteria in the context of government agencies (Title VI) and employment (Title VII). Two initial foci of fairness research were unfairness of standardized tests in higher education/university contexts  as well as discrimination in employment-based concepts . The first years after the Civil Rights act saw the emergence of a variety of definitions, metrics, and scholarly disputes about the applicability of various definitions and fairness concepts as well as the realizations that some concepts (such as group-based vs individual notions of fairness) can be incompatible. \nWhen comparing current work in \\ml with initial work in fairness, it is noteworthy that much of the early literature considers regression settings as well a correlation-based definition of fairness properties of an underlying mechanism due to the focus on test fairness with a continuous target variable (see e.g., ). However, the general notions transfer to (binary) classification settings and thus define essential concepts such as protected/demographic variables (e.g. ), notions of group vs individual fairness (e.g. ), impossibility conditions between different fairness conditions (), and fairness quantification based on metrics (e.g., true positive rates, ).\nDespite the increased discussion of different aspects and viewpoints of fairness in the 1970s as well as the founding of many modern fairness concepts, no general consensus as to what constitutes fairness or if it can/should be quantified emerged based on this first wave of fairness research. As  note, some of the related discussions resonate with current discussions in \\ml, e.g., the difficulty that different notions can be incompatible with each other, or the fact that each specific quantified measurement of fairness seems to have particular downsides. \nMore recently, many researchers (e.g. ), governments (e.g. the EU in , and the US in ), policies like the \\gdpr, NGOs (e.g. the Association of Internet Researchers ) and the media have fervently called for more societal accountability and social understanding of \\ml . There is a recognition in the literature that often data is the problem, i.e. intrinsic biases in the sample will manifest themselves in any model built on the data , inappropriate uses of data leading to (un)conscious bias(es) , data veracity and quality , data relativity and context shifts , and subjectivity filters . Even for skilled \\ml researchers, the array of challenges can be overwhelming and current \\ml libraries often do not yet accommodate means to ascertain social accountability. Note that data is not the only source of bias and discrimination, here we refer to  for a general discussion on the main types of bias and discrimination in \\ml . \n\\ml researchers have responded to this call, developing a large number of metrics to quantify fairness in decisions (automated or otherwise) and mitigate any bias and unfairness issues in \\ml . Figure \\ref{fig:numpapers} shows the number of papers, starting in 2010, that have been published in the fairness in \\ml domain. \nThese numbers are based on the articles referenced in our survey. The figure shows a clear uptick in papers starting in 2016 and 2017.\\footnote{The numbers for 2020 are incomplete as the survey was submitted during the year.} \nIn this respect, this article aims to provide an entry-level overview of the current state of the art for fairness in \\ml . This article builds on other similarly themed reviews that have focused on the history of fairness in \\ml , a multidisciplinary survey of discrimination analysis , a discussion on key choices and assumptions  and finally  who review different types of biases,  also introduce a number of methods to mitigate these. In this article we assume that the reader has a working knowledge of applied \\ml , i.e. they are familiar with the basic structure of data mining methodologies such as  and how to apply and evaluate ``standard'' \\ml methods. \nUpon this basis, this article aims to: 1) Provide an easy introduction into the area fairness in \\ml (\\autoref{sec:overview}); 2) Summarise the current approaches to measure fairness in \\ml within a standardised notation framework discussing the various trade-offs of each approach as well as their overarching objectives (\\autoref{sec:metrics}); 3) Define a two-dimensional taxonomy of approach categories to act as a point of reference. Within this taxonomy, we highlight the main approaches, assumptions, and general challenges for binary classification (\\autoref{sec:approaches}) and beyond binary classification (\\autoref{sec:beyond}); 4) Highlight currently available toolkits for fair \\ml (\\autoref{sec:platforms}); and 5) Outline the dilemmas for fairness research as avenues of future work to improve the accessibility of the domain (\\autoref{sec:dilemmas}).\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=.5\\textwidth]{images/NumPapers.png}\n    \\caption{Number of Papers related to Fairness in \\ml research based on cited articles in this survey.}\n    \\label{fig:numpapers}\n    \\tightenfig\n\\end{figure}\n\\input{overview}\n\\label{sec:variables}\nBefore discussing the different approaches adopted in the literature to improve the fairness of \\ml applications, a discussion on the nature of protected variables is needed. Many researchers make assumptions about what does or does not constitute a protected variable; and the question of which variables should be protected quickly arises. We note that many variables are explicitly defined as ``sensitive'' by corresponding legal frameworks. \\simon{use }\nHowever, the notion of a protected or sensitive variable typically encompasses any feature of the data that involves or concerns people . While there are some readily available sets of candidate sensitive variables, there are relatively few works (beyond case studies within papers presenting novel fairness measures) that actively seek to determine whether such variables should be protected or not.  both present an approaches specifically looking at the variable importance (or model influence) of sensitive variables, and as such could act as a means to ascertain prior to the application of any fair \\ml approaches discussed in this review whether there is a cause for concern.\nWhilst this provides some guidance for \\ml researchers, there is still the question detecting variables that are not strictly sensitive, but have a relationship with one or more sensitive variables.  notes that many definitions of fairness express model output in terms of sensitive variables, without considering ``related'' variables. Not considering these related variables could erroneously assume fair \\ml model has been produced. Understanding ``related'' variables is a well studied area, yet typically not within the fairness literature, but the privacy and data archiving literature. Although the relationship between discrimination and privacy was noted in . The cyber/information security and data privacy domains view the challenges of sensitive data differently. Looking more from the perspective of sensitive data disclosure being a long-standing challenge in protecting citizen anonymity when data is published and/or analysed . Key approaches in this area are (e.g. ) seek to protect specific individuals and groups from being identifiable within a given dataset, i.e. minimise disclosure risk. Yet, these approaches can still struggle to handle multiple sensitive attributes at once . Whilst, these approaches (and many others) have been successful in anonymising datasets, they still require a list of features to protect. For explicit identifiers (such as name, gender, zip etc.) such lists exist within legal frameworks (such as those discussed above). For correlated or otherwise related variables (often referred to as proxies or quasi-identifiers), much of the literature assumes a priori knowledge of the set of quasi-identifiers , or seeks to discover them on a case-by-case basis, e.g. , and moves towards a notion of privacy preserving data mining as introduced by .  also discuss the notion of proxy groups, a set of ``similar'' instances of the data that could correspond to a protected set. They give the example of inferring ethnic backgrounds through surname similarities and argue that proxy groups may generalise to unobservable groups of interest which have significant fairness implications, which they show experimentally.\nMore recently, researchers have begun to investigate graph- and network-based methods for discovering quasi-indicators from a given dataset either with respect to anonymity criteria (e.g. ) or specific notions of fairness (e.g. ).  provides a brief overview of different theoretical applications to algorithmic fairness, with  noting how different causal graph-based models can help use variable relationships to distill different biases in the model and/or data. \\autoref{tbl:proxies} provides some examples of sensitive variables and potential respective proxies.\n\\begin{table}[h]\n    \\scriptsize\n    \\centering\n    \\begin{tabularx}{0.8\\textwidth}{p{.25\\textwidth}X}\n        \\toprule\n        Sensitive Variable & Proxy \\\\%\\\\\\hline\n        \\midrule\n        Gender &    Education Level \\\\\n                    & Felony Descriptive Variables (in general) \\\\\n                    & Income \\\\\n                    & Occupation \\\\\n                    & Keywords in User Generated Content (e.g. CV, Social Media etc.)\\\\\n                    & University Faculty \\\\\n                    & Working Hours \\\\ \n        \\midrule\n        Marital Status &    Education Level\\\\\n                       &    Income \\\\\n        \\midrule\n        Race &  Felony Descriptive Variables (in general) \\\\\n            &   Keywords in User Generated Content (e.g. CV, Social Media etc.) \\\\\n            &   Zipcode \\\\ \n        \\midrule\n        Disabilities & Personality Test Data \\\\ \n        \\bottomrule        \n    \\end{tabularx}\n    \\caption{Example Proxies of Sensitive Variables based on findings from }\n    \\label{tbl:proxies}\n\\end{table}", "cites": [5617, 6333, 6334, 8788, 3901, 8703, 3907, 9001, 4079, 9000, 6336, 6335, 8702, 6332], "cite_extract_rate": 0.19444444444444445, "origin_cites_number": 72, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a broad range of literature, integrating historical, legal, and technical perspectives on fairness. It critically examines the limitations and challenges in fairness research, such as incompatible fairness definitions and data-related issues. The section also abstracts key principles, like the distinction between group and individual fairness, and the role of causal reasoning in addressing biases."}}
{"id": "d6a04f1e-4dd6-4921-8777-ee2894367ea1", "title": "Measuring Fairness and Bias", "level": "section", "subsections": ["43caf93f-1c43-4c8c-b980-931dca126218", "df5cc326-2b88-46cc-86d0-cec9d2ab59f0", "b64a2657-0180-4a07-9c66-ece5e19834b2", "f9a75db4-9343-48cf-85fe-a6959dc9a310"], "parent_id": "f62ce4ec-ed89-4ca6-9540-c6a7826047e5", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"]], "content": "\\label{sec:metrics}\nBehind intervention-based approaches are a myriad of definitions and metrics (e.g. ) to mathematically represent bias, fairness, and/or discrimination; but they lack consistency in naming conventions  and notation. More so, there are many different interpretations of what it means for an algorithm to be ``fair''. Several previous publications provide a (limited) overview of multiple fairness metrics and definitions, e.g., . We extend these prior summaries by including additional perspectives for types of biases and a larger set of metrics and definitions that are included as compared to previous publications.\nTypically, metrics fall under several categories, for example: 1) \\emph{statistical parity}: where each group receives an equal fraction of possible [decision] outcomes ; 2) \\emph{disparate impact}: a quantity that captures whether wildly different outcomes are observed in different [social] groups ; 3) \\emph{equality of opportunity} , 4) \\emph{calibration} : where false positive rates across groups are enforced to be similar (defined as \\emph{disparate mistreatment} by  when this is not the case), 5) \\emph{counterfactual fairness} which states that a decision is fair towards an individual if it coincides with one that would have been taken were the sensitive variable(s) different . \nAlthough the literature has defined a myriad of notions to quantify fairness, each measures and emphasizes different aspects of what can be considered ``fair''. Many are difficult / impossible to combine , but ultimately, we must keep in mind (as noted in ) there is no universal means to measure fairness, and also at present no clear guideline(s) on which measures are ``best''. Thus, in this section we provide an overview of fairness measures and seek to provide a lay interpretation to help inform decision making. Table \\ref{tbl:metricoverview} presents an overview of the categories of fairness metrics presented with \\autoref{tbl:notationclassification} introducing key notation. \n\\begin{table}[h]\n    \\scriptsize\n    \\centering\n    \\begin{tabularx}{\\textwidth}{p{.07\\textwidth}@{\\hskip 0.15in}X@{\\hskip 0.15in}X@{\\hskip 0.15in}X@{\\hskip 0.15in}X@{\\hskip 0.15in}p{0.15\\textwidth}}\n        \\toprule\n        & \\multicolumn{4}{c}{Group-based Fairness} & Individual and Counterfactual Fairness \\\\\n        \\cmidrule(lr){2-5} \\cmidrule(lr){6-6}\n         & Parity-based Metrics & Confusion Matrix-based Metrics & Calibration-based Metrics & Score-based Metrics & Distribution-based Metrics  \\\\%\\\\\\hline\n        \\midrule\n        Concept & Compare predicted positive rates across groups & Compare groups by taking into account potential underlying differences between groups & Compare based on predicted probability rates (scores) & Compare based on expected scores & Calculate distributions based on individual classification outcomes \\\\\n        Abstract Criterion & Independence & Separation & Sufficiency & - & - \\\\\n        Examples & Statistical Parity, Disparate Impact & Accuracy equality, Equalized Odds, Equal Opportunity & Test fairness, Well calibration & Balance for positive and negative class, Bayesian Fairness & Counterfactual Fairness, Generalized Entropy Index \\\\\n        \\bottomrule        \n    \\end{tabularx}\n    \\caption{Overview of suggested fairness metrics for binary classification}\n    \\label{tbl:metricoverview}\n    \\tightentab\n\\end{table}\n\\newcommand\\MyBox[2]{\n  \\fbox{\\lower0.75cm\n    \\vbox to 1.7cm{\\vfil\n      \\hbox to 1.7cm{\\hfil\\parbox{1.4cm}{#1\\\\#2}\\hfil}\n      \\vfil}\n  }\n}\n\\begin{table}{}\n    \\centering\n    \\scriptsize\n    \\begin{tabularx}{0.8\\textwidth}{p{3cm}X}  \n        \\toprule\n        Symbol   & Description \\\\\n        \\midrule\n        $y\\in {0, 1}$ & Actual value / outcome \\\\\n        $\\hat{y}\\in {0, 1}$ & Predicted value / outcome \\\\\n        $s=Pr(\\hat{y}_i=1)$ & Predicted score of an observation $i$. Probability of $y=1$ for observation $i$ \\\\\n        $g_i, g_j$ & Identifier for groups based on protected attribute \\\\\n        \\bottomrule\n    \\end{tabularx}\n    \\caption{Notation for Binary Classification}\n    \\label{tbl:notationclassification}\n    \\tightentab\n\\end{table}", "cites": [3900, 3935, 8788, 6338, 6337, 3899, 8702, 8704], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key fairness metrics from multiple cited works, organizing them into a structured and coherent framework with clear categories and abstract criteria. It also provides abstraction by generalizing the types of fairness (e.g., group-based vs. individual/counterfactual) and highlights the lack of consensus in the field. While it offers some critique, such as the difficulty in combining metrics and the absence of universally accepted measures, the analysis could be deeper to reach the highest critical insight level."}}
{"id": "68ef59f4-78cb-4790-a8e1-84a9bce3a431", "title": "Parity-based Metrics", "level": "subsubsection", "subsections": [], "parent_id": "df5cc326-2b88-46cc-86d0-cec9d2ab59f0", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Group Fairness Metrics"], ["subsubsection", "Parity-based Metrics"]], "content": "\\hfill\\\\\nParity-based metrics typically consider the predicted positive rates, i.e., $Pr(\\hat{y}=1)$, across different groups. This is related to the Independence criterion that was defined in \\autoref{subsec:abstractfairnesscriteria}.\n\\textbf{Statistical/Demographic Parity}: \nOne of the earliest definitions of fairness, this metric defines fairness as an equal probability of being classified with the positive label . I.e., each group has the same probability of being classified with the positive outcome. A disadvantage of this notion, however, is that potential differences between groups are not being taken into account. \n\\begin{align}\n    Pr(\\hat{y}=1 | g_i) = Pr(\\hat{y}=1 | g_j)\n\\end{align}\n\\textbf{Disparate Impact}: Similar to statistical parity, this definition looks at the probability of being classified with the positive label. In contrast to parity, Disparate Impact considers the ratio between unprivileged and privileged groups. Its origins are in legal fairness considerations for selection procedures which sometimes use an 80\\% rule to define if a process has disparate impact (ratio smaller than 0.8) or not . \n\\begin{align}\n   \\frac{Pr(\\hat{y}=1 | g_1)}{Pr(\\hat{y}=1 | g_2)} \n\\end{align}\nWhile often used in the (binary) classification setting, notions of Disparate Impact are also used to define fairness in other domains, e.g., dividing a finite supply of items among participants .", "cites": [6339, 3903], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of parity-based fairness metrics, such as Statistical/Demographic Parity and Disparate Impact. It makes minimal connections between the cited papers, only noting that Disparate Impact has origins in legal considerations and is applied in other domains. While it does mention a limitation of Statistical Parity, it lacks deeper critical evaluation or abstraction into broader fairness principles."}}
{"id": "0f27b46e-5acd-419e-a6f8-a58b3715ef74", "title": "Confusion Matrix-based Metrics", "level": "subsubsection", "subsections": [], "parent_id": "df5cc326-2b88-46cc-86d0-cec9d2ab59f0", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Group Fairness Metrics"], ["subsubsection", "Confusion Matrix-based Metrics"]], "content": "\\hfill\\\\\nWhile parity-based metrics typically consider variants of the predicted positive rate $Pr(\\hat{y}=1)$, confusion matrix-based metrics take into consideration additional aspects such as \\tpr, \\tnr, \\fpr, and \\fnr. The advantage of these types of metrics is that they are able to include underlying differences between groups who would otherwise not be included in the parity-based approaches. This is related to the Separation criterion that was defined in \\autoref{subsec:abstractfairnesscriteria}.\n\\textbf{Equal Opportunity}: As parity and disparate impact do not consider potential differences in groups that are being compared,  consider additional metrics that make use of the \\fpr and \\tpr between groups. Specifically, an algorithm is considered to be fair under equal opportunity if its \\tpr is the same across different groups. \n\\begin{align}\n   Pr(\\hat{y}=1 |y=1 \\& g_i) &= Pr(\\hat{y}=1 | y=1 \\& g_j) \n\\end{align}\n\\textbf{Equalized Odds} (Conditional procedure accuracy equality ): Similarly to equal opportunity, in addition to \\tpr equalized odds simultaneously considers \\fpr as well, i.e., the percentage of actual negatives that are predicted as positive. \n\\begin{align}\n   Pr(\\hat{y}=1 |y=1 \\& g_i) = Pr(\\hat{y}=1 | y=1 \\& g_j) &\\And Pr(\\hat{y}=1 |y=0 \\& g_i) = Pr(\\hat{y}=1 | y=0 \\& g_j)\n\\end{align}\n\\textbf{Overall accuracy equality} : Accuracy, i.e., the percentage of overall correct predictions (either positive or negative), is one of the most widely used classification metrics.  adjusts this concept by looking at relative accuracy rates across different groups. If two groups have the same accuracy, they are considered equal based on their accuracy. \n\\begin{align}\n   Pr(\\hat{y}=0 |y=0 \\& g_i) + Pr(\\hat{y}=1 |y=1 \\& g_i) &= Pr(\\hat{y}=0 |y=0 \\& g_j) + Pr(\\hat{y}=1 |y=1 \\& g_j)\n\\end{align}\n\\textbf{Conditional use accuracy equality} : As an adaptation of the overall accuracy equality, the following conditional procedure and conditional use accuracy do not look at the overall accuracy for each subgroup, but rather at the positive and negative predictive values. \n\\begin{align}\n   Pr(y=1 | \\hat{y}=1 \\& g_i) &= Pr(y=1 | \\hat{y}=1 \\& g_j) \\And \n   Pr(y=0 | \\hat{y}=0 \\& g_i) &= Pr(y=0 | \\hat{y}=0 \\& g_j)\n\\end{align}\n\\textbf{Treatment equality} : Treatment equality considers the ratio of False Negative Predictions (\\fnr) to False Positive Predictions. \n\\begin{align}\n  \\frac{Pr(\\hat{y}=1 |y=0 \\& g_i)}{Pr(\\hat{y}=0 |y=1 \\& g_i)} &= \\frac{Pr(\\hat{y}=1 |y=0 \\& g_j)}{Pr(\\hat{y}=0 |y=1 \\& g_j)}\n\\end{align}\n\\textbf{Equalizing disincentives} : The Equalizing disincentives metric compares the difference of two metrics, \\tpr and \\fpr, across the groups and is specified as:\n\\begin{align}\n   Pr(\\hat{y}=1 |y=1 \\& g_i) - Pr(\\hat{y}=1 |y=0 \\& g_i) &= Pr(\\hat{y}=1 |y=1 \\& g_j) - Pr(\\hat{y}=1 |y=0 \\& g_j)\n\\end{align}\n\\textbf{Conditional Equal Opportunity} : As some metrics can be dependent on the underlying data distribution,  provide an additional metric that specifies equal opportunity on a specific attribute $a$ our of a list of attributes $A$, where $\\tau$ is a threshold value:\n\\begin{align}\n   Pr(\\hat{y}\\geq \\tau |g_i \\& y<\\tau \\& A=a) &= Pr(\\hat{y}= \\geq \\tau | g_j \\& y<\\tau \\& A=a)\n\\end{align}", "cites": [6340, 6341, 3899, 8702, 3937], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a clear, descriptive overview of confusion matrix-based fairness metrics, including definitions and mathematical formulations for each. While it references cited papers, it does so in a limited way, primarily to support the presentation of concepts rather than to synthesize or contrast their contributions. There is minimal critical evaluation or abstraction beyond the presentation of individual metrics."}}
{"id": "d099f9ac-57f3-49a5-9745-2ab4f3991b88", "title": "Calibration-based Metrics", "level": "subsubsection", "subsections": [], "parent_id": "df5cc326-2b88-46cc-86d0-cec9d2ab59f0", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Group Fairness Metrics"], ["subsubsection", "Calibration-based Metrics"]], "content": "\\hfill\\\\\nIn comparison to the previous metrics which are defined based on the predicted and actual values, \ncalibration-based metrics take the predicted probability, or score, into account. This is related to the Sufficiency criterion that was defined in Section \\ref{subsec:abstractfairnesscriteria}.\n\\textbf{Test fairness/ calibration / matching conditional frequencies} (, ): Essentially, test fairness or calibration wants to guarantee that the probability of $y=1$ is the same given a particular score. I.e., when two people from different groups get the same predicted score, they should have the same probability of belonging to $y=1$. \n\\begin{align}\n   Pr(y=1 | S=s \\& g_i) &= Pr(y=1 | S=s \\& g_j) \n\\end{align}\n\\textbf{Well calibration} : An extension of regular calibration where the probability for being in the positive class also has to equal the particular score.\n\\begin{align}\n   Pr(y=1 | S=s \\& g_i) &= Pr(y=1 | S=s \\& g_j) = s\n\\end{align}", "cites": [3900, 3899, 8704], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of calibration-based fairness metrics, referencing three relevant papers but without deeply connecting or synthesizing their contributions. It lacks critical evaluation or comparison of the cited works and does not generalize beyond the specific definitions to highlight broader principles or trends in fairness measurement."}}
{"id": "0f3e3404-2224-4e9d-95ec-91f5f462a08b", "title": "Score-based Metrics", "level": "subsubsection", "subsections": [], "parent_id": "df5cc326-2b88-46cc-86d0-cec9d2ab59f0", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Group Fairness Metrics"], ["subsubsection", "Score-based Metrics"]], "content": "\\hfill\\\\\n\\textbf{Balance for positive and negative class} : The expected predicted score for the positive and negative class has to be equal for all groups:\n\\begin{align}\n   E(S=s | y=1 \\& g_i) &= E(S=s | y=1 \\& g_j), E(S=s | y=0 \\& g_i) = E(S=s | y=0 \\& g_j)\n\\end{align}\n\\textbf{Bayesian Fairness}  extend the balance concept from  when model parameters themselves are uncertain. Bayesian fairness considers scenarios where the expected utility of a decision maker has to be balanced with fairness of the decision. The model takes into account the probability of different scenarios (model parameter probabilities) and the resulting fairness / unfairness.", "cites": [9002, 3900], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of score-based fairness metrics, particularly focusing on the concept of Bayesian fairness. It synthesizes the key idea from the cited paper by extending the balance concept into a probabilistic framework. However, the critical analysis is limited, as it does not evaluate the strengths or weaknesses of the approach or contrast it with others. The abstraction is moderate, as it generalizes the idea of balance under uncertainty but does not elevate it to a meta-level principle."}}
{"id": "b64a2657-0180-4a07-9c66-ece5e19834b2", "title": "Individual and Counterfactual Fairness Metrics", "level": "subsection", "subsections": [], "parent_id": "d6a04f1e-4dd6-4921-8777-ee2894367ea1", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Individual and Counterfactual Fairness Metrics"]], "content": "As compared to group-based metrics which compare scores across different groups, individual and counterfactual fairness metrics do not focus on comparing two or more groups as defined by a sensitive variable, but consider the outcome for each participating individual.  propose the concept of counterfactual fairness which builds on causal fairness models and is related to both individual and group fairness concepts.  proposes a generalized entropy index which can be parameterized for different values of $\\alpha$ and measures the individual impact of the classification outcome. This is similar to established distribution indices such as the Gini Index in economics.\n\\textbf{Counterfactual Fairness}: Given a causal model $(U,V,F)$, where $U$ are latent (background) variables, $V = S \\cup X$ are observable variables including the sensitive variable $S$, and $F$ is a set of functions defining structural equations such that $V$ is a function of $U$, counterfactual fairness is:\n\\begin{align} \n    P(\\hat{y}_{A \\leftarrow a}(U) = y | X = x, A=a ) = P(\\hat{y}_{A \\leftarrow a^{'}}(U) = y | X = x, A=a )\n\\end{align}\nEssentially, the definition ensures that the prediction for an individual coincides with the decision if the sensitive variable would have been different.\n\\textbf{Generalized Entropy Index}:  defines the Generalized Entropy Index (GEI) which considers differences in an individual's prediction ($b_i$) to the average prediction accuracy ($\\mu$). It can be adjusted based on the parameter $\\alpha$, where $b_i=\\hat{y}_i-y_i+1$ and $\\mu=\\frac{\\sum_i b_i}{n}$: \n\\begin{align}\n    GEI = \\frac{1}{n\\alpha(\\alpha-1)}\\sum_{i=1}^{n}\\left[ (\\frac{b_i}{\\mu})^\\alpha -1\\right]\n\\end{align}\n\\textbf{Theil Index}: a special case of the GEI for $\\alpha = 1$. In this case, the calculation simplifies to:\n\\begin{align}\n    Theil = \\frac{1}{n}\\sum_{i=1}^{n}(\\frac{b_i}{\\mu})log(\\frac{b_i}{\\mu}))\n\\end{align}", "cites": [3905], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of individual and counterfactual fairness metrics, including definitions and mathematical formulations. However, it lacks synthesis of ideas from the cited papers, critical evaluation of their strengths or limitations, and broader abstraction or pattern identification across works. The presentation is informative but not insight-driven."}}
{"id": "f9a75db4-9343-48cf-85fe-a6959dc9a310", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "d6a04f1e-4dd6-4921-8777-ee2894367ea1", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Measuring Fairness and Bias"], ["subsection", "Summary"]], "content": "The literature is at odds with respect to whether individual or group fairness should be prioritized.  note that many approaches to group fairness tackle only between-group issues, as a consequence they demonstrate that within-group issues are worsened through this choice. Consequently, users must decide on where to place emphasis, but be mindful of the trade off between any fairness measure and model accuracy . With a reliance on expressing fairness and bias mathematically,  argue that these definitions often do not map to normative social, economic, or legal understandings of the same concepts. This is corroborated by  who note an over emphasis in the literature on disparate treatment.  criticize ad hoc and implicit choices concerning distributional assumptions or realities of relative group sizes.", "cites": [6043, 6342, 8703, 3899, 3903, 3912, 6343], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by highlighting tensions in the literature, such as the trade-off between individual and group fairness, and between fairness and model accuracy. While it integrates several papers to present a coherent narrative on these themes, it lacks deeper synthesis or a novel framework. Critical analysis is present but somewhat limited to identifying conceptual mismaps and methodological issues rather than offering a comprehensive evaluation of the approaches."}}
{"id": "e98aee45-e1e2-4474-932f-2ab784fc5142", "title": "Binary Classification Approaches", "level": "section", "subsections": [], "parent_id": "f62ce4ec-ed89-4ca6-9540-c6a7826047e5", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Binary Classification Approaches"]], "content": "\\label{sec:approaches}\nBuilding on the metrics discussed in \\autoref{sec:metrics}, fairness in \\ml researchers seek to mitigate unfairness by ``protecting'' sensitive sociodemographic attributes (as introduced in \\autoref{sec:variables}). The literature is dominated by approaches for mitigating bias and unfairness in \\ml within the problem class of binary classification . There are many reasons for this, but most notably: 1) many of the most contentious application areas that motivated the domain are binary decisions (hiring vs. not hiring; offering a loan vs. not offering a loan; re-offending vs. not re-offending etc.); 2) quantifying fairness on a binary dependent variable is mathematically more convenient; addressing multi-class problems would at the very least add terms in the fairness quantity. \n\\input{approaches/forest}\nIn this section, we discuss the main approaches for tackling fairness in the binary classification case. We begin by arranging mitigation methods into a visual taxonomy according to the location in the \\ml framework (\\autoref{fig:components}), i.e. pre-processing: \\autoref{fig:pre}, in-processing: \\autoref{fig:inproc}, and post-processing: \\autoref{fig:postproc}. We note an abundance of pre- and in-processing vs. post-processing methods and that method families, e.g. methods leveraging adversarial learning, can belong to multiple stages (pre- and in-processing in this case). It is also noteworthy that many of the approaches listed (i.e. the overall mitigation strategy applied by researchers) do not belong to a single category or stage, but several: approaches tend to be hybrid and this is becoming more prevalent in more recent approaches. However, we are yet to find an approach using methods from all three stages, even if there are papers comparing methods from multiple stages. Finally, we also note that we do not comment on the advantages of specific approaches over others, yet where relevant we outline challenges researchers must navigate. The literature has an urgent need for a structured meta review of approaches to fairness. Whilst many papers compare specific subsets of the approaches in this section, they do not and realistically cannot offer a holistic comparison. \n\\input{approaches/blinding}\n\\input{approaches/causalmethods}\n\\input{approaches/sampling}\n\\input{approaches/transformation}\n\\input{approaches/relabelling}\n\\input{approaches/reweighing}\n\\input{approaches/regularisation}\n\\input{approaches/adverserial}\n\\input{approaches/bandits}\n\\input{approaches/calibration}\n\\input{approaches/thresholding}", "cites": [3912], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of binary classification approaches to fairness in ML, organizing methods into pre-, in-, and post-processing categories. While it acknowledges the hybrid nature of some approaches and points to the need for a structured meta-review, it does not deeply synthesize, critically evaluate, or abstract broader principles from the cited works. The section remains largely expository with limited analytical depth."}}
{"id": "49073552-a7bd-4b89-9ac5-8b51ee217b64", "title": "Beyond Binary Classification", "level": "section", "subsections": [], "parent_id": "f62ce4ec-ed89-4ca6-9540-c6a7826047e5", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Beyond Binary Classification"]], "content": "\\label{sec:beyond}\nThe bulk of the fairness literature focuses on binary classification  . In this section, we provide an overview and discussion beyond approaches for binary classification (albeit less comprehensive) and note that there is a sufficient need for fairness researchers to also focus on other \\ml problems. \n\\input{approaches/regression}\n\\input{approaches/recsys}\n\\input{approaches/unsupervised}\n\\input{approaches/nlp}", "cites": [3912], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited work, merely mentioning one paper on fair regression without connecting it to other cited areas like recommender systems, unsupervised learning, or NLP. There is no critical evaluation of the methods or limitations discussed, and the section lacks abstraction or broader conceptual insights. It serves more as a placeholder for additional content rather than a coherent, insightful discussion."}}
{"id": "aff97c9d-314d-44b1-8eaf-81201ff720e7", "title": "Current Platforms", "level": "section", "subsections": [], "parent_id": "f62ce4ec-ed89-4ca6-9540-c6a7826047e5", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Current Platforms"]], "content": "\\label{sec:platforms}\nWhile many researchers publish their individual approaches on github and similar platforms, a few notable projects have emerged that address fairness in \\ml from a more general perspective. \\autoref{tbl:platformoverview} describes some of these approaches. We note the existence of proprietary software, yet here emphasize readily tools to discuss the current state of the art for \\ml researchers and practitioners. \n\\begin{table}[h]\n    \\scriptsize\n    \\centering\n    \\begin{tabularx}{\\textwidth}{p{.2\\textwidth}@{\\hskip 0.15in}X}\n        \\toprule\n         Project &  Features   \\\\%\\\\\\hline\n        \\midrule\n        AIF360  &  Set of tools that provides several pre-, in-, and post-processing approaches for binary classification as well as several pre-implemented datasets that are commonly used in Fairness research \n        \\\\\n        Fairlean\\footnotemark & Implements several parity-based fairness measures and algorithms  for binary classification and regression as well as a dashboard to visualize disparity in accuracy and parity. \n        \\\\\n        Aequitas   &  Open source bias audit toolkit. Focuses on standard \\ml metrics and their evaluation for different subgroups of a protective attribute.\n        \\\\\n        Responsibly   & Provides datasets, metrics, and algorithms to measure and mitigate bias in classification as well as NLP (bias in word embeddings). \n        \\\\\n        Fairness\\footnotemark &  Tool that provides commonly used fairness metrics (e.g., statistical parity, equalized odds) for R projects.  \n        \\\\\n        FairTest   & Generic framework that provides measures and statistical tests to detect unwanted associations between the output of an algorithm and a sensitive attribute.  \n        \\\\\n        Fairness Measures\\footnotemark &  Project that considers quantitative definitions of discrimination in classification and ranking scenarios. Provides datasets, measures, and algorithms (for ranking) that investigate fairness.  \n        \\\\ \n        Audit AI\\footnotemark & Implements various statistical significance tests to detect discrimination between groups and bias from standard machine learning procedures.  \n        \\\\\n        Dataset Nutrition Label  & Generates qualitative and quantitative measures and descriptions of dataset health to assess the quality of a dataset used for training and building \\ml models. \n        \\\\\n        ML Fairness Gym  & Part of Google's Open AI project, a simulation toolkit to study long-run impacts of ML decisions.\\footnotemark   Analyzes how algorithms that take fairness into consideration change the underlying data (previous classifications) over time (see e.g. ). \n        \\\\\n        \\bottomrule        \n    \\end{tabularx}\n    \\caption{Overview of projects addressing Fairness in Machine Learning.}\n    \\label{tbl:platformoverview}\n    \\tightentab\n\\end{table}\n\\addtocounter{footnote}{-5}\n\\footnotetext{\\url{https://github.com/fairlearn/fairlearn}}\n\\addtocounter{footnote}{1}\n\\footnotetext{\\url{https://github.com/kozodoi/Fairness}}\n\\addtocounter{footnote}{1}\n\\footnotetext{\\url{https://github.com/google-research/tensorflow_constrained_optimization}}\n\\addtocounter{footnote}{1}\n\\footnotetext{\\url{https://github.com/pymetrics/audit-ai}}\n\\addtocounter{footnote}{1}\n\\footnotetext{\\url{http://www.fairness-measures.org/}, \\url{https://github.com/megantosh/fairness_measures_code/tree/master}}\n\\addtocounter{footnote}{1}\n\\footnotetext{\\url{https://github.com/google/ml-fairness-gym}} \n\\addtocounter{footnote}{1}\n\\label{sec:challenges}", "cites": [3930, 6347, 1204, 6344, 6346, 3899, 6039, 6043, 3902, 3924, 6345, 3892], "cite_extract_rate": 0.9230769230769231, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive overview of fairness-focused platforms, listing features and linking them to their repositories. It integrates limited information from the cited papers, often only mentioning their titles or abstract concepts like 'statistical parity' without deeper synthesis. There is minimal critical evaluation or abstraction to broader themes or principles."}}
{"id": "f9ff220a-ff92-49e0-bf46-530f799fa035", "title": "Dilemma 1: Fairness vs. Model Performance", "level": "subsection", "subsections": [], "parent_id": "d4a5fc0d-3bca-4b19-a5e4-f2a96a7ce547", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Concluding Remarks: The Fairness Dilemmas"], ["subsection", "Dilemma 1: Fairness vs. Model Performance"]], "content": "\\label{sec:tradeoff}\nA lack of consideration for the sociocultural context of the application can result in \\ml solutions that are biased, unethical, unfair, and often not legally permissible . The \\ml community has responded with a variety of mechanisms to improve the fairness of models as outlined in this article. However, when implementing fairness measures, we must emphasize either fairness or model performance as improving one can often detriment the other .  do, however, note that a reduction in accuracy may in fact be the desired result, if it was discrimination in the first place that raised accuracy. Note that even prior to recognizing this trade-off, we need to be cautious in our definition of model performance. \\ml practitioners can measure performance in a multitude of ways, and there has been much discussion concerning the choice of different performance measures and approaches . The choice of performance measure(s) itself may even harbor, disguise, or create new underlying ethical concerns. We also note that currently, there is little runtime benchmarking of methods outside of clustering approaches (see ). This is an observation as opposed to a criticism, but we note that potential users of fairness methods will likely concern themselves with computational costs, especially if they increase.", "cites": [8703, 3899, 3925, 8702, 3903, 6343], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the fairness vs. model performance dilemma, drawing on multiple cited papers to highlight the conceptual and practical tensions involved. It synthesizes ideas about the trade-off and the need for careful performance metric selection. While it offers some critical reflection on how accuracy might sometimes reflect discrimination, the analysis remains at a high level without deeper comparative evaluation or detailed critique of specific approaches."}}
{"id": "29123ce1-859d-4eef-be88-84b940d2d607", "title": "Dilemma 2: (Dis)agreement and Incompatibility of ``Fairness''", "level": "subsection", "subsections": [], "parent_id": "d4a5fc0d-3bca-4b19-a5e4-f2a96a7ce547", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Concluding Remarks: The Fairness Dilemmas"], ["subsection", "Dilemma 2: (Dis)agreement and Incompatibility of ``Fairness''"]], "content": "\\label{sec:balance}\nOn top of the performance trade-off, there is no consensus in literature whether individual or group fairness should be prioritized. Fairness metrics usually either emphasize individual or group fairness, but cannot combine both  .  also note that many approaches to group fairness often tackle between-group issues, as a consequence they demonstrate that within-group issues are worsened through this choice. To further complicate things,  argue that with a reliance on expressing fairness mathematically these definitions often do not map to normative social, economic, or legal understandings of the same concepts. This is corroborated by  who note an over-emphasis in the literature on specific measures of fairness and insufficient dialogue between researchers and affected communities. Thus, improving fairness in \\ml is challenging and simultaneously there are many different notions for researchers and practitioners to navigate. Further adding to this discussion is the notion of differing views of the root(s) of fairness and bias.  study the differing views of people in this regard and observe that this is not a trivial challenge to address. E.g.,  notes that women have differing views in the inclusion / exclusion of gender as a protected variable to men.  note that a similar discussion was left unresolved in the early days of fairness research in the context of test scores and employment/hiring practices, indicating that this is one of the main challenges of \\ml fairness research in the future.  have noted that this dilemma can be articulated as a bias in, bias out property of \\ml: i.e. addressing one form of bias results in another. \nThus, the community as articulated in  needs to explore ways to either handle combinations of fairness metrics, even if only approximately due to specific incompatibilities, or implement a significant meta review of measures to help categorise specific differences, ideological trade-offs, and preferences. This will enable researchers and practitioners to consider a balance of the fairness measures they are using. This is a challenging undertaking and whilst the tools discussed in \\autoref{sec:platforms} go some way to facilitate this, there is a need for more general toolkits and methodologies for comparing fairness approaches. We note a number of comparative studies, i.e. , but these only scratch the surface.", "cites": [8151, 6039, 6006, 6342, 6348, 3901, 6337, 6351, 6349, 6350, 8704], "cite_extract_rate": 0.6111111111111112, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 4.3}, "insight_level": "high", "analysis": "The section synthesizes a range of cited works to articulate a coherent narrative around the incompatibility of different fairness notions in ML. It critically evaluates the limitations of current mathematical fairness definitions and highlights the need for a more nuanced, community-informed approach. The analysis abstracts from specific papers to identify broader challenges, such as the 'bias in, bias out' property and the ideological trade-offs in fairness metric selection."}}
{"id": "7a75f3b7-2a96-4c33-8984-e3464bf5067f", "title": "Dilemma 3: Tensions with Context and Policy", "level": "subsection", "subsections": [], "parent_id": "d4a5fc0d-3bca-4b19-a5e4-f2a96a7ce547", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Concluding Remarks: The Fairness Dilemmas"], ["subsection", "Dilemma 3: Tensions with Context and Policy"]], "content": "\\label{sec:context}\nThe literature typically hints toward ``optimizing'' fairness without transparency of the root(s) of (un)fairness  rarely extending beyond ``(un)fair''  typically to mirror current legal thought . This is true for both metrics and methods. As such, platforms are needed to assist practitioners in ascertaining the cause(s) of unfairness and bias. However, beyond this, critics of current research  argue that efforts will fail unless contextual, sociocultural, and social policy challenges are better understood. Thus, there is an argument that instead of striving  to ``minimize'' unfairness, more awareness of context-based aspects of discrimination is needed. There is the prevalent assumption that ``unfairness'' has a uniform context-agnostic egalitarian valuation function for decision makers when considering different (sub)populations . This suggests a disconnect between organizational realities and current research, which undermines advancements . Other suggestions have been for \\ml researchers and practitioners to better understand the limitations of human decision making .  \nIt is easy to criticize, however, the underlying challenge is availability of realistic data. Currently, the literature relies unilaterally on convenience datasets (enabling comparative studies), often from the UCI repository  or similar with limited industry context and engagement  .  note that there is an additional challenge in the datasets used to train models: data represent past decisions, and as such inherent bias(es) in these decisions are amplified. This is a problem referred to as selective labels . Similarly, there may be differences in the distribution(s) of the data between the data the model is trained on, and deployed on: dataset shift as discussed by . As such, data context cannot be disregarded. \nThus, researchers need to better engage with (industry) stakeholders to study models in vivo and engage proactively in open debate on policy and standardization. This is a hard problem to solve: companies cannot simply hand out data to researchers and researchers cannot fix this problem on their own. There is a tension here between advancing the fairness state of the art, privacy , and policy.  notes that policy makers are generally not considered or involved in the \\ml fairness domain. We are seeing an increasing number of working groups on best practices for ethics, bias, and fairness, where Ireland's NSAI/TC 002/SC 18 Artificial Intelligence working group, the IEEE P7003 standardization working group on algorithmic bias, and the Big Data Value Association are just three examples of many, but this needs to be pushed harder at national and international levels by funding agencies, policy makers, and researchers themselves.", "cites": [3935, 6353, 3903, 7621, 6333, 6342, 3907, 6352], "cite_extract_rate": 0.32, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to highlight the tension between algorithmic fairness and real-world context, particularly sociocultural and policy factors. It critically engages with the idea of context-agnostic fairness metrics and emphasizes the limitations of current datasets and practices. The discussion abstracts beyond individual papers to identify broader systemic issues in data availability and stakeholder engagement."}}
{"id": "6ecccda2-2d6f-4354-a223-a78deadb21de", "title": "Dilemma 4: Democratisation of ML vs the Fairness Skills Gap", "level": "subsection", "subsections": [], "parent_id": "d4a5fc0d-3bca-4b19-a5e4-f2a96a7ce547", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Concluding Remarks: The Fairness Dilemmas"], ["subsection", "Dilemma 4: Democratisation of ML vs the Fairness Skills Gap"]], "content": "\\label{sec:skills}\nToday, \\ml technologies are more accessible than ever. This has occurred through a combination of surge in third level courses and the wide availability of \\ml tools such as WEKA , RapidMiner\\footnote{\\url{https://rapidminer.com}}, and SPSS Modeler\\footnote{\\url{https://www.ibm.com/ie-en/products/spss-modeler}}. Alternatively, Cloud-based solutions such as Google's Cloud AutoML , Uber AI's Ludwig,\\footnote{https://uber.github.io/ludwig/} and Baidu's EZDL\\footnote{\\url{https://ai.baidu.com/ezdl/}} remove the need to even run models locally. The no-/low-code \\ml movement is arguably enabling more  companies to adopt \\ml technologies.\nIn addition, there is a growing trend in the use of Automated Machine Learning (AutoML)  to train \\ml models. AutoML abstracts much of the core methodological expertise (e.g., KDD , and CRISP-DM ) by automated feature extraction and training multiple models often combining them into an ensemble of models that maximizes a set of performance measures. Collectively, each of these advancements positively democratizes \\ml, as it means lower barriers of use: ``push button operationalization''  with online marketplaces\\footnote{E.g.: Amazon's \\url{https://aws.amazon.com/marketplace/solutions/machinelearning/} and Microsoft's \\url{https://gallery.azure.ai} ML Marketplaces.} selling third party \\ml solutions. \nLowering the entry barrier to \\ml through democratization will (if it hasn't already) mean an increase in (un)intentional socially insensitive uses of \\ml technologies. The challenge is that \\ml application development follows a traditional software development model: it is modular, sequential, and based on large collections of (often) open source libraries, but methods to highlight bias, fairness, or ethical issues assume high expertise in \\ml development and do not consider ``on-the-street'' practitioners . This was our motivation in writing this survey. However, the fairness domain is only just starting to provide open source tools available for practitioners (\\autoref{sec:platforms}). Yet, in general there is little accommodation for varying levels of technical proficiency, and this undermines current advancement . There is a tension between educational programs (as called for in ) and the degree of proficiency needed to apply methods and methodologies for fair \\ml.  have advocated this as the formalization of exploratory fairness analysis: similar to exploratory data analysis, yet for informed decision making with regard to ``fair'' methodological decisions. Similarly,  call for core \\ml educational resources and courses to better include ethical reasoning and deliberation and provide an overview of potential materials. Thus, the fourth dilemma currently facing the fair \\ml domain is its own democratization to keep up with the pace of \\ml proliferation across sectors. This means a shift in terms of scientific reporting, open source comprehensive frameworks for repeatable and multi-stage (i.e., pipelined models) decision making processes where one model feeds into another . Currently under-addressed is bias and fairness transitivity: where one \\ml model is downstream to another.", "cites": [6342, 6354], "cite_extract_rate": 0.11764705882352941, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two papers to discuss the broader issue of the fairness skills gap amid ML democratization, connecting them to the theme of accessible fair ML practices. It provides a critical perspective by highlighting limitations in current fairness tools and educational resources, though the critique is not deeply nuanced. The section abstracts the issue into a meta-level dilemma but stops short of offering a comprehensive new framework."}}
{"id": "703633f7-f5f0-45ce-8780-918f503d2751", "title": "Concluding Remarks", "level": "subsection", "subsections": [], "parent_id": "d4a5fc0d-3bca-4b19-a5e4-f2a96a7ce547", "prefix_titles": [["title", "Fairness in Machine Learning: A Survey"], ["section", "Concluding Remarks: The Fairness Dilemmas"], ["subsection", "Concluding Remarks"]], "content": "The literature almost unilaterally focuses on supervised learning with an overwhelming emphasis on binary classification : diversification is needed. With very few exceptions, the approaches discussed in this article operate on the assumption of some set of (usually a priori known) ``protected variables''. This doesn't help practitioners. Tools potentially based on causal methods (\\autoref{sec:repair}) are needed to assist in the identification of protected variables and groups as well as their proxies. \nMore realistic datasets are needed:  argue that approaches tend to operate on too small a subset of features raising stability concerns. This should go hand in hand with more industry-focused training. Tackling fairness from the perspective of protected variables or groups needs methodological care, as ``fixing'' one set of biases may inflate another  rendering the model as intrinsically discriminatory as a random model .  There is also the risk of redlining, where although the sensitive attribute is ``handled'' sufficiently, correlated variables are still present , amplifying instead of reducing unfairness . \nWe also note specific considerations of pre-processing vs. in-processing vs. post-processing interventions. Pre-processing methods, which modify the training data, are at odds with policies like \\gdpr 's right to an explanation, and can introduce new subjectivity biases . They also assume sufficient knowledge of the data, and make assumptions over its veracity . Uptake of in-processing approaches requires better integration with standard \\ml libraries to overcoming  porting challenges.  noted that generally post-processing methods have suboptimal accuracy compared to other ``equally fair'' classifiers, with  noting that often test-time access to protected attributes is needed, which may not be legally permissible, and have other undesirable effects .\nAs a closing thought many approaches to reduce discrimination may themselves be unethical or impractical in settings where model accuracy is critical such as in healthcare, or criminal justice scenarios . This is not to advocate that models in these scenarios should be permitted to knowingly discriminate, but rather that a more concerted effort is needed to understand the roots of discrimination.  Perhaps, as  note, it may often be better to fix the underlying data sample (e.g. collect more data, which better represents minority or protected groups and delay the modeling phase) than try to fix a discriminatory \\ml model.\n\\label{sec:conclusion}\n\\bibliographystyle{plainnat}\n\\bibliography{refs}\n\\end{document}\n\\endinput", "cites": [3907, 3935, 6350, 6347, 8703, 6043, 3912, 3902, 3937], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key insights from the cited works to highlight limitations and dilemmas in fairness research, particularly in the context of supervised learning and protected variables. It offers critical evaluation of the trade-offs between different fairness approaches and their practical and ethical implications. The abstraction level is strong, as it generalizes findings to broader concerns such as model deployment, data collection, and long-term societal impact."}}
