{"id": "8e64701a-213e-4b68-9539-a0d24a8c4e0f", "title": "Introduction", "level": "section", "subsections": ["be808f8d-48ba-4fb9-acdb-0d9ad75ed802", "790807fe-dc89-49b7-a024-2c449d535b42", "5ebe623d-a5a3-4db3-9c6d-dde5f719b3d4"], "parent_id": "be806312-3b5b-4083-ab29-bf4d337ae115", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{T}{o} date, deep learning  has substantially promoted the development of computer vision. People are no longer satisfied with simple visual understanding tasks such as object detection and recognition. Higher-level visual understanding and reasoning tasks are often required to capture the relationship between objects in the scene as a driving force. The emergence of scene graphs is precisely to solve this problem. Scene graphs were first proposed  as a data structure that describes the object instances in a scene and the relationships between these objects. A complete scene graph can represent the detailed semantics of a dataset of scenes, but not a single image or a video; moreover, it contains powerful representations that encode 2D/3D images  and videos  into their abstract semantic elements without restricting either the types and attributes of objects or the relationships between objects. \nRelated research into scene graphs greatly promotes the understanding of various tasks such as vision, natural language, and their cross-domains.\nAs early as 2015, the idea of utilizing the visual features of different objects contained in the image and the relationships between them was proposed as a means of achieving a number of visual tasks, including action recognition , image captioning  and other relevant computer vision tasks . Visual relationship mining has been demonstrated to significantly improve the performance of related visual tasks, as well as to effectively enhance people's ability to understand and reason about visual scenes. Subsequently, visual relationship was incorporated into scene graph theory by Johnson et al. , in which the definition of scene graphs was formally provided. In , a scene graph is generated manually from a dataset of real-world scene graphs, enabling the detailed semantics of a scene to be captured. Since then, the research on scene graphs has received extensive attention .\nAt present, the work related to scene graph generation (SGG) is explosively increasing, but there is a lack of a comprehensive and systematic survey of SGG. In order to fill this gap, we will mainly review the methods and applications of SGG. Fig.~\\ref{fig:SGG_statistics} shows the main structure of our survey. In addition, in Section \\ref{sec:Datasets and performance evaluation}, we summarize the datasets and evaluation methods commonly used in scene graphs and compare the performance of the models. In Section \\ref{sec:FutureDirection}, we discuss the future direction of SGG. Finally, we present our concluding remarks in Section \\ref{sec:Conclusion}.\n\\begin{figure} \n  \\centering\n  \\includegraphics[width=0.45\\textwidth]{figure/SGGMethodsApplication}\n  \\vspace{-1 em}\n  \\caption{A classification of the methods and applications of SGG.}\n  \\vspace{-1.5 em}\n  \\label{fig:SGG_statistics}\n\\end{figure} \n\\vspace{-1em}", "cites": [2329, 6411, 6415, 5516, 6413, 6412, 6414], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction provides a general overview of scene graphs and their role in advancing visual understanding, with minimal synthesis or integration of the cited papers. It mentions a few papers in passing but does not connect their ideas in a coherent or insightful manner. There is no critical evaluation or comparison of approaches, and no broader conceptual or theoretical abstraction is offered beyond basic definitions and applications."}}
{"id": "790807fe-dc89-49b7-a024-2c449d535b42", "title": "Construction Process", "level": "subsection", "subsections": [], "parent_id": "8e64701a-213e-4b68-9539-a0d24a8c4e0f", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Introduction"], ["subsection", "Construction Process"]], "content": "\\label{sec:Construction process}\n\\textcolor{black}{\nReferring to the expression in , a general SGG process is shown in Fig.~\\ref{fig:dog_ride_surfboard}. Fig.~\\ref{fig:dog_ride_surfboard}(left) shows an abstract representation of this SGG process, and Fig.~\\ref{fig:dog_ride_surfboard}(right) shows a concrete example. Specifically, node $I$ represents a given image, node $X$ represents the feature of the object, and node $Z$ represents the category of the object. The node $Y$ represents the category of the prediction predicate and the corresponding triple $\\langle s-p-o \\rangle$, which uses a fusion function to receive the output from the three branches to generate the final logits. The node $Y$ represents the true triplet label. The explanation of the corresponding link is as follows:\\\\\n\\textbf{Link $I\\rightarrow X$ (Object Feature Extraction).} A pre-trained Faster R-CNN  is often used to extract a set of bounding boxes $B=\\{b_i|i=1,...,m\\}$ and the corresponding feature maps $X=\\{x_i|i=1,...,m\\}$ of the input image $I$. This process can be expressed as:\n\\begin{equation}\n    Input:\\{I\\}\\Rightarrow Output:\\{x_i|i=1,...,m\\}.\n\\end{equation}\nThrough this process, the visual context is encoded for each object.\\\\\n\\textbf{Link $X\\rightarrow Z$ (Object Classification).} This process can be simply expressed as:\n\\begin{equation}\n    Input:\\{x_i\\}\\Rightarrow Output:\\{z_i, z_i\\in O\\}, i=1,...,m.\n\\end{equation}\n\\textbf{Link $Z\\rightarrow \\widetilde{Y}$ (Object Class Input for SGG).} The paired object label $(z_i,z_j)$ is used, and the predicate $\\widetilde{y}_{ij}$ between the object pair is predicted by a combined embedding layer $M$. This process can be expressed as:\n\\begin{equation}\n    Input:\\{(z_i,z_j)\\} \\overset{M}{\\Longrightarrow} Output:\\{\\widetilde{y}_{ij}\\}, i\\neq j; i,j=1,...,m.\n\\end{equation}\nSome prior knowledge (e.g., language prior  and statistical prior ) can be calculated in this link.\\\\\n\\textbf{Link $X\\rightarrow \\widetilde{Y}$ (Object Feature Input for SGG).} The combination $[x_i,x_j]$ of paired object features is used as input to predict the corresponding predicate. This process can be expressed as:\n\\begin{equation}\n    Input:\\{[x_i,x_j]\\} \\Rightarrow Output:\\{\\widetilde{y}_{ij}\\}, i\\neq j; i,j=1,...,m.\n\\end{equation} The rich context information can be fully excavated in this link .\\\\\n\\textbf{Link $I\\rightarrow \\widetilde{Y}$ (Visual Context Input for SGG).} The visual context feature $v_{ij}=Convs(RoIAlign(I,b_i\\bigcup b_j))$  of the joint region $b_i\\bigcup b_j$ is extracted in this link and predicts the corresponding triplet. This process can be expressed as:\n\\begin{equation}\n    Input:\\{v_{ij}\\} \\Rightarrow Output:\\{\\widetilde{y}_{ij}\\}, i\\neq j; i,j=1,...,m.\n\\end{equation}\n\\textbf{Training Loss.} Most of the models are trained by using the conventional cross-entropy losses of the object label and the predicate label. In addition, to avoid any single link spontaneously dominating\nthe generation of logits $\\widetilde{Y}$,  add auxiliary cross-entropy losses that individually predict\n$\\widetilde{Y}$ from each branch. Further,  improve the training loss from the perspective of the structure of the output space.}\n\\textcolor{black}{More concise, the generation of scene graphs can be roughly divided into three parts: feature extraction, contextualization, and graph construction and reasoning.\n\\begin{itemize}\n    \\item \\textit{Feature extraction}. This process is mainly responsible for encoding objects or object pairs in the image. For example, Link $I\\rightarrow X$ and Link $I\\rightarrow \\widetilde{Y}$.\n    \\item \\textit{Contextualization}. It plays the role of associating different entities and is mainly used to enhance the contextual information between entities. For example, Link $I\\rightarrow X$, Link $Z\\rightarrow \\widetilde{Y}$, Link $X\\rightarrow \\widetilde{Y}$ and Link $I\\rightarrow \\widetilde{Y}$.\n    \\item \\textit{Graph construction and inference}. Finally, use these contextual information to predict the predicate and complete the construction and inference of the graph. For example, the prediction of node $\\widetilde{Y}$ label.\n\\end{itemize}}\nOn the other hand, as shown in Fig. \\ref{fig:SGG_Example} (a), from the perspective of the SGG process, the generation of scene graphs can be currently divided into two types . The first approach has two stages, namely object detection and pairwise relationship recognition . The first stage involved in identifying the categories and attributes of the detected objects is typically achieved using Faster-RCNN . This method is referred to as the \\textit{bottom-up} method.\nIt can be expressed in the following form:\n\\begin{equation}\n    P(SG|I)=P(B|I)*P(O|B,I)*P(R|B,O,I),\n    \\label{eq:bottom_up}\n\\end{equation}\nwhere $P(B|I)$, $P(O|B,I)$ and $P(R|B,O,I)$ represent the bounding box ($B$), object ($O$) and relational ($R$) prediction model, respectively.\nThe other approach involves jointly detecting and recognizing the objects and their relationships . This method is referred to as the \\textit{top-down} method. \nThe corresponding probability model can be expressed as:\n\\begin{equation}\n    P(SG|I)=P(B|I)*P(O,R|B,I),\n\\end{equation}\nwhere $P(O,R|B,I)$ represents the joint inference model of objects and their relationships based on the object region proposals.\nAt the high level, the inference tasks and other visual tasks involved include recognizing objects, predicting the objects' coordinates, and detecting/recognizing pairwise relationship predicates between objects . Therefore, most current works focus on the key challenge of reasoning the visual relationship.\n\\vspace{-1em}", "cites": [6421, 6419, 6417, 6422, 8714, 4016, 6418, 8155, 209, 267, 6423, 4013, 6424, 9022, 284, 6416, 2329, 6420, 9021, 520], "cite_extract_rate": 1.0, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a well-structured analytical overview of the scene graph generation (SGG) process by integrating multiple cited works. It synthesizes the construction pipeline across feature extraction, contextualization, and graph inference, while connecting different papers to illustrate technical approaches like bottom-up and top-down methods. It abstracts key components and challenges, such as training bias and feature interactions, but offers limited deep critique of individual methods despite identifying some limitations."}}
{"id": "cfa8afb0-5858-42f0-a294-1e923c5b96d6", "title": "CRF-based SGG", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "CRF-based SGG"]], "content": "In the visual relationship triples $\\langle s-r-o \\rangle$, a strong statistical correlation exists between the relationship predicate and the object pair. Effective use of this information can greatly aid in the recognition of visual relationships. The CRF (Conditional Random Field)  is a classical tool capable of incorporating statistical relationships into the discrimination task. CRF has been widely used in various graph inference tasks, including image segmentation , named-entity recognition  and image retrieval . In the context of visual relations, the CRF can be expressed as follows:\n\\begin{equation}\nP(r_{s\\rightarrow o}|X)=\\frac{1}{Z}exp(\\Phi(r_{s\\rightarrow o}|X;W)), X=x_r,x_s,x_o,\n\\end{equation}\nwhere $x_r$ refers to the appearance feature and spatial configuration of the object pair, $x_s$ and $x_o$ denote the appearance features of the subject and object respectively. \\textcolor{black}{In general, most of these features are one-dimensional tensors after ROI pooling, and have the size of $1\\times N$. $N$ is the dimension of tensors, and its specific value can be controlled by network parameters}. $W$ is the parameter of the model, $Z$ is a normalization constant, and $\\Phi$ represents the joint potential. Similar CRFs are widely utilized in computer vision tasks  and have been demonstrated effective in capturing statistical correlations in visual relationships. Fig.\\ref{fig:CRF} summarizes the basic structure of CRF-based SGG, which comprises two parts: object detection and relationship prediction. Object detection models are used to obtain the regional visual features of subjects and objects, while relationship prediction models predict the relationships between subjects and objects using their visual features. The other improved CRF-based SGG models achieved better performances by using more suitable object detection models and relation prediction models with stronger inference ability . For example, Deep Relational Network (DR-Net) and Semantic Compatibility Network (SCN) were proposed in  and .\nInspired by the success of deep neural networks  and CRF  models, to explore statistical relationships in the context of visual relationships, DR-Net  opts to incorporate statistical relationship modeling into the deep neural network framework. DR-Net unrolls the inference of relational modeling into a feedforward network. In addition, DR-Net is different from previous CRFs. More specifically, the statistical inference procedure in DR-Net is embedded in a deep relational network through iteration unrolling. The performance of the improved DR-Net is not only superior to classification-based methods but also better than deep potential-based CRFs. \nFurthermore, SG-CRF (SGG via Conditional Random Fields)  can be defined as maximizing the following probability function by finding the best prediction of $o_i,o_i^{bbox},r_{i\\rightarrow j}$:\n\\begin{equation}\n    P(SG|I)=\\prod_{o_i\\in O}P(o_i,o_i^{bbox}|I)\\prod_{r_{i\\rightarrow j\\in R}}P(r_{i\\rightarrow j}|I),\n    \\label{eq:EBM_0}\n\\end{equation}\nwhere $o_i^{bbox} \\in\\mathbb{R}^4$ represents the bounding box coordinate of the $i$-th object instance. It has been observed that some previous methods  tend to ignore the semantic compatibility (that is, the likelihood distribution of all 1-hop neighbor nodes of a given node) between instances and relationships, which results in a significant decrease in the model performance when faced with real-world data. For example, this may cause the model to incorrectly recognize $\\langle dog-sitting~inside-car \\rangle$ as $\\langle dog-driving-car \\rangle$. Moreover, these models ignore the order of the two, leading to confusion between subject and object, which may produce absurd predictions such as $\\langle car-sitting~inside-dog \\rangle$. To solve these problems, an end-to-end scene graph constructed via conditional random fields was proposed by SG-CRF to improve the quality of SGG. More specifically, to learn the semantic compatibility of nodes in the scene graph, SG-CRF proposes a new semantic compatibility network based on conditional random fields. To distinguish between the subject and object in the relationship, SG-CRF proposes an effective relation sequence layer that can capture the subject and object sequence in the visual relationship. \nIn general, the CRF-based SGG can effectively model the statistical correlation in the visual relationship. This statistically relevant information modeling remains a classic tool in visual relationship recognition tasks.\n\\begin{figure} \n    \\center{\\includegraphics[width=0.49\\textwidth] {figure/unsee_relation}} \n    \\vspace{-1.8 em}\n    \\caption{Examples of the sparsity and variability of visual relationships. \n    }\n    \\vspace{-1.2 em}\n    \\label{fig:unsee_relation}\n\\end{figure}\n\\vspace{-1em}", "cites": [1737, 6419, 97, 1745, 6425, 5257, 6427, 6426, 9021, 284], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes CRF-based SGG methods by connecting foundational CRF concepts (Paper 1, 4, 7) with specific visual relationship applications (Paper 2, 8). It provides a coherent narrative by highlighting how CRFs model statistical correlations and how models like DR-Net and SG-CRF improve upon earlier methods. The analysis includes critical insights, such as the importance of semantic compatibility and the confusion between subject-object order, and abstracts beyond individual papers to discuss general principles of modeling relationships in scene graphs."}}
{"id": "aa565ce5-2332-4a69-9352-c5670e10938e", "title": "TransE-based SGG", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "TransE-based SGG"]], "content": "\\label{sec:TransE-based SGG}\nThe knowledge graph is similar to the scene graph; it also has a large number of fact triples, and these multi-relational data are denoted in the form $(head,label,tail)$ (abbreviated as $(h,l,t)$). Among them, $h,t\\in O$ are the head entity and the tail entity respectively, while $l$ is the relationship label between the two entities. \\textcolor{black}{These entities are the objects in scene graph, so we use $O$ to denote the set of entities for avoiding confusion with $E$ (the edges between the objects)}. Knowledge graphs represent learning to embed triples into low-dimensional vector spaces, with TransE (Translation Embedding)-based  models having been demonstrated particularly effective. Furthermore, TransE regards the relationship as a translation between the head entity and the tail entity. The model is required to learn vector embeddings of entities and relationships. That is, for the tuple $(h,l,t)$, $h+l\\approx t$ ($t$ should be the nearest neighbor of $h+l$; otherwise, they should be far away from each other). This embedding learning can be achieved by minimizing the following margin-based loss function:\n\\begin{equation}\n\\mathcal{L}=\\sum_{(h,l,t)\\in S}\\sum_{(h',l,t')\\in S'_{(h,l,t)}}[\\gamma+d(h+l,t)-d(h'+l,t')]_+,\n\\end{equation}\nwhere $S$ represents the training set, $d$ is used to measure the distance between the two embeddings, $\\gamma$ is a marginal hyperparameter, $(h,l,t)$ is a positive tuple, $(h',l,t')$ is a negative tuple, and\n\\begin{equation}\nS'_{h,l,t}=\\{(h',l,t)|h'\\in \\mathbb{E}\\}\\cup \\{(h,l,t')|t'\\in O\\}.\n\\end{equation}\nThe relationship tuples in the scene graph also have similar definitions and attributes, meaning that the learning of this visual relationship embedding is also very helpful for the scene graph. \nInspired by the advances made by TransE in the relational representation learning of knowledge bases , VTransE  (based on TransE) explored how visual relations could be modeled by mapping the features of objects and predicates in low-dimensional space, and is the first TransE-based SGG method that works by extending TransE networks . Subsequently, as shown in Fig. \\ref{TransE}, attention mechanisms and visual context information are introduced for designing MATransE (Multimodal Attentional Translation Embeddings)  and UVTransE (Union Visual Translation Embedding)  respectively. Furthermore, TransD and analogy transformation are used to replace TransE for visual relationship prediction in RLSV (Representation Learning via Jointly Structural and Visual Embedding)   and AT (Analogies Transfer)  respectively.\n\\begin{figure} \n\\center{\\includegraphics[width=0.4\\textwidth] {figure/trans.png}} \n\\vspace{-.8 em}\n   \\caption{Relevant TransE-based SGG models.}\n\\vspace{-1.5 em}\n \\label{TransE}\n\\end{figure}\nMore specifically, VTransE maps entities and predicates into a low-dimensional embedding vector space, in which the predicate is interpreted as the translation vector between the embedded features of the subject and the object's bounding box regions. \\textcolor{black}{Similar to the tuple of knowledge graph, the relationship in scene graph is modeled as a simple vector transformation, i.e., $s + p \\approx o$.}\nIt can be consided as a basic vector transformation for TransE-based SGG methods. While this is a good start, VTransE considers only the features of the subject and the object, and not those of the predicate and context information , despite these having been demonstrated to be useful for the recognition of relations . To this end, MATransE , an approach based on VTransE, combines the complementary nature of language and vision , along with an attention mechanism  and deep supervision , to propose a multimodal attention translation embedding method. \\textcolor{black}{MATransE tries to learn the projection matrices $W_s, W_p$ and $W_o$ for a projection of $\\langle s, p, o \\rangle$ into a score space, and the binary masksâ€™ convolutional features $m$ are used in the attention module. Then, $s + p \\approx o$ becomes:} \n\\begin{equation}\n    W_s(s, o, m)s+W_p (s, o, m)p \\approx W_o(s, o, m)o.\n\\end{equation}\nMATransE designed two separate branches to deal directly with those of the predicate and the features of the subject-object, achieving good results.\nIn addition to drastically changing the visual appearance of the predicate, both the sparsity of the predicate representation in the training set  and the very large predicate feature space also make the task of visual relationship detection increasingly difficult. \nLet us take the Stanford VRD dataset  as an example. This dataset contains 100 classes of objects, 70 classes of predicates, and a total of $30k$ training relationship annotations. The number of possible $\\langle s-p-o \\rangle$ triplets is $100^2*70=700k$, which means that a large number of possible real relationships do not even have a training example. These invisible relationships should not be ignored, even though they are not included in the training set. Fig. \\ref{fig:unsee_relation} presents an example of this case.\nHowever, VTransE and MATransE are not well-suited to dealing with this issue. Therefore, the detection of unseen/new relationships in scenes is essential to the building of a complete scene graph. Inspired by VTransE , the goal of UVTransE  is to improve the generalization for rare or unseen relations.\nBased on VTransE, UVTransE introduces a joint bounding box \\textcolor{black}{or union feature $u$} of subject and object to facilitate better capturing of contextual information and learns the embeddings \\textcolor{black}{ of the predicate} guided by the constraint $p \\approx u-s-o$.\nUVTransE introduces the union of subject and object and uses a context-augmented translation embedding model to capture both common and rare relations in scenes. This type of exploration is highly beneficial for constructing a relatively complete scene graph. Finally, UVTransE combines the scores of vision, language, and object detection modules to sort the predictions of the triple relationship. \nThe architectural details of UVTRansE are illustrated in Fig. \\ref{fig:UVTransE}. UVTransE treats predicate embedding as $p \\approx u(s, o)-(s+o)$. While, VTransE  models visual relationships by mapping the features of objects and predicates in a low-dimensional space, where the relationship triples can be interpreted as vector translation: $s+p\\approx o$.\n\\begin{figure} \n\\center{\\includegraphics[width=0.43\\textwidth] {figure/UVTransE.png}} \n\\vspace{-.8 em}\n   \\caption{The overall structure of UVTransE's  visual detection model.}\n\\vspace{-1.2 em}\n \\label{fig:UVTransE}\n\\end{figure}\nIn addition, to solve the same problem associated with new relationship discovery in scenes to generate incomplete scene graphs, RLSV  attempts to use existing scene graphs and images to predict the new relationship between two entities, enabling it to achieve scene graph completion. RLSV begins with the relevant knowledge of the knowledge graph, incorporating the characteristics of the scene graph, and proposes an end-to-end representation learning model of joint structure and visual embedding. Unlike TransE-based SGG methods, RLSV uses TransD  (an improved version of TransR/CTransR ) to project the entities (subjects and objects) from the entity space to the relation space by means of two mapping matrices \\textcolor{black}{($M_s$ and $M_o$), and the triplets $\\langle s-p-o \\rangle$ have new representations $\\langle s\\perp-p\\perp-o \\perp \\rangle$. Then, following $s + p \\approx o$ of TransE, the relationship in scene graph can be modeled as: $\\langle s\\perp+ p\\perp \\approx o \\perp \\rangle$.} \nUnlike UVTransE and RLSV, which aim to find existing visual relationships but lack corresponding annotations in the image, AT (Analogies Transfer)  tries to detect those visual relationships that are not visible in the training set. As shown in Fig. \\ref{fig:unsee_relation}, the individual entities of $\\langle person-ride-dog \\rangle$ and $\\langle dog-ride-bike \\rangle$ are available in the training set; however, either their combination is not seen in the training set, or the visual relationship is extremely rare. As is evident, AT studies a more general phenomenon, specifically those unseen relationships that are visible in the training set for a single entity but not for the combination of $\\langle s-p-o \\rangle$. The whole network model utilizes analogy transformation to compute the similarity between the unseen triplet and its similar triplets in order to estimate this unseen relationship and has achieved good results in unseen relationship detection. \\textcolor{black}{Therefore, the transformation learned in AT is to transform the visual phrase embedding $\\omega $ of a source triplet $\\langle s, p, o\\rangle$ to $\\omega '$ of a target triplet $\\langle s' , p' , o'\\rangle$, rather than visual feature space transformations in TransE-based SGG methods.} Compared with the commonly used TransD/TransE-based SGG methods, this SGG method using Analogies Transfer has good research prospects.\nBased on the insights obtained from knowledge graph-related research, the TransE-based SGG method has developed rapidly and attracted strong interest from researchers. Related research results have also proven that this method is effective. In particular, the TransE-based SGG method is very helpful for the mining of unseen visual relationships, which will directly affect the integrity of the scene graph. Related research is thus still very valuable.\n\\begin{figure} \n\\center{\\includegraphics[width=0.45\\textwidth] {figure/CNN-BASED.png}} \n\\vspace{-1 em}\n   \\caption{The mainstream CNN-based SGG models.}\n\\vspace{-0.5 em}\n \\label{fig:CNN}\n\\end{figure}\n\\vspace{-1em}", "cites": [6420, 6430, 6431, 9021, 6428, 6429, 9023, 7859], "cite_extract_rate": 0.47058823529411764, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple TransE-based SGG approaches, tracing their evolution from VTransE to UVTransE and AT, while connecting them through a common theme of vector translation in embedding spaces. It includes critical points, such as the limitations of VTransE in ignoring predicate and context features, and the need for generalization to unseen relations. The section abstracts some patterns, like the use of multimodal attention and union features, but stops short of forming a fully novel meta-framework."}}
{"id": "4b05107c-06ce-4cb8-8eed-e9c50cc120b5", "title": "CNN-based SGG", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "CNN-based SGG"]], "content": "CNN-based SGG methods attempt to extract the local and global visual features of the image using convolutional neural networks (CNN), followed by predicting the relationships between the subjects and objects via classification. \\textcolor{black}{From most CNN-based SGG methods, we can conclude that this type of SGG methods mainly includes three parts: region proposal, feature learning and relational classification. Among these parts, feature learning is one of the key parts, and we can use the following formulation to express the feature learning at level $l$ of the subject, predicate and object respectively:\n\\begin{equation}\n   v_{\\ast}^{l}=f(W_{\\ast}^{l}\\otimes v_{\\ast}^{l-1}+c_{\\ast}^{l}),\n   \\label{eq:cnn}\n\\end{equation}\nwhere $\\ast$ can be $\\left \\{s, p, o\\right \\}$ and $\\otimes$ is the matrix-vector product, $W_{\\ast}^{l}$ and $c_{\\ast}^{l}$ are the parameters of FC or Conv layers. The subsequent CNN-based SGG methods are devoted to designing new modules to learn optimal features $v'$.} \nFig. \\ref{fig:CNN} presents the mainstream CNN-based SGG methods. The final features used for relationship identification are obtained by jointly considering the local visual features of multiple objects in LinkNet  or introducing a box attention mechanism in BAR-CNN (Box Attention Relational CNN) . In an attempt to improve the efficiency of SGG models, Rel-PN  and IM-SGG (Interpretable Model for SGG)  aim to select the most effective ROIs for visual relational prediction. ViP-CNN (Visual Phrase-guided Convolutional Neural Network)  and Zoom-Net  pay more attention to the interactions between local features. As CNN performs well at extracting the visual features of the image, the related SGG method based on CNN has been extensively studied.\nIn this part, we will elaborate on these CNN-based SGG methods. \nThe scene graph is generated by analyzing the relationships between multiple objects in the image dataset. It is accordingly necessary to consider the connection between related objects as much as possible, rather than focusing on a single object in isolation. LinkNet  was proposed to improve SGG by explicitly modeling inter-dependency among all related objects. More specifically, LinkNet designs a simple and effective relational embedding module that jointly learns the connections between all related objects. In addition, LinkNet also introduces a global context encoding module and a geometrical layout encoding module, which extract global context information and spatial information between object proposals from the entire image and thereby further improve the performance of the algorithm. The specific LinkNet is divided into three main steps: bounding box proposal, object classification, and relationship classification. However, LinkNet considers the relation proposal of all objects, which makes it computationally expensive.\n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/CNN-BASED1.png}} \n\\vspace{-2 em}\n   \\caption{Comparison of the brief schematic diagrams of three CNN-based SGG methods.}\n\\vspace{-1.7 em}\n   \\label{fig:CNN_BASED1}\n\\end{figure}\nOn the other hand, as deep learning technology has developed, the corresponding object detection research has become increasingly mature . By contrast, the recognition of associations between different entities for higher-level visual task understanding has become a new challenge; this is also the key to scene graph construction. As analyzed in Section \\ref{sec:TransE-based SGG}, to detect all relationships, it is inefficient and unnecessary to first detect all single objects and then classify all pairs of relationships, as the visual relationship that exists in the quadratic relationship is very sparse. Using visual phrases  to express this visual relationship may therefore be a good solution. \nRel-PN  has conducted corresponding research in this direction. Similar to the region proposals of objects provided by Region Proposal Networks (RPN), Rel-PN  utilizes a proposal selection module to select meaningful subject-object pairs for subsequent relationship prediction. This operation will greatly reduce the computational complexity of SGG. The model structure of Rel-PN is illustrated in Fig. \\ref{fig:CNN_BASED1}(a). Rel-PN's compatibility evaluation module uses two types of modules: a visual compatibility module and a spatial compatibility module. The visual compatibility module is mainly used to analyze the coherence of the appearance of the two boxes, while the spatial compatibility module is primarily used to explore the locations and shapes of the two boxes. Furthermore, IM-SGG , based on Rel-PN, considers three types of features, namely visual, spatial, and semantic, which are extracted by three corresponding models. Subsequently, similar to Rel-PN, these three types of features are fused for the final relationship identification. Different from Rel-PN, IM-SGG utilizes an additional semantic module to capture the strong prior knowledge of the predicate., and achieve better performance (see Fig. \\ref{fig:CNN_BASED1}(b)). This method effectively improves the interpretability of SGG.\nMore directly, using a similar method to Rel-PN, ViP-CNN  also clearly treats the visual relationship as a visual phrase containing three components. ViP-CNN  attempts to jointly learn the specific visual features for the interaction to facilitate the consideration of the visual dependency. In ViP-CNN, the PMPS (Phrase-guided Message Passing Structure) is proposed to model the interdependency information among local visual features using a gather-broadcast message passing flow mechanism. ViP-CNN has achieved significant improvements in speed and accuracy. \nIn addition, to further improve the SGG accuracy, some methods have also studied the interaction between different features with the goal of more accurately predicting the visual relationship between different entities. This is because the independent detection and recognition of a single object provide little assistance in fundamentally recognizing visual relationships. Fig. \\ref{fig:object_detection} presents an example of a case in which even the most perfect object detector finds it difficult to distinguish people standing beside horses from people feeding horses. \n\\begin{figure} \n\\center{\\includegraphics[width=0.35\\textwidth] {figure/object_detection.png}}\n\\vspace{-.8 em}\n   \\caption{Schematic diagram of object detection. \n   }\n\\vspace{-1.8 em}\n   \\label{fig:object_detection}\n\\end{figure}\nTherefore, the information interaction between different objects is extremely important to the understanding of visual relationships. Many related works have been published on this subject.\nFor example, the interactions between detected object pairs are used for visual relationship recognition in Zoom-Net . Zoom-Net achieves compelling performance by successfully recognizing complex visual relationships through the use of deep message propagation and the interaction between local object features and global predicate features, without the use of any linguistic priors. VIP-CNN  also uses similar feature interactions. The key difference is that the CA-M (Context-Appearance Module) proposed by VIP-CNN attempts to directly fuse pairwise features to capture contextual information, while the SCA-M (Spatiality-Context-Appearance Module) proposed by Zoom-Net  performs spatially-aware channel-level local and global context information fusion. Therefore, SCA-M has more advantages while capturing the spatial and contextual relationships between the subject, predicate, and object features. Fig. \\ref{fig:ViP-CNN&Zoom-Net} presents the structure comparison diagram of the Appearance Module (A-M) without information interaction, along with the Context-Appearance Module (CA-M) and Spatiality-Context-Appearance Module (SCA-M).\n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth]\n{figure/ViP-CNN-Zoom-Net.png}}\n\\vspace{-2 em}\n   \\caption{(a) The ROI-pooled feature of the subject (S), predicate (P), and object (O) of a given input image. (b) Appearance Module (A-M) without information interaction. (c) Context-Appearance Module (CA-M) in ViP-CNN . (d) Spatiality-Context-Appearance Module (SCA-M) in Zoom-Net .}\n   \\vspace{-1 em}\n   \\label{fig:ViP-CNN&Zoom-Net}\n\\end{figure}\nAn attention mechanism is also a good tool for improving visual relationship detection. BAR-CNN  observed that the receptive field of neurons in the most advanced feature extractors  may still be limited, meaning that the model may cover the entire attention map. To this end, BAR-CNN proposes a box attention mechanism; this enables visual relationship detection tasks to use existing object detection models to complete the corresponding relationship recognition tasks without introducing additional complex components. This is a very interesting concept, and BAR-CNN has also obtained competitive recognition performance. A schematic illustration of BAR-CNN is presented in Fig. \\ref{fig:CNN_BASED1}(c).\nRelated CNN-based SGG methods have been extensively studied. However, there are still many remaining challenges requiring further research, including those of how to reduce the computational complexity as much as possible while ensuring deep interaction between the triplet's different features, how to deal with the real but very sparse visual relationship in reality, etc. Identifying solutions to these problems will further deepen the research related to the CNN-based SGG method.\n\\vspace{-1em}", "cites": [6432, 6420, 301, 6424, 8155, 97, 9022, 6435, 206, 6433, 209, 6434, 802], "cite_extract_rate": 0.8666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple CNN-based SGG methods into a coherent narrative by categorizing their components and highlighting key innovations like feature fusion, proposal selection, and message passing. It offers some critical analysis by noting computational costs and inefficiencies (e.g., in LinkNet) and comparing the effectiveness of different modules (e.g., CA-M vs. SCA-M). However, it lacks deeper abstraction or a novel framework to generalize beyond the discussed methods."}}
{"id": "995cafa0-f77d-4663-a56f-b05f28381044", "title": "RNN/LSTM-based SGG", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "RNN/LSTM-based SGG"]], "content": "\\label{sec:RNN/LSTM-based SGG}\nA scene graph is a structured representation of an image. The information interaction between different objects and the contextual information of these objects is crucial to the recognition of the visual relationship between them. Models based on RNN and LSTM have natural advantages in capturing the contextual information in the scene graph and reasoning on the structured information in the graph structure. RNN/LSTM-based methods are thus also a popular research direction. As shown in Fig. \\ref{fig:rnn-lstm}, based on the standard RNN/LSTM networks, several improved SGG models have been proposed. For example, the feature interaction of local and global context information is considered in IMP (Iterative Message Passing)  and MotifNet (Stacked Motif Network)  respectively. Similarly, instance-level and scene-level context information are used for SGG in PANet (predicate association network) , \\textcolor{black}{ and attention-based RNN is also introduced in SIG (Sketching Image Gist)  for SGG. The corresponding probabilistic interpretation of these RNN/LSTM-based SGG methods can be simplified in the conditional form of Eq.(\\ref{eq:bottom_up}), while these methods mainly utilize standard/improved RNN/LSTM networks to inference the relationship by optimizing $P(R|B,O,I)$.} Later, RNN/LSTM-based models have attempted to learn different types of contextual information by designing structural RNN/LSTM modules; examples include AHRNN (attention-based hierarchical RNN) , VCTree (Visual Context Tree model) . \\textcolor{black}{This type of SGG methods considers the scene graph as a hierarchical graphical structure, so they need to construct a hierarchical entity tree based on the region proposals. Then the hierarchy contextal information can be encoded by}\n\\begin{equation}\n   D = BiTreeLSTM(\\left \\{ z_i \\right \\}{_{i=0}^{n}}),\n    \\label{eq:bilstm}\n\\end{equation}\n\\noindent \\textcolor{black}{where $z_i$ is the feature of the input nodes in the constracted hierarchical entity tree. Finally, a MLP (Multi-layer Perceptron) classifier is used to predict the predicate $p$}.\n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/rnn-lstm.png}} \n\\vspace{-1.8 em}\n   \\caption{RNN/LSTM-based SGG models.}\n\\vspace{-1.5 em}\n \\label{fig:rnn-lstm}\n\\end{figure}\nAs discussed above, to make full use of the contextual information in the image to improve the accuracy of SGG, IMP  was proposed. IMP attempts to use standard RNN to solve the scene graph inference problem and iteratively improve the model's prediction performance through message passing. The main highlight of this approach is its novel primal-dual graph, which enables the bi-directional flow of node information and edge information, and updates the two GRUs  of node and edge in an iterative manner. This form of information interaction helps the model to more accurately identify the visual relationships between objects. \nUnlike cases of interaction between local information, such as IMP, MotifNet  begins from the assumption that the strong independence assumption in the local predictor  limits the quality of global prediction. To this end, MotifNet encodes global context information through recurrent sequential architecture LSTMs (Long Short-term Memory Networks) .\nHowever, MotifNet  only considers the context information between objects while failing to take scene information into account. There have also been some works  that investigate the classification of relationships by exchanging the context between nodes and edges. \nHowever, the above-mentioned SGG methods focus primarily on the structural-semantic features in a scene while ignoring the correlations among different predicates. For this reason,  proposed a two-stage predicate association network (PANet). The main goal of the first stage is to extract instance-level and scene-level context information, while the second stage is mainly used to capture the association between predicate alignment features. In particular, an RNN module is used to fully capture the association between alignment features. This kind of predicate association analysis has also achieved good results.\nHowever, the methods discussed above often rely on object detection and predicate classification between objects. There are two inherent limitations of this approach: first, the object bounding box or relationship pairs generated via the object detection method are not always necessary for the generation of the scene graph; second, SGG depends on the probabilistic ranking of the output relationships, which will lead to semantically redundant relationships .\nFor this reason, AHRNN  proposed a hierarchical recurrent neural network based on a visual attention mechanism. This approach first uses the visual attention mechanism  to resolve the first limitation. Secondly, AHRNN regards the recognition of relational triples as a sequence learning problem using recurrent neural networks (RNN). In particular, it employs hierarchical RNN to model relational triples to more effectively process long-term context information and sequence information , thereby avoiding the need to rank the probability of output relationships.\nOn the other hand, VCTree  observed that the previous scene graphs either adopted chains  or a fully-connected graph . However, VCTree proposes that these two prior structures may not be optimal, as the chain structure is too simple and may only capture simple spatial information or co-occurrence bias; moreover, the fully connected graph lacks the distinguishing structure of hierarchical and parallel relationships. To solve this problem, VCTree proposed composite dynamic tree structures, which can use TreeLSTM  for efficient context coding and thus effectively represent the hierarchical and parallel relationships in visual relationships. This tree structure provides a new research direction for scene graph representation. Fig. \\ref{fig:Chain_Graph_VCTree} presents a comparison of the chain structure, fully connected graph structure, subgraph, and dynamic tree structure of the scene graph. \n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/Chain_Graph_VCTree.png}} \n\\vspace{-1.5 em}\n   \\caption{A comparison of the chains , fully connected graph , subgraph  and dynamic tree  structure of the scene graph. The dynamic tree structure on the left shows a left-child right-sibling binary tree, where the left branches (red) represent the hierarchical relationships, while the right branches (blue) represent the parallel relationship. \n   }\n   \\vspace{-1.8 em}\n   \\label{fig:Chain_Graph_VCTree}\n\\end{figure}\nSIG  also proposed a scene graph with a similar tree structure; the key difference stems from the observation that humans tend to describe the subjects and key relationships in the image first when analyzing scenes, meaning that a hierarchy analysis with primary and secondary order is more in line with human habits. To this end, SIG proposed a human-mimetic hierarchical SGG method. Under this approach, the scene is represented by a human-mimetic HET (Hierarchical Entity Tree) composed of a series of image regions, while Hybrid-LSTM (Hybrid Long Short-Term Memory) is used to parse HET, thereby enabling the hierarchical structure  and siblings context  information in HET to be obtained.\n\\vspace{-1em}", "cites": [6419, 271, 8714, 6438, 3760, 267, 6436, 6424, 284, 6430, 6437, 6420, 277, 303, 8412], "cite_extract_rate": 0.6521739130434783, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple RNN/LSTM-based scene graph generation (SGG) methods, highlighting their design choices and contextual modeling strategies. It critically evaluates their limitations, such as over-reliance on object detection and failure to capture predicate correlations. The section abstracts these works into a broader narrative about hierarchical and structured modeling of visual relationships, offering insights into how these models align with human perception and reasoning patterns."}}
{"id": "60d13ecb-395c-44da-98f6-d95ab0515d43", "title": "GNN-based SGG", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "GNN-based SGG"]], "content": "\\label{sec:Graph-based SGG}\nThe scene graph can be regarded as a graph structure. An intuitive approach would therefore be to improve the generation of scene graphs with the help of graph theory. The GCN (Graph Convolutional Network)  is just such a method. This approach is designed to process graph structure data, local information  can be effectively learned between neighboring nodes. GCN has been proven to be highly effective in tasks such as relational reasoning , graph classification , node classification in large graphs , and visual understanding . Accordingly, many researchers have directly studied the SGG method based on GCN. \\textcolor{black}{ Similar to the conditional form (Eq.(\\ref{eq:bottom_up})) of RNN/LSTM-based SGG methods, and following the expression form of the relevant variables in this paper, the SGG process based on GCN can also be factorized into three parts:}\n\\begin{equation}\nP(\\left \\langle V,E,O,R \\right \\rangle|I) =P(V |I)*P(E|V, I)*P(R, O|V, E, I)\n\\label{eq:GCN-based}\n\\end{equation}\n\\noindent \\textcolor{black}{ where $V$ is the set of nodes (objects in images), $E$ is the edges (relationships between objects) in a graph. Based on this basic conditional form, the subsequent improved GNN-based SGG methods are presented. Most of these methods attempt to optimize the terms of $P(E|V,I)$ and $P(R, O|V, E, I)$ by designing relevant modules, and GCN-based networks are designed for the graph labeling process $P(R, O|V, E, I)$. Fig. \\ref{graph} presents some classic GNN-based SGG models.} F-Net (Factorizable Net)  completes final SGG by decomposing and merging the graphs, after which the attention mechanism is introduced to design different types of GNN modules for SGG, such as Graph R-CNN , GPI  and ARN (Attentive Relational Network) . Few-shot training and multi-agent training are applied to few-shot SGP  and CMAT (Counterfactual critic Multi-Agent Training)  respectively. Probabilistic Graph Network (PGN) is designed for DG-PGNN , while multi-modal graph convNet was developed for SGVST . Furthermore, other improved GNN-based network modules have been proposed for SGG, which we will describe in detail.\n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/graph.png}} \n\\vspace{-2 em}\n   \\caption{Improved SGG models based on GNN. }\n\\vspace{-1.5 em}\n \\label{graph}\n\\end{figure}\nAs discussed in Section \\ref{sec:Construction process}, the current SGG methods can be roughly divided into two categories: \\textit{bottom-up} and \\textit{top-down} methods. However, these types of frameworks build a quadratic number of objects, which is time-consuming. Therefore, an efficient subgraph-based framework for SGG, called Factorizable Net (F-Net) , is proposed to promote the generation efficiency of the scene graph. With this approach, the detected object region proposals are paired to facilitate the construction of a complete directed graph. Thereafter, a more precise graph is generated by merging edges corresponding to similar union regions into a subgraph; each subgraph has several objects and their relationships are represented as edges. By substituting the original scene graph with these subgraphs, Factorizable Net can achieve higher generation efficiency of the scene graph.\nFurthermore, Graph R-CNN  attempts to trim the original scene graph (removing those unlikely relationships) so as to generate a sparse candidate graph structure. Finally, an attention graph convolutional network (AGCN) is used to integrate global context information to promote more efficient and accurate SGG.\nThe graph-based attention mechanism also has important research value in the generation of scene graphs. For example, previous SGG work  often requires prior knowledge of the graph structure. In addition, these methods tend to ignore the overall structure and information of the entire image, as they capture the representation of nodes and edges in a step-by-step manner. Moreover, the one-by-one detection of the visual relationship of the paired regions  is also poorly suited to describing the structure of the entire scene.\nFor this reason, in ARN , a semantic transformation module is proposed that produces semantic embeddings by transforming label embeddings and visual features into the same space, while a relation inference module is used to predict the entity category and relationship as the final scene graph result. In particular, to facilitate describing the structure of the entire scene, ARN proposed a graph self-attention-based model aimed at embedding a joint graph representation to describe all relationships. This module helps to generate more accurate scene graphs. \\textcolor{black}{In addition, an intuition is that when recognizing an image of a \"person riding a horse\", the interaction between the human leg and the horseback can provide strong visual evidence for the recognition of the predicate. For this reason, RAAL-SGG (Toward Region-Aware Attention Learning for SGG)  points out that it is limited to use only coarse-grained bounding boxes to study SGG. Therefore, RAAL-SGG proposed a region-aware attention learning method that uses an object-wise attention graph neural network for more fine-grained object region reasoning. The probability function of this model can be expressed as}\n\\begin{equation}\n\\begin{split}\nP(SG|I)=&P(B|I)*P(F|B,I)*P(O|B,I,F)*\\\\&P(R|B,I,F,O),F=\\{f^{(n)}\\}_{n=1}^N,\n\\end{split}\n\\label{eq:RAAL-SGG}\n\\end{equation}\n\\textcolor{black}{where $f^{(n)}$ is a region set of the \\textit{n}th object. Different from Eq.(\\ref{eq:bottom_up}), Eq.(\\ref{eq:RAAL-SGG}) considers the object area set $F$ which is more fine-grained than the coarse-grained bounding box $B$. This helps the model to reason about the predicate with the help of the object interaction area.} \nWhen predicting the visual relationship of the scene graph, the reading order of entities in the context encoded using RNN/LSTM  also has a crucial influence on the SGG. A fixed reading order may not be optimal under these circumstances. A scene graph generator should reveal the connection between objects and relations to improve the prediction precision, even if different types of inputs are present. Formally, given the same features,  the same result should be obtained by a framework or a function $\\mathcal{F}$ even if the input has been permuted. Motivated by this observation, the architecture of a neural network for SGG should ideally remain invariant to a particular type of input permutation. Herzig et al.  accordingly proved this property based on the fact that such an architecture or framework can gather information from the holistic graph in a permutation-invariant manner. Based on this feature, these authors proposed several common architecture structures and obtained competitive performance.\nFor most SGG approaches , the long-tailed distribution of relationships remains a challenge to relational feature learning. Existing methods are often unable to deal with unevenly distributed predicates. Therefore, Dornadula et al.  attempted to construct a scene graph via few-shot learning of predicates, which can scale to new predicates. The SGG model based on few-shot learning attempts to fully train the graph convolution model and the spatial and semantic shift functions on relationships with abundant data. For their part, the new shift functions are fine-tuned to new, rare relationships of a few examples. When compared to conventional SGG methods, the novelty of this model is that predicates are defined as functions, such that object notations are useful for few-shot predicate forecasting; these include a forward function that turns subject notations into objects and a corresponding function that changes the object representation back into subjects. The model achieves good performance in the learning of rare predicates.\nA comprehensive, accurate, and coherent scene graph is what we expect to achieve, and the semantics of the same node in different visual relationships should also be consistent. However, the currently widely used supervised learning paradigm based on cross-entropy may not guarantee the consistency of this visual context. This is because this paradigm tends to predict pairwise relationships in an independent way , while hub nodes (those that belong to multiple visual relationships at the same time) and non-hub nodes are given the same penalty.  This is unreasonable. For this reason,  proposed a Counterfactual critic Multi-Agent Training (CMAT) approach. More specifically, CMAT is the first work to define SGG as a cooperative multi-agent problem. This approach solves the problems of graph consistency and graph-level local sensitivity by directly maximizing a graph-level metric as a reward (corresponding to the hub and non-hub nodes being given different penalties).\nSimilarly, RelDN (Relationship Detection Network)  also found that applying cross-entropy loss alone may have an adverse effect on predicate classification; examples include Entity Instance Confusion (confusion between different instances of the same type) and Proximal Relationship Ambiguity (subject-object pairing problems in different triples with the same predicate).\nRelDN is proposed to tackle these two problems. In RelDN, three types of features for semantic, visual, and spatial relationship proposals are combined by means of entity-wise addition. These features are then applied to obtain a distribution of predicate labels via softmax normalization. Thereafter, contrastive losses between graphs are specifically constructed to solve the aforementioned problems. \nScene graphs provide a natural representation for reasoning tasks. Unfortunately, given their non-differentiable representations, it is difficult to use scene graphs directly as intermediate components of visual reasoning tasks. Therefore, DSG (Differentiable Scene-Graphs)  are employed to solve the above obstacles. The visual features of objects are used as inputs to the differentiable scene-graph generator module of DSGs, which is a set of the new node and edge features. The novelty of the DSG architecture lies in its decomposition of the scene graph components, enabling each element in a triplet to be represented by a dense descriptor. Thus, DSGs can be directly used as the intermediate representation of downstream inference tasks.\nAlthough we have investigated many GNN-based SGG methods, there are still many other related methods. For example,  proposes a deep generative probabilistic graph neural network (DG-PGNN) to generate a scene graph with uncertainty. SGVST  introduces a scene graph-based method to generate story statements from image streams. This approach uses GCN to capture the local fine-grained region representation of objects in the scene graph. We can conclude from the above that the GNN-based SGG method has attracted significant research attention due to its obvious ability to capture structured information.\n\\vspace{-1em}", "cites": [6446, 6421, 6441, 6442, 6419, 242, 6439, 6445, 6428, 8313, 271, 8714, 4016, 9024, 566, 6440, 6443, 267, 6444, 1125, 6424, 4015, 7212, 216, 284, 6416, 211, 6420, 9021], "cite_extract_rate": 0.6904761904761905, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple GNN-based SGG methods, integrating their approaches into a coherent narrative. It provides critical analysis by highlighting limitations such as computational inefficiency and long-tailed predicate distributions. The section abstracts key design principles (e.g., permutation invariance, attention mechanisms, subgraph efficiency) and presents them as general strategies for improving SGG."}}
{"id": "6ec82603-3144-4160-9716-8a92a48e3c55", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "b1509635-571c-4c31-a74e-95c0af7e8ce8", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Scene graph generation"], ["subsection", "Discussion"]], "content": "\\textcolor{black}{As mentioned previously, ``the recognition of predicates is interrelated, and contextual information plays a vital role in the generation of scene graphs.''} To this end, researchers are increasingly focusing on methods based on RNN/LSTM or graphs. This is primarily because RNN/LSTM has better relational context modeling capabilities. While the graph structure properties of the scene graph itself also enable the GNN-based SGG to obtain the corresponding attention. In addition, TransE-based SGG has been welcomed by researchers due to its intuitive modeling method, which makes the model very interpretable. Due to the strong visual feature learning ability of CNN, future CNN-based SGG methods will remain mainstream SGG methods.\nFrom the perspective of objects, relationships, and messages, the above methods can also be divided into object contextualization methods, relationship representation methods, message-passing methods, and relationship context methods. In the scene graph parsing task, all objects and relationships are related; accordingly, the corresponding context information should be fully considered. Most of the bounding box-based SGG methods consider the context information of the object . \\textcolor{black}{Because objects in these methods can be adequately interacting in the process of contexualizaiton of Link $I\\rightarrow \\widetilde{Y}$, Link $I\\rightarrow X$ and subsequent $X\\rightarrow \\widetilde{Y}$.} Furthermore, due to the good modeling ability of RNN/LSTM in a relational context, the related SGG method is also favored by many researchers (as discussed in Section.\\ref{sec:RNN/LSTM-based SGG}). There is a large amount of related work contained herein, including RNN , GRU , LSTM , TreeLSTM . \\textcolor{black}{Specifically, for example, in feature extraction (Link $I\\rightarrow X$), in order to encode the visual context for each object, Motifs  chooses to use Bi-LSTMs (bidirectional LSTMs). In PANet , it uses the combination of class embedding, spatial information, and object visual features as the input of RNN in the contextualization process to obtain instance-level and scene-level context information.} The relationship representation method is also the main research direction at present because it directly affects the accuracy, completeness, and hierarchy of the relationship modeling. It mainly includes chains , fully connected graph , subgraph , and tree  structures (see Fig. \\ref{fig:Chain_Graph_VCTree}). The related research helps in establishing a better explanatory model. \\textcolor{black}{For example, MSDN (Multi-level Scene Description Network)  builds a dynamic graph from three different levels of object, phrase, and region features in the process of contexualizaiton of Link $I\\rightarrow X$, Link $X\\rightarrow \\widetilde{Y}$, and Link $I\\rightarrow \\widetilde{Y}$, in which the feature refining phase message can be passed on the edges of the graph. VCTree  uses Bi-TreeLSTMs (bidirectional TreeLSTMs)  to encode visual context in feature extraction (Link $I\\rightarrow X$).} In addition, message passing  is also an important research direction, as it directly affects the degree of information interaction between objects and relationship nodes (as discussed in Section.\\ref{sec:Graph-based SGG}). In the future, the mining of relevant contextual information will continue to be a highly promising research direction. In addition, it is also necessary to add reasoning capabilities to the model, as this will aid in solving the long-tail problem.\n\\textcolor{black}{In addition to the above-discussed method of using full supervision for SGG,  proposes a new method of using weak supervision to generate scene graphs. It uses the image and the corresponding caption as weak supervision to learn the entities in the image and the relationship between them. This further expands the available dataset for scene graph related research. \nIn addition, PUM (Probabilistic Uncertainty Modeling)  proposes that in order to generate an â€˜accurateâ€™ scene graph, it is inappropriate to predict the relationship between entities in a deterministic way. This is mainly due to the semantic ambiguity (e.g., synonymy ambiguity, hyponymy ambiguity, and multi-view ambiguity) of visual relationship. For this reason, PUM uses probabilistic uncertainty modeling to replace the previous deterministic relationship with predictive modeling in the construction and inference stage of the graph. This strategy enables PUM to generate more realistic and diversified scene graphs.\nSimilar multi-directional research is necessary for the development of scene graphs.}\n\\vspace{-1em}", "cites": [6419, 8714, 6448, 4016, 6438, 8155, 267, 6436, 6447, 284, 6437, 6416, 6449, 6420, 277, 8412], "cite_extract_rate": 0.8, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers by organizing them into categories like object contextualization, relationship representation, and message passing, highlighting how different methods leverage contextual and relational modeling. It offers some critical analysis, especially in addressing the limitations of deterministic models and the need for probabilistic uncertainty handling. The section abstracts beyond individual papers by discussing overarching modeling strategies and identifying trends such as the importance of context and the role of graph-based reasoning."}}
{"id": "8772339b-bc4e-4047-be04-d488f4ce57ad", "title": "SGG with Language Prior", "level": "subsection", "subsections": [], "parent_id": "9453c957-a952-4d6f-a11e-dcd69c81b6e0", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "SGG with prior knowledge"], ["subsection", "SGG with Language Prior"]], "content": "\\begin{figure} \n\\center{\\includegraphics[width=0.48\\textwidth] {figure/girl_women_ride.png}}\n\\vspace{-1 em}\n   \\caption{Examples of semantic similarity helping with visual relationship inference. \n   }\n   \\vspace{-1.5 em}\n \\label{fig:girl_women_ride}\n\\end{figure}\nLanguage priors typically use the information embedded in semantic words to fine-tune the possibility of relationship prediction, thereby improving the accuracy of visual relationship prediction. Language priors can help the recognition of visual relationships through the observation of semantically related objects. For example, horses and elephants may be arranged in a semantically similar context, e.g., \"a person riding a horse\" and \"a person riding an elephant\". Therefore, although the co-occurrence of elephants and persons is not common in the training set, through the introduction of language priors and the study of more common examples (such as \"a person riding a horse\"), we can still easily infer that the relationship between a person and an elephant may be one of riding. This idea is illustrated in Fig.\\ref{fig:girl_women_ride}. This approach also helps to resolve the long tail effect in visual relationships.\nMany researchers have conducted detailed studies on the introduction of language priors. For example, Lu et al. suggested training a visual appearance module and a language module simultaneously, then combining the two scores to infer the visual relationship in the image. In particular, the language a priori module projects the semantic-like relationships into a tighter embedding space.  This helps the model to infer a similar visual relationship (\"person riding an elephant\") from the \"person riding a horse\" example. Similarly, VRL (deep Variation-structured Reinforcement Learning)  and CDDN (Context-Dependent Diffusion Network)  also use language priors to improve the prediction of visual relationships; the difference is that  uses semantic word embedding  to fine-tune the possibility of predicting relationships, while VRL follows the variational-structured traversal scheme over a directed semantic action graph from the language prior, meaning that the latter can provide richer and more compact semantic association representation than word embedding. \nMoreover, CDDN finds that similar objects have close internal correlations, which can be used to infer new visual relationships. To this end, CDDN uses word embedding to obtain a semantic graph, while simultaneously constructing a spatial scene graph to encode global context interdependency. CDDN can effectively learn the latent representation of visual relations through the combination of prior semantics and visual scenes; furthermore, considering its isomorphic invariance to graphs, it can cater well to visual relation detection.\nOn the other hand, although the language prior can compensate for the difference between model complexity and dataset complexity, its effectiveness will also be affected when the semantic word embedding falls short . For this reason,  further introduces a relation learning module with a priori predicate distribution on the basis of IMP  to better learn visual relations. \nIn more detail, a pre-trained tensor-based relation module is added to  as a dense relation prior to fine-tuned relation estimation, while an iterative message-passing scheme with GRUs is used as a GCN method for promoting the SGG performance with better feature representation. \nIn addition to using language priors,  also combines visual cues to identify visual relationships in images and locate phrases. For its part,  models the appearance, size, location, and attributes of entities, along with the spatial relationship between object pairs connected by verbs or prepositions, and jointly infers visual relationships through automatically learning and combining the weights of these clues.\n\\vspace{-1em}", "cites": [6450, 2329, 6423, 7165, 9021, 6451, 284], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers, integrating their contributions to show how language priors are used in different models for SGG, such as IMP, VRL, and CDDN. It provides a coherent narrative on how semantic information aids relationship prediction and the long tail problem. Some critical analysis is present, particularly regarding the limitations of semantic word embedding, but deeper evaluation of trade-offs between methods is limited. The section identifies general patterns in the use of language priors and their combination with visual cues, showing moderate abstraction."}}
{"id": "99be734b-a45c-4c75-be41-c84d3ced7036", "title": "SGG with Statistical Prior", "level": "subsection", "subsections": [], "parent_id": "9453c957-a952-4d6f-a11e-dcd69c81b6e0", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "SGG with prior knowledge"], ["subsection", "SGG with Statistical Prior"]], "content": "Statistical prior is also a form of prior knowledge widely used by SGG, as objects in the visual scene usually have strong structural regularity . For example, people tend to wear shoes, while mountains tend to have water around them. In addition, $\\langle cat-eat-fish \\rangle$ is common, while $\\langle fish-eat-cat \\rangle$ and $\\langle cat-ride-fish \\rangle$ are very unlikely. This relationship can thus be expressed using prior knowledge of statistical correlation. Modeling the statistical correlation between object pairs and relationships can help us in correctly identifying visual relationships.\nDue to the spatially large and long-tailed nature of relationship distributions, simply using the annotations contained in the training set would be insufficient.  Moreover, it is difficult to collect an adequate amount of labeled training data. For this reason, LKD (Linguistic Knowledge Distillation)  uses not only the annotations inside the training set but also text publicly available on the Internet (Wikipedia) to collect external language knowledge. This is mainly achieved by tallying the vocabulary and expressions used by humans to describe the relationships between pairs of objects in the text, then calculating the conditional probability distribution ($P(pred|subj, obj)$) of the predicate given a pair of $\\langle subj, obj \\rangle$. A novel contribution is that of using knowledge distillation  to acquire prior knowledge from internal and external linguistic data in order to solve the long-tail relationship problem.\nSimilarly, DR-Net (Deep Relational Networks)  also noticed the strong statistical correlation between the triples $\\langle subj-pred-obj \\rangle$.  The difference is that DR-Net proposed a deep relationship network to take advantage of this statistical correlation. DR-Net first extracts the local regions and spatial masks of each pair of objects, then inputs them together with the appearance of a single object into the deep relational network for joint analysis, thereby obtaining the most likely relational category.\nIn addition, MotifNet  performed a statistical analysis of the co-occurrences between the relationships and object pairs on the Visual Genome dataset , finding that these statistical co-occurrences can provide strong regularization for relationship prediction. To this end, MotifNet uses LSTM  to encode the global context of objects and relationships, thus enabling the scene graph to be parsed. \nHowever, although the above methods  also observed the statistical co-occurrence of the triple, the depth model they designed implicitly mined this statistical information through message transmission.\nKERN (Knowledge-Embedded Routing Network)  also took note of this statistical co-occurrence. The difference is that KERN formally expresses this statistical knowledge in the form of a structured graph, which is incorporated into the deep propagation network as additional guidance. This can effectively regularize the distribution of possible relationships, thereby reducing the ambiguity of prediction.\nIn addition, similar statistical priors are also used in complex indoor scene analysis .  Statistical priors can effectively improve performance on the corresponding scene analysis tasks.\n\\vspace{-1em}", "cites": [6438, 4013, 6419, 5257, 6452, 5721, 6416], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by highlighting the common theme of statistical co-occurrence in scene graph generation and explaining how different methods leverage this. It provides a critical perspective by noting that prior methods implicitly mined this knowledge, while newer approaches like KERN formalize it. The abstraction is moderate, identifying a general trend in using statistical priors for regularization, but it does not fully generalize to a meta-level framework."}}
{"id": "80c717ba-d27f-4119-85df-3d3ed7628090", "title": "SGG with Knowledge Graph", "level": "subsection", "subsections": [], "parent_id": "9453c957-a952-4d6f-a11e-dcd69c81b6e0", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "SGG with prior knowledge"], ["subsection", "SGG with Knowledge Graph"]], "content": "Knowledge graphs are a rich knowledge base that encodes how the world is structured. Common-sense knowledge graphs have thus been used as prior knowledge to effectively help the generation of scene graphs.  \nTo this end, GB-Net (Graph Bridging Network)  proposes a new perspective, which constructs scene graphs and knowledge graphs into a unified framework. More specifically, GB-Net regards the scene graph as the image-conditioned instantiation of the commonsense knowledge graph. Based on this perspective, the generation of scene graphs is redefined as a bridge mapping between scene graphs and common sense graphs. \nIn addition, the deviations in the existing label dataset on object pairs and relationship labels, along with the noise and missing annotations they contain, all increase the difficulty of developing a reliable scene graph prediction model. For this reason, KB-GAN (knowledge base and auxiliary image generation)  proposed a SGG algorithm based on external knowledge and image reconstruction loss to overcome the problems found in datasets. More specifically, KB-GAN uses ConceptNet's  English subgraph as the knowledge graph; the knowledge-based module of KB-GAN improves the feature refinement process by reasoning on a basket of common sense knowledge retrieved from ConceptNet. Similarly, there are many related works  that use knowledge graphs as prior knowledge to assist relationship prediction.\n\\vspace{-1em}", "cites": [4013, 4017, 4824, 6453, 9024], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by introducing a conceptual framework (image-conditioned instantiation of knowledge graphs) and highlighting how knowledge graphs can bridge gaps in scene graph generation. It synthesizes the core ideas of GB-Net and KB-GAN, connecting them to the broader challenge of dataset noise and imbalance. However, it lacks deeper critical evaluation of the cited papers' limitations or a more abstract, meta-level discussion of underlying principles."}}
{"id": "df65d60c-12f7-4735-913e-189357423eff", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "9453c957-a952-4d6f-a11e-dcd69c81b6e0", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "SGG with prior knowledge"], ["subsection", "Discussion"]], "content": "\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/Prior.png}} \n\\vspace{-1.5 em}\n   \\caption{A schematic diagram of a SGG framework assisted by prior knowledge. Such models often consist of two branches: a visual module and an additional prior knowledge module. \nPrior knowledge includes (a) language priors (such as directed semantic action graphs , semantic graphs , or word embedding ), (b) statistical priors (such as linguistic knowledge \ncollection , statistical co-occurrence probabilities  and Deep Relational Networks ) and (c) knowledge graphs . \n}\n\\vspace{-1.5 em}\n \\label{fig:priors}\n\\end{figure}\nFig.\\ref{fig:priors} presents the pipeline of SGG models with different types of prior knowledge. \nPrior knowledge has been proven to significantly improve the quality of SGG. Existing methods use either an external curated knowledge base, such as ConceptNet , or the statistical information found in the annotation corpus to obtain commonsense data. However, the former is limited by incomplete external knowledge , while the latter is often based on hard-coded heuristic algorithms such as the co-occurrence probability of a given category. Therefore, the latest research  attempts to use visual commonsense as a machine learning task for the first time, and automatically obtains visual commonsense data directly from the dataset to improve the robustness of scene understanding. While this exploration is very valuable, the question of how to acquire and make full use of this prior knowledge remains a difficult one that merits further attention.\n\\vspace{-1em}", "cites": [2329, 6438, 6423, 4013, 6419, 4017, 7046, 9021, 6453, 3631, 6454, 6416, 6455], "cite_extract_rate": 0.9285714285714286, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a schematic overview and integrates prior knowledge approaches into a structured framework, linking the cited works under the categories of language, statistical, and knowledge graph priors. It offers some critical perspective by highlighting limitations, such as incomplete external knowledge and the use of heuristic algorithms. However, it lacks deeper comparative analysis or synthesis into a novel framework, and while it identifies broader patterns, it does not fully abstract to a meta-level understanding of the field."}}
{"id": "006d8642-906f-44af-97db-03919f44cec8", "title": "Long-tailed distribution on SGG", "level": "section", "subsections": [], "parent_id": "be806312-3b5b-4083-ab29-bf4d337ae115", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Long-tailed distribution on SGG"]], "content": "\\label{sec:long-tail}\nLong-tail distribution is a key challenge in visual relationship recognition. Predicates in visual relationships are often unevenly distributed, and common head predicates (such as ``on'', ``have'', ``in'', etc.) occupy most of the relationship expression (as shown in Fig. \\ref{fig:Long-tail} (a)). However, these overly general relationship expressions have very limited significance for the analysis of visual relationships. We would prefer to see more accurate expressions, such as ``people walking on the beach'' or ``people lying on the beach'' rather than ``people on the beach'' (as shown in Fig. \\ref{fig:Long-tail} (b)). \nWhen analyzing the complexity of visual relationships, it should be noted that because visual relationships combine objects and predicates, the complexity of this relationship is $\\mathcal{O}(N^2R)$ for $N$ objects and $R$ predicates. Even if learning the models for the object and predicate separately reduces the complexity to $\\mathcal{O}(N+R)$, the dramatic changes in the appearance of the predicate remain very challenging. For example, there is a significant difference in the visual appearance of $\\langle person-ride-bike \\rangle$ and $\\langle person-ride-horse \\rangle$ (as shown in Fig. \\ref{fig:unsee_relation}). The distribution of such object-predicate combinations tends to be more long-tailed than that of objects alone. \nIn addition, due to the inherent complexity of real scenes, the fact that some visual relationships have never appeared in the training set does not mean that they do not exist, and it is impossible to exhaust them in the training set (e.g., the right side of Fig. \\ref{fig:unsee_relation}). Therefore, the study of the long-tail problem is urgent and necessary.\n\\begin{figure} \n\\center{\\includegraphics[width=0.4\\textwidth] {figure/Long-tail.png}}  \n\\vspace{-1 em}\n   \\caption{An example of SGG. (a) The distribution of the 20 most common predicates in Visual Genome . (b) A labeled bounding boxes output image. \n   Copied from .\n}\n\\vspace{-1.5 em}\n \\label{fig:Long-tail}\n\\end{figure}\nIn scene graph datasets, the actually existing long-tailed distribution of relationships directly affects the accuracy and completeness of the generated scene graph and is thus a problem that many scholars have been trying to solve for the SGG context. For example, zero-shot , one-shot  and few-shot  learning approaches attempt to address the challenges of scaling relationship recognition to the long tail of categories in the datasets. Specifically,  learns a meaningful embedding space by treating the predicate as a function between object representations so that the predicate can be transferred between objects of the same kind, thereby enhancing the learning ability of the model in the few-shot problem. \nIn addition,  proposes that the standard scene graph density loss function can result in individual edges in a large sparse graph being readily ignored during the training process, and the ``frequent'' relationship prediction leads to a ``blind'' model that can also achieve good performance. For this reason,  introduced a density-normalized edge loss and proposed a novel weighted metric for zero/few shots, which achieved good results.  Moreover, the language prior information  and statistical prior   are used to project relationships, enabling similar, rare relationships to be predicted so as to alleviate the problem of the long tail of infrequent relationships. Such types of prior information or analogies between similar relationships are very helpful for detecting infrequent relationships. \nIn addition,  aims to solve the long-tail distribution problem in the scene graph by transferring the knowledge learned from the head relationship (relationship with a larger order of magnitude instances) to the tail (relationship with a smaller order of magnitude instance) by means of knowledge transfer. \nThe deviation caused by the long-tail problem is widespread. Traditional debiasing methods are often unable to distinguish between good deviation (e.g., people riding a bicycle instead of eating) and bad deviation (e.g., on instead of walking/lying on). For this reason, TDE (Total Direct Effect)  explored a method based on causal inference biased training. TDE finds and eliminates bad bias by extracting counterfactual causality from the training graph.\n\\textcolor{black}{Recently, EBM (Energy Based Modeling)  pointed out that those methods  that use standard cross-entropy loss still essentially treat the prediction of objects and relationships in the scene graph as independent entities.\n\\begin{equation}\n    log P(SG|I)=\\sum_{o_i\\in O}log P(o_i,o_i^{bbox}|I)+\\sum_{r_{i\\rightarrow j\\in R}}log P(r_{i\\rightarrow j}|I).\n    \\label{eq:EBM}\n\\end{equation}\nThe modification of Eq.(\\ref{eq:EBM_0}), Eq(\\ref{eq:EBM}), explains the nature of this independent prediction. To this end, EBM proposes to use an energy-based learning framework  to generate scene graphs. This framework allows the use of \"loss\" that explicitly incorporates structure in the output space to train the SGG model. It still achieved good results under zero- and few-shot settings.} Similarly, exploring in different directions is very beneficial to alleviating the long-tail problem.\nAlthough researchers have done a lot of related research, the long-tail problem will remain a continuing hot issue in scene graph research.\n\\vspace{-1em}", "cites": [6419, 6456, 6422, 6418, 6438, 38, 6423, 4013, 6436, 284, 6416, 6457, 2329, 9021], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on long-tailed distributions in SGG, connecting ideas about predicate imbalance, training biases, and knowledge transfer. It provides critical analysis by highlighting the limitations of standard approaches, such as the independence assumption in cross-entropy loss. The section also abstracts the problem by identifying overarching patterns like the need for structured prediction and the use of prior knowledge to generalize to rare or unseen relationships."}}
{"id": "49619156-111e-4907-910f-8fa7848ab90a", "title": "Applications of Scene Graph", "level": "section", "subsections": ["bbf38020-b38a-4952-97b4-c4b58924d7f9", "09f71dca-623b-4f6c-8d16-8c2356c278cf", "74a4637e-c278-4356-8e4f-e1ba0a9f6488", "f3b20a97-4e99-4906-ab63-807fdfd13172", "72ded03d-f079-4bca-afb1-c7c577045dc5", "f8792837-a361-496a-ba37-a6f97e21820c"], "parent_id": "be806312-3b5b-4083-ab29-bf4d337ae115", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"]], "content": "\\label{sec:SGG_Applications}\n\\begin{figure*} \n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{figure/SGG_Applications}\n\t\\vspace{-1 em}\n\t\\caption{Examples of scene graph application scenarios. These applications include visual-textual transformers , image text retrieval , visual question answering , image understanding and reasoning , 3D scene graphs , and the detection and recognition of human-object and human-human interaction .}\n\t\\vspace{-1.5 em}\n\t\\label{fig:SGG_Applications}\n\\end{figure*}\nThe scene graph can describe the objects in a scene and the relationships between the objects, meaning that it provides a better representation for scene understanding-related visual and textual tasks and can greatly improve the model performance of these tasks. \nFig.\\ref{fig:SGG_Applications} presents some examples of scene graph application scenarios. Next, we will conduct a detailed review of these scene graph applications one by one. \n\\vspace{-1em}", "cites": [6459, 6462, 6461, 7014, 6460, 6464, 6412, 6463, 6458], "cite_extract_rate": 0.5, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by listing several application areas of scene graphs and citing relevant papers, but does not connect or integrate the ideas across sources. There is no critical analysis or evaluation of the cited works, and the section lacks abstraction or identification of broader patterns. It remains at a descriptive level."}}
{"id": "79514acc-2090-4abd-943d-7c7ad7b3f1a8", "title": "Image Generation", "level": "subsubsection", "subsections": [], "parent_id": "bbf38020-b38a-4952-97b4-c4b58924d7f9", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Visual-Textual Transformer"], ["subsubsection", "Image Generation"]], "content": "Although text-based image generation has made exciting progress in the context of simple textual descriptions, it is still difficult to generate images based on complex textual descriptions containing multiple objects and their relationships. Image generation based on scene graphs is better able to deal with complex scenes containing multiple objects and desired layouts. \nGenerating complex images from layouts is more controllable and flexible than text-based image generation. However, it remains a difficult one-to-many problem, and only limited information can be conveyed by a bounding box and its corresponding label. To generate a realistic image according to the corresponding scene graph with object labels and their relationships, Johnson et al.  proposed an end-to-end image generation network model. Compared with text-based image generation methods, a final complex image with many recognizable objects can be generated using this method by explicitly inferring the objects and relationships based on structured scene graphs. However, this image generation method  cannot introduce new or additional information to existing descriptions and is limited to generating images at one time. Therefore, Mittal et al. proposed a recursive network architecture  that preserves the image content generated in previous steps and modifies the accumulated images based on newly provided scene information. This method allows the context of sequentially generated images to be preserved by subjecting certain information to subsequent image generation conditions. However, there are still problems associated with ensuring that the generated image conforms to the scene graph and measuring the performance of the image generation models. Subsequently, an image generation method was proposed that harnesses the scene graph context and improves image generation . The scene context network encourages the generated images not only to appear realistic but also to respect the scene graph relationships. Similarly, Layout2Im (Layout-based Image generation)  is also an end-to-end model for generating images from layouts. Different from other related methods, Layout2Im breaks down the representations of each object into specified and unspecified parts, and individual object representations are grouped together for encoding the layouts. \nFrom , we can see that generating a layout from a scene graph is an important step in layout-based image generation. Therefore, Herzig et al. and Tripathi et al.  attempted to improve the quality of images generated from scene graphs by generating better layouts. Generating realistic images with complex visual scenes is a challenging task, especially when we want to control the layouts of the generated images. Herzig et al.  present a novel SRC (Soft Relations Closure) Module to inherently learn the canonical graph representations, with the weighted graph representations obtained from a GCN used to generate the scene layouts. SRC can canonicalize graphs to improve layout generation. Moreover, Tripathi et al.  proposed a scene layout generation system that generates structured scene layouts. \nSimilarly, to solve the layout prediction problem, Schroeder et al. proposed a layout prediction framework based on Triplet-Aware Scene Graph Embeddings . Triplet embeddings with supervisory signals are used to improve scene layout prediction, while a data augmentation technique is utilized to maximize triplet numbers during training. These two methods of additional supervision and data augmentation can enhance the embedding representation, enabling better layout outputs to be obtained.\nThe above-mentioned image generation method of the scene graph is based on the layout, and aims to ensure the semantic consistency between the generated image and the scene graph. However, they often overlook the visual details of the images and objects. PasteGAN  is a semi-parametric method for image generation based on scene graphs and object cropping, designed to improve the visual quality of generated images. The proposed Crop Refining Network and Object-Image Fuser with attention mechanism in PasteGAN can encode objects' spatial arrangement, appearances, and interactions. Compared with most scene graph image generation methods, PasteGAN can parameterize control of the appearance of objects in the generated images. In addition, one interesting approach to image generation utilizes image collections to generate a narrative collage based on scene graphs . In this process, the object relationship is crucial to the object positions in the narrative collage. However, the scene graphs used here to build and evaluate are primarily rule-based. \nFurthermore, SS-SGN (Spatio-Semantic Scene Graph Network)  proposes a scene graph-based image modification method that can interact with users. More specifically, the user only needs to change a certain node or edge of the scene graph and then apply this change to edit the image. This provides users with more flexibility to modify or integrate new content into the image.\n\\vspace{-1em}", "cites": [6465, 6459, 6462, 6466, 6470, 7014, 6468, 6469, 6467], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section demonstrates strong synthesis by connecting multiple scene graph-based image generation approaches, emphasizing layout generation, control over object appearance, and user interaction. It provides some critical analysis by highlighting limitations such as the inability to introduce new information and the challenge of ensuring semantic consistency. While it identifies some patterns (e.g., the importance of layout in realism), it does not fully abstract to a meta-level framework or principle."}}
{"id": "0d96d762-3dde-45d9-8835-db015b0cfd98", "title": "Image/Video Captioning", "level": "subsubsection", "subsections": [], "parent_id": "bbf38020-b38a-4952-97b4-c4b58924d7f9", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Visual-Textual Transformer"], ["subsubsection", "Image/Video Captioning"]], "content": "Traditional image captioning methods rely on the visual features of the objects detected in images. While, the natural language description is generated by models of natural language reasoning, such as RNN or LSTM. However, these methods cannot make full use of the semantic relationships between objects, which leads to the generated language description being inaccurate. The image captioning methods based on scene graphs solve this problem by capturing the relationship information between objects. \nThe image captioning method in  tried to learn the semantic representation by embedding a scene graph as an intermediate state. This method is easy to execute, does not require complex image preprocessing, and is competitive with existing methods. Since graphical representations with conceptual positional binding can improve image captioning, TPsgtR (Tensor Product Scene-Graph-Triplet Representation)  is proposed for image caption generation using regional visual features. In TPsgtR, the technique of neuro-symbolic embedding can embed the relationships into concrete forms (neural symbolic representations), rather than relying on the model to form all possible combinations. These neural symbolic representations aid in defining the neural symbolic space and can be transformed into better captions. In addition, the visual relational features can be learned from a neural scene graph generator (Stacked Motif Network) , and will facilitates the grounding of language in visual relations.\nFrom the perspective of human cognition, vision-based language generation is related to high-level abstract symbols. Abstracting the scenes into symbols will accordingly provide a clear path to language description generation. Therefore, SGAE (Scene Graph Auto-Encoder)  is proposed to incorporate these inductive biases into the encoder-decoder models for image captioning, an approach expected to help this encoder-decoder model exhibit less overfitting to the dataset bias. \nSimilarly, to be able to generate the type of image descriptions desired by human, Chen et al. proposed an ASG (Abstract Scene Graph) structure  to represent user intentions, as well as to control the generated descriptions and detailed description of the scenes. ASG can identify users' intention and semantics in graphs, which enables it to generate the required caption, and actively considers users' intentions to produce the desired image caption, which significantly enhances the image caption diversity. Unlike ASG, which generates diversified captions by focusing on different combinations of objects, SGD (Scene Graph Decomposition)  is to decompose the scene graph into a set of subgraphs, then use a deep model to select the most important subgraphs. SGD can obtain accurate, diverse, and controllable subtitles through the use of subgraphs.\nIn previous work, entities in images are often considered separately, which leads to the lack of structured information in the generated sentences. Scene graphs are structured by leveraging both visual features and semantic knowledge, and image captioning frameworks are proposed based on the structural-semantic information . In , a hierarchical-attention-based module is designed to learn the correlation scores of the visual features and semantic relationship features, which are used to obtain the final context feature vector rather than simply concentrating the two feature vectors into a single vector. SGC (Scene Graph Captioner)  captures the integrated structural-semantic features from visual scenes, after which LSTM-based models translate these semantic features into the final text description. \nFurthermore, a scene graph can also be used to generate the story from an image stream; the SGVST proposed in  can model the visual relations both within one image and across images, which is conducive to image captioning. This method significantly improves the fluency and richness of the generated stories.\nAt present, most image captioning models rely heavily on image-caption pair datasets, while unpaired image captioning presents great challenges when it comes to extracting and mapping different features of visual and textual modalities. Therefore, there are high costs associated with obtaining large-scale paired data of images and texts. To solve this problem, an unpaired scene graph-based image captioning approach is presented in  to capture rich semantic information from scenes. It further proposes an unsupervised feature extraction method to learn the scene graph features by mapping from the visual features of the image to the textual features of the sentences. \n\\vspace{-1em}", "cites": [6471, 6473, 6474, 6472], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information well by connecting multiple scene graph-based captioning approaches (TPsgtR, SGAE, ASG, SGD) under a common theme of leveraging structured semantic relationships. It provides some critical perspective by highlighting limitations of traditional captioning methods and the challenges of unpaired data. While it identifies patterns in how scene graphs improve captioning, it does not fully abstract into a meta-level framework or theory."}}
{"id": "09f71dca-623b-4f6c-8d16-8c2356c278cf", "title": "Cross-Modal Retrieval", "level": "subsection", "subsections": [], "parent_id": "49619156-111e-4907-910f-8fa7848ab90a", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Cross-Modal Retrieval"]], "content": "Cross-modal retrieval is also a common application in scene graph research.\nImage-text retrieval is a classic multi-modal retrieval task. The key to image-text cross-modal retrieval concerns learning a comprehensive and unified representation to represent multi-modal data. The scene graph is a good choice in this context. \nImage retrieval via scene graph was first applied in . By replacing textual descriptions with scene graphs for image retrieval, the model can accurately describe the semantics of images without unstructured texts, and can further retrieve related images in more open and interpretable retrieval tasks. Subsequently, another scene graph-based image retrieval method was proposed by Schuster et al.. These may be the two earliest methods to involve the construction and applications of the scene graph. The results of their experiments show that image retrieval using scene graphs achieves better results than traditional image retrieval methods based on low-level visual features.\nAt present, most existing cross-modal scene retrieval methods ignore the semantic relationship between objects in the scene and the embedded spatial layout information. Moreover, these methods adopt a batch learning strategy and are thus unsuitable for processing stream data. To solve these problems, an online cross-modal scene retrieval method  is proposed that utilizes binary representations and semantic graphs. \nThe semantic graph can serve as a bridge between the scene graph and the corresponding text that enables the measurement of the semantic correlation between different modal data. \nHowever, most text-based image retrieval models experience difficulties when searching large-scale image data, such that the model needs to resort to an interactive retrieval process through multiple iterations of question-answering. To solve this problem, Ramnath et al. proposed an image retrieval framework based on scene graph , which models the retrieval task as a learnable graph-matching problem between query graphs and catalog graphs. Their approach incorporated the strategies and structural constraints of the retrieval task into inference modules using multi-modal graph representation. \nSimilarly, SQIR (Structured Query-based Image Retrieval using scene graphs)  is also a scene graph based image retrieval framework. The difference is that SQIR determined that structured queries (e.g. \"little girl riding an elephant\") are more likely to capture the interaction between objects than single-object queries (e.g. \"little girl\", \"elephant\"). To this end, SQIR proposes an image retrieval method based on scene graph embedding, which treats visual relationships as directed subgraphs of the scene graph for the purposes of the structured query.\nIn addition, Wang et al.  proposed two kinds of scene graphs (visual scene graph (VSG) and textual scene graph (TSG)) to represent image and text respectively, and matched two different modalities through the unified representation of scene graphs.\nHowever, the above methods are often based on fixed text or images for cross-modal retrieval. GEDR (Graph Edit Distance Reward)  proposes a more creative and interactive image retrieval method. More specifically, similar to SS-SGN , GEDR attempts to edit the scene graph, while GEDR edits the scene graph according to the user's text instructions on the given image prediction scene graph to perform image retrieval. This makes image retrieval more flexible and promotes easier user interaction. \nIn addition, the scene graph is also used as an intermediate representation of 2D-3D and 3D-3D matching . Although there is not much related work, scene graphs are still expected to perform well in other similar cross-modal retrieval tasks.\n\\begin{table*}[t]\n    \\centering\n    \\caption{Statistics of the scene graph dataset. \"-\" indicates that this attribute is not released, \\textcolor{black}{and \"\\XSolidBrush\" indicates that the attribute is not applicable.}}\n    \\vspace{-1 em}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{|c|l|l|c|cc|ccc|cc|}\n        \\hline\n        &\\multirow{3}{*}{Dataset} &\\multirow{3}{*}{Size}&\\makecell[c]{\\#Action}& \\multicolumn{2}{c|}{Objects}& \\multicolumn{3}{c|}{Relationships} & \\multicolumn{2}{c|}{Predicates}\\\\ \n        & && categories & \\#bbox & \\#categories & \\#triplet & \\#categories &\\makecell[c]{\\#zero-shot\\\\w \\& w/o (categories)}&\\#categories &  \\makecell[c]{\\#predicates per\\\\object category}\\\\ \\hline\n        \\multirow{10}{*}{Image}&RW-SGD&5K&\\XSolidBrush&94K&7K & 113K&1K&-&-&3.3\\\\\n        &VRD &5K&\\XSolidBrush&-&100&38K&7K&-&-&24.25\\\\\n        &UnRel &1K&\\XSolidBrush&-&-&76&-&-&-&-\\\\\n        &HCVRD &53K&\\XSolidBrush&-&1.8K&257K&28K&18.5K \\& 9.8k&927 &10.63\\\\\n        &Open Images  & - &\\XSolidBrush& 3M & 57 & 375K & 10 & - &- & -\\\\\n        &Visual Phrase  &3K&\\XSolidBrush&3K&8&1.8K & 17& -&- & 120\\\\\n        &Scene Graph & 5K &\\XSolidBrush& 69K & 266 & 110K & 23K& -&- &2.3\\\\\n        &VG &108K &\\XSolidBrush &3.8M & 33.8K &-& 42K& -&- &-\\\\\n        &VrR-VG &59K & \\XSolidBrush & 282K & 1.6K & 203K & 117& -&- &-\\\\\n        &VG150 & 88K & \\XSolidBrush & 739K & 150 & 413K & 50& -&- &-\\\\ \\hline\n        \\multirow{2}{*}{Video}&CAD120++  & 0.5K (0.57\\textit{h}) & 10 & 64K & 13 & 32K & 6& -&- &-\\\\\n        &Action Genome &10K (82\\textit{h}) & 157 & 0.4M & 35 & 1.7M & 25& -&- &-\\\\ \\hline\n        \\multirow{4}{*}{3D}&Armeni et al.  & \\makecell[l]{35 buildings\\\\727 rooms} & \\XSolidBrush & 3K & 28 & - & 4& -&- &-\\\\\n        &3DSSG &\\makecell[l]{1482 scans \\\\478 scenes} & \\XSolidBrush & 48K & 534 & 544K & 40& -&- &-\\\\\n        \\hline\n    \\end{tabular}\n    }\n    \\label{tab:datasets}\n    \\vspace{-1 em}\n\\end{table*}\n\\vspace{-1em}", "cites": [6441, 6478, 6475, 6480, 6476, 6477, 6467, 6479, 5257, 284, 9025, 9021, 6412], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple scene graph-based cross-modal retrieval approaches, linking them to common challenges such as semantic relationships and scalability. It critically discusses limitations like ignoring spatial layout and unsuitability for stream data, while introducing frameworks like GEDR and SQIR. It abstracts to some extent by identifying the role of semantic graphs as a bridge between modalities but does not provide deep meta-level insights."}}
{"id": "74a4637e-c278-4356-8e4f-e1ba0a9f6488", "title": "Visual Question Answering", "level": "subsection", "subsections": [], "parent_id": "49619156-111e-4907-910f-8fa7848ab90a", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Visual Question Answering"]], "content": "VQA is also a multimodal feature learning task. Compared with traditional VQA methods, scene graphs can capture the essential information of images in the form of graph structures, which helps scene graph-based VQA methods outperform traditional algorithms. \nInspired by the application of traditional QA systems on knowledge graphs, an alternative scene graph-based approach was investigated . Zhang et al. explored how to effectively use scene graphs derived from images for visual feature learning, and further applied graph networks (GN) to encode the scene graph and perform reasoning according to the questions provided. Moreover, Yang et al.  aimed to improve performance on VQA tasks through the use of scene graphs, and accordingly proposed a new model named Scene GCN (Scene Graph Convolutional Network)  to solve the relationship reasoning problem in a visual question-and-answer context. To effectively represent visual relational semantics, a visual relationship encoder is built to yield discriminative and type-aware visual relationship embeddings, constrained by both the visual context and language priors. To confirm the reliability of the results predicted by VQA models, Ghosh et al.  proposed an approach named XQA (eXplainable Question Answering), which may be the first VQA model to generate natural language explanations. In XQA, natural language explanations that comprise evidence are generated to answer the questions, which are asked regarding images using two sources of information: the entity annotations generated from the scene graphs and the attention map generated by a VQA model. As can be determined from these research works, since scene graphs are able to provide information regarding the relationships between objects in visual scenes, there is significant scope for future research into scene graph-based VQA.\n\\vspace{-1em}", "cites": [6460, 6481], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates ideas from multiple papers by highlighting how scene graphs are used for relationship reasoning in VQA, and connects their approaches to broader themes such as structured visual representations and explainability. While it provides a coherent narrative, the critical evaluation is limitedâ€”there is no direct comparison or critique of the methods' strengths and weaknesses. It offers some abstraction by framing scene graphs as a valuable tool for relational understanding but stops short of proposing a novel conceptual framework."}}
{"id": "f3b20a97-4e99-4906-ab63-807fdfd13172", "title": "Image Understanding and Reasoning", "level": "subsection", "subsections": [], "parent_id": "49619156-111e-4907-910f-8fa7848ab90a", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Image Understanding and Reasoning"]], "content": "Fully understanding an image necessitates the detection and recognition of different visual components, as well as the inference of higher-level events and activities through the combination of visual modules, reasoning modules, and priors.  Therefore, the scene graph with triplets of $\\langle subject-relation-object \\rangle $ contains information that is essential to image understanding and reasoning. \nVisual understanding requires the model to have visual reasoning ability. However, existing methods tend to pay less attention to how to make a machine (model) \"think\", and instead attempt to extract the pixel-level features directly from the images; this is despite the fact that it is difficult to carry out accurate reasoning using pixel-level visual features alone. The task of image reasoning should be based directly on the detected objects, rather than on pixel-level visual features. \nMore specifically, similar to traditional visual understanding methods, the objects, scenes, and other constituent visual components first need to be detected by a deep learning perception system from input images. A common-sense knowledge base is then built by a Bayesian Network based on the image annotations, while the object interactions are predicted by an intermediate knowledge structure called SDG (Scene Description Graph) . These object interaction priors can be used as the input for image reasoning models and applied to other visual reasoning tasks. In addition, we should focus on teaching a machine (model) to ``think'' for visual reasoning tasks, such as, by using XNMS (Explicit and Explicit Neural Modules). XNMS defines several neural modules that are responsible for specific functions (such as object location, attention transformation, etc.) based on scene graphs. \nXNMS separates \"high-level\" visual reasoning from \"low-level\" visual perception and forces the model to focus on how to \"think\" rather than on simple visual recognition. Since image reasoning is based on object detection and recognition, we hope to learn the mapping from the shared visual feature space by objects and relations to two independent semantic embedding spaces (objects and relations). Moreover, to avoid confusion between these two feature spaces, the visual features of the relationships are not transferred to the objects; instead, only the object features are transferred . Visual reasoning based on scene graphs has its applications for reasoning civic issues , which are mainly reflected by the relationships between the objects. Furthermore, generating a semantic layout from a scene graph is a crucial intermediate task in the process of connecting textual descriptions to the relevant images. \n\\vspace{-1em}", "cites": [6464, 6482, 6458], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from the cited papers, particularly connecting the idea of using scene graphs for visual reasoning with methods like XNMS and SDG. It provides a coherent narrative by emphasizing the shift from perception-based models to reasoning-focused approaches. While it offers a critical view of current methods by pointing out their overreliance on pixel-level features, it could delve deeper into the limitations of the proposed solutions. The section abstracts the concepts to a meta-level by discussing broader implications for image understanding and civic issue resolution."}}
{"id": "72ded03d-f079-4bca-afb1-c7c577045dc5", "title": "3D Scene Understanding", "level": "subsection", "subsections": [], "parent_id": "49619156-111e-4907-910f-8fa7848ab90a", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "3D Scene Understanding"]], "content": "Similar to the 2D scene graph generated from 2D images, a scene graph can also be constructed from 3D scenes as a 3D scene graph, which can provide numerically accurate quantification of the object relationships in 3D scenes. A 3D scene graph succinctly describes the environments by abstracting the objects and their relationships in 3D space in the form of graphs. The construction of a 3D scene is very helpful for the understanding of the indoor complex environment and other tasks.\nTo construct a 3D scene graph, it is necessary to locate the different objects, identify the elements, attributes, and relationships between the objects in 2D images, and then use all of this information to construct a 3D scene. \nIn  and , the basic process used to generate 3D scene graphs is similar, and there are several similar methods (Faster RCNN or Mask RCNN) used to extract the required information from a number of 2D images. However, there are differences in the specific details. For example, different methods have been proposed for constructing 3D scene graphs using the relevant information obtained from 2D images. Specifically, in , Armeni et al. tried to construct a 3D scene graph of a building. The constructed 3D scene graph consists of four layers: the building, rooms, objects, and cameras. In each layer, there are a set of nodes with their attributes and edges representing the relationships between nodes. Moreover, in , Kim et al. proposed the 3D scene graph to promote the intelligent agents gathering the semantics within the environments, then applying the 3D scene graph to other downstream tasks. \nFurthermore, the applicability of the 3D scene graph  is verified by demonstrating two major applications of VQA (Visual Question and Answering) and task planning, achieving better performance than the traditional 2D scene graph-based methods. \nSimilarly, 3DSSG (3D Semantic Scene Graphs)  and 3D-DSG (3D Dynamic Scene Graphs)  also studied the scene understanding of indoor 3D environments. More specifically, 3DSSG proposes a learning method based on PointNet and GCN that moves from the scene point cloud regression to the scene graph. This method has achieved good performance in the 3D scene retrieval task. \n3D-DSG attempts to narrow the perception gap between robots and humans in a 3D environment by capturing the metrics and semantics of the dynamic environment. These works have effectively deepened people's understanding of 3D scenes and promoted related applications.\n\\vspace{-1em}", "cites": [6415, 6479, 6412], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.7, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of methods and approaches for 3D scene graph generation, mentioning key works and their structures. However, it lacks deeper synthesis by not clearly connecting the different contributions into a unified narrative. There is minimal critical analysis or identification of broader patterns or principles, focusing instead on surface-level descriptions of each paper's approach."}}
{"id": "f8792837-a361-496a-ba37-a6f97e21820c", "title": "Human-Object / Human-Human Interaction", "level": "subsection", "subsections": [], "parent_id": "49619156-111e-4907-910f-8fa7848ab90a", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Applications of Scene Graph"], ["subsection", "Human-Object / Human-Human Interaction"]], "content": "\\label{sec:HOI/HHI}\nThere are many fine-grained categories of things in the scene, which can be generally divided into humans and objects. Therefore, some scene graph-related research works have focused on the detection and recognition of HOI (Human-Object Interaction)  and HHI (Human-Human Interaction)  in scenes. In these works, the long tail of relationships remains a problem to be solved , while the detection and recognition of interpersonal relationships have also been proposed ; these human-human relationships can be used to further infer the visual social relationships in a scene. In this section, we will discuss the existing method-based scene graph for the detection and recognition of human-object interactions and human-human interaction.\nFor HOI, there are two main benchmarks: HICO-DET  and HCVRD . This type of visual relational HOI dataset has a natural long-tail distribution and has one-shot or zero-shot HOI detection, which makes it very difficult to conduct model training for most HOI methods in order to achieve better performance. In addition, the task of HOI relies on object detection and involves the construction of human and object pairs with high complexity . The zero-shot learning approach is introduced to address the challenges of scaling HOI recognition to the long tail of categories in the HOI dataset . In addition, HOI recognition is an important means of distinguishing between different types of human actions that occur in the real world. Most HOI methods consist of two steps: human-object pair detection and HOI recognition . The detected proposals of paired human-object regions are passed into a multi-stream network (HO-RCNN  and iCAN ) to facilitate the classification of HOIs by extracting the features from the detected humans, objects, and the spatial relations between them. Moreover, the structural knowledge from the images is also beneficial for HOI recognition, with GCN being a commonly used model for learning structural features. For example, GPNN (Graph Parsing Neural Network) is proposed in  to infer the HOI graph structure represented by an adjacency matrix and node labels. Furthermore, in order to reduce the number of human-object pairs, some inter-activeness priors can be explored for HOI detection; these indicate whether a human and object have interactions with each other , and can be learned from the HOI datasets, regardless of HOI category settings.\nThe above HOI approaches focus primarily on the detection, selection, and recognition of human-object pairs. However, they do not consider whether the approach adopted for the corresponding HOI tasks should be human-centric or object-centric. In a given scene, however, most human-object interactions are human-centric. Therefore, some HOI works have adopted human-centric approaches such as human-to-object  and human-to-human . Inspired by a human-centric approach, we can first identify a human in a scene, then select the human-object pairs of interest to facilitate the recognition of human-object pairs in multi-stream networks; of these, HO-RCNN  is a representative example. In addition, the information of HOI can be used for action recognition. InteractNet may be the first proposed multi-task network for human-centric HOI detection and action recognition. This network model can achieve the task of detecting $\\langle human-verb-object \\rangle $ triplets in challenging images. \nMoreover, it was hypothesized that the visual features of the detected persons contain powerful cues for localizing the objects with which they interact so that the model learns to predict the action-specific density over the object locations based on the visual features of the detected persons.\nFurthermore, interactions can also occur between humans in a scene, which indicate social relationships. The identification of social relationships in a scene requires a deeper understanding of the scene, along with a focus on human-to-human rather than human-to-object interaction. Therefore, social relationship detection is a task of human-centric HHI, and related works mainly consist of human-human pair detection and social relationship recognition using two network branches . For social relationship recognition, contextual cues can be exploited by a CNN model with attention mechanisms . Adaptive Focal Loss is designed by leveraging the ambiguous annotations so that the model can more effectively learn the relationship features; the goal here is to solve the problem of uncertainty arising during the visual identification of social relationships. The global visual features and mid-level details are also beneficial for social relationship recognition, and GCN is a commonly used model for predicting human social relationships by integrating the global CNN features .\n\\vspace{-1em}", "cites": [6484, 6461, 262, 6483, 6463], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a coherent narrative on HOI and HHI detection, integrating concepts such as human-centric vs. object-centric approaches, multi-stream networks, and graph-based methods like GCN and GPNN. It identifies challenges like long-tail distribution and ambiguous annotations, showing some critical evaluation. However, it could offer deeper comparative analysis and more abstract principles to elevate the insight level further."}}
{"id": "ee2b18be-16f9-4018-8ddf-0dc93a3c6f02", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "0691aff5-a1e8-40f0-bc70-7fc842d4b942", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Datasets and performance evaluation"], ["subsection", "Datasets"]], "content": "\\label{sec:SGG_Datasets}\nIn this section, we present a detailed summary of the datasets commonly used in SGG tasks, so that interested readers can make their selection accordingly. \nWe investigated a total of 14 commonly used datasets, including 10 static image scene graph datasets, 2 video scene graph datasets and 2 3D scene graph datasets.\n\\begin{itemize}\n    \\item \\textit{RW-SGD}  is constructed by manually selecting 5,000 images from YFCC100m  and Microsoft COCO datasets , after which AMT (Amazonâ€™s Mechanical Turk) is used to produce a human-generated scene graph from these selected images.\n    \\item \\textit{Visual Relationship Dataset} (VRD)  is constructed for the task of visual relationship prediction. The construction of VRD highlights the long tail of infrequent relationships.\n    \\item \\textit{UnRel Dataset} (UnRel-D)  is a new challenging dataset of unusual relations, and contains more than 1,000 images, which can be queried with 76 triplet queries. The relatively small amount of data and visual relationships in UnRel-D make the long-tail distribution of relationships in this dataset not obvious.\n    \\item \\textit{HCVRD Dataset}  contains 52,855 images with 1,824 object categories and 927 predicates, along with 28,323 relationship types. Similar to VRD, HCVRD also has a long-tail distribution of infrequent relationships. \n    \\item \\textit{Open Images}  is a large-scale dataset, which provides a large number of examples for object detection and relationship detection.\n    \\item \\textit{Visual Phrase}  is a dataset that contains visual relationships, and it is mainly used to improve object detection. It contains 13 common types of relationships.\n    \\item \\textit{Scene Graph}  is a dataset containing visual relationships, which is designed to improve image retrieval tasks. Although it contains 23,190 relationship types, it has only 2.3 predicates per object category.\n    \\item \\textit{Visual Genome} (VG) , \\textit{VG150}  and \\textit{VrR-VG} (Visually-Relevant Relationships Dataset) . VG is a large-scale visual dataset consisting of various components, including objects, attributes, relationships, question-answer pairs, and so on.\n    VG150 and VrR-VG are two datasets constructed based on VGD. VG150 uses VGD to eliminate objects with poor annotation quality, overlapping bounding boxes, and/or ambiguous object names, and retains 150 commonly used object categories.\n    VrR-VG is constructed based on VGD by filtering out visually irrelevant relationships. The top 1,600 objects and 500 relationships are selected from VG by applying a hierarchical clustering algorithm on the relationshipsâ€™ word vectors. Therefore, VrR-VG is a scene graph dataset used to highlight visually relevant relationships; the long-tail distribution of relationships is largely suppressed on this dataset.\n    \\item \\textit{CAD120++}  and \\textit{Action Genome}  are two video action reasoning datasets containing scenes of human daily life. They can be used for the analysis of tasks related to spatio-temporal scene graphs.\n    \\item \\textit{Armeni et al.}  and \\textit{3DSSG}  are two large-scale 3D semantic scene graphs containing indoor architecture or 3D reconstructions of real scenes. They are widely used as part of research in areas related to 3D scene understanding (robot navigation, augmented and virtual reality, etc.).\n\\end{itemize}\nThe information of these datasets is summarized in TABLE \\ref{tab:datasets}. This includes various attributes of datasets commonly used in SGG tasks. \n\\vspace{-1em}", "cites": [6476, 6441, 6479, 486, 6477, 9021, 5257, 6412, 284, 6475], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a detailed listing of datasets relevant to scene graph generation, with some basic synthesis of their purposes and characteristics, such as long-tail distributions and relevance to specific tasks. However, it largely describes each dataset individually without deep comparative analysis or critical evaluation of their strengths and weaknesses. There is minimal abstraction to broader principles or frameworks beyond the specific datasets."}}
{"id": "51cff6cc-e78a-4da1-ad8d-17c7d56e2e91", "title": "Evaluation method and performance comparison", "level": "subsection", "subsections": [], "parent_id": "0691aff5-a1e8-40f0-bc70-7fc842d4b942", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Datasets and performance evaluation"], ["subsection", "Evaluation method and performance comparison"]], "content": "\\label{sec:Evaluation method and performance comparison}\n\\begin{figure} \n\\center{\\includegraphics[width=0.49\\textwidth] {figure/Evaluation1.png}}\n\\vspace{-2 em}\n\\caption{Four commonly used evaluation methods in visual relationship detection.}\n\\vspace{-1.5 em}\n \\label{fig:Evaluation1}\n\\end{figure}\nVisual relationship detection is the core of SGG. The commonly used evaluation methods for visual relationship detection are as follows :\n\\begin{itemize}\n    \\item \\textit{Predicate detection (Predicate Det.)} (Fig.\\ref{fig:Evaluation1}(a)). It is required to predict possible predicates between object pairs given a set of localized objects and object labels. The goal is to study relationship prediction performance without being restricted by object detection .\n    \\item \\textit{Phrase detection (Phrase Det.)} (Fig.\\ref{fig:Evaluation1}(c)). It is required to predict a $\\langle subject-predicate-object \\rangle$ triplets for a given image and locate a bounding box for the entire relationship that overlaps the ground truth box by at least 0.5 .\n    \\item \\textit{Relationship detection (Relationship Det.)} (Fig.\\ref{fig:Evaluation1}(d)). It is required to output a set of triplets of a given image, while localizing the objects in the image.\n\\end{itemize}\nOn the other hand, the above three visual relationship detection evaluation methods do not consider the long-tail distribution phenomenon and the graph-level coherence that are common in real scenes. For this reason,  further improves the evaluation method of SGG; that is, SGG diagnosis. SGG diagnosis is based on the following three key metrics:\n\\begin{itemize}\n    \\item \\textit{Relationship Retrieval (RR).} This can be further divided into three sub-tasks. \n    (1) Predicate Classification (PredCls): the same as Predicate Det.. \n    (2) Scene Graph Classification (SGCls) (Fig.\\ref{fig:Evaluation1}(b)): its input is a ground true bounding box without labels.\n    (3) Scene Graph Detection (SGDet): detects scene graphs from scratch. It is the same as Relationship Det..\n    \\item \\textit{Zero-Shot Relationship Retrieval (ZSRR)} . The visual relationship that ZSRR requires to be tested has never been observed in the training set. Similar to RR, ZSRR has the same three sub-tasks.\n    \\item \\textit{Sentence-to-Graph Retrieval (S2GR).} Both RR and ZSRR are evaluated at the triplet level. S2RR aims to evaluate the scene graph at the graph level. It uses image caption sentences as queries to retrieve images represented by scene graphs.\n\\end{itemize}\n\\begin{table*}[t]\n\\centering\n\\caption{\\textcolor{black}{Performance comparison of the SGG method with graph constraints on VG150.} . '-' indicates that there is no corresponding result released in the paper. Note that some methods use multiple different types of networks or priors, and these are categorized according to the most prominent method they use. $\\dagger$ denotes the re-implemented version from . * indicates that the method evaluates on other datasets. The best performance is shown in bold.}\n\\vspace{-1 em}\n\\label{tab:PerformanceComparison}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|l|l|cc|cc|cc|}\n\\hline\n&&& \\multicolumn{2}{c|}{PredCls} & \\multicolumn{2}{c|}{SGCls} & \\multicolumn{2}{c|}{SGDet} \\\\\n&&Method & R@\\{20/50/100\\} & mR@\\{20/50/100\\} & R@\\{20/50/100\\} & mR@\\{20/50/100\\} & R@\\{20/50/100\\} & mR@\\{20/50/100\\} \\\\ \\hline\n\\multirow{28}{*}{SGG}&\\multirow{4}{*}{TransE}& VTransE*  &  -/-/- &  -/-/- &  -/-/- &  -/-/- &  -/5.5/6.0 &  -/-/- \\\\\n&& GPS-Net  & \\textbf{67.6/69.7/69.7} & -/-/22.8 & \\textbf{41.8/42.3/42.3} & -/-/12.6 & 22.3/\\textbf{28.9/33.2} & -/-/9.8 \\\\\n&&VTransE-TDE  & -/-/- & \\textbf{18.9}/25.3/28.4 & -/-/- & \\textbf{9.8/13.1}/14.7 & -/-/- & \\textbf{6.0/8.5/10.2} \\\\\n&&Motifs-TDE  & 33.6/46.2/51.4 & 18.5/\\textbf{25.5/29.1}  & 21.7/27.7/29.9 & \\textbf{9.8/13.1/14.9} & 12.4/16.9/20.3& 5.8/8.2/9.8 \\\\\n&&MemNet  & 42.1/53.2/57.9 & -/-/- & 23.3/27.8/29.5 & -/-/- & 7.7/11.4/13.9 & -/-/- \\\\\n&&LSBR & 60.3/66.2/68.0 & -/-/- & 33.6/37.5/38.3 & -/-/- &  \\textbf{23.6}/28.2/31.4  & -/-/- \\\\ \n\\cline{2-9}\n&\\multirow{5}{*}{CNN}& Pixels2Graphs  & 47.9/54.1/55.4 & -/-/- & 18.2/21.8/22.6 & -/-/- & 6.5/8.1/8.2 & -/-/- \\\\\n&&Graph R-CNN  & -/54.2/59.1 & -/-/- & -/29.6/31.6 & -/-/- & -/11.4/13.7 & -/-/- \\\\\n&&HCNet  & 59.6/66.4/68.8 & -/-/- & 34.2/36.6/\\textbf{37.3} & -/-/- & \\textbf{22.6}/28.0/31.2 & -/-/- \\\\\n&&Assisting-SGG & \\textbf{67.2/68.9/68.9} & -/-/- & \\textbf{36.4/37.0}/37.0 & -/-/- & 21.7/\\textbf{28.3/32.6}  & -/-/- \\\\ \n&&\\textcolor{black}{FCSGG}  & 33.4/41.0/45.0 & \\textbf{4.9/6.3/7.1} & 19.0/23.5/25.7 & \\textbf{2.9/3.7/4.1} & 16.1/21.3/25.1  & \\textbf{2.7/3.6/4.2} \\\\\n\\cline{2-9}\n&\\multirow{11}{*}{\\makecell{RNN/\\\\LSTM}} & IMP  & -/44.8/53.0 & -/6.1/8.0 & -/21.7/24.4 & -/3.1/3.8 & -/3.4/4.2 & -/0.6/0.9 \\\\ \n&&\\textcolor{black}{IMP$\\dagger$}  & 58.5/65.2/67.1 & -/9.8/10.5 & 31.7/34.6/35.4 & -/5.8/6.0 & 14.6/20.7/24.5  & -/3.8/4.8 \\\\\n&&FREQ  & 49.4/59.9/64.1 & 8.3/13.0/16.0 & 27.7/32.4/34.0 & 5.1/7.2/8.5 & 17.7/23.5/27.6  & 4.5/6.1/7.1 \\\\\n&&FREQ+Overlap  &  53.6/60.6/62.2  & -/-/- & 29.3/32.3/32.9 & -/-/- & 20.1/26.2/30.1   &-/-/- \\\\\n&&VCTree-SL  & 59.8/66.2/67.9 & -/-/- & 35.0/37.9/38.6 & -/-/- & 21.7/27.7/31.1 & -/-/- \\\\\n&&VCTree-HL  & 60.1/66.4/68.1 & 14.0/17.9/19.4 & 35.2/38.1/38.8 & 8.2/10.1/10.8 & 22.0/27.9/31.3 & 5.2/6.9/8.0 \\\\\n&&VCTree L2+cKD & 59.0/65.4/67.1 & 14.4/18.4/20.0 & 41.4/45.2/46.1 & 9.7/12.1/13.1 & 24.8/32.0/36.1  & 5.7/7.7/9.1 \\\\ \n&&VCTree-TDE$_{Cross Entropy}$  & -/-/- &16.3/22.85/26.26& -/-/- &11.85/15.81/17.99& -/-/- &6.59/8.99/10.78 \\\\\n&&VCTree-TDE$_{EBM\\_Loss}$ & -/-/- &\\textbf{19.87/26.66/29.97}& -/-/- &\\textbf{13.86/18.2/20.45}& -/-/- &\\textbf{7.1/9.69/11.6} \\\\\n&&SIG & 59.8/66.3/68.1 & -/-/- & 33.8/36.6/37.3 & -/-/- & 21.6/27.5/30.9 & -/-/- \\\\\n&&PANet &  59.7/66.0/67.9 & -/-/- & 37.4/40.9/41.8 & -/-/- & 21.5/26.9/29.9  & -/-/- \\\\\n&&BGT-Net &  \\textbf{60.9/67.1/68.9} & -/-/- & \\textbf{41.7/45.9/47.1} & -/-/- & \\textbf{25.5/32.8/37.3}  & -/-/- \\\\ \n\\cline{2-9}\n&\\multirow{13}{*}{Graph}&CMAT  & 60.2/66.4/68.1 & -/-/- & 35.9/\\textbf{39.0/39.8} & -/-/- & \\textbf{22.1}/27.9/31.2 & -/-/- \\\\\n&&CMAT-XE  & -/-/- & -/-/- & 34.0/36.9/37.6 & -/-/- & -/-/- & -/-/- \\\\\n&&RelDN  & \\textbf{66.9}/68.4/68.4 & -/-/- & \\textbf{36.1}/36.8/36.8 & -/-/- & 21.1/28.3/32.7 & -/-/- \\\\\n&&MSDN  & -/67.0/\\textbf{71.0} & -/-/- & -/-/- & -/-/- & -/10.7/14.2 & -/-/- \\\\\n&&FactorizableNet*  & -/22.8/28.6 & -/-/- & -/-/- & -/-/- & -/13.1/16.5 & -/-/- \\\\ \n&&ARN & -/65.1/66.9 & -/-/- & -/36.5/38.8 & -/-/- & -/-/-  & -/-/- \\\\ \n&&VRF & -/56.7/57.2 & -/-/- & -/23.7/24.7 & -/-/- & -/13.2/13.5  & -/-/- \\\\ \n&&HOSE-Net & -/\\textbf{70.1}/70.1 & -/-/- & -/ 37.3/37.3& -/-/- & -/\\textbf{28.9/33.3} & -/-/- \\\\ \n&&IMP+GLAT+Fusion & -/-/- & -/12.1/12.9 & -/-/- & -/6.6/ 7.0 & -/-/-  & -/-/- \\\\ \n&&SNM+GLAT+Fusion & -/-/- & -/14.1/15.3 & -/-/- & -/7.5/7.9 & -/-/-  & -/-/- \\\\ \n&&KERN+GLAT+Fusion & -/-/- & -/17.8/19.3 & -/-/- & -/9.9/10.4 & -/-/-  & -/-/- \\\\ \n&&Dual-ResGCN & 60.2/66.6/68.2 & \\textbf{15.6/19.7/21.5} & 35.4/38.3/39.1 & \\textbf{9.1/11.1/12.0} & \\textbf{22.1}/28.1/31.5  & \\textbf{6.1/8.4/9.5} \\\\\n&&\\textcolor{black}{RAAL-SGG} & 59.1/66.2/68.4 & 14.4/18.3/19.9  & 33.5/36.7/37.6 & 7.9/9.6/10.3 & 21.7/27.3/29.9  & 4.9/6.5/7.4\\\\\n\\hline\n\\multirow{12}{*}{\\makecell[l]{Prior\\\\knowledge}}&\\multirow{2}{*}{\\makecell[l]{Language\\\\Prior}} & SG-LP  & -/27.9/35.0 & -/-/- & -/11.8/14.1 & -/-/- & -/0.3/0.5 & -/-/- \\\\\n&&TFR & \\textbf{40.1/51.9/58.3} & -/-/- & \\textbf{19.6/24.3/26.6} & -/-/- & \\textbf{3.4/4.8/6.0}  & -/-/- \\\\\n\\cline{2-9}\n&\\multirow{4}{*}{\\makecell[l]{Statistical\\\\Prior}}&KERN & \\textbf{59.1/65.8}/67.6 & -/17.7/19.2 & 32.3/\\textbf{36.7}/37.4 & -/9.4/10.0 & \\textbf{22.3}/27.1/29.8  & -/6.4/7.3 \\\\\n&&MotifNet  & 58.5/65.2/67.1 & \\textbf{10.8}/14.0/15.3 & \\textbf{32.9}/35.8/36.5 & \\textbf{6.3}/7.7/8.2 & 21.4/\\textbf{27.2}/30.3 & \\textbf{4.2}/5.7/6.6 \\\\\n&&MotifNet-freq   & -/41.8/48.8 & -/-/- & -/23.8/27.2 & -/-/- & -/6.9/9.1 & -/-/- \\\\\n&&\\textcolor{black}{ResCAGCN + PUM}  & -/-/\\textbf{68.3} & -/\\textbf{20.2/22.0} & -/-/\\textbf{39.0} & -/\\textbf{11.9/12.8} & -/-/\\textbf{31.3} & -/\\textbf{7.7/8.9} \\\\\n\\cline{2-9}\n&\\multirow{7}{*}{\\makecell[l]{Knowledge\\\\Graph}} & IMP+  & \\textbf{52.7}/59.3/61.3 & -/9.8/10.5 & \\textbf{31.7}/34.6/35.4 & -/5.8/6.0 & \\textbf{14.6}/20.7/24.5 & -/3.8/4.8 \\\\\n&&GPI  & -/65.1/66.9 & -/-/- & -/36.5/38.8 & -/-/- & -/-/- & -/-/- \\\\\n&&KB-GAN  & -/-/- & -/-/- & -/-/- & -/-/- & -/13.7/17.6 & -/-/- \\\\ \n&&GB-Net & -/66.6/68.2 & -/\\textbf{19.3/20.9} & -/38.0/38.8 & -/9.6/10.2 & -/26.4/30.0  & -/\\textbf{6.1}/\\textbf{7.3} \\\\ \n&&DG-PGNN & -/\\textbf{70.1/73.0} & -/-/- & -/\\textbf{39.5/40.8} & -/-/- & -/\\textbf{32.1/33.1}  & -/-/- \\\\ \n&&Schemata & -/66.9/68.4 & -/19.1/20.7 & -/39.1/39.8 & -/\\textbf{10.1/10.9} & -/-/-  & -/-/- \\\\ \n\\hline\n\\end{tabular}\n}\n\\vspace{-1 em}\n\\end{table*}\nRecall@K (R@K) is often used as an evaluation metric for the above tasks. In addition, due to the existence of reporting bias in R@K , R@K is easily disturbed by high-frequency predicates. Therefore, mean Recall@K (mR@K)  was proposed. mR@K retrieves each predicate separately and then averages R@K for all predicates. \nGraph constraints are also a factor that should be considered. Some previous work  constrained only one relationship for a given object pair when calculating R@K, while other work  omitted this constraint and enabled multiple relationships to be obtained. In addition, to better evaluate the model's ability to conduct rare visual relationship prediction,  introduces density-normalized edge loss and explores a novel weighted metric.\nAt present, most existing SGG methods use three subtasks in \\textit{RR} with graph constraints for performance evaluation. Referring to the classification outlined in Section \\ref{sec:SGG_Alone} and Section \\ref{sec:SGG_Proir}, the performance of the related SGG methods are summarized in TABLE \\ref{tab:PerformanceComparison}. Most current methods use the SGG method based on graph and RNN/LSTM. Compared with R@K, mR@K is generally lower. For datasets with obvious long-tail distribution, mR@K is a fairer evaluation metric. \n\\textcolor{black}{In particular, due to VTransE* , FactorizableNet* , IMP , IMP$\\dagger$ , Pixels2Graphs , FCSGG , and VRF , only visual features are used, and the performance of these methods is generally low. In contrast, VCTree , KERN , GPS-Net , GB-NET , in addition to using visual features, they also use other knowledge (for example, language embedding, statistical information, and counterfactual causality, etc.). This allows these methods to gain more additional knowledge and thus more performance gains. In addition, currently almost all methods propose that the prediction of objects and relationships is associated instead of independent. They try to consider the contextual information between objects , or use GNN , LSTM , message passing  and other methods to capture this relationship. However, these methods often use cross entropy loss training, as shown in Eq.\\ref{eq:EBM}, which is essentially considering objects and relationships as independent entities. A new energy-based loss function is proposed in EBM . This well-designed loss calculates the joint likelihood of objects and relationships. Compared with the traditional cross-entropy loss, it has achieved a consistent improvement in mR@K on multiple classic models . At the same time, EBM  has achieved the best performance among the algorithms currently investigated.}\nIn addition, ZSRR is also an important task, but most current methods fail to evaluate the ZSRR task. Paying attention to the evaluation of the ZSRR task is helpful to the study of the long-tail distribution of the scene graph. \n\\vspace{-1em}", "cites": [1223, 6417, 5297, 6486, 6428, 6422, 271, 8714, 6456, 6448, 4016, 6454, 9024, 6418, 6438, 6487, 6440, 4017, 8155, 4824, 4174, 8156, 267, 4013, 6436, 4015, 6453, 6447, 284, 6437, 6416, 6485, 6420, 9021], "cite_extract_rate": 0.7555555555555555, "origin_cites_number": 45, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear comparative overview of evaluation methods for scene graph generation and includes performance metrics across various models and datasets. While it organizes and integrates different approaches under categories like TransE, CNN, RNN/LSTM, and Graph-based methods, the synthesis remains largely structured around method types rather than deeper thematic connections. It identifies some limitations (e.g., ignoring long-tail and graph-level coherence), but the critical analysis is modest and not consistently applied across all cited works."}}
{"id": "3ac9f30f-d4d2-477d-9a40-a55f8672e108", "title": "Future Research", "level": "section", "subsections": [], "parent_id": "be806312-3b5b-4083-ab29-bf4d337ae115", "prefix_titles": [["title", "A Comprehensive Survey of Scene Graphs: Generation and Application"], ["section", "Future Research"]], "content": "\\label{sec:FutureDirection}\nSGG aims at mining the relationships between objects in images or scenes and forming relationship graphs. Although SGG currently has a lot of related research, there are still many directions worthy of attention.\n\\textbf{Long-tailed distribution in visual relations.} The inexhaustibility of visual relations and the complexity of real scenes determine the inevitability of long-tailed distribution in visual relations. The data balance required by model training and the long-tailed distribution in real data presents a pair of unavoidable contradictions. Associative reasoning through similar objects or similar relationships across scenes may be a suitable research direction to pursue, as it may aid in solving the long-tail distribution problem of relationships on the current scene graph dataset to a certain extent. In addition, a targeted long-tail distribution evaluation metric or task design is also a potential research direction, as it could aid in fairer evaluation of the model's learning ability in zero/one/few-shot contexts; however, related research is still very limited. Although the long-tail problem has received extensive attention from researchers (Section \\ref{sec:long-tail}), there are large numbers of potential, unfrequent, non-focused, or even unseen relationships in the scene that still need to be explored.\n\\textbf{Relationships detection between distant objects.} Currently, a scene graph is generated based on large numbers of small-scale relationship graphs, which are abstracted from small scenes in scene graph datasets by means of relevant relationship prediction and reasoning models. The selection of potential effective relationships  and the establishment of the final relationships in the scene graph are largely dependent on the spatial distance between objects, such that no relationship will exist between two distant objects. However, in the case of a large scene, there are still more such relationships . Therefore, an appropriate proportion of large-scale images can be added to the existing scene graph datasets, while the relationships between objects separated by long distances can be properly considered during SGG, which will improve the integrity of the scene graph.\n\\textbf{SGG based on dynamic images.} The scene graph is generated based on static images in scene graph datasets, and object relationship prediction is also carried out for the static objects in the images by means of related reasoning models. \nThere are very few related research works , and little attention has been paid to the role played by the dynamic behaviors of objects in the prediction and inference of the relationships.\nIn practice, however, it may be necessary to predict large numbers of relationships by means of successive actions or events; that is, relationship detection and reasoning based on video scenes. This is because, compared with static images, it is obvious that the analysis of spatio-temporal scene graphs of dynamic images offers a wider range of application scenarios. \nWe therefore contend that it will be necessary to focus on relationship prediction based on the dynamic actions of the objects in the video.\n\\textbf{Social relationship detection based on scene graph.} From Section \\ref{sec:HOI/HHI}, we can see that the detection of HOI (human-object interaction) and human-human interaction is an important application of scene graphs, and these types of relationships can be further extended to detect social relationships. We believe that social relationship detection can be used to understand the scenes in more depth, and is thus also a very important research direction. The SGG models based on large-scale datasets can even mine unseen social relationships from the visual data, which yields a wider range of practical application values.\n\\textbf{Models and methods of visual reasoning.} For SGG, the mainstream methods have been developed based on object detection and recognition.\nHowever, due to the limitations in the current scene graph datasets and the limited capability of relationship prediction models derived using these datasets, it is difficult for existing models to continuously enhance their ability to predict relationships. Therefore, we believe that online learning, reinforcement learning, and active learning may be relevant methods or strategies that could be introduced into future SGG methods, as this would enable the SGG models to continuously enhance their relationship prediction abilities by drawing on a large number of constantly updated realistic datasets.\nIn general, research in the field of scene graphs has developed rapidly and has broad application prospects. Scene graphs are expected to further promote the understanding and reasoning of higher-level visual scenes. At present, however, scene graph-related research is not sufficiently mature, meaning that it requires more effort and exploration.\n\\vspace{-1em}", "cites": [6464, 6434, 6458], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates several ideas from the cited papers to form a coherent narrative about future research directions in SGG, particularly in relation to long-tailed distributions and dynamic images. It provides some level of abstraction by identifying recurring themes such as data imbalance and spatial limitations. However, the critical evaluation is somewhat limited, focusing more on describing gaps rather than deeply analyzing the cited works or their implications."}}
