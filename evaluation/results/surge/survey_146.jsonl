{"id": "7b23b7de-94bb-4463-a00d-65ebc4b0144c", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "af93dea2-321c-4620-ba4b-082ab5c8e3bd", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{W}{ith} the rapid development of 3D acquisition technologies, 3D sensors are becoming increasingly available and affordable, including various types of 3D scanners, LiDARs, and RGB-D cameras (such as Kinect, RealSense and Apple depth cameras) . 3D data acquired by these sensors can provide rich geometric, shape and scale information . Complemented with 2D images, 3D data provides an opportunity for a better understanding of the surrounding environment for machines. 3D data has numerous applications in different areas, including autonomous driving, robotics, remote sensing, and medical treatment . \n3D data can usually be represented with different formats, including depth images, point clouds, meshes, and volumetric grids. As a commonly used format, point cloud representation preserves the original geometric information in 3D space without any discretization. Therefore, it is the preferred representation for many scene understanding related applications such as autonomous driving and robotics. Recently, deep learning techniques have dominated many research areas, such as computer vision, speech recognition, and natural language processing. However, deep learning on 3D point clouds still face several significant challenges , such as the small scale of datasets, the high dimensionality and the unstructured nature of 3D point clouds. On this basis, this paper focuses on the analysis of deep learning methods which have been used to process 3D point clouds.\nDeep learning on point clouds has been attracting more and more attention, especially in the last five years. Several publicly available datasets are also released, such as ModelNet , ScanObjectNN , ShapeNet , PartNet , S3DIS , ScanNet , Semantic3D , ApolloCar3D , and the KITTI Vision Benchmark Suite . These datasets have further boosted the research of deep learning on 3D point clouds, with an increasingly number of methods being proposed to address various problems related to point cloud processing, including 3D shape classification, 3D object detection and tracking,  \\Gary{3D point cloud segmentation, 3D point cloud registration, 6-DOF pose estimation, and 3D reconstruction }. Few surveys of deep learning on 3D data are also available, such as . \\Gary{However, our paper is the first to specifically focus on deep learning methods for point cloud understanding.} \nA taxonomy of existing deep learning methods for 3D point clouds \\why{is} shown in Fig. \\ref{fig:structure}. \nCompared with the existing literatures, the major contributions of this work can be summarized as follows:\n\\begin{enumerate}\n    \\item To the best of our knowledge, this is the \\textit{first} survey paper to comprehensively cover deep learning  methods for several important point cloud understanding tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation.\n    \\item As opposed to existing reviews , we specifically focus on deep learning methods for \\textit{3D point clouds} rather than all types of 3D data. \n    \\item This paper covers the \\textit{most recent and advanced progresses} of deep learning on point clouds. Therefore, it provides the readers with the state-of-the-art methods.\n    \\item Comprehensive \\textit{comparisons of existing methods} on several publicly available datasets are provided (e.g., in Tables \\ref{Tab:ModelNet}, \\ref{Tab:KITTI3D}, \\ref{Tab:KITTIBEV}, \\ref{tab:segmentation_results}), with brief summaries and insightful discussions being presented. \n\\end{enumerate}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=\\textwidth]{Figure/structure_v2.pdf}\n\\caption{A taxonomy of deep learning methods for 3D point clouds.\n\\label{fig:structure}}\n\\end{figure*}\n\\begin{table*}[t]\n\\caption{A summary of existing datasets for 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. \\protect\\textsuperscript{1} The number of classes used for evaluation and the number of annotated classes (shown in brackets).\\label{tab:Overview-dataset}}\n\\begin{tabular}{|r|c|c|c|c|c|c|c|}\n\\hline \n\\multicolumn{8}{|c|}{\\textbf{Datasets for 3D Shape Classification}}\\\\\n\\hline \nName and Reference & Year & \\#Samples & \\#Classes & \\#Training & \\#Test & Type & Representation\\\\\n\\hline \nMcGill Benchmark  & 2008 & 456 & 19 & 304 & 152 & Synthetic & Mesh\\\\\n\\hline \nSydney Urban Objects  & 2013 & 588 & 14 & - & - & Real-World & Point Clouds\\\\\n\\hline \nModelNet10  & 2015 & 4899 & 10 & 3991 & 605 & Synthetic & Mesh\\\\\n\\hline \nModelNet40  & 2015 & 12311 & 40 & 9843 & 2468 & Synthetic & Mesh\\\\\n\\hline \nShapeNet  & 2015 & 51190 & 55 & - & - & Synthetic & Mesh\\\\\n\\hline \nScanNet  & 2017 & 12283 & 17 & 9677 & 2606 & Real-World & RGB-D\\\\\n\\hline \nScanObjectNN  & 2019 & 2902 & 15 & 2321 & 581 & Real-World & Point Clouds\\\\\n\\hline \n\\multicolumn{8}{|c|}{\\textbf{Datasets for 3D Object Detection and Tracking}}\\\\\n\\hline \nName and Reference & Year & \\#Scenes & \\#Classes & \\#Annotated Frames & \\#3D Boxes & Secne Type & Sensors\\\\\n\\hline \nKITTI  & 2012  & 22  & 8  & 15K  & 200K  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nSUN RGB-D  & 2015  & 47  & 37  & 5K  & 65K  & Indoor  & RGB-D \\\\\n\\hline \nScanNetV2  & 2018  & 1.5K  & 18  & -  & -  & Indoor  & RGB-D \\& Mesh \\\\\n\\hline \nH3D  & 2019  & 160  & 8  & 27K  & 1.1M  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nArgoverse  & 2019  & 113  & 15  & 44K  & 993K  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nLyft L5  & 2019  & 366  & 9  & 46K  & 1.3M  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nA{*}3D  & 2019  & -  & 7  & 39K  & 230K  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nWaymo Open  & 2020  & 1K  & 4  & 200K  & 12M  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \nnuScenes  & 2020  & 1K  & 23  & 40K  & 1.4M  & Urban (Driving)  & RGB \\& LiDAR \\\\\n\\hline \n\\multicolumn{8}{|c|}{\\textbf{Datasets for 3D Point Cloud Segmentation}}\\\\\n\\hline \nName and Reference & Year & \\#Points & \\#Classes\\textsuperscript{1} & \\#Scans & Spatial Size & RGB & Sensors\\\\\n\\hline \nOakland & 2009 & 1.6M & 5(44) & 17 & - & N/A & MLS\\\\\n\\hline \nISPRS & 2012 & 1.2M & 9 & - & - & N/A &  ALS\\\\\n\\hline \nParis-rue-Madame & 2014 & 20M & 17 & 2 & - & N/A & MLS\\\\\n\\hline \nIQmulus & 2015 & 300M & 8(22) & 10 & - & N/A & MLS\\\\\n\\hline \nScanNet & 2017 & - & 20(20) & 1513 & 8$\\times$4$\\times$4 & Yes & RGB-D\\\\\n\\hline \nS3DIS & 2017 & 273M & 13(13) & 272 & 10$\\times$5$\\times$5 & Yes & Matterport\\\\\n\\hline \nSemantic3D & 2017 & 4000M & 8(9) & 15/15 & 250$\\times$260$\\times$80 & Yes &  TLS\\\\\n\\hline \nParis-Lille-3D & 2018 & 143M & 9(50) & 3 & 200$\\times$280$\\times$ 30 & N/A & MLS\\\\\n\\hline \nSemanticKITTI & 2019 & 4549M & 25(28) & 23201/20351 & 150$\\times$100$\\times$10 & N/A & MLS\\\\\n\\hline \nToronto-3D & 2020 & 78.3M & 8(9) & 4 & 260$\\times$350$\\times$ 40 & Yes & MLS\\\\\n\\hline \nDALES & 2020 & 505M & 8(9) & 40 & 500$\\times$500$\\times$65 & N/A &  ALS\\\\\n\\hline \n\\end{tabular}\n\\end{table*}\nThe structure of this paper is as follows. \\Gary{Section \\ref{sec:background} introduces the datasets and evaluation metrics for the respective tasks.} \nSection \\ref{sec:shape_classification} reviews the methods for 3D shape classification. Section \\ref{sec:object_detection} provides a survey of existing methods for 3D object detection and tracking. Section \\ref{sec:scene_segmentation}  presents a review of methods for point cloud segmentation, including semantic segmentation, instance segmentation, and part segmentation. Finally, Section \\ref{sec:conclusion} concludes the paper. We also provide a regularly updated project page on: \\url{https://github.com/QingyongHu/SoTA-Point-Cloud}.\n\\qy{", "cites": [1518, 7381, 1519, 7379, 7377, 7375, 7378, 1517, 7374, 7376, 7380], "cite_extract_rate": 0.2972972972972973, "origin_cites_number": 37, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of datasets relevant to deep learning on 3D point clouds, listing their characteristics and purposes. While it includes some synthesis by categorizing the datasets into tasks and summarizing their attributes, it lacks deeper integration of ideas or critical evaluation of the works. Abstraction is minimal, with no clear meta-level insights or broader principles identified."}}
{"id": "a999e26f-7629-4315-9ca3-5b199ad663dd", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "301783d9-71ae-4424-ae13-85c7e16e2136", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "Background"], ["subsection", "Datasets"]], "content": "}\n\\why{A large number of datasets have been collected to evaluate the performance of deep learning algorithms for different 3D point clouds applications. Table \\ref{tab:Overview-dataset} lists some typical datasets used for 3D shape classification, \\qy{3D object detection and tracking, and 3D point cloud segmentation}. In particular, the attributes of these datasets are also summarized.}\n\\why{For 3D shape classification, there are two types of datasets: synthetic datasets  and real-world datasets . Objects in the synthetic datasets are complete, without any occlusion and background. In contrast, objects in the real-world datasets are occluded at different levels and some objects are contaminated with background noise.}\n\\hao{For 3D object detection and tracking, there are two types of datasets: indoor scenes  and outdoor urban scenes . The point clouds in the indoor datasets are either converted from dense depth maps or sampled from 3D  meshes. The outdoor urban datasets are designed for autonomous driving, where objects are spatially well separated and these point clouds are sparse.}\n\\qy{For 3D point cloud segmentation, these datasets are acquired by different types of sensors, including Mobile Laser Scanners (MLS) , Aerial Laser Scanners (ALS) , static Terrestrial Laser Scanners (TLS) , RGB-D cameras  and other 3D scanners .\nThese datasets can be used to develop algorithms for various challenges including similar distractors, shape incompleteness, and class imbalance.}\n\\qy{", "cites": [1518, 1519, 7378, 1517, 7374, 7376, 7375], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of datasets used in point cloud learning, categorizing them by application and acquisition method. It makes minimal synthesis by grouping datasets under the three main tasks but does not deeply connect ideas or present a novel framework. There is little critical analysis or identification of broader patterns, with most content focused on summarizing attributes and sources of the datasets."}}
{"id": "8b4c5c7b-5a2d-4561-a66d-02ec3d1b43bd", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "301783d9-71ae-4424-ae13-85c7e16e2136", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "Background"], ["subsection", "Evaluation Metrics"]], "content": "}\n\\qy{Different evaluation metrics have been proposed to test these methods for various point cloud understanding tasks.} \\why{For 3D shape classification, \\textit{Overall Accuracy} (OA) and \\textit{mean class accuracy} (mAcc) are the most frequently used performance criteria. `OA' represents the mean accuracy for all test instances and `mAcc' represents the mean accuracy for all shape classes}. \n\\hao{For 3D object detection, \\textit{Average Precision} (AP) is the most frequently used criterion. It is calculated as the area under the precision-recall curve. \\textit{Precision} and \\textit{Success} are commonly used to evaluate the overall performance of a 3D single object tracker. \\textit{Average Multi-Object Tracking Accuracy} (AMOTA) and \\textit{Average Multi-Object Tracking Precision} (AMOTP) are the most frequently used criteria for the evaluation of 3D multi-object tracking.}\n\\qy{For 3D point cloud segmentation, OA, \\textit{mean Intersection over Union} (mIoU) and \\textit{mean class Accuracy} (mAcc)  are the most frequently used criteria for performance evaluation. In particular, \\textit{mean Average Precision} (mAP)  is also used in instance segmentation of 3D point clouds.}", "cites": [7381, 7378, 7375], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of evaluation metrics used in different point cloud understanding tasks but does not synthesize or connect ideas from the cited papers in a meaningful way. It lacks critical evaluation of the metrics or the datasets, and only describes commonly used terms without abstracting broader trends or principles."}}
{"id": "3eb38c0d-2272-44a8-8144-807c37cad28e", "title": "\\why{Multi-view based Methods", "level": "subsection", "subsections": [], "parent_id": "9c28278c-9219-4c2b-b2f9-5d6e789894bb", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "\\why{Multi-view based Methods"]], "content": "}\n\\why{These methods first project a 3D shape into multiple views and extract   view-wise features, and then fuse these features for accurate shape classification. How to aggregate multiple view-wise features into a discriminative global representation is a key challenge for these methods.}\nMVCNN  is a pioneering work, which simply max-pools multi-view features into a global descriptor. However, max-pooling only retains the maximum elements from a specific view, resulting in information loss. MHBN  integrates local convolutional features by harmonized bilinear pooling to produce a compact global descriptor. Yang et al.  first leveraged a relation network to exploit the inter-relationships (e.g., region-region relationship and view-view relationship) over a group of views, and then aggregated these views to obtain a discriminative 3D object representation. In addition, several other methods  have also been proposed to improve the recognition accuracy. \\why{Unlike previous methods, Wei et al.  used a directed graph in View-GCN by considering multiple views as grpah nodes. The core layer composing of local graph convolution, non-local message passing and selective view-sampling is then applied to the constructed graph. The concatenation of max-pooled node features at all levels is finally used to form the global shape descriptor.}", "cites": [7383, 7382], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers effectively by organizing them around a central idea—multi-view based methods for 3D shape classification—and explaining how each contributes to solving the challenge of feature aggregation. It provides some critical analysis by pointing out limitations, such as information loss in max-pooling. While it identifies patterns in the evolution of methods (e.g., moving from simple pooling to graph-based aggregation), it does not yet reach the level of meta-insights or a novel overarching framework."}}
{"id": "e88ecfee-d369-446c-80cb-f62030814842", "title": "\\why{Volumetric-based Methods", "level": "subsection", "subsections": [], "parent_id": "9c28278c-9219-4c2b-b2f9-5d6e789894bb", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "\\why{Volumetric-based Methods"]], "content": "}\n\\why{These methods usually voxelize a point cloud into 3D grids, and then apply a 3D Convolution Neural Network (CNN) on the volumetric representation for shape classification.}\n \\why{Maturana et al.}  introduced a volumetric occupancy network called VoxNet to achieve robust 3D object recognition. Wu et al.  proposed a convolutional deep belief-based 3D ShapeNets to learn the distribution of points from various 3D shapes (which are represented by a probability distribution of binary variables on voxel grids). Although encouraging performance has been achieved, these methods are unable to scale well to dense 3D data since the computation and memory footprint grow cubically with the resolution. \nTo this end, a hierarchical and compact structure (such as octree) is introduced to reduce the computational and memory costs of these methods. OctNet  first hierarchically partitions a point cloud using a hybrid grid-octree structure, which represents the scene with several shallow octrees along a regular grid. The structure of octree is encoded efficiently using a bit string representation, and the feature vector of each voxel is indexed by simple arithmetic. Wang et al.  proposed an Octree-based CNN for 3D shape classification. The average normal vectors of a 3D model sampled in the finest leaf octants are fed into the network, and 3D-CNN is applied on the octants occupied by the 3D shape surface. Compared to a baseline network based on dense input grids, OctNet requires much less memory and runtime for high-resolution point clouds. Le et al.  proposed a hybrid network called PointGrid,  which integrates the point and grid representation for efficient point cloud processing. A constant number of points is sampled within each embedding volumetric grid cell, which allows the network to extract geometric details by using 3D convolutions. Ben-Shabat et al.  transformed the input point cloud into 3D grids which are further represented by 3D modified Fisher Vector (3DmFV) method, and then learned the global representation through a conventional CNN architecture.", "cites": [7384], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by organizing them around the concept of volumetric-based methods and highlighting their common approach of using voxel grids and 3D CNNs. It identifies a key limitation—cubic growth in computation and memory with resolution—and discusses how hierarchical structures like octrees address this. While it provides some level of critical evaluation, it could offer a more nuanced comparison of the effectiveness and trade-offs of different approaches."}}
{"id": "e5e20a05-173a-490c-8409-dd9b1129b86d", "title": "\\bf Pointwise MLP Methods", "level": "subsubsection", "subsections": [], "parent_id": "7217bd1d-086f-477a-b305-d19d6968b507", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "\\bf Pointwise MLP Methods"]], "content": "These methods model each point independently with several shared Multi-Layer Perceptrons (MLPs) and then aggregate a global feature using a symmetric aggregation function, as shown in Fig. \\ref{fig:pointnet}.\n\\begin{figure}[b]\n\\centering\n\\includegraphics[scale=0.65]{Figure/Pointnet.pdf}\n\\caption{\\why{A lightweight architecture of PointNet}. $n$ denotes the number of input points, $M$ denotes the dimension of the learned features for each point.}\n\\label{fig:pointnet}\n\\end{figure}\nTypical deep learning methods for 2D images cannot be directly applied to 3D point clouds due to their inherent data irregularities. As a pioneering work, PointNet  directly takes point clouds as its input and achieves permutation invariance with a symmetric function. Specifically, PointNet learns pointwise features independently with several MLP layers and extracts global features with a max-pooling layer. Deep sets  achieves permutation invariance by summing up all representations and applying nonlinear transformations. Since features are learned independently for each point in PointNet , the local structural information between points cannot be captured. Therefore, Qi et al.  proposed a hierarchical network PointNet++ to capture fine geometric structures from the neighborhood of each point. As the core of PointNet++ hierarchy, its set abstraction level is composed of three layers: the sampling layer, the grouping layer and the PointNet based learning layer. By stacking several set abstraction levels, PointNet++ learns features from a local geometric structure and abstracts the local features layer by layer.\nBecause of its simplicity and strong representation ability, many networks have been developed based on PointNet . The architecture of Mo-Net  is similar to PointNet  but it takes a finite set of moments as its input. Point Attention Transformers (PATs)  represents each point by its own absolute position and relative positions with respect to its neighbors and learns high dimensional features through MLPs. Then, Group Shuffle Attention (GSA) is used to capture relations between points, and a permutation invariant, differentiable and trainable end-to-end Gumbel Subset Sampling (GSS) layer is developed to learn hierarchical features. Based on PointNet++ , PointWeb  utilizes the context of the local neighborhood to improve point features using Adaptive Feature Adjustment (AFA). Duan et al. \\cite {Duan2019} proposed a Structural Relational Network (SRN) to learn structural relational features between different local structures using MLP. Lin et al.  accelerated the inference process by constructing a lookup table for both input and function spaces learned by PointNet. The inference time on the ModelNet and ShapeNet datasets is sped up by 1.5 ms and 32 times over PointNet on a moderate machine. SRINet  first projects a point cloud to obtain rotation invariant representations, and then utilizes PointNet-based backbone to extract a global feature and graph-based aggregation to extract local features. \\why{In PointASNL, Yan et al.  utilized an Adaptive Sampling (AS) module to adaptively adjust the coordinates and features of points sampled by the Furthest Point Sampling (FPS) algorithm, and proposed a local-non-local (L-NL) module to capture the local and long range dependencies of these sampled points.}", "cites": [1520, 1523, 1522, 7385, 1521], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes pointwise MLP-based methods by connecting PointNet, Deep Sets, PointNet++, and their derivatives into a coherent narrative, highlighting the evolution from basic permutation invariance to capturing local and nonlocal dependencies. It critically evaluates the limitations of PointNet (e.g., inability to capture local structure) and explains how subsequent works addressed these issues. The abstraction is strong, as it identifies broader design patterns such as hierarchical feature learning, attention mechanisms, and robustness to noise, which transcend individual papers."}}
{"id": "87a117cf-d994-44cc-b135-6ed580a8a273", "title": "\\bf Convolution-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "7217bd1d-086f-477a-b305-d19d6968b507", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "\\bf Convolution-based Methods"]], "content": "Compared with kernels defined on 2D grid structures (e.g., images), convolutional kernels for 3D point clouds are hard to design due to the irregularity of point clouds. According to the type of convolutional kernels, current 3D convolution methods can be divided into continuous and discrete convolution methods, as shown in Fig. \\ref{fig:3D Convolution Network}.\n\\textbf{3D Continuous Convolution Methods.} \nThese methods define convolutional kernels on a continuous space, where the weights for neighboring points are related to the spatial distribution with respect to the center point.\n3D convolution can be interpreted as a weighted sum over a given subset. As the core layer of RS-CNN , RS-Conv takes a local subset of points around a certain point as its input, and the convolution is implemented using an MLP by learning the mapping from low-level relations (such as Euclidean distance and relative position) to high-level relations between points in the local subset.  In , kernel elements are selected randomly in a unit sphere. An MLP-based continuous function is then used to establish relation between the locations of the kernel elements and the point cloud. In DensePoint , convolution is defined as a Single-Layer Perceptron (SLP) with a nonlinear activator. Features are learned by concatenating features from all previous layers to sufficiently exploit the contextual information. Thomas et al.  proposed both rigid and deformable Kernel Point Convolution (KPConv) operators for 3D point clouds using a set of learnable kernel points. ConvPoint  separates the convolution kernel into spatial and feature parts. The locations of the spatial part are randomly selected from a unit sphere and the weighting function is learned through a simple MLP.\nSome methods also use existing algorithms to perform convolution. In PointConv , convolution is defined as a Monte Carlo estimation of the continuous 3D convolution with respect to an importance sampling. The convolutional kernels consist of a weighting function (which is learned with MLP layers) and a density function (which is learned by a kernelized density estimation and an MLP layer). To improve memory and computational efficiency, the 3D convolution is further reduced into two operations: matrix multiplication and 2D convolution. With the same parameter setting, its memory consumption can be reduced by about 64 times. In MCCNN , convolution is  considered as a Monte Carlo estimation process relying on a sample's density function (which is implemented with MLP). Poisson disk sampling is then used to construct a point cloud hierarchy. This convolution operator can be used to perform convolution between two or multiple sampling methods and can handle varying sampling densities. In SpiderCNN , SpiderConv is proposed to define convolution as the product of a step function and a Taylor expansion defined on the $k$ nearest neighbors. The step function captures the coarse geometry by encoding the local geodesic distance, and the Taylor expansion captures the intrinsic local geometric variations by interpolating arbitrary values at the vertices of a cube. Besides, a convolution network PCNN  is also proposed for 3D point clouds based on the radial basis function. \n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\columnwidth,keepaspectratio]{Figure/3D_Convolution.pdf}\n\\caption{\\label{fig:3D Convolution Network} An illustration of a continuous and discrete convolution for local neighbors of a point. (a) represents a local neighborhood ${q_i}$ centered at point $p$; (b) and (c) represent 3D continuous and discrete convolution, respectively.}\n\\end{figure}\nSeveral methods have been proposed to address the rotation equivariant problem faced by 3D convolution networks. Esteves et al.  proposed 3D Spherical CNN to learn rotation equivariant representation for 3D shapes, which takes multi-valued spherical functions as its input. Localized convolutional filters are obtained by parameterizing spectrum with anchor points in the spherical harmonic domain. Tensor field networks  are proposed to define the point convolution operation as the product of a learnable radial function and  spherical harmonics, which are locally equivariant to 3D rotations, translations, and permutations. The convolution in  is defined based on the spherical cross-correlation and implemented using a generalized Fast Fourier Transformation (FFT) algorithm. Based on PCNN, SPHNet  achieves rotation invariance by incorporating spherical harmonic kernels during convolution on volumetric functions.  \nTo accelerate computing speed, Flex-Convolution  defines weights of convolution kernel as standard scalar product over $k$ nearest neighbors, which can be accelerated using CUDA. Experimental results have demonstrated its competitive performance on a small dataset with fewer parameters and lower memory consumption.\n\\textbf{3D Discrete Convolution Methods.} \\label{3DGN}\nThese methods define convolutional kernels on regular grids, where the weights for neighboring points are related to the offsets with respect to the center point.\nHua et al.  transformed non-uniform 3D point clouds into uniform grids and defined convolutional kernels on each grid. The proposed 3D kernel assigns the same weights to all points falling into the same grid. For a given point, the mean features of all the neighboring points that are located on the same grid are computed from the previous layer. Then, mean features of all grids are weighted and summed to produce the output of the current layer. Lei et al.  defined a spherical convolutional kernel by partitioning a 3D spherical neighboring region into multiple volumetric bins and associating each bin with a learnable weighting matrix. The output of the spherical convolutional kernel for a point is determined by the non-linear activation of the mean of weighted activation values of its neighboring points. In GeoConv , the geometric relationship between a point and its neighboring points is explicitly modeled based on six bases. Edge features along each direction of the basis are weighted independently by a direction-associated learnable matrix. These direction-associated features are then aggregated according to the angles formed by the given point and its neighboring points. For a given point, its feature at the current layer is defined as the sum of features of the given point and its neighboring edge features at the previous layer. \nPointCNN  transforms the input points into a latent and potentially canonical order through a $\\chi$-conv transformation (which is implemented through MLP) and then applies typical convolutional operator on the transformed features. By interpolating point features to neighboring discrete convolutional kernel-weight coordinates, Mao et al.  proposed an interpolated convolution operator InterpConv to measure the geometric relations between input point clouds and kernel-weight coordinates. Zhang et al.  proposed a RIConv operator to achieve rotation invariance, which takes low-level rotation invariant geometric features as input and then turns the convolution into 1D by a simple binning approach. A-CNN  defines an annular convolution by looping the array of neighbors with respect to the size of kernel on each ring of the query point and learns the relationship between neighboring points in a local subset. \nTo reduce the computational and memory cost of 3D CNNs, Kumawat et al.  proposed a Rectified Local Phase Volume (ReLPV) block to extract phase in a 3D local neighborhood based on 3D Short Term Fourier Transform (STFT), which significantly reduces the number of parameters. In SFCNN , a point cloud is projected onto regular icosahedral lattices with aligned spherical coordinates. Convolutions are then conducted upon the features concatenated from vertices of spherical lattices and their neighbors through convolution-maxpooling-convolution structures. SFCNN is resistant to rotations and perturbations.", "cites": [7391, 7393, 1527, 7388, 7390, 7389, 7387, 7392, 1524, 7395, 7386, 7290, 1525, 1526, 7394], "cite_extract_rate": 0.6521739130434783, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing methods into continuous and discrete categories and highlighting their shared goals and differences. It provides critical insights by discussing limitations such as rotation equivariance and computational efficiency, and how different works address them. The abstraction is evident in the identification of common strategies (e.g., spherical harmonics, interpolation) and the generalization of point cloud convolution design principles."}}
{"id": "43040137-7682-4a1b-907d-1ba17a33632a", "title": "\\bf Graph-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "7217bd1d-086f-477a-b305-d19d6968b507", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "\\bf Graph-based Methods"]], "content": "Graph-based networks consider each point in a point cloud as a vertex of a graph, and generate directed edges for the graph based on the neighbors of each point. Feature learning is then performed in spatial or spectral domains . A typical graph-based network is shown in Fig. \\ref{fig:Graph_based_Network}. \n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\columnwidth,keepaspectratio]{Figure/graph.pdf}\n\\caption{An illustration of a graph-based network.}\n\\label{fig:Graph_based_Network}\n\\end{figure}\n\\textbf{Graph-based Methods in Spatial Domain.}\nThese methods define operations (e.g., convolution and pooling) in spatial domain. Specifically, convolution is usually implemented through MLP over spatial neighbors, and pooling is adopted to produce a new coarsened graph by aggregating information from each point's neighbors. Features at each vertex are usually assigned with coordinates, laser intensities or colors, while features at each edge are usually assigned with geometric attributes between two connected points.\nAs a pioneering work, Simonovsky et al.  considered each point as a vertex of the graph, and connected each vertex to all its neighbors by a directed edge. Then, Edge-Conditioned Convolution (ECC) is proposed using a filter-generating network (e.g., MLP). Max pooling is adopted to aggregate neighborhood information and graph coarsening is implemented based on VoxelGrid . In DGCNN , a graph is constructed in the feature space and dynamically updated after each layer of the network. As the core layer of EdgeConv, an MLP is used as the feature learning function for each edge, and channel-wise symmetric aggregation is applied onto the edge features associated with the neighbors of each point. Further, LDGCNN  removes the transformation network and links the hierarchical features from different layers in DGCNN  to improve its performance and reduce the model size. An end-to-end unsupervised deep AutoEncoder network (namely, FoldingNet ) is also proposed to use the concatenation of a vectorized local covariance matrix and point coordinates as its input. Inspired by Inception  and DGCNN , Hassani and Haley  proposed an unsupervised multi-task autoencoder to learn point and shape features. The encoder is constructed based on mutli-scale graphs. The decoder is constructed using three unsupervised tasks including clustering, self-supervised classification and reconstruction, which are trained jointly with a mutli-task loss. Liu et al.  proposed a Dynamic Points Agglomeration Module (DPAM) based on graph convolution to simplify the process of points agglomeration (sampling, grouping and pooling) into a simple step, which is implemented through multiplication of the agglomeration matrix and points feature matrix. Based on the PointNet architecture, a hierarchical learning architecture is constructed by stacking multiple DPAMs. Compared with the hierarchy strategy of PointNet++ , DPAM dynamically exploits the relation of points and agglomerates points in a semantic space. \nTo exploit the local geometric structures, KCNet  learns features based on kernel correlation. Specifically, a set of learnable points characterizing geometric types of local structures are defined as kernels. Then, affinity between the kernel and the neighborhood of a given point is calculated. In G3D ,  convolution is defined as a variant of polynomial of adjacency matrix, and pooling is defined as multiplying the Laplacian matrix and the vertex matrix by a coarsening matrix.\nClusterNet  utilizes a rigorously rotation-invariant module to extract rotation-invariant features from $k$ nearest neighbors for each point, and constructs hierarchical structures of a point cloud based on the unsupervised agglomerative hierarchical clustering method  with ward-linkage criteria . The features in each sub-cluster are first learned through an EdgeConv block and then aggregated through max pooling.\n\\why{To address the time-consuming problem of current data structuring methods (such as FPS and neighbor points querying), Xu et al.  proposed to blend the advantages of volumetric based and point based methods to improve the computational efficiency. Experiments on the ModelNet classification task demonstrate that  the computational efficiency of the proposed Grid-GCN network is 5$\\times$ faster than other models in average.}\n\\textbf{Graph-based Methods in Spectral Domain.}\nThese methods define convolutions as spectral filtering, which is implemented as the multiplication of signals on graph with eigenvectors of the graph Laplacian matrix . \nRGCNN  constructs a graph by connecting each point with all other points in the point cloud and updates the graph Laplacian matrix in each layer. To make features of adjacent vertices more similar, a graph-signal smoothness prior is added into the loss function. To address the challenges caused by diverse graph topology of data, the SGC-LL layer in AGCN  utilizes a learnable distance metric to parameterize the similarity between two vertices on the graph. The adjacency matrix obtained from graph is normalized using Gaussian kernels and learned distances. HGNN  builds a hyperedge convolutional layer by applying spectral convolution on a hypergraph. \nAforementioned methods operate on full graphs. To exploit local structural information, Wang et al.  proposed an end-to-end spectral convolution network LocalSpecGCN to work on a local graph (which is constructed from the $k$ nearest neighbors). This method does not require any offline computation of the graph Laplacian matrix and graph coarsening hierarchy. In PointGCN , a graph is constructed based on $k$ nearest neighbors from a point cloud and each edge is weighted using a Gaussian kernel. Convolutional filters are defined as Chebyshev polynomials in  graph spectral domain. Global pooling and multi-resolution pooling are used to capture global and local features of the point cloud.  Pan et al.  proposed 3DTI-Net by applying convolution on the $k$ nearest neighboring graphs in spectral domain. The invariance to geometry transformation is achieved by learning from relative Euclidean and direction distances.", "cites": [7385, 1529, 1528, 1530, 241, 9112, 305, 8460, 7396, 278, 8313, 7215, 7397, 213, 275], "cite_extract_rate": 0.6818181818181818, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of graph-based methods for 3D point cloud classification, organizing them into spatial and spectral domains and connecting related works like DGCNN, ECC, and AGCN. It includes critical evaluation, such as the time-consuming nature of data structuring and the trade-offs in network design (e.g., removing transformation networks in LDGCNN). The section abstracts the core principles of graph-based learning, such as exploiting local structures and handling irregular data, offering a structured understanding of the approaches."}}
{"id": "8d65d374-1481-4e71-836d-2ae8f2c8b9d9", "title": "\\bf \\why{Hierarchical Data Structure-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "7217bd1d-086f-477a-b305-d19d6968b507", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "\\bf \\why{Hierarchical Data Structure-based Methods"]], "content": "}\nThese networks are constructed based on different \\why{hierarchical data structures} (e.g., octree and kd-tree). In these methods, point features are learned hierarchically from leaf nodes to the root node along a tree.  \nLei et al.  proposed an octree guided CNN using spherical convolutional kernels (as described in Section \\ref{3DGN}). Each layer of the network corresponds to one layer of the octree and a spherical convolutional kernel is applied at each layer. The values of neurons in the current layer are determined as the mean values of all relevant children nodes in the previous layer. Unlike OctNet  which is based on octree, Kd-Net  is built using multiple K-d trees with different splitting directions at each iteration. Following a bottom-up approach, the representation of a non-leaf node is computed from representations of its children using MLP. The feature of the root node (which describes the whole point cloud) is finally fed to fully connected layers to predict classification scores. Note that, Kd-Net shares parameters at each level according to the splitting type of nodes. \n3DContextNet  uses a standard balanced K-d tree to achieve feature learning and aggregation. At each level, point features are first learned through MLP based on local cues (which models inter-dependencies between points in a local region) and global contextual cues (which models the relationship for one position with respect to all other positions). Then, the feature of a non-leaf node is computed from its child nodes using MLP and aggregated by max pooling. For classification, the above process is repeated until the root node is attained. \nThe hierarchy of SO-Net network is constructed by performing point-to-node $k$ nearest neighbor search . Specifically, a modified permutation invariant Self-Organizing Map (SOM) is used to model the spatial distribution of a point cloud. Individual point features are learned from normalized point-to-node coordinates through a series of fully connected layers. The feature of each node in SOM is extracted from point features associated with this node using channel-wise max pooling. The final feature is then learned from node features using an approach similar to PointNet . Compared to PointNet++ , the hierarchy of SOM is more efficient and the spatial distribution of the point cloud is fully explored. \n\\begin{table*}[]\n\\centering\n\\caption{Comparative 3D shape classification results on the ModelNet10/40 benchmarks. Here, we only focus on point-based networks. `\\#params' represents the number of parameters of a model, `OA' represents \\why{the mean accuracy for all test instances} and `mAcc' represents the mean accuracy \\why{ for all shape classes} in the table. The symbol `-' means the results are unavailable.}\n\\label{Tab:ModelNet}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|r|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{2}{|c|}{\\bf Methods} & \\bf Input & \\bf \\#params (M) & \\begin{tabular}[c]{@{}c@{}}\\bf ModelNet40\\\\ \\bf (OA)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\bf ModelNet40\\\\ \\bf (mAcc)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\bf ModelNet10\\\\ \\bf (OA)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\bf ModelNet10\\\\\\bf (mAcc)\\end{tabular} \\\\ \\hline\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}c@{}}Pointwise MLP \\\\ Methods\\end{tabular}} \n& PointNet  & Coordinates & 3.48 & 89.2\\% & 86.2\\% & - & - \\\\ \\cline{2-8} \n& PointNet++  & Coordinates & 1.48 & 90.7\\% & - & - & - \\\\ \\cline{2-8}\n& MO-Net  & Coordinates & 3.1 & 89.3\\% & 86.1\\% & - & - \\\\ \\cline{2-8}\n& Deep Sets  & Coordinates & - & 87.1\\% & - & - & - \\\\ \\cline{2-8}\n& PAT  & Coordinates & - & 91.7\\% & - & - & -\\\\ \\cline{2-8}\n& PointWeb  & Coordinates & - & 92.3\\% & 89.4\\% & - & -\\\\ \\cline{2-8}\n& SRN-PointNet++   & Coordinates & - & 91.5\\% & - & - & -\\\\ \\cline{2-8}\n& JUSTLOOKUP  & Coordinates & - & 89.5\\% & 86.4\\% & 92.9\\% & 92.1\\%\\\\ \\cline{2-8}\n& \\why{PointASNL }  & Coordinates & - & 92.9\\% & - & 95.7\\% & -\\\\ \\cline{2-8}\n& \\why{PointASNL }  & Coordinates+Normals & - & 93.2\\% & - & 95.9\\% & -\\\\ \\cline{2-8}\n\\hline\n\\multirow{20}{*}{\\begin{tabular}[c]{@{}c@{}}Convolution-based\\\\ Methods\\end{tabular}} \n& Pointwise-CNN  & Coordinates & - & 86.1\\% & 81.4\\% & - & -\\\\ \\cline{2-8}\n& PointConv  & Coordinates+Normals & - & 92.5\\% & - & - & -\\\\ \\cline{2-8}\n& MC Convolution  & Coordinates & - & 90.9\\% & - & - & -\\\\ \\cline{2-8}\n& SpiderCNN  & Coordinates+Normals & - & 92.4\\% & - & - & -\\\\ \\cline{2-8}\n& PointCNN  & Coordinates & 0.45 & 92.2\\% & 88.1\\% & - & -\\\\ \\cline{2-8}\n& Flex-Convolution  & Coordinates & - & 90.2\\% & - & - & -\\\\ \\cline{2-8}\n& PCNN  & Coordinates & 1.4 & 92.3\\% & - & 94.9\\% & -\\\\ \\cline{2-8}\n& Boulch  & Coordinates & - & 91.6\\% & 88.1\\% & - & -\\\\ \\cline{2-8}\n& RS-CNN  & Coordinates & - & 93.6\\% & - & - & -\\\\ \\cline{2-8}\n& Spherical CNNs  & Coordinates & 0.5 & 88.9\\% & - & - & -\\\\ \\cline{2-8}\n& GeoCNN  & Coordinates & - & 93.4\\% & 91.1\\% & - & -\\\\ \\cline{2-8}\n& $\\Psi$-CNN  & Coordinates & - & 92.0\\% & 88.7\\% & 94.6\\% & 94.4\\%\\\\ \\cline{2-8}\n& A-CNN  & Coordinates & - & 92.6\\% & 90.3\\% & 95.5\\% & 95.3\\%\\\\ \\cline{2-8}\n& SFCNN  & Coordinates & - & 91.4\\% & - & - & -\\\\ \\cline{2-8}\n& SFCNN  & Coordinates+Normals & - & 92.3\\% & - & - & -\\\\ \\cline{2-8}\n& DensePoint  & Coordinates  & 0.53 & 93.2\\% & - & 96.6\\% & -\\\\ \\cline{2-8}\n& KPConv rigid  & Coordinates & - & 92.9\\% & - & - & -\\\\ \\cline{2-8}\n& KPConv deform  & Coordinates & - & 92.7\\% & - & - & -\\\\ \\cline{2-8}\n& InterpCNN  & Coordinates & 12.8 & 93.0\\% & - & - & -\\\\ \\cline{2-8}\n& ConvPoint  & Coordinates & - & 91.8\\% & 88.5\\% & - & -\\\\ \\hline \n\\multirow{11}{*}{\\begin{tabular}[c]{@{}c@{}}Graph-based\\\\ Methods\\end{tabular}} \n& ECC  & Coordinates & - & 87.4\\% & 83.2\\% & 90.8\\% & 90.0\\%\\\\ \\cline{2-8}\n& KCNet  & Coordinates & 0.9 & 91.0\\% & - & 94.4\\% & -\\\\ \\cline{2-8}\n& DGCNN  & Coordinates & 1.84 & 92.2\\% & 90.2\\% & - & -\\\\ \\cline{2-8}\n& LocalSpecGCN  & Coordinates+Normals & - & 92.1\\% & - & - & -\\\\ \\cline{2-8}\n& RGCNN  & Coordinates+Normals & 2.24 & 90.5\\% & 87.3\\% & - & -\\\\ \\cline{2-8}\n& LDGCNN  & Coordinates & - & 92.9\\% & 90.3\\% & - & -\\\\ \\cline{2-8}\n& 3DTI-Net  & Coordinates & 2.6 & 91.7\\% & - & - & -\\\\ \\cline{2-8}\n& PointGCN  & Coordinates & - & 89.5\\% & 86.1\\% & 91.9\\% & 91.6\\%\\\\ \\cline{2-8}\n& ClusterNet  & Coordinates & - & 87.1\\% & - & - & -\\\\ \\cline{2-8}\n& Hassani et al.  & Coordinates & - & 89.1\\% & - & - & -\\\\ \\cline{2-8}\n& DPAM  & Coordinates & - & 91.9\\% & 89.9\\% & 94.6\\% & 94.3\\%\\\\ \\cline{2-8}\n& \\why{Grid-GCN } & Coordinates & - & 93.1\\% & 91.3\\% & 97.5\\% & 97.4\\%\\\\ \\cline{2-8}\n\\hline \n\\multirow{6}{*}{\\begin{tabular}[c]{@{}c@{}}\\why{Hierarchical Data Structure} \\\\-based Methods\\end{tabular}} \n& KD-Net  & Coordinates & 2.0  & 91.8\\% & 88.5\\% & 94.0\\% & 93.5\\%\\\\ \\cline{2-8}\n& SO-Net  & Coordinates & - & 90.9\\% & 87.3\\% & 94.1\\% & 93.9\\%\\\\ \\cline{2-8}\n& SCN  & Coordinates & - & 90.0\\% & 87.6\\% & - & -\\\\ \\cline{2-8}\n& A-SCN  & Coordinates & - & 89.8\\% & 87.4\\% & - & -\\\\ \\cline{2-8}\n& 3DContextNet  & Coordinates & - & 90.2\\% & - & - & -\\\\ \\cline{2-8}\n& 3DContextNet  & Coordinates+Normals & - & 91.1\\% & - & - & -\\\\ \\hline \n\\multirow{9}{*}{Other Methods}\n& 3DmFV-Net  & Coordinates & 4.6 & 91.6\\% & - & 95.2\\% & -\\\\ \\cline{2-8}\n& PVNet  & Coordinates+Views & - & 93.2\\% & - & - & -\\\\ \\cline{2-8}\n& PVRNet   & Coordinates+Views & - & 93.6\\% & - & - & -\\\\ \\cline{2-8}\n& 3DPointCapsNet  & Coordinates & - & 89.3\\% & - & - & -\\\\ \\cline{2-8}\n& DeepRBFNet  & Coordinates & 3.2 & 90.2\\% & 87.8\\% & - & -\\\\ \\cline{2-8}\n& DeepRBFNet  & Coordinates+Normals & 3.2 & 92.1\\% & 88.8\\% & - & -\\\\ \\cline{2-8}\n& Point2Sequences  & Coordinates & - & 92.6\\% & 90.4\\% & 95.3\\% & 95.1\\%\\\\ \\cline{2-8}\n& RCNet  & Coordinates & - & 91.6\\% & - & 94.7\\% & -\\\\ \\cline{2-8}\n& RCNet-E  & Coordinates & - & 92.3\\% & - & 95.6\\% & -\\\\ \\hline \n\\end{tabular}\n}\n\\end{table*}", "cites": [7385, 7393, 1528, 1529, 1530, 9112, 1523, 1527, 7388, 7389, 7401, 1521, 7400, 1524, 7396, 278, 7215, 1520, 7386, 1522, 7290, 7398, 1531, 275, 1525, 7397, 1526, 7399, 7390], "cite_extract_rate": 0.5686274509803921, "origin_cites_number": 51, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section primarily lists and briefly describes several hierarchical data structure-based methods for 3D point cloud classification, but lacks deeper synthesis of ideas across works. There is minimal critical analysis or evaluation of limitations, and while some patterns like the use of tree structures (octree, K-d tree) are identified, the abstraction remains limited to basic observations rather than conceptual generalizations."}}
{"id": "d40ac51c-b270-4244-9b38-7e1a626df4e5", "title": "\\bf Other Methods", "level": "subsubsection", "subsections": [], "parent_id": "7217bd1d-086f-477a-b305-d19d6968b507", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Shape Classification"], ["subsection", "Point-based Methods"], ["subsubsection", "\\bf Other Methods"]], "content": "In addition, many other schemes have also been proposed. \nRBFNet  explicitly models the spatial distribution of points by aggregating features from sparsely distributed Radial Basis Function (RBF) kernels with learnable kernel positions and sizes. 3DPointCapsNet  learns point independent features with pointwise MLP and convolutional layers, and extracts global latent representation with multiple max-pooling layers. Based on unsupervised dynamic routing, powerful representative latent capsules are then learned. \\why{Qin et al.  proposed an end-to-end unsupervised domain adaptation network PointDAN for 3D point cloud representation. To capture semantic properties of a point cloud, a self-supervised method is proposed to reconstruct the point cloud, whose parts have been randomly rearranged  . Li et al.  proposed an auto-augmentation framework, PointAugment, to automatically optimize and augment point cloud samples for network training. Specifically, shape-wise transformation and point-wise displacement for each input sample are automatically learned, and the network is trained by alternatively optimizing and updating the learnable parameters of its augmentor and classifier. Inspired by shape context , Xie et al.  proposed a ShapeContextNet architecture by combining affinity point selection and compact feature aggregation into a soft alignment operation using dot-product self-attention . To handle noise and occlusion in 3D point clouds, Bobkov et al.   fed handcrafted point pair function based 4D rotation invariant descriptors into a 4D convolutional neural network. Prokudin et al.  first randomly sampled a basis point set with a uniform distribution from a unit ball, and then encoded a point cloud as minimal distances to the basis point set. Consequently, the point cloud is converted to a vector with a relatively small fixed length. The encoded representation can then be processed with existing machine learning methods.}\nRCNet  utilizes standard RNN and 2D CNN to construct a permutation-invariant network for 3D point cloud processing. The point cloud is first partitioned into parallel beams and sorted along a specific dimension, and each beam is then fed into a shared RNN. The learned features are further fed into an efficient 2D CNN for hierarchical feature aggregation. To enhance its description ability, RCNet-E is proposed to ensemble multiple RCNets along different partition and sorting directions. Point2Sequences  is another RNN-based model that captures correlations between different areas in local regions of point clouds. It considers features learned from a local region at multiple scales as sequences and feeds these sequences from all local regions into an RNN-based encoder-decoder structure to aggregate local region features. \nSeveral methods also learn from both 3D point clouds and 2D images. In PVNet , high-level global features extracted from multi-view images are projected into the subspace of point clouds through an embedding network, and fused with point cloud features through a soft attention mask. Finally, a residual connection is employed for fused features and multi-view features to perform shape recognition. Later, PVRNet  is further proposed to exploit the relation between a 3D point cloud and its multiple views by a relation score module. Based on the relation scores, the original 2D global view features are enhanced for point-single-view fusion and point-multi-view fusion.", "cites": [7403, 38, 1532, 1531, 7400, 7399, 7402, 1533], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists several point-based methods for 3D shape classification, describing the technical components of each. While it provides a basic synthesis by grouping them under common themes (e.g., RBF kernels, RNN-based encoding, multi-modal fusion), it lacks deeper integration or comparison of the approaches. There is little critical evaluation or identification of broader trends or principles, making the narrative primarily descriptive."}}
{"id": "91e76de0-2caf-451a-8444-c82e48bfe222", "title": "3D Object Detection", "level": "subsection", "subsections": ["e6d7b97d-ddf5-4a2e-bae4-08626f36756a", "76e5660f-7054-4bdb-9f88-31ccea48ca70"], "parent_id": "88793bd8-cfbb-4a97-ae03-17f5c34f0614", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Object Detection and Tracking"], ["subsection", "3D Object Detection"]], "content": "\\hao{A typical 3D object detector takes the point cloud of a scene as its input and produces an oriented 3D bounding box around each detected object, as shown in Fig. \\ref{fig:object-detection}.} Similar to object detection in images , 3D object detection methods can be divided into two categories: region proposal-based and single shot methods. Several milestone methods are presented in Fig. \\ref{fig:milestone-detection}.\n\\begin{figure}[h]\n\\noindent \\begin{centering}\n\\noindent\\subfloat[{ScanNetV2  dataset}]{\\noindent \\begin{centering}\n\\includegraphics[scale=1]{Figure/ScanNet_v2.JPG}\n\\par\\end{centering}\n}\n\\subfloat[{KITTI  dataset}]{\\noindent \\begin{centering}\n\\includegraphics[scale=0.2]{Figure/KITTI.JPG}\n\\par\\end{centering}\n}\n\\par\\end{centering}\n\\caption{\\label{fig:object-detection} An illustration of 3D object detection. (a) and (b) are originally shown in  and , respectively.}\n\\end{figure}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=6.5in]{Figure/milestones_of_detection_v4.pdf}\n\\caption{Chronological overview of the most relevant deep learning-based 3D object detection methods.\\label{fig:milestone-detection}}\n\\end{figure*}", "cites": [1534, 7404, 7405, 7376], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic categorization of 3D object detection methods and references relevant papers, but it lacks meaningful synthesis of the cited works into a coherent narrative. There is minimal critical analysis or evaluation of the approaches, and no abstraction or generalization of patterns or principles across the methods. The content is largely descriptive with minimal insight."}}
{"id": "e6d7b97d-ddf5-4a2e-bae4-08626f36756a", "title": "\\bf Region Proposal-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "91e76de0-2caf-451a-8444-c82e48bfe222", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Object Detection and Tracking"], ["subsection", "3D Object Detection"], ["subsubsection", "\\bf Region Proposal-based Methods"]], "content": "These methods first propose several possible regions (also called proposals) containing objects, and then extract region-wise features to determine the category label of each proposal. According to their object proposal generation approach, these methods can further be divided into three categories: multi-view based, segmentation-based and frustum-based methods.\n\\textbf{Multi-view based Methods.} These methods fuse proposal-wise features from different view maps (e.g., LiDAR front view, Bird's Eye View (BEV), and image) to obtain 3D rotated boxes, as shown in Fig. \\ref{fig:detection_frameworks}(a). The computational cost of these methods is usually high. \n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\columnwidth,keepaspectratio]{Figure/Frameworks_object_detection.pdf}\n\\caption{Typical networks for three categories of region proposal-based 3D object detection methods. From top to bottom: (a) multi-view based, (b) segmentation-based and (c) frustum-based methods.}\n\\label{fig:detection_frameworks}\n\\end{figure}\nChen et al.  generated a group of highly accurate 3D candidate boxes from the BEV map and projected them to the feature maps of multiple views (e.g., LiDAR front view image, RGB image). They then combined these region-wise features from different views to predict oriented 3D bounding boxes, as shown in Fig. \\ref{fig:detection_frameworks}(a). Although this method achieves a recall of 99.1\\% at an Intersection over Union (IoU) of 0.25 with only 300 proposals, its speed is too slow for practical applications. Subsequently, several approaches have been developed to improve multi-view 3D object detection methods from two aspects. \n\\textbf{First}, several methods have been proposed to efficiently fuse the information of different modalities. To generate 3D proposals with a high recall for small objects, Ku et al.  proposed a multi-modal fusion-based region proposal network. They first extracted equal-sized features from both BEV and image views using cropping and resizing operations, and then fused these features using element-wise mean pooling. Liang et al.  exploited continuous convolutions to enable effective fusion of image and 3D LiDAR feature maps at different resolutions. Specifically, they extracted nearest corresponding image features for each point in the BEV space and then used bilinear interpolation to obtain a dense BEV feature map by projecting image features into the BEV plane. Experimental results show that dense BEV feature maps are more suitable for 3D object detection than discrete image feature maps and sparse LiDAR feature maps. Liang et al.  presented a multi-task multi-sensor 3D object detection network for end-to-end training. Specifically, multiple tasks (e.g., 2D object detection, ground estimation and depth completion) are exploited to help the network learn better feature representations. \nThe learned cross-modality representation is further exploited to produce highly accurate object detection results. Experimental results show that this method achieves a significant improvement on 2D, 3D and BEV detection tasks, and outperforms previous state-of-the-art methods on the TOR4D  benchmark .\n\\textbf{Second}, different methods have been investigated to extract robust representations of the input data. Lu et al.  explored multi-scale contextual information by introducing a Spatial Channel Attention (SCA) module, which captures the global and multi-scale context of a scene and highlights useful features. They also proposed an Extension Spatial Unsample (ESU) module to obtain high-level features with rich spatial information by combining multi-scale low-level features, thus generating reliable 3D object proposals. Although better detection performance can be achieved, the aforementioned multi-view methods take a long runtime since they perform feature pooling for each proposal. Subsequently, Zeng et al.  used a pre-RoI pooling convolution to improve the  efficiency of . Specifically, they moved the majority of convolution operations to be ahead of the RoI pooling module. Therefore, RoI convolutions are performed once for all object proposals. \nExperimental results show that this method can run at a speed of 11.1 fps, which is 5 times faster than MV3D .\n\\textbf{Segmentation-based Methods.} These methods first leverage existing semantic segmentation techniques to remove most background points, and then generate a large amount of high-quality proposals on foreground points to save computation, as shown in Fig. \\ref{fig:detection_frameworks}(b). Compared to multi-view methods , these methods achieve higher object recall rates and are more suitable for complicated scenes with highly occluded and crowded objects.\nYang et al.  used a 2D segmentation network to predict foreground pixels and projected them into point clouds to remove most background points. They then generated proposals on the predicted foreground points and designed a new criterion named PointsIoU to reduce the redundancy and ambiguity of proposals. Following , Shi et al.  proposed a PointRCNN framework. Specifically, they directly segmented 3D point clouds to obtain foreground points and then fused semantic features and local spatial features to produce high-quality 3D boxes. Following the Region Proposal Network (RPN) stage of , Jesus et al.  proposed a pioneering work to leverage Graph Convolution Network (GCN) for 3D object detection. Specifically, two modules are introduced to refine object proposals using graph convolution. The first module R-GCN utilizes all points contained in a proposal to achieve per-proposal feature aggregation. The second module C-GCN fuses per-frame information from all proposals to regress accurate object boxes by exploiting contexts. \nSourabh et al.  projected a point cloud into the output of the image-based segmentation network and appended the semantic prediction scores to the points. The painted points are fed into existing detectors  to achieve significant performance improvement. Yang et al.  associated each point with a spherical anchor. \nThe semantic score of each point is then used to remove redundant anchors. Consequently, this method achieves a higher recall with lower computational cost as compared to previous methods . In addition, a PointsPool layer is proposed to learn compact features for interior points in proposals and a parallel IoU branch is introduced to improve localization accuracy and detection performance. \n\\textbf{Frustum-based Methods.} These methods first leverage existing 2D object detectors to generate 2D candidate regions of objects and then extract a 3D frustum proposal for each 2D candidate region, as shown in Fig. \\ref{fig:detection_frameworks}(c). Although these methods can efficiently propose possible locations of 3D objects, the step-by-step pipeline makes their performance limited by 2D image detectors.\nF-PointNets  is a pioneering work in this direction. It generates a frustum proposal for each 2D region and applies PointNet  (or PointNet++ ) to learn point cloud features of each 3D frustum for amodal 3D box estimation. \nIn a follow-up work, Zhao et al.  proposed a Point-SENet module to predict a set of scaling factors, which were further used to adaptively highlight useful features and suppress informative-less features. They also integrated the PointSIFT  module into the network to capture orientation information of point clouds, which achieved strong robustness to shape scaling. This method achieves significant improvement on both indoor and outdoor datasets  as compared to F-PointNets . \nXu et al.  leveraged both 2D image region and its corresponding frustum points to accurately regress 3D boxes. \nTo fuse image features and global features of point clouds, they presented a global fusion network for direct regression of box corner locations. They also proposed a dense fusion network for the prediction of point-wise offsets to each corner. \nShin et al.  first estimated 2D bounding boxes and 3D poses of objects from a 2D image, and then extracted multiple geometrically feasible object candidates. These 3D candidates are fed into a box regression network to predict accurate 3D object boxes. Wang et al.  generated a sequence of frustums along the frustum axis for each 2D region and applied PointNet  to extract features for each frustum. The frustum-level features are reformed to generate a 2D feature map, which is then fed into a fully convolutional network for 3D box estimation. This method achieves the state-of-the-art performance among 2D image-based methods and was ranked in the top position of the official KITTI leaderboard. \\why{Johannes} et al.  first obtained a preliminary detection results on the BEV map, and then extracted small point subsets (also called patches) based on the BEV predictions. A local refinement network is applied to learn the local features of patches to predict highly accurate 3D bounding boxes.\n\\begin{table*}[t]\n\\centering\n\\caption{Comparative 3D object detection results on the KITTI test 3D detection benchmark. 3D bounding box IoU threshold is 0.7 for cars and 0.5 for pedestrians and cyclists. The modalities are LiDAR (L) and image (I). `E', `M' and `H' represent easy, moderate and hard classes of objects, respectively. For simplicity, we omit the `\\%' after the value. The symbol `-' means the results are unavailable.}\n\\label{Tab:KITTI3D}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|r|c|c|ccc|ccc|ccc|}\n\\hline\n \\multicolumn{3}{|c|}{\\multirow{2}{*}{\\bf Method}} & \\multirow{2}{*}{\\bf Modality} & \\multirow{2}{*}{\\bf \\tabincell{c}{Speed \\\\ (fps)}} & \\multicolumn{3}{|c|}{\\bf Cars} & \\multicolumn{3}{|c|}{\\bf Pedestrians} & \\multicolumn{3}{|c|}{\\bf Cyclists} \\\\\\cline{6-14}\n \\multicolumn{3}{|c|}{} & & & {\\bf E} & {\\bf M} & {\\bf H} & {\\bf E} & {\\bf M} & {\\bf H} & {\\bf E} & {\\bf M} & {\\bf H} \\\\\\hline\n\\multirow{19}{*}{\\tabincell{c}{Region \\\\ Proposal \\\\-based \\\\ Methods}} & \\multirow{6}{*}{\\tabincell{c}{Multi-view \\\\ Methods}} & {MV3D } & {L \\& I} & 2.8 & 74.97 & 63.63 & 54.00 & - & - & - & - & - & - \\\\\n  & & {AVOD } & {L \\& I} & 12.5 & 76.39 & 66.47 & 60.23 & 36.10 & 27.86 & 25.76 & 57.19 & 42.08 & 38.29 \\\\\n  & & {ContFuse } & {L \\& I} & 16.7 & 83.68 & 68.78 & 61.67 & - & - & - & - & - & - \\\\\n  & & {MMF } & {L \\& I} & 12.5 & 88.40 & 77.43 & 70.22 & - & - & - & - & - & - \\\\\n  & & {SCANet } & {L \\& I} & 11.1 & 79.22 & 67.13 & 60.65 & - & - & - & - & - & - \\\\\n  & & {RT3D } & {L \\& I} & 11.1 & 23.74 & 19.14 & 18.86 & - & - & - & - & - & - \\\\\\cline{2-14}\n  & \\multirow{3}{*}{\\tabincell{c}{Segmentation \\\\ -based \\\\ Methods}} & {IPOD } & {L \\& I} & 5.0 & 80.30 & 73.04 & 68.73 & 55.07 & 44.37 & 40.05 & 71.99 & 52.23 & 46.50 \\\\\n  & & {PointRCNN } & {L} & 10.0 & 86.96 & 75.64 & 70.70 & 47.98 & 39.37 & 36.01 & 74.96 & 58.82 & 52.53 \\\\\n  & & {PointRGCN } & {L} & 3.8 & 85.97 & 75.73 & 70.60 & - & - & - & - & - & - \\\\\n  & & {PointPainting } & {L \\& I} & 2.5 & 82.11 & 71.70 & \t67.08 & 50.32 & 40.97 & 37.87 & 77.63 & 63.78 & 55.89 \\\\\n  & & {STD } & {L} & 12.5 & 87.95 & 79.71 & 75.09 & 53.29 & 42.47 & 38.35 & 78.69 & 61.59 & 55.30 \\\\\\cline{2-14}\n  & \\multirow{6}{*}{\\tabincell{c}{Frustum \\\\ -based \\\\ Methods}} & {F-PointNets } & {L \\& I} & 5.9 & 82.19 & 69.79 & 60.59 & 50.53 & 42.15 & 38.08 & 72.27 & 56.12 & 49.01 \\\\\n  & & {SIFRNet } & {L \\& I} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {PointFusion } & {L \\& I} & - & 77.92 &   63.00 & 53.27 & 33.36 & 28.04 & 23.38 & 49.34 & 29.42 & 26.98 \\\\\n  & & {RoarNet } & {L \\& I} & 10.0 & 83.71 & 73.04 & 59.16 & - & - & - & - & - & - \\\\\n  & & {F-ConvNet } & {L \\& I} & 2.1 & 87.36 & 76.39 & 66.69 & 52.16 & 43.38 & 38.80 & 81.98 & 65.07 & 56.54 \\\\\n  & & {\\tabincell{c}{Patch Refinement }} & {L} & 6.7 & 88.67 & 77.20 & 71.82 & - & - & - & - & - & - \\\\\\cline{2-14}\n  & \\multirow{4}{*}{\\tabincell{c}{Other \\\\ Methods}} & {3D IoU loss } & {L} & 12.5 & 86.16 & 76.50 & 71.39 & - & - & - & - & - & - \\\\\n  & & {\\tabincell{c}{Fast Point R-CNN }} & {L} & 16.7 & 84.80 & 74.59 & 67.27 & - & - & - & - & - & - \\\\\n  & & {PV-RCNN } & {L} & 12.5 & 90.25 & 81.43 & 76.82 & - & - & - & - & - & - \\\\\n  & & {VoteNet } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {Feng et al. } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {ImVoteNet } & {L \\& I} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {Part-A\\^{}2 } & {L} & 12.5 & 87.81 & 78.49 & 73.51 & - & - & - & - & - & - \\\\\\hline\n  \\multirow{13}{*}{\\tabincell{c}{Single \\\\ Shot \\\\ Methods}} & \\multirow{3}{*}{\\tabincell{c}{BEV-based \\\\ Methods}} & {PIXOR } & {L} & 28.6 & - & - & - & - & - & - & - & - & - \\\\\n  & & {HDNET } & {L} & 20.0 & - & - & - & - & - & - & - & - & - \\\\\n  & & {BirdNet } & {L} & 9.1 & 13.53 & 9.47 & 8.49 & 12.25 & 8.99 & 8.06 & 16.63 & 10.46 & 9.53 \\\\\\cline{2-14}\n   & \\multirow{8}{*}{\\tabincell{c}{Discretization \\\\ -based \\\\ Methods}} &  {VeloFCN } & {L} & 1.0 & - & - & - & - & - & - & - & - & - \\\\\n  & & {3D FCN } & {L} & {\\textless0.2} & - & - & - & - & - & - & - & - & - \\\\\n  & & {Vote3Deep } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {3DBN } & {L} & 7.7 & 83.77 & 73.53 & 66.23 & - & - & - & - & - & - \\\\\n  & & {VoxelNet } & {L} & 2.0 & 77.47 & 65.11 & 57.73 & 39.48 & 33.69 & 31.51 & 61.22 & 48.36 & 44.37 \\\\\n  & & {SECOND } & {L} & 26.3 & 83.34 & 72.55 & 65.82 & 48.96 & 38.78 & 34.91 & 71.33 & 52.08 & 45.83 \\\\\n  & & {MVX-Net } & {L \\& I} & 16.7 & 84.99 & 71.95 & 64.88 & - & - & - & - & - & - \\\\\n  & & {PointPillars } & {L} & 62.0 & 82.58 & 74.31 & 68.99 & 51.45 & 41.92 & 38.89 & 77.10 & 58.65 & 51.92 \\\\\n  & & {SA-SSD } & {L} & 25.0 & 88.75 & 79.79 & 74.16 & - & - & - & - & - & - \\\\\\cline{2-14}\n  & \\tabincell{c}{Point-based \\\\ Methods} & {3DSSD } & {L} & 25.0 & 88.36 & 79.57 & 74.55 & 54.64 & 44.27 & 40.23 & 82.48 & 64.10 & 56.90 \\\\\\hline\n  & \\multirow{2}{*}{\\tabincell{c}{Other \\\\ Methods}} & {LaserNet } & {L} & 83.3 & - & - & - & - & - & - & - & - & - \\\\\n  & & {LaserNet++ } & {L \\& I} & 26.3 & - & - & - & - & - & - & - & - & - \\\\\n  & & {OHS-Dense } & {L} & 33.3 & 88.12 & 78.34 & 73.49 & 47.14 & 39.72 & 37.25 & 79.09 & 62.72 & 56.76 \\\\\n  & & {OHS-Direct } & {L} & 33.3 & 86.40 & 77.74 & 72.97 & 51.29 & 44.81 & 41.13 & 77.70 & 63.16 & 57.16 \\\\\n  & & {Point-GNN } & {L} & 1.7 & 88.33 & 79.47 & 72.29 & 51.92 & 43.77 & 40.14 & 78.60 & 63.48 & 57.08\\\\\\hline\n\\end{tabular}}\n\\end{table*}\n\\textbf{Other Methods.} Motivated by the success of axis-aligned IoU in object detection in images, Zhou et al.  integrated the IoU of two 3D rotated bounding boxes into several state-of-the-art detectors  to achieve consistent performance improvement. Chen et al.  proposed a two-stage network architecture to use both point cloud and voxel representations. First, point clouds are voxelized and fed to a 3D backbone network to produce initial detection results. Second, the interior point features of initial predictions are further exploited for box refinements. Although this design is conceptually simple, it achieves comparable performance to  while maintaining a speed of 16.7 fps. \nShi et al.  proposed PointVoxel-RCNN (PV-RCNN) to leverage both 3D convolutional network and PointNet-based set abstraction for the learning of point cloud features. Specifically, the input point clouds are first voxelized and then fed into a 3D sparse convolutional network to generate high-quality proposals. The learned voxel-wise features are then encoded into a small set of key points via a voxel set abstraction module. In addition, they also proposed a keypoint-to-grid ROI abstraction module to capture rich context information for box refinement. Experimental results show that this method outperforms previous methods by a remarkable margin and is ranked first\\footnote{The ranking refers to the time of the submission: \n\\hao{12th June}, 2020} on the $Car$ class of the KITTI 3D detection benchmark.\nInspired by Hough voting-based 2D object detectors, Qi et al.  proposed VoteNet to directly vote for virtual center points of objects from point clouds and to generate a group of high-quality 3D object proposals by aggregating vote features. VoteNet significantly outperforms previous approaches using only geometric information, and achieves the state-of-the-art performance on two large indoor benchmarks (i.e., ScanNet  and SUN RGB-D ). However, the prediction of virtual center point is unstable for a partially occluded object. Further, Feng et al.  added an auxiliary branch of direction vectors to improve the prediction accuracy of virtual center points and 3D candidate boxes. In addition, a 3D object-object relationship graph between proposals is built to emphasize useful features for accurate object detection. \\hao{Qi et al.  proposed an ImVoteNet detector by fusing 2D object detection cues (e.g., geometric and semantic/texture cues) into a 3D voting pipeline.}\nInspired by the observation that the ground truth boxes of 3D objects provide accurate locations of intra-object parts, Shi et al.  proposed  the Part-$A^{2}$ Net, which is composed of a part-aware stage and a part-aggregation stage. The part-aware stage applies a UNet-like  network with sparse convolution and sparse deconvolution to learn point-wise features for the prediction and coarse generation of intra-object part locations. The part-aggregation stage adopts RoI-aware pooling to aggregate predicted part locations for box refinement.", "cites": [7408, 7416, 7417, 7385, 7411, 7413, 7420, 7407, 7409, 1535, 7406, 7404, 7419, 7376, 825, 7405, 7415, 4349, 7412, 7414, 7410, 7418], "cite_extract_rate": 0.4489795918367347, "origin_cites_number": 49, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of region proposal-based 3D object detection methods, organizing them into three coherent categories and highlighting their key innovations and trade-offs. It includes critical comparisons of efficiency and performance, such as noting the slow speed of early methods versus improvements in later works. The section abstracts broader patterns, such as the role of multi-modal fusion and the use of graph-based refinement for better detection accuracy."}}
{"id": "76e5660f-7054-4bdb-9f88-31ccea48ca70", "title": "\\bf Single Shot Methods", "level": "subsubsection", "subsections": [], "parent_id": "91e76de0-2caf-451a-8444-c82e48bfe222", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Object Detection and Tracking"], ["subsection", "3D Object Detection"], ["subsubsection", "\\bf Single Shot Methods"]], "content": "These methods directly predict class probabilities and regress 3D bounding boxes of objects using a single-stage network. They do not need region proposal generation and post-processing. As a result, they can run at a high speed.\nAccording to the type of input data, single shot methods can be divided into three categories: BEV-based, \\hao{discretization-based and point-based methods.}\n\\textbf{BEV-based Methods.} \nThese methods mainly take BEV representation as their input. Yang et al.  discretized the point cloud of a scene with equally spaced cells and encoded the reflectance in a similar way, resulting in a regular representation. A Fully Convolution Network (FCN) network is then applied to estimate the locations and heading angles of objects. This method outperforms most single shot methods (including VeloFCN , 3D-FCN  and Vote3Deep ) while running at 28.6 fps. Later, Yang et al.  exploited the geometric and semantic prior information provided by High-Definition (HD) maps to improve the robustness and detection performance of . Specifically, they obtained the coordinates of ground points from the HD map and then used the distance relative to the ground for BEV representation \nto remedy the translation variance caused by the slope of the road. In addition, they concatenated a binary road mask with the BEV representation along the channel dimension to focus on moving objects. Since HD maps are not available everywhere, they also proposed an online map prediction module to estimate the map priors from single LiDAR point cloud. This map-aware method significantly outperforms its baseline on the TOR4D  and KITTI  datasets. However, its generalization performance to point clouds with different densities is poor. To solve this problem, Beltr{\\'a}n et al.  proposed a normalization map to consider the differences among different LiDAR sensors. The normalization map is a 2D grid with the same resolution as the BEV map, and it encodes the maximum number of points contained in each cell. It is shown that this normalization map significantly improves the generalization ability of BEV-based detectors.\n\\textbf{\\hao{Discretization-based Methods.}} These methods convert a point cloud into a regular discrete representation, and then apply CNN to predict both categories and 3D boxes of objects.\nLi et al.  proposed the first method to use a FCN for 3D object detection. They converted a point cloud into a 2D point map and used a 2D FCN to predict the bounding boxes and confidences of objects. Later, they  discretized the point cloud into a 4D tensor with dimensions of length, width, height and channels, and extended the 2D FCN-based detection technologies to 3D domain for 3D object detection. Compared to , 3D FCN-based method  obtains a gain of over 20\\% in accuracy, but inevitably costs more computing resources due to 3D convolutions and the sparsity of the data. To address the sparsity problem of voxels, Engelcke et al.  leveraged a feature-centric voting scheme to generate a set of votes for each non-empty voxel and to obtain the convolutional results by accumulating the votes. Its computational complexity is proportional to the number of occupied voxels. Li et al.  constructed a 3D backbone network by stacking multiple sparse 3D CNNs. This method is designed to save memory and accelerate computation by fully using the sparsity of voxels. This 3D backbone network extracts rich 3D features for object detection without introducing heavy computational burden.\nZhou et al.  presented a voxel-based end-to-end trainable framework VoxelNet. They partitioned a point cloud into equally spaced voxels and encoded the features within each voxel into a 4D tensor. A region proposal network is then connected to produce detection results. Although its performance is strong, this method is very slow due to the sparsity of voxels and 3D convolutions. Later, Yan et al.  used the sparse convolutional network  to improve the inference efficiency of . They also proposed a sine-error angle loss to solve the ambiguity between orientations of 0 and $\\pi$. Sindagi et al.  extended VoxelNet by fusing image and point cloud features at early stages. Specifically, they projected non-empty voxels generated by  into the image and used a pre-trained network to extract image features for each projected voxel. These image features are then concatenated with voxel features to produce accurate 3D boxes. Compared to , this method can effectively exploit multi-modal information to reduce false positives and negatives. Lang et al.  proposed a 3D object detector named PointPillars. This method leverages PointNet  to learn the feature of point clouds organized in vertical columns (Pillars) and encodes the learned features as a pesudo image. A 2D object detection pipeline is then applied to predict 3D bounding boxes. PointPillars outperforms most fusion approaches (including MV3D , RoarNet  and AVOD ) in terms of Average Precision (AP). Moreover, PointPillars can run at a speed of 62 fps on both the 3D and BEV KITTI  benchmarks, making it highly suitable for practical applications.\n\\hao{Inspired by the observation that partial spatial information of a point cloud is inevitably lost in progressively downscaled feature maps of existing single shot detectors, He et al.  proposed a SA-SSD detector to leverage the fine-grained structure information to improve localization accuracy. Specifically, they first converted a point cloud to a tensor and fed it into a backbone network to extract multi-stage features. In addition, an auxiliary network with point-level supervision is employed to guide the features to learn the structure of point clouds. Experimental results show that SA-SSD ranks the first\\footnote{\\hao{The ranking refers to the time of the submission: \n12th June, 2020}} on the $Car$ class of the KITTI BEV detection benchmark.}\n\\textbf{\\hao{Point-based Methods.}} \\hao{These methods directly take raw point clouds as their inputs. 3DSSD  is a pioneering work in this direction. It introduces a fusion sampling strategy for Distance-FPS (D-FPS) and Feature-FPS (F-FPS) to remove time-consuming Feature Propagation (FP) layers and the refinement module in . Then, a Candidate Generation (CG) layer is used to fully exploit representative points, which are further fed into an anchor-free regression head with a 3D centerness label to predict 3D object boxes. Experimental results show that 3DSSD outperforms the two-stage point-based method PointRCNN  while maintaining a speed of 25 fps.} \n\\textbf{Other Methods.} Meyer et al.  proposed an efficient 3D object detector called LaserNet. This method predicts a probability distribution over bounding boxes for each point and then combines these per-point distributions to generate final 3D object boxes. Further, the dense Range View (RV) representation of point cloud is used as input and a fast mean-shift algorithm is proposed to reduce the noise produced by per-point prediction. LaserNet achieves the state-of-the-art performance at the range of 0 to 50 meters, and its runtime is significantly lower than existing methods. Meyer et al.  then extended LaserNet  to exploit the dense texture provided by RGB images (e.g., 50 to 70 meters). Specifically, they associated LiDAR points with image pixels by projecting 3D point clouds onto 2D images and exploited this association to fuse RGB information into 3D points. They also considered 3D semantic segmentation as an auxiliary task to learn better representations. This method achieves a significant improvement in both long-range (e.g., 50 to 70 meters) object detection and semantic segmentation while maintaining high efficiency of LaserNet. \nInspired by the observation that points on an isolated object part can provide abundant information about position and orientation of the object, Chen et al.  proposed a novel $Hotspot$ representation and the first hotspot-based anchor-free detector. Specifically, raw point clouds are first voxelized and then fed into a backbone network to produce 3D feature maps. These feature maps are used to classify hotspots and predict 3D bounding boxes simultaneously. Note that, hotspots are assigned at the last convolutional layer of the backbone network. Experimental results show that this method achieves comparable performance and is robust to sparse point clouds.\n\\hao{Shi et el.  proposed a graph neural network Point-GNN to detect 3D objects from lidar point clouds. They first encoded an input point cloud as a graph of near neighbors with a fixed radius and then fed the graph into Point-GNN to predict both the categories and boxes of objects.}\n\\begin{table*}[t]\n\\centering\n\\caption{Comparative 3D object detection results on the KITTI test BEV detection benchmark. 3D bounding box IoU threshold is 0.7 for cars and 0.5 for pedestrians and cyclists. The modalities are LiDAR (L) and image (I). `E', `M' and `H' represent easy, moderate and hard classes of objects, respectively. For simplicity, we omit the `\\%' after the value. The symbol `-' means the results are unavailable.}\n\\label{Tab:KITTIBEV}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|r|c|c|ccc|ccc|ccc|}\n\\hline\n \\multicolumn{3}{|c|}{\\multirow{2}{*}{\\bf Method}} & \\multirow{2}{*}{\\bf Modality} & \\multirow{2}{*}{\\bf \\tabincell{c}{Speed \\\\ (fps)}} & \\multicolumn{3}{|c|}{\\bf Cars} & \\multicolumn{3}{|c|}{\\bf Pedestrians} & \\multicolumn{3}{|c|}{\\bf Cyclists} \\\\\\cline{6-14}\n \\multicolumn{3}{|c|}{} & & & {\\bf E} & {\\bf M} & {\\bf H} & {\\bf E} & {\\bf M} & {\\bf H} & {\\bf E} & {\\bf M} & {\\bf H} \\\\\\hline\n\\multirow{19}{*}{\\tabincell{c}{Region \\\\ Proposal \\\\-based \\\\ Methods}} & \\multirow{6}{*}{\\tabincell{c}{Multi-view \\\\ Methods}} & {MV3D } & {L \\& I} & 2.8 & 86.62 & 78.93 & 69.80 & - & - & - & - & - & - \\\\\n  & & {AVOD } & {L \\& I} & 12.5 & 89.75 & 84.95 & 78.32 & 42.58 & 33.57 & 30.14 & 64.11 & 48.15 & 42.37 \\\\\n  & & {ContFuse } & {L \\& I} & 16.7 & 94.07 & 85.35 & 75.88 & - & - & - & - & - & - \\\\\n  & & {MMF } & {L \\& I} & 12.5 & 93.67 & 88.21 & 81.99 & - & - & - & - & - & - \\\\\n  & & {SCANet } & {L \\& I} & 11.1 & 90.33 & 82.85 & 76.06 & - & - & - & - & - & - \\\\\n  & & {RT3D } & {L \\& I} & 11.1 & 56.44 & 44.00 & 42.34 & - & - & - & - & - & - \\\\\\cline{2-14}\n  & \\multirow{3}{*}{\\tabincell{c}{Segmentation \\\\ -based \\\\ Methods}} & {IPOD } & {L \\& I} & 5.0 & 89.64 & 84.62 & 79.96 & 60.88 & 49.79 & 45.43 & 78.19 & 59.40 & 51.38 \\\\\n  & & {PointRCNN } & {L} & 10.0 & 92.13 & 87.39 & 82.72 & 54.77 & 46.13 & 42.84 & 82.56 & 67.24 & 60.28 \\\\\n  & & {PointRGCN } & {L} & 3.8 & 91.63 & 87.49 & 80.73 & - & - & - & - & - & - \\\\\n  & & {PointPainting } & {L \\& I} & 2.5 & 92.45 & 88.11 & 83.36 & 58.70 & 49.93 & 46.29 & 83.91 & 71.54 & 62.97 \\\\ \n  & & {STD } & {L} & 12.5 & 94.74 & 89.19 & 86.42 & 60.02 & 48.72  44.55 & 81.36 & 67.23 & 59.35 \\\\\\cline{2-14}\n  & \\multirow{6}{*}{\\tabincell{c}{Frustum \\\\ -based \\\\ Methods}} & {F-PointNets } & {L \\& I} & 5.9 & 91.17 & 84.67 & 74.77 & 57.13 & 49.57 & 45.48 & 77.26 & 61.37 & 53.78 \\\\\n  & & {SIFRNet } & {L \\& I} & - & - & - & - & - & - & - & - & - & -\\\\\n  & & {PointFusion } & {L \\& I} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {RoarNet } & {L \\& I} & 10.0 & 88.20 & 79.41 & 70.02 & - & - & - & - & - & - \\\\\n  & & {F-ConvNet } & {L \\& I} & 2.1 & 91.51 & 85.84 & 76.11 & 57.04 & 48.96 & 44.33 & 84.16 & 68.88 & 60.05 \\\\\n  & & {\\tabincell{c}{Patch Refinement }} & {L} & 6.7 & 92.72 & 88.39 & 83.19 & - & - & - & - & - & -\\\\\\cline{2-14}\n  & \\multirow{4}{*}{\\tabincell{c}{Other \\\\ Methods}} & {3D IoU loss } & {L} & 12.5 & 91.36 & 86.22 & 81.20 & - & - & - & - & - & - \\\\\n  & & {\\tabincell{c}{Fast Point R-CNN }} & {L} & 16.7 & 90.76 & 85.61 & 79.99 & - & - & - & - & - & - \\\\\n  & & {PV-RCNN } & {L} & 12.5 & 94.98 & 90.65 & 86.14 & - & - & - & 82.49 & 68.89 & 62.41 \\\\\n  & & {VoteNet } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {Feng et al. } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {ImVoteNet } & {L \\& I} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {Part-A\\^{}2 } & {L} & 12.5 & 91.70 & 87.79 & 84.61 & - & - & - & 81.91 & 68.12 & 61.92 \\\\\\hline\n  \\multirow{13}{*}{\\tabincell{c}{Single \\\\ Shot \\\\ Methods}} & \\multirow{3}{*}{\\tabincell{c}{BEV-based \\\\ Methods}} & {PIXOR } & {L} & 28.6 & 83.97 & 80.01 & 74.31 & - & - & - & - & - & - \\\\\n  & & {HDNET } & {L} & 20.0 & 89.14 & 86.57 & 78.32 & - & - & - & - & - & - \\\\\n  & & {BirdNet } & {L} & 9.1 & 76.88 & 51.51 & 50.27 & 20.73 & 15.80 & 14.59 & 36.01 & 23.78 & 21.09 \\\\\\cline{2-14}\n  & \\multirow{8}{*}{\\tabincell{c}{Discretization \\\\ -based \\\\ Methods}} &  {VeloFCN } & {L} & 1.0 & 0.02 & 0.14 & 0.21 & - & - & - & - & - & - \\\\\n  & & {3D FCN } & {L} & {\\textless0.2} & 70.62 & 61.67 & 55.61 & - & - & - & - & - & - \\\\\n  & & {Vote3Deep } & {L} & - & - & - & - & - & - & - & - & - & - \\\\\n  & & {3DBN } & {L} & 7.7 & 89.66 & 83.94 & 76.50 & - & - & - & - & - & -\\\\\n  & & {VoxelNet } & {L} & 2.0 & 89.35 & 79.26 & 77.39 & 46.13 & 40.74 & 38.11 & 66.70 & 54.76 & 50.55 \\\\\n  & & {SECOND } & {L} & 26.3 & 89.39 & 83.77 & 78.59 & 55.99 & 45.02 & 40.93 & 76.50 & 56.05 & 49.45 \\\\\n  & & {MVX-Net } & {L \\& I} & 16.7 & 92.13 & 86.05 & 78.68 & - & - & - & - & - & - \\\\\n  & & {PointPillars } & {L} & 62.0 & 90.07 & 86.56 & 82.81 & 57.60 & 48.64 & 45.78 & 79.90 & 62.73 & 55.58 \\\\\n  & & {SA-SSD } & {L} & 25.0 & 95.03 & 91.03 &\t85.96 & - & - & - & - & - & - \\\\\\cline{2-14}\n  & \\tabincell{c}{Point-based \\\\ Methods} & {3DSSD } & {L} & 25.0 & 92.66 & 89.02 & 85.86 & 60.54 & 49.94 & 45.73 & 85.04 & 67.62 & 61.14 \\\\\\hline\n  & \\multirow{2}{*}{\\tabincell{c}{Other \\\\ Methods}} & {LaserNet } & {L} & 83.3 & 79.19 & 74.52 & 68.45 & - & - & - & - & - & - \\\\\n  & & {LaserNet++ } & {L \\& I} & 26.3 & - & - & - & - & - & - & - & - & - \\\\\n  & & {OHS-Dense } & {L} & 33.3 & 93.73 & 88.11 & 84.98 & 50.87 & 44.59 & 42.14 & 82.13 & 66.86 & 60.86 \\\\\n  & & {OHS-Direct } & {L} & 33.3 & 93.59 & 87.95 & 83.21 & 55.90 & 49.48 & 45.79 & 79.66 & 67.20 & 61.04 \\\\\n  & & {Point-GNN } & {L} & 1.7 & 93.11 & 89.17 & 83.90 & 55.36 & 47.07 & 44.61 & 81.17 & 67.28 & 59.67\\\\\\hline\n\\end{tabular}}\n\\end{table*}", "cites": [7408, 7416, 7417, 7411, 7413, 7420, 7407, 7409, 1535, 7406, 7404, 7405, 7415, 4349, 7412, 7414, 7410, 7418], "cite_extract_rate": 0.4, "origin_cites_number": 45, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information across multiple papers by categorizing single-shot methods into BEV-based, discretization-based, and point-based approaches, highlighting how each class addresses point cloud challenges. It provides critical insights, such as trade-offs between accuracy and computational cost, and limitations in generalization across different point cloud densities. While it identifies patterns like the use of sparsity-aware techniques and sensor fusion, the abstraction remains somewhat constrained to specific detection paradigms rather than broader meta-level principles."}}
{"id": "0d6c8ca4-1474-4b97-8978-ecc6f07813b6", "title": "3D Object Tracking", "level": "subsection", "subsections": [], "parent_id": "88793bd8-cfbb-4a97-ae03-17f5c34f0614", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Object Detection and Tracking"], ["subsection", "3D Object Tracking"]], "content": "Given the locations of an object in the first frame, the task of object tracking is to estimate its state in subsequent frames . Since 3D object tracking can use the rich geometric information in point clouds, it is expected to overcome several drawbacks faced by image-based tracking, including occlusion, illumination and scale variation. \nInspired by the success of Siamese network  for imaged-based object tracking, Giancola et al.  proposed a 3D Siamese network with shape completion regularization. \nSpecifically, they first generated candidates using a Kalman filter, and encoded model and candidates into a compact representation using shape regularization. The cosine similarity is then used to search the location of the tracked object in the next frame. This method can be used as an alternative for object tracking, and significantly outperforms most 2D object tracking methods, including $\\mathrm{STAPLE_{CA}}$  and SiamFC . To efficiently search the target object, Zarzar et al.  leveraged a 2D Siamese network to generate a large number of coarse object candidates on BEV representation. They then refined the candidates by exploiting the cosine similarity in 3D Siamese network. This method significantly outperforms  in terms of both precision (i.e., by 18\\%) and success rate (i.e., by 12\\%). Simon et al.  proposed a 3D object detection and tracking architecture for semantic point clouds. They first generated voxelized semantic point clouds by fusing 2D visual semantic information, and then utilized the temporal information to improve accuracy and robustness of multi-target tracking. In addition, they introduced a powerful and simplified evaluation metric (i.e., Scale-Rotation-Translation score (SRFs)) to speed up training and inference. Complexer-YOLO achieves promising tracking performance and can still run in real-time. \\hao{Further, Qi et al.  proposed a Point-to-Box (P2B) network. They fed template and search areas into the backbone to obtain their seeds. The search area seeds are augmented with target-specific features and then the potential target centers are regressed by Hough voting. Experimental results show that P2B outperforms  by over 10\\% while running at 40 fps.}\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth,keepaspectratio]{Figure/Scene-flow.JPG}\n\\caption{A 3D scene flow between two KITTI point clouds, originally shown in . Point clouds $\\mathcal{X}$, $\\mathcal{Y}$ and the translated point cloud of $\\mathcal{X}$ are highlighted in red, green, and blue, respectively.}\n\\label{fig:scene-flow}\n\\end{figure}", "cites": [9113, 7422, 7423, 7421], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates multiple papers to present a comparative view of 3D object tracking methods, particularly those based on Siamese networks and fusion techniques. It highlights performance improvements (e.g., precision and success rate) and introduces key features like shape completion and evaluation metrics. However, it lacks deeper abstraction or a novel framework, and the critical analysis is limited to stating performance advantages without discussing underlying trade-offs or limitations."}}
{"id": "ae0c2d30-9e72-483a-8141-90d8b14bb4b6", "title": "3D Scene Flow Estimation", "level": "subsection", "subsections": [], "parent_id": "88793bd8-cfbb-4a97-ae03-17f5c34f0614", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Object Detection and Tracking"], ["subsection", "3D Scene Flow Estimation"]], "content": "\\hao{Given two point clouds $\\mathcal{X}$ and $\\mathcal{Y}$, 3D scene flow $D=\\{d_i\\}^N$ describes the movement of each point $x_i$ in $\\mathcal{X}$ to its corresponding position $x_i'$ in $\\mathcal{Y}$, such that $x_i'=x_i+d_i$. Figure \\ref{fig:scene-flow} shows a 3D scene flow between two KITTI point clouds.} Analogous to optical flow estimation in 2D vision, several methods have started to learn useful information (e.g. 3D scene flow, spatial-temporary information) from a sequence of point clouds. \nLiu et al.  proposed FlowNet3D to directly learn scene flows from a pair of consecutive point clouds. FlowNet3D learns both point-level features and motion features through a flow embedding layer. However, there are two problems with FlowNet3D. First, some predicted motion vectors differ significantly from the ground truth in their directions. Second, it is difficult to apply FlowNet to non-static scenes, especially for the scenes which are dominated by deformable objects. To solve this problem, Wang et al.  introduced a cosine distance loss to minimize the angle between the predictions and the ground truth. In addition, they also proposed a point-to-plane distance loss to improve the accuracy for both rigid and dynamic scenes. Experimental results show that these two loss terms improve the accuracy of FlowNet3D from 57.85\\% to 63.43\\%, and speed up and stabilize the training process. Gu et al.  proposed a Hierarchical Permutohedral Lattice FlowNet (HPLFlowNet) to directly estimate scene flow from large-scale point clouds. Several bilateral convolution layers are proposed to restore structural information from raw point clouds, while reducing the computational cost. \nTo effectively process sequential point clouds, Fan and Yang  proposed PointRNN, PointGRU and PointLSTM networks and a sequence-to-sequence model to track moving points. PointRNN, PointGRU, and PointLSTM are able to capture the spatial-temporary information and model dynamic point clouds. Similarly, Liu et al.  proposed MeteorNet to directly learn a representation from dynamic point clouds. This method learns to aggregate information from spatiotemporal neighboring points. Direct grouping and chained-flow grouping are further introduced to determine the temporal neighbors. However, the performance of the aforementioned methods is limited by the scale of datasets. Mittal et al.  proposed two self-supervised losses to train their network on large unlabeled datasets. Their main idea is that a robust scene flow estimation method should be effective in both forward and backward predictions. Due to the unavailability of scene flow annotation, the nearest neighbor of the predicted transformed point is considered as pesudo ground truth. However, the true ground truth may not be the same as the nearest point. To avoid this problem, they computed the scene flow in the reverse direction and proposed a cycle consistency loss to translate the point to the original position. Experimental results show that this self-supervised method exceeds the state-of-the-art performance of supervised learning-based methods.", "cites": [7424, 1536, 7425], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by connecting methods like FlowNet3D, HPLFlowNet, PointRNN, and self-supervised approaches into a coherent narrative on 3D scene flow estimation. It critically evaluates the limitations of prior methods (e.g., direction errors, scalability) and highlights how subsequent approaches address these issues. While it identifies patterns in the use of loss functions and temporal modeling, it primarily operates at the level of specific techniques rather than abstracting overarching principles."}}
{"id": "1a0c8661-9ada-4fd1-8878-8aed12a4a51b", "title": "3D Semantic Segmentation", "level": "subsection", "subsections": ["2e5fc633-9e47-4881-a4e7-c4a571c04a34", "da2922a3-cf9a-40b5-a1c7-a3ac579ccab0", "34485851-b230-4e04-af6f-5efd313f66b0", "e9575b95-4a2f-4bbd-bd5b-667a693dad5d"], "parent_id": "fc9d3465-3d91-42d6-8707-1bc7429ba4c0", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "3D Semantic Segmentation"]], "content": "Given a point cloud, the goal of semantic segmentation is to separate it into several subsets according to the semantic meanings of points. Similar to the taxonomy for 3D shape classification (Section \\ref{sec:shape_classification}), there are \\qy{four paradigms for semantic segmentation: projection-based, discretization-based,  point-based, and hybrid methods.}\n\\qy{The first step of both the projection and discretization-based methods is to transform a point cloud to an intermediate regular representation, such as multi-view  , spherical  , volumetric  , permutohedral lattice  , and hybrid representations , as shown in Fig. \\ref{fig:intermediate_representation}. The intermediate segmentation results are then projected back to the raw point cloud. In contrast, point-based methods directly work on irregular point clouds. Several representative methods are shown in Fig. \\ref{fig:milestone_sem}.}", "cites": [7427, 1537, 7426, 7429, 7428], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by organizing them into four distinct paradigms for 3D semantic segmentation, which provides a structured and coherent overview. It offers a critical perspective by contrasting projection/discretization-based methods with point-based methods, highlighting their different input handling strategies. However, while it begins to generalize these methods into broader categories, it does not fully explore overarching trends or principles that unify or differentiate them in a deeper, meta-level analysis."}}
{"id": "2e5fc633-9e47-4881-a4e7-c4a571c04a34", "title": "\\bf{\\qy{Projection-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "1a0c8661-9ada-4fd1-8878-8aed12a4a51b", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "3D Semantic Segmentation"], ["subsubsection", "\\bf{\\qy{Projection-based Methods"]], "content": "}}\n\\qy{These methods usually project a 3D point cloud into 2D images, including multi-view and spherical images. }\n\\textbf{Multi-view Representation.} \nLawin et al.  first projected a 3D point cloud onto 2D planes from multiple virtual camera views. Then, a multi-stream FCN is used to predict pixel-wise scores on  synthetic images. The final semantic label of each point is obtained by fusing the re-projected scores over different views. Similarly, Boulch et al.  first generated several RGB and depth snapshots of a point cloud using multiple camera positions. They then performed pixel-wise labeling on these snapshots using 2D segmentation networks. The scores predicted from RGB and depth images are further fused using residual correction . Based on the assumption that point clouds are sampled from locally Euclidean surfaces, Tatarchenko et al.  introduced tangent convolutions for dense point cloud segmentation. This method first projects the local surface geometry around each point to a virtual tangent plane. Tangent convolutions are then directly operated on the surface geometry. This method shows great scalability and is able to process large-scale point clouds with millions of points. Overall, the performance of multi-view segmentation methods is sensitive to viewpoint selection and occlusions. Besides, these methods have not fully exploited the underlying geometric and structural information, as the projection step inevitably introduces information loss. \n\\begin{figure}[bht]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{Figure/representation_new.pdf}\n\\caption{An illustration of the intermediate representation. (a) and (b) are originally shown in  and , respectively. \\label{fig:intermediate_representation}}\n\\end{figure}\n\\textbf{Spherical Representation.} \nTo achieve fast and accurate segmentation of 3D point clouds,  Wu et al.  proposed an end-to-end network based on SqueezeNet  and Conditional Random Field (CRF). To further improve segmentation accuracy, SqueezeSegV2  is introduced to address domain shift by utilizing an unsupervised domain adaptation pipeline. Milioto et al.  proposed RangeNet++ for real-time semantic segmentation of LiDAR point clouds. The semantic labels of 2D range images are first transferred to 3D point clouds, an efficient GPU-enabled KNN-based post-processing step is further used to alleviate the problem of discretization errors and blurry inference outputs.\nCompared to single view projection, spherical projection retains more information and is suitable for the labeling of LiDAR point clouds. However, this intermediate representation inevitably brings several problems such as discretization errors and occlusions.", "cites": [1538, 7431, 7428, 7430], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on projection-based methods for 3D point cloud segmentation, grouping them into multi-view and spherical representations. It provides a coherent narrative by comparing their approaches and outcomes. The critical analysis is moderate, highlighting limitations such as information loss and sensitivity to viewpoint selection. The abstraction is reasonable, as it generalizes common issues like discretization errors and occlusions across different projection strategies."}}
{"id": "da2922a3-cf9a-40b5-a1c7-a3ac579ccab0", "title": "\\bf{\\qy{Discretization-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "1a0c8661-9ada-4fd1-8878-8aed12a4a51b", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "3D Semantic Segmentation"], ["subsubsection", "\\bf{\\qy{Discretization-based Methods"]], "content": "} }\n\\qy{These methods usually convert a point cloud into a dense/sparse discrete representation, such as  volumetric and sparse permutohedral lattices.}\n\\textbf{Dense Discretization Representation.}\nEarly methods usually voxelized the point clouds as dense grids and then leverage the standard 3D convolutions. Huang et al.  first divided a point cloud into a set of occupancy voxels, then fed these intermediate data to a fully-3D CNN for voxel-wise segmentation. Finally, all points within a voxel are assigned the same semantic label as the voxel. The performance of this method is severely limited by the granularity of the voxels and the boundary artifacts caused by the point cloud partition. Further, Tchapmi et al.  proposed SEGCloud to achieve fine-grained and global consistent semantic segmentation. This method introduces a deterministic trilinear interpolation to map the coarse voxel predictions generated by 3D-FCNN  back to the point cloud, and then uses Fully Connected CRF (FC-CRF) to enforce spatial consistency of these inferred per-point labels. Meng et al.  introduced a kernel-based interpolated variational autoencoder architecture to encode the local geometrical structures within each voxel. Instead of a binary occupancy representation, RBFs are employed for each voxel to obtain a continuous representation and capture the distribution of points in each voxel. VAE is further used to map the point distribution within each voxel to a compact latent space. Then, both symmetry groups and an equivalence CNN are used to achieve robust feature learning. \n\\qy{Thanks to the good scalability of 3D CNN, volumetric-based networks are free to be trained and tested on point clouds with different spatial sizes.}\nIn Fully-Convolutional Point Network (FCPN) , different levels of geometric relations are first hierarchically abstracted from point clouds, 3D convolutions and weighted average pooling are then used to extract features and incorporate long-range dependencies. This method can process large-scale point clouds and has good scalability during inference. Dai et al.  proposed ScanComplete to achieve 3D scan completion and per-voxel semantic labeling. This method leverages the scalability of fully-convolutional neural networks and can adapt to different input data sizes during training and test. A coarse-to-fine strategy is used to hierarchically improve the resolution of the predicted results.\nOverall, the volumetric representation naturally preserves the neighborhood structure of 3D point clouds. Its regular data format also allows direct application of standard 3D convolutions. These factors lead to a steady performance improvement in this area. However, the voxelization step inherently introduces discretization artifacts and information loss. Usually, a high resolution leads to high memory and computational costs, while a low resolution introduces loss of details. It is non-trivial to select an appropriate grid resolution in practice. \n\\textbf{Sparse Discretization Representation.}\nVolumetric representation is naturally sparse, as the number of non-zero values only accounts for a small percentage. Therefore, it is inefficient to apply dense convolution neural networks on the spatially-sparse data. To this end, Graham et al.  proposed submanifold sparse convolutional networks based on the indexing structure. This method significantly reduces memory and computational costs by restricting the output of convolution to be only related to occupied voxels. Meanwhile, its sparse convolution can also control the sparsity of the extracted features. This submanifold sparse convolution is suitable for efficient processing of high-dimensional and spatially-sparse data. Further, Choy et al.  proposed a 4D spatio-temporal convolutional neural network called MinkowskiNet for 3D video perception. A generalized sparse convolution is proposed to effectively process high-dimensional data. A trilateral-stationary conditional random field is further applied to enforce consistency.\nOn the other hand, Su et al.  proposed the Sparse Lattice Networks (SPLATNet) based on Bilateral Convolution Layers  (BCLs). This method first interpolates a raw point cloud to a permutohedral sparse lattice, BCL is then applied to convolve on occupied parts of the sparsely populated lattice. The filtered output is then interpolated back to the raw point cloud. In addition, this method allows flexible joint processing of multi-view images and point clouds. Further, Rosu et al.  proposed LatticeNet to achieve efficient processing of large point clouds. A data-dependent interpolation module called DeformsSlice is also introduced to back project the lattice feature to point clouds.", "cites": [810, 7433, 7427, 1537, 7429, 7432], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, connecting them within the framework of discretization-based methods for 3D semantic segmentation. It provides critical analysis by discussing the limitations of voxelization and the trade-offs between resolution and computational cost. The section also abstracts beyond individual methods, highlighting broader design principles such as the efficiency of sparse convolutions and the role of interpolation for consistency."}}
{"id": "34485851-b230-4e04-af6f-5efd313f66b0", "title": "\\bf{Hybrid Methods", "level": "subsubsection", "subsections": [], "parent_id": "1a0c8661-9ada-4fd1-8878-8aed12a4a51b", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "3D Semantic Segmentation"], ["subsubsection", "\\bf{Hybrid Methods"]], "content": "}\nTo further leverage all available information, several methods have been proposed to learn multi-modal features from 3D scans. Dai and Nie{\\ss}ner  \\why{presented} a joint 3D-multi-view network to combine RGB features and geometric features. A 3D CNN stream and several 2D streams are used to extract features, and a differentiable back-projection layer is proposed to jointly fuse the learned 2D embeddings and 3D geometric features. Further, Chiang et al.  proposed a unified point-based framework to learn 2D textural appearance, 3D structures and global context features from point clouds. This method directly applies point-based networks to extracts local geometric features and global context from sparsely sampled point sets without any voxelization. Jaritz et al.  proposed Multi-view PointNet (MVPNet) to aggregate appearance features from 2D multi-view images and spatial geometric features in the canonical point cloud space.", "cites": [7426, 7434], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of hybrid methods for 3D semantic segmentation, mentioning the approaches of three different papers. It offers minimal synthesis beyond listing the methods, and does not compare them or evaluate their strengths and limitations. There is no abstraction or identification of broader patterns or principles across the cited works."}}
{"id": "e9575b95-4a2f-4bbd-bd5b-667a693dad5d", "title": "\\bf{Point-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "1a0c8661-9ada-4fd1-8878-8aed12a4a51b", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "3D Semantic Segmentation"], ["subsubsection", "\\bf{Point-based Methods"]], "content": "}\nPoint-based networks directly work on irregular point clouds. However, point clouds are orderless and unstructured, making it infeasible to directly apply standard CNNs. To this end, the pioneering work PointNet  is proposed to learn per-point features using shared MLPs and global features using symmetrical pooling functions. Based on PointNet, a series of point-based networks have been proposed recently. Overall, these methods can be roughly divided into pointwise MLP methods, point convolution methods, RNN-based methods, and graph-based methods.\n\\textbf{Pointwise MLP Methods.} \nThese methods usually use shared MLP as the basic unit in their network for its high efficiency. However, point-wise features extracted by  shared MLP cannot capture the local geometry in point clouds and the mutual interactions between points . To capture wider context for each point and learn richer local structures, several dedicated networks have been introduced, including methods based on neighboring feature pooling, attention-based aggregation, and local-global feature concatenation.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{Figure/point_based.pdf}\n\\caption{An illustration of point-based methods. (a)-(d) are originally shown in , respectively.}\n\\label{fig:Pointnet++}\n\\end{figure}\n\\textit{Neighboring feature pooling:} \nTo capture local geometric patterns, these methods learn a feature for each point by aggregating the information from local neighboring points. In particular, PointNet++  groups points hierarchically and progressively learns from larger local regions, as illustrated in Fig. \\ref{fig:Pointnet++}(a). Multi-scale grouping and multi-resolution grouping are also proposed to overcome the problems caused by non-uniformity and varying density of point clouds. Later, Jiang et al.  proposed a PointSIFT module to achieve orientation encoding and scale awareness. This module stacks and encodes the information from eight spatial orientations through a three-stage ordered convolution. Multi-scale features are concatenated to achieve adaptivity to different scales. Different from the grouping techniques used in PointNet++ (i.e., ball query), \\why{Engelmann} et al.  utilized $K$-means clustering and KNN to separately define two neighborhoods in the world space and feature space. Based on the assumption that points from the same class are expected to be closer in feature space, a pairwise distance loss and a centroid loss are introduced to further regularize feature learning. To model the mutual interactions between different points, Zhao et al.  proposed PointWeb to explore the relations between all pairs of points in a local region by densely constructing a locally fully-linked web. \nAn Adaptive Feature Adjustment (AFA) module is proposed to achieve information interchange and feature refinement. This aggregation operation helps the network to learn a discriminative feature representation. Zhang et al.  proposed a permutation invariant convolution called Shellconv based on the statistics from concentric spherical shells. This method first queries a set of multi-scale concentric spheres, the max-pooling operation is then used within different shells to summarize the statistics, MLPs and 1D convolution are used to obtain the final convolution output. Hu et al.  proposed an efficient and lightweight network called RandLA-Net for large-scale point cloud segmentation. This network utilizes random point sampling to achieve remarkably high efficiency in terms of memory and computation. A local feature aggregation module is further proposed to capture and preserve geometric features. \n\\textit{Attention-based aggregation:} To further improve segmentation accuracy, an attention mechanism  is introduced to point cloud segmentation. Yang et al.  \nproposed a group shuffle attention to model the relations between points, and presented a permutation-invariant, task-agnostic and differentiable Gumbel Subset Sampling (GSS) to replace the widely used FPS approach. This module is less sensitive to outliers and can select a representative subset of points. To better capture the spatial distribution of a point cloud, Chen et al.  proposed a Local Spatial Aware (LSA) layer to learn spatial awareness weights based on the spatial layouts and the local structures of point clouds. \nSimilar to CRF, Zhao et al.  proposed an Attention-based Score Refinement (ASR) module to post-process the segmentation results produced by the network. The initial segmentation result is refined by pooling the scores of neighboring points with learned attention weights. This module can be easily integrated into existing deep networks to improve segmentation performance. \n\\textit{Local-global concatenation:} Zhao et al.  proposed a permutation-invariant PS${^2}$-Net to incorporate local structures and global context from point clouds. Edgeconv  and NetVLAD  are repeatedly stacked to capture the local information and scene-level global features. \n\\begin{table*}[]\n\\centering\n\\caption{Comparative semantic segmentation results on the S3DIS (including both Area5 and 6-fold cross validation) , Semantic3D (including both \\textit{semantic-8} and \\textit{reduced-8} subsets)  , ScanNet , and SemanticKITTI  datasets. Overall Accuracy (OA), Mean Intersection over Union (mIoU) are the main evaluation metric. For simplicity, we omit the `\\%' after the value. The symbol `-' means the results are unavailable.}\n\\label{tab:segmentation_results}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|r|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{3}{|c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{4}{c|}{\\textbf{S3DIS}} & \\multicolumn{4}{c|}{\\textbf{Semantic3D}} & \\multicolumn{2}{c|}{\\textbf{ScanNet(v2)}} & \\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Sem.\\\\ KITTI\\\\ (mIoU)\\end{tabular}}} \\\\ \\cline{4-13}\n\\multicolumn{3}{|c|}{} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Area5\\\\ (OA)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Area5\\\\ (mIoU)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}6-fold\\\\ (mIoU)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}6-fold\\\\ (mIoU)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}sem.\\\\ (OA)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}sem.\\\\ (mIoU)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}red.\\\\ (OA)\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}red.\\\\ (mIoU)\\end{tabular}} & \\textbf{OA} & \\textbf{mIoU} &  \\\\ \\hline\n\\multirow{6}{*}{\\begin{tabular}[c]{@{}c@{}}Projection\\\\-based \\\\ Methods\\end{tabular}} \n& \\multirow{3}{*}{Multi-view}\n& DeePr3SS  & - & - & - & - & - & - & 88.9 & 58.5 & - & - & - \\\\ \\cline{3-14} \n &  & SnapNet  & - & - & - & - & 91.0 & 67.4 & 88.6 & 59.1 & - & - & - \\\\ \\cline{3-14}\n  &  & TangentConv  & 82.5 & 52.8 & - & - & - & - & - & - & 80.1 & 40.9 & 40.9 \\\\ \\cline{2-14} \n & \\multirow{3}{*}{Spherical} & SqueezeSeg  & - & - & - & - & - & - & - & - & - & - & 29.5 \\\\ \\cline{3-14} \n &  & SqueezeSegV2  & - & - & - & - & - & - & - & - & - & - & 39.7 \\\\ \\cline{3-14} \n &  & RangeNet++  & - & - & - & - & - & - & - & - & - & - & 52.2 \\\\  \\hline \n\\multirow{6}{*}{\\begin{tabular}[c]{@{}c@{}}Discretization\\\\-based \\\\ Methods\\end{tabular}} \n & \\multirow{4}{*}{Volumetric} & SEGCloud  & - & 48.9 & - & - & - & - & 88.1 & 61.3 & - & - & - \\\\ \\cline{3-14} \n &  & SparseConvNet  & - & - & - & - & - & - & - & - & - & 72.5 & - \\\\ \\cline{3-14} \n &  & MinkowskiNet  & - & - & - & - & - & - & - & - & - & 73.6 & - \\\\ \\cline{3-14} \n &  & VV-Net  & - & - & 87.8 & 78.2 & - & - & - & - & - & - & - \\\\ \\cline{2-14} \n & \\multirow{2}{*}{{\\begin{tabular}[c]{@{}c@{}}Permutohedral\\\\lattice \\end{tabular}}} \n & SPLATNet  & - & - & - & - & - & - & - & - & - & 39.3 & 18.4 \\\\ \\cline{3-14} \n &  & LatticeNet  & - & - & - & - & - & - & - & - & - & 64.0 & 52.2 \\\\ \\hline \n \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}} Hybrid \\\\ Methods\\end{tabular}} \n & \\multirow{3}{*}{Hybrid} & 3DMV  & - & - & - & - & - & - & - & - & - & 48.4 & - \\\\ \\cline{3-14} \n &  & UPB  & - & - & - & - & - & - & - & - & - & 63.4 & - \\\\ \\cline{3-14} \n &  & MVPNet  & - & - & - & - & - &  & - & - & - & 64.1 & - \\\\ \\hline\n\\multirow{30}{*}{\\begin{tabular}[c]{@{}c@{}}Point\\\\-based\\\\ Methods\\end{tabular}} & \\multirow{11}{*}{\\begin{tabular}[c]{@{}c@{}}Point-wise\\\\ MLP\\end{tabular}} & PointNet  & - & 41.1 & 78.6 & 47.6 & - & - & - & - & - & - & 14.6 \\\\ \\cline{3-14} \n &  & PointNet++  & - & - & 81.0 & 54.5 & 85.7 & 63.1 & - & - & 84.5 & 33.9 & 20.1 \\\\ \\cline{3-14} \n &  & PointSIFT  & - & - & 88.7 & 70.2 & - & - & - & - & 86.2 & 41.5 & - \\\\ \\cline{3-14} \n &  & Engelmann  & 84.2 & 52.2 & 84.0 & 58.3 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & 3DContextNet  & - & - & 84.9 & 55.6 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & A-SCN  & - & - & 81.6 & 52.7 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & PointWeb  & 87.0 & 60.3 & 87.3 & 66.7 & - & - & - & - & 85.9 & - & - \\\\ \\cline{3-14} \n &  & PAT  &  & 60.1 &  & 64.3 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & LSANet  & - & - & 86.8 & 62.2 & - & - & - & - & 85.1 & - & - \\\\ \\cline{3-14} \n &  & ShellNet  & - & - & 87.1 & 66.8 & - & - & 93.2 & 69.3 & 85.2 & - & - \\\\ \\cline{3-14} \n &  & RandLA-Net  & - & - & 88.0 & 70.0 & 94.6 & 74.8 & 94.8 & 77.4 & - & - & 55.9 \\\\ \\cline{2-14} \n & \\multirow{7}{*}{\\begin{tabular}[c]{@{}c@{}}Point\\\\convolution \\end{tabular}} & PointCNN  & 85.9 & 57.3 & 88.1 & 65.4 & - & - & - & - & 85.1 & 45.8 & - \\\\ \\cline{3-14} \n &  & PCCN  & - & 58.3 & - & - & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & A-CNN  & - & - & 87.3 & - & - & - & - & - & 85.4 & - & - \\\\ \\cline{3-14} \n &  & ConvPoint  & - & - & 88.8 & 68.2 & 93.4 & 76.5 & - & - & - & - & - \\\\ \\cline{3-14} \n &  & KPConv  & - & 67.1 & - & 70.6 & - & - & 92.9 & 74.6 & - & 68.4 & - \\\\ \\cline{3-14} \n &  & DPC  & 86.8 & 61.3 & - & - & - & - & - & - & - & 59.2 & - \\\\ \\cline{3-14} \n &  & InterpCNN  & - & - & 88.7 & 66.7 & - & - & - & - & - & - & - \\\\ \\cline{2-14} \n & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}RNN \\\\-based\\end{tabular}} & RSNet  & - & 51.9 & - & 56.5 & - & - & - & - & 84.9 & 39.4 & - \\\\ \\cline{3-14} \n &  & G+RCU  & - & 45.1 & 81.1 & 49.7 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & 3P-RNN  & 85.7 & 53.4 & 86.9 & 56.3 & - & - & - & - & - & - & - \\\\ \\cline{2-14} \n & \\multirow{9}{*}{\\begin{tabular}[c]{@{}c@{}}Graph\\\\-based\\end{tabular}} & DGCNN  & - & - & 84.1 & 56.1 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & SPG  & 86.4 & 58.0 & 85.5 & 62.1 & 92.9 & 76.2 & 94.0 & 73.2 & - & - & 17.4 \\\\ \\cline{3-14} \n &  & SSP+SPG  & 87.9 & 61.7 & 87.9 & 68.4 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & GACNet  & 87.8 & 62.9 & - & - & - & - & 91.9 & 70.8 & - & - & - \\\\ \\cline{3-14} \n &  & PAG  & 86.8 & 59.3 & 88.1 & 65.9 & - & - & - & - & - & - &  \\\\ \\cline{3-14} \n &  & HDGCN  & - & 59.3 & - & 66.9 & - & - & - & - & - & - & - \\\\ \\cline{3-14} \n &  & HPEIN  & 87.2 & 61.9 & 88.2 & 67.8 & - & - & - & - & - & 61.8 & - \\\\ \\cline{3-14} \n &  & SPH3D-GCN  & 87.7 & 59.5 & 88.6 & 68.9 & - & - & - & - & - & 61.0 & - \\\\ \\cline{3-14} \n &  & DPAM  & 86.1 & 60.0 & 87.6 & 64.5 & - & - & - & - & - & - & - \\\\ \\hline\n\\end{tabular}\n}\n\\end{table*}\n\\textbf{Point Convolution Methods.} \nThese methods tend to propose effective convolution operators for point clouds. Hua et al.  proposed a point-wise convolution operator, where the neighboring points are binned into kernel cells and then convolved with kernel weights. As shown in Fig. \\ref{fig:Pointnet++}(b), Wang et al.  proposed a network called PCCN based on parametric continuous convolution layers. The kernel function of this layer is parameterized by MLPs and spans the continuous vector space. \\why{Thomas} et al.  proposed a Kernel Point Fully Convolutional Network (KP-FCNN) based on Kernel Point Convolution (KPConv). Specifically, the convolution weights of KPConv are determined by the Euclidean distances to kernel points, and the number of kernel points is not fixed. The positions of the kernel points are formulated as an optimization problem of best coverage in a sphere space. Note that, the radius neighbourhood is used to keep a consistent receptive field, while grid subsampling is used in each layer to achieve high robustness under varying densities of point clouds. In , \\why{Engelmann} et al. provided rich ablation experiments and visualization results to show the impact of receptive field on the performance of aggregation-based methods. They also proposed a Dilated Point Convolution (DPC) operation to aggregate dilated neighboring features, instead of the $\\textit{K}$ nearest neighbours. This operation is demonstrated to be very effective in increasing the receptive field and can be easily integrated into existing aggregation-based networks. \n\\textbf{RNN-based Methods.} To capture inherent context features from point clouds, Recurrent Neural Networks (RNN) have also been used for semantic segmentation of point clouds. Based on PointNet , \\why{Engelmann} et al.  first transformed a block of points into multi-scale blocks and grid blocks to obtain input-level context. Then, the block-wise features extracted by PointNet are sequentially fed into Consolidation Units (CU) or Recurrent Consolidation Units (RCU) to obtain output-level context. Experimental results show that incorporating spatial context is important for the improvement of the segmentation performance. Huang et al.  proposed a lightweight local dependency modeling module, and utilized a slice pooling layer to convert unordered point feature sets into an ordered sequence of feature vectors. As shown in Fig. \\ref{fig:Pointnet++}(c), Ye et al.  first proposed a Pointwise Pyramid Pooling (3P) module to capture the coarse-to-fine local structure, and then utilized two-direction hierarchical RNNs to further obtain long-range spatial dependencies. RNN is then applied to achieve an end-to-end learning. However, these methods lose rich geometric features and density distribution from point clouds when aggregating the local neighbourhood features with global structure features . To alleviate the problems caused by the rigid and static pooling operations, Zhao et al.  proposed a Dynamic Aggregation Network (DAR-Net) to consider both global scene complexity and local geometric features. The inter-medium features are dynamically aggregated using a self-adapted receptive field and node weights. Liu et al.  proposed 3DCNN-DQN-RNN for efficient semantic parsing of large-scale point clouds. This network first learns the spatial distribution and color features using a 3D CNN network, DQN is further used to localize objects belonging to a specific class. The final concatenated feature vector is fed into a residual RNN to obtain the final segmentation results.  \n\\textbf{Graph-based Methods.} \nTo capture the underlying shapes and geometric structures of 3D point clouds, several methods resort to graph networks. As shown in Fig. \\ref{fig:Pointnet++}(d), \\why{Landrieu} et al.  represented a point cloud as a set of interconnected simple shapes and superpoints, and used an attributed directed graph (i.e., superpoint graph) to capture the structure and context information. Then, the large-scale point cloud segmentation problem is spilt into three sub-problems, i.e., geometrically homogeneous partition, superpoint embedding, and contextual segmentation. To further improve the partition step, \\why{Landrieu and Boussaha}  proposed a supervised framework to oversegment a point cloud into pure superpoints. This problem is formulated as a deep metric learning problem structured by an adjacency graph. In addition, a graph-structured contrastive loss is also proposed to help the recognition of borders between objects. \nTo better capture the local geometric relationships in high-dimensional space, Kang et al.  proposed a PyramNet based on Graph Embedding Module (GEM) and Pyramid Attention Network (PAN). The GEM module formulates a point cloud as a directed acyclic graph and utilzes a covariance matrix to replace the Euclidean distance for the construction of adjacent similarity matrix. Convolution kernels with four different sizes are used in the PAN module to extract features with different semantic intensities. In , Graph Attention Convolution (GAC) is proposed to selectively learn relevant features from a local neighboring set. This operation is achieved by dynamically assigning attention weights to different neighboring points and feature channels based on their spatial positions and feature differences. GAC can learn to capture discriminative features for segmentation, and has similar characteristics to the commonly used CRF model.\n\\Gary{Ma et al.  proposed a Point Global Context Reasoning (PointGCR) module to capture global contextual information along the channel dimension using an undirected graph representation. PointGCR is a plug-and-play and end-to-end trainable module. It can easily be integrated into an existing segmentation network to achieve performance improvement.}\n\\qy{In addition, several very recent work tries to achieve semantic segmentation of point clouds under weak supervision. Wei et al.  proposed a two-stage approach to train a segmentation network with subcloud level labels. Xu et al.  investigated several inexact supervision schemes for  semantic segmentation of point clouds. They also proposed a network that is able to be trained with only partially labeled points (e.g. 10\\%). }", "cites": [7385, 38, 7427, 1527, 7440, 7389, 7431, 270, 7434, 1542, 278, 7375, 7432, 7438, 7437, 7419, 7433, 1541, 7426, 7378, 1540, 7435, 7436, 7376, 1520, 7386, 7439, 7290, 1539, 7429, 7428], "cite_extract_rate": 0.5166666666666667, "origin_cites_number": 60, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers by grouping them into coherent categories (e.g., neighboring feature pooling, attention-based aggregation, local-global concatenation), highlighting shared design principles and innovations. It critically discusses method limitations, such as the inability of shared MLPs to capture local geometry, and introduces comparisons of techniques like ball query vs. K-means clustering. The abstraction level is strong, as it identifies overarching strategies for feature extraction and aggregation in point-based methods."}}
{"id": "9aad4c07-9aad-4558-a28c-b5e780667732", "title": "\\bf{Proposal-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "76fc9b71-036f-4904-86be-b8a384bf63e7", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "Instance Segmentation"], ["subsubsection", "\\bf{Proposal-based Methods"]], "content": "} \nThese methods convert the instance segmentation problem into two sub-tasks: 3D object detection and instance mask prediction. \nHou et al.  proposed a 3D fully-convolutional Semantic Instance Segmentation (3D-SIS) network to achieve semantic instance segmentation on RGB-D scans. This network learns from both color and geometry features. Similar to 3D object detection, a 3D Region Proposal Network (3D-RPN) and a 3D Region of Interesting (3D-RoI) layer are used to predict bounding box locations, object class labels and instance masks. Following the analysis-by-synthesis strategy, Yi et al.  proposed a Generative Shape Proposal Network (GSPN) to generate high-objectness 3D proposals. These proposals are further refined by a Region-based PointNet (R-PointNet). The final label is obtained by predicting a per-point binary mask for each class label. Different from direct regression of 3D bounding boxes from point clouds, this method removes a large amount of meaningless proposals by enforcing geometric understanding. \nBy extending 2D panoptic segmentation to 3D mapping, \\why{Narita} et al.  proposed an online volumetric 3D mapping system to jointly achieve large-scale 3D reconstruction, semantic labeling, and instance segmentation. They first utilized 2D semantic and instance segmentation networks to obtain pixel-wise panoptic labels and then integrated these labels to the volumtric map. A fully-connected CRF is further used to achieve accurate segmentation. This semantic mapping system can achieve high-quality semantic mapping and discriminative object recognition. Yang et al.  proposed a single-stage, anchor-free and end-to-end trainable network called 3D-BoNet to achieve instance segmentation on point clouds. This method directly regresses rough 3D bounding boxes for all potential instances, and then utilizes a point-level binary classifier to obtain instance labels. Particularly, the bounding box generation task is formulated as an optimal assignment problem. In addition, a multi-criteria loss function is also proposed to regularize the generated bounding boxes. This method does not need any post-processing and is computationally efficient. Zhang et al.  proposed a network for instance segmentation of large-scale outdoor LiDAR point clouds. This method learns a feature representation on the bird's-eye view of point clouds using self-attention blocks. The final instance labels are obtained based on the predicted horizontal center and the height limits. Shi et al.  proposed a hierarchy-aware Variational Denoising Recursive AutoEncoder (VDRAE) to predict the layout of indoor 3D space. The object proposals are iteratively generated and refined by recursive context aggregation and propagation.\nOverall, proposal-based methods  are intuitive and straightforward, and the instance segmentation results usually have good objectness. However, these methods require multi-stage training and pruning of redundant proposals. Therefore, they are usually time-consuming and computationally expensive.", "cites": [7441, 7443, 7442, 1543], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information by grouping the cited papers under the common theme of proposal-based methods and explaining their shared two-stage structure. It provides a critical perspective by highlighting the computational inefficiency and the need for multi-stage training. While it captures some patterns (e.g., the use of 3D-RPN and refinement strategies), it stops short of proposing a meta-level framework or identifying overarching principles that unify the methods."}}
{"id": "255d07fb-7b15-424f-8710-a67b019f8cb0", "title": "\\bf{Proposal-free Methods", "level": "subsubsection", "subsections": [], "parent_id": "76fc9b71-036f-4904-86be-b8a384bf63e7", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "Instance Segmentation"], ["subsubsection", "\\bf{Proposal-free Methods"]], "content": "}  \nProposal-free methods  do not have an object detection module. Instead, they usually consider instance segmentation as a subsequent clustering step after semantic segmentation. In particular, most existing methods are based on the assumption that points belonging to the same instance should have very similar features. Therefore, these methods mainly focus on discriminative feature learning and point grouping.\nIn a pioneering work, Wang et al.  first introduced a Similarity Group Proposal Network (SGPN). This method first learns a feature and semantic map for each point, and then introduces a similarity matrix to represent the similarity between each paired features. To learn more discriminative features, they use a double-hinge loss to mutually adjust the similarity matrix and semantic segmentation results. Finally, a heuristic and non-maximal suppression method is adopted to merge similar points into instances. \nSince the construction of a similarity matrix requires large memory consumption, the scalability of this method is limited.\nSimilarly, Liu et al.  first leveraged submanifold sparse convolution  to predict semantic scores of each voxel and affinity between neighboring voxels. They then introduced a clustering algorithm to group points into instances based on the predicted affinity and the mesh topology. Mo et al.  introduced a detection-by-segmentation network in PartNet to achieve instance segmentation. PointNet++ is used as the backbone to predict semantic labels of each point and disjoint instance masks.\nFurther, Liang et al.  proposed a structure-aware loss for the learning of discriminative embeddings. This loss considers both the similarity of features and the geometric relations among points. An attention-based graph CNN is further used to adaptively refine the learned features by aggregating different information from neighbors. \nSince the semantic category and instance label of a point are usually dependent on each other, several methods have been proposed to couple these two tasks into a single task.\nWang et al.  integrated these two tasks by introducing an end-to-end and learnable Associatively Segmenting Instances and Semantics (ASIS) module. Experiments show that semantic features and instance features can mutually support each other to achieve an improved performance through this ASIS module. Similarly, Zhao et al.  proposed JSNet to achieve both semantic and instance segmentation. Further, Pham et al.  first introduced a Multi-Task Point-wise Network (MT-PNet) to assign a label to each point and regularized the embeddings in the feature space by introducing a discriminative loss . They then fused the predicted semantic labels and embeddings to a Multi-Value Conditional Random Field (MV-CRF) model for joint optimization. Finally, mean-field variational inference is used to produce semantic labels and instance labels. Hu et al.  first proposed a Dynamic Region Growing (DRG) method to dynamically separate a point cloud into a set of disjoint patches, and then used an unsupervised K-means++ algorithm to group all these patches. Multi-scale patch segmentation is then performed with the guidance of contextual information between patches. Finally, these labeled patches are merged into object level to obtain final semantic and instance labels.\nTo achieve instance segmentation on full 3D scenes,  \\why{Elich} et al.  presented a hybrid 2D-3D network to jointly learn global consistent instance features from a BEV representation and local geometric features of point clouds. The learned features are then combined to achieve semantic and instance segmentation. Note that, rather than heuristic \\textit{GroupMerging} algorithms , a more flexible Meanshift  algorithm is used to group these points into instances.\nAlternatively, multi-task learning is also introduced for instance segmentation. \\why{Lahoud} et al.  learned both the unique feature embedding of each instance and the directional information to estimate the object's center. Feature embedding loss and directional loss are proposed to adjust the learned feature embeddings in latent feature space. Mean-shift clustering and non-maximum suppression are adopted to group voxels into instances. This method achieves the state-of-the-art performance on the ScanNet  benchmark. Besides, the predicted directional information is particularly useful to determine the boundary of instances. Zhang et al.  introduced probabilistic embeddings to instance segmentation of point clouds. This method also incorporates uncertainty estimation and proposes a new loss function for the clustering step. \\qy{Jiang et al.  proposed a PointGroup network, which is composed of a semantic segmentation branch and an offset prediction branch. A  dual-set clustering algorithm and the ScoreNet is further utilized to achieve better grouping results.}\nIn summary, proposal-free methods do not require computationally expensive region-proposal components. However, the objectness of instance segments grouped by these methods is usually low since these methods do not explicitly detect object boundaries.", "cites": [9114, 7444, 7445, 1545, 1546, 7447, 7446, 7376, 1544], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple proposal-free methods, connecting common themes such as discriminative feature learning and clustering-based instance formation. It provides a critical perspective by pointing out limitations like high memory consumption and low objectness in grouping. The abstraction is strong, identifying broader patterns like the coupling of semantic and instance segmentation tasks, and the use of clustering algorithms as a unifying theme across different approaches."}}
{"id": "7fe39e58-d8c4-42e8-91a5-b8b2c9e736e8", "title": "Part Segmentation", "level": "subsection", "subsections": [], "parent_id": "fc9d3465-3d91-42d6-8707-1bc7429ba4c0", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "Part Segmentation"]], "content": "The difficulties for part segmentation of 3D shapes are twofold. First, shape parts with the same semantic label have a large geometric variation and ambiguity. Second, the number of parts in objects with the same semantic meanings may be different.\nVoxSegNet  is proposed to achieve fine-grained part segmentation on 3D voxelized data under a limited solution. A Spatial Dense Extraction (SDE) module (which consists of stacked atrous residual blocks) is proposed to extract multi-scale discriminative features from sparse volumetric data. The learned features are further re-weighted and fused by progressively applying an Attention Feature Aggregation (AFA) module. \\why{Kalogerakis} et al.  combined FCNs and surface-based CRFs to achieve end-to-end 3D part segmentation. They first generated images from multiple views to achieve optimal surface coverage and fed these images into a 2D network to produce confidence maps. Then, these confidence maps are aggregated by a surface-based CRF, which is responsible for a consistent labeling of the entire scene. \nYi et al.  introduced a Synchronized Spectral CNN (SyncSpecCNN) to perform convolution on irregular and non-isomorphic shape graphs. A spectral parameterization of dilated convolutional kernels and a spectral transformer network is introduced to solve the problem of multi-scale analysis in parts and information sharing across shapes. \nWang et al.  first performed shape segmentation on 3D meshes by introducing Shape Fully Convolutional Networks (SFCN) and taking three low-level geometric features as its input. They then utilized voting-based multi-label graph cuts to further refine the segmentation results. Zhu et al.  proposed a weakly-supervised CoSegNet for 3D shape co-segmentation. This network takes a collection of unsegmented 3D point cloud shapes as input, and produces shape part labels by iteratively minimizing a group consistency loss. Similar to CRF, a pre-trained part-refinement network is proposed to further refine and denoise part proposals. Chen et al.  proposed a Branched AutoEncoder network (BAE-NET) for unsupervised, one-shot and weakly supervised 3D shape co-segmentation. This method formulates the shape co-segmentation task as a representation learning problem and aims at finding the simplest part representations by minimizing the shape reconstruction loss. Based on the encoder-decoder architecture, each branch of this network can learn a compact representation for a specific part shape. The features learned from each branch and the point coordinate are then fed to the decoder to produce a binary value (which indicates whether the point belongs to this part). This method has good generalization ability and can process large 3D shape collections (up to 5000+ shapes). However, it is sensitive to initial parameters and does not incorporate shape semantics into the network, which hinders this method to obtain a robust and stable estimation in each iteration. Yu et al.  proposed a top-down recursive part decomposition network (PartNet) for hierarchical shape segmentation. Different from existing methods that segment a shape to a fixed label set, this network formulates part segmentation as a problem of cascade binary labeling, and decompose the input point cloud   to an arbitrary number of parts based on the geometric structure. Luo et al.  introduced a learning-based grouping framework for the task of zero-shot 3D part segmentation. To improve the cross-category generalization ability, this method tends to learn a grouping policy that restricts the network to learn part-level features within the part local context.", "cites": [7449, 7448, 283], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of part segmentation methods by describing their key components and approaches. It integrates several works by highlighting their technical contributions, such as network architectures and segmentation strategies, but lacks a deeper synthesis into a unifying framework. Some limitations are mentioned, such as sensitivity to parameters and lack of semantic integration in BAE-NET, showing basic critical analysis. The section identifies patterns like the use of graph structures and attention mechanisms but does not abstract them into broader theoretical principles."}}
{"id": "dc382f9b-2eca-4763-9871-ccb879df59f6", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "fc9d3465-3d91-42d6-8707-1bc7429ba4c0", "prefix_titles": [["title", "Deep Learning for 3D Point Clouds: A Survey"], ["section", "3D Point Cloud Segmentation"], ["subsection", "Summary"]], "content": "Table \\ref{tab:segmentation_results} shows the results achieved by existing methods on public benchmark, including S3DIS , Semantic3D , ScanNet , and SemanticKITTI . The following issues need to be further investigated:\n\\begin{enumerate}\n    \\item[$\\bullet$] \\qy{Thanks to the regular data representation, both  projection-based methods and discretization-based methods can leverage the mature network architecture from their 2D image counterparts. However, the main limitation of projection-based methods lies in the information loss caused by 3D-2D projection, while the main bottleneck for discretization-based methods is the cubically increased computational and memory costs caused by the increase of the resolution. To this end, sparse convolution building upon indexing structures would be a feasible solution and worth further exploration.}\n    \\item[$\\bullet$] \\qy{Point-based networks are the most frequently investigated methods. However, point representation naturally does not have explicit neighboring information, most existing point-based methods resort to expensive neighbor searching mechanisms  (e.g., KNN  or ball query ). This inherently limits the efficiency of these methods, the recently proposed point-voxel joint representation  would be an interesting direction for further investigation.}\n   \\item[$\\bullet$]  Learning from imbalanced data is still a challenging problem in point cloud segmentation. Although several approaches  have achieved a remarkable overall performance, their performance on minority classes is still limited. For example, RandLA-Net  achieves an overall IoU of 76.0\\% on the \\textit{reduced-8} subset of Semantic3D, but a very low IOU of 41.1\\% on the class of \\textit{hardscape}.\n   \\item[$\\bullet$] The majority of existing approaches  work on small point clouds (e.g., 1m$\\times$1m with 4096 points). In practice, the point clouds acquired by depth sensors are usually immense and large-scale. Therefore, it is desirable to further investigate the problem of efficient segmentation of large-scale point clouds.\n   \\item[$\\bullet$] A handful of works  have started to learn spatio-temporal information from dynamic point clouds. It is expected that the spatio-temporal information can help to improve the performance of subsequent tasks such as 3D object recognition, segmentation, and completion. \n\\end{enumerate}", "cites": [7424, 7433, 7290, 7385, 7378, 7450, 270, 7425, 7436, 7375], "cite_extract_rate": 0.625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 4.3}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple cited papers to identify overarching challenges and potential solutions in 3D point cloud segmentation. It critically evaluates limitations of projection-based, discretization-based, and point-based methods, and highlights promising research directions such as sparse convolutions and point-voxel joint representations. The abstraction level is strong, as the section generalizes patterns in data representation, computational efficiency, and class imbalance across the field."}}
