{"id": "36ca0416-4ac1-46dd-b029-a454f0bd52c2", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "9cc41caf-627e-4604-8046-5481f76b9b4f", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Introduction"]], "content": "\\label{sect:introduction}\n  Time series are vital in real-world applications. However, due to unexpected accidents, for example broken sensors or missing of the signals, \n  missing values are everywhere in time series. In some datasets, the missing rate can reach 90\\%, which makes the data hard to be utilized . \n  The missing values significantly do harm to the downstream applications such as traditional classification or regression, sequential data integration \n  and forecasting tasks , leading to high demand for data imputation. \n  Our preliminary study  shows that imputing the missing values indeed helps significantly the prediction of fuel consumption.\n  In the scenarios of fuel consumption prediction, missing values happen due to the errors of sensors. \n  We propose an imputation approach named FuelNet to deal with such errors. The FuelNet generates proper values to impute missing data. With imputed data, the \n  fuel consumption can be reduced by around 45.5\\%.\n  In current stages, time series data imputation is a well studied problem with different categories of methods including \n  deletion methods, simple imputation methods and learning based methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, thus losing \n  the information from the time data.\n  Fortunately, with the increasing development of deep learning, a large quantity of deep learning methods are researched, among which RNN is one of the typical methods to handle sequence data. \n  The intuition on why deep learning models could advance imputation tasks is that, they are proven to have the ability to mine information hidden in the time series. These characteristics could enable them \n  to impute missing values with such models. \n  Recently, deep learning methods have been applied to multivariable time series imputation and show positive progress in imputing \n  the missing values. In this paper, we mainly survey three papers about time series imputation with deep learning methods \n  among which RNN, GRU and GAN are adopted separately or in combination. We will review these papers about their model structure, the common parts they all adopted and the advantages and disadvantages through comparison.\n  The remainder of the paper is organized as follows. In the next section, we categorize existing data imputation methods and mainly give an introduction to deep learning imputation methods.\n  Section 3 will show the definition of the problems and the symbols. Section 4 will give a detailed discussion of deep learning methods, mainly about their concrete structure, advantages and disadvantages. \n  And finally in Section 5 we summarize the survey and give our conclusions.\n\\begin{table*}[t]\n  \\centering\n  \\caption{Comparison of different methods addressing time series imputation}\n  \\label{tab:comparison-of-methods}\n  \\resizebox{\\textwidth}{50mm}{ \n  \\begin{tabular}{|l|l|l|l|l|}\n    \\hline\n    Methodologies                     & Sample approaches from the literature & Time interval    & Value type               & Time series dimension \\\\ \\hline\n    \\multirow{2}{*}{Deletion}         & Listwise Deletion                      & regular/irregular  & qualitative & multidimensional        \\\\ \\cline{2-5} \n                                      & Pairwise Deletion                      & regular/irregular  & qualitative & multidimensional                      \\\\ \\hline\n    \\multirow{2}{*}{Neighbor Based} & QDORC                                         & regular/irregular                & quantitative/qualitative &   multidimensional                     \\\\ \\cline{2-5} \n&  SRKN  & regular/irregular & quantitative/qualitative &   multidimensional \\\\ \\hline \n    \\multirow{2}{*}{Constraint Based} & DERAND          &  regular/irregular                                      & quantitative/qualitative &  multidimensional                      \\\\ \n\\cline{2-5} & SCREEN  & regular/irregular & qualitative & single dimensional \\\\ \\hline\n    \\multirow{2}{*}{Regression Based} \n                                      & ARX                                    &   regular                        &  qualitative            & single dimensional                      \\\\ \\cline{2-5} \n                                      & IMR                                    &  regular               &   qualitative                       & single dimensional                      \\\\ \\hline\n    \\multirow{2}{*}{Statistical}      & DPC                                    &  regular               &  qualitative                        &  single dimensional                     \\\\ \\cline{2-5} \n                                      & IIM                                    &  regular                 &  qualitative                        &  multidimensional                       \\\\ \\hline\n    \\multirow{2}{*}{MF Based}         & TRMF                                   & regular                 & qualitative                         &  multidimensional                     \\\\ \\cline{2-5} \n                                      & NMF                                    & regular                & qualitative                          & multidimensional                      \\\\ \\hline\n    \\multirow{2}{*}{EM Based}         & EM                                     &  regular              & qualitative                         & multidimensional                      \\\\ \\cline{2-5}\n                                      & EM-GMM                                     &  regular              & qualitative                         & multidimensional                      \\\\ \\hline\n    \\multirow{2}{*}{MLP Based}        & MLP                                     &  regular              & qualitative                         & single dimensional                     \\\\ \\cline{2-5} \n                                      & ANN                                     &  regular              & qualitative                         & single dimensional                     \\\\ \\hline \n    \\multirow{5}{*}{DL Based}         & GRU-D                                  &   regular/irregular              &     qualitative                     & multidimensional                      \\\\ \\cline{2-5} \n                                      & GRUI-GAN                               & regular/irregular                 &  qualitative                        & multidimensional                       \\\\ \\cline{2-5} \n                                      & BRITS                                  & regular/irregular               &    qualitative                      & multidimensional                      \\\\ \\cline{2-5} \n                                      & E2GAN                                  & regular/irregular               &  qualitative                         & multidimensional                       \\\\ \\cline{2-5} \n                                      & NAOMI                                  & regular/irregular                  &   qualitative                       & multidimensional                       \\\\ \\hline\n    \\end{tabular}\n  }\n\\end{table*}", "cites": [6177, 8109, 1005], "cite_extract_rate": 0.11538461538461539, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a basic overview of time series imputation and introduces some deep learning-based methods, but it lacks meaningful synthesis of the cited papers. It merely lists methods and mentions their adoption of RNN, GRU, and GAN without connecting underlying ideas or themes across the works. There is minimal critical analysis or abstraction beyond the specific models named."}}
{"id": "e9bf6801-c8c7-4824-b43e-886c2740da23", "title": "Categorization", "level": "section", "subsections": [], "parent_id": "9cc41caf-627e-4604-8046-5481f76b9b4f", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Categorization"]], "content": "\\label{sect:classification}\nIn this section, we will give a brief introduction of the major approaches to time series imputation. Moreover, we will classify existing time series imputation methods according to the \nprinciples and techniques they rely on. \nIn order to impute the missing values, researchers have proposed many imputation methods to handle the missing values in time series. In this paper,\nwe mainly conclude 8 kinds of the missing value imputation methods including \\textbf{deletion methods}, \\textbf{neighbor based methods}, \\textbf{constraint based methods},\n\\textbf{regression based methods}, \\textbf{statistical based methods}, \\textbf{MF based methods}, \\textbf{EM based mathods}, \\textbf{MLP based mathods} and \\textbf{DL based methods}.\nTable~\\ref{tab:comparison-of-methods} shows the comparison of these methods we conclude. We will introduce each kind of method respectively as follows.\n\\textbf{Deletion methods} take a simple strategy that they directly erase the observations that contain missing values from the raw data . \nIt is also a commonly adopted strategy when the missing value is not \nhigh and the deletion of the missing values will not influence the downstream applications. However, when the missing rate reaches some level (in , it is 5\\%), \nignoring the missing values and deleting them make the data incomplete and not suitable for downstream applications. \n\\textbf{Neighbor based methods}  find out the imputation value from neighbors, e.g., identified by clustering methods like KNN or DBSCAN.\nThey first find the nearest neighbors of the missing values through other attributes, and then update the missing values with the mean value of these neighbors. \nMoreover, considering the local similarity, some methods take the last observed valid value to replace \nthe blank . \nSRKN (Swapping Repair with K Neighbors)  in our preliminary study could also be adapted to impute the missing values that are misplaced in other dimensions. \n\\textbf{Constraint based methods}  discover the rules in dataset, and take advantage of these rules to impute. \nTo apply to time series data, similarity rules such as differential dependencies  or comparable dependencies  could be employed that study the distances or similarities of timestamps as well as values . \nMore advanced constraints could be specified in a graph structure , such as Petri net, and employed to impute the qualitative values of events in time series .\nThese methods are effective when the data is highly continuous or satisfies certain patterns. For example, when the data is increasing linearly, it is effective and efficient\nto take simple methods or clustering methods. And when the rules or constraints are satisfied, constraints based methods outperform others in both time and accuracy .\nHowever, multivariable time series in the real world are not usually satisfied with such rules, thus more general methods are required and learning based methods are \nresearched to impute the time series automatically.\n\\textbf{Regression based methods} \nLOESS~ learns a regression model from nearest neighbors for predicting the missing value referring to the complete attributes.\nFor time series data, autoregressive (AR) models (e.g., ARX  and ARIMA ) try to predict missing values from historical data.\nMore advanced IMR (iterative minimum repairing ) provides both anomaly detection and data repair for both anomalies and missing values.\nThese methods mostly benefit from historical data as well as the accuracy of the nearest neighbors. Thus they could be applied when neighbors are reliable and the time series are highly relative.\n\\textbf{Statistical based methods} rely on statistical models to impute the missing values . \nSimple statistical methods just utilize the data in the original data to impute the missing values, such as take the mean value or median value of the \nattribute to impute . \n estimates probability values by statistics on speeds as well as the changes. \nRecently, more advanced IIM (Imputation via Individual models)  adaptively learns individual models for various number of neighbors.\nUnlike regression based methods which based on just historical data, statistical based models are learned from the whole dataset, including historical data and future data. \nTherefore, they may capture more information from raw data. \n\\textbf{Matrix Factorization based methods}\nThe Matrix Factorization (MF) algorithm tries to impute the value with the Matrix Factorization and reconstruction to find the correlations among the data and complete the missing values which is a classical method of \ncollaborative filtering . In recent years MF based approaches are introduced into time series imputation fields . \nIn general, MF based approaches decompose the data matrix into 2 low-dimensional matrices in the meantime extracting the features from original data. And then they try to reconstruct the original matrix and \nin this processing, missing values are imputed. \n\\textbf{Expectation-Maximization based methods}\nExpectation-Maximization (EM) based methods have been successfully applied to missing data imputation problems . EM based methods follow a two-stage strategy consisting of the E (Expectation) step and the \nM (Maximization) step which iteratively imputes the missing values with the statistical model parameters\nand then updates the statistical model parameters to maximize the possibility of the distribution of the filled data.\n\\textbf{Multi-Layer Perceptron based methods}\nMulti-Layer Perceptron (MLP) based methods employee MLP, which is also called fully connected networks. MLP tries to predict missing value by complete values. It can be divided into 3 parts: input layers, hidden layers and output layers.\nIn this approach, by minimizing the loss function, the perceptron learns a function to impute missing values by input variables. In , MLP is used to predict missing values in neural network-based diagnostic systems.\nAnd in , MLP is employed to impute Population Census.\nRecently, \\textbf{deep learning based methods}  \nmainly deploy Recurrent Neural Network (RNN), since RNN is capable of capturing the time information. In these papers, time information is handled separately and \nattached with more importance. To impute the time series, not only RNN is used, they also combine the models like Gated Recurrent Unit (GRU)\n \nto extract the long-term information, Generative Adversarial Networks (GAN)  to generate the imputed values \nand Bidirectional Recurrent Networks to improve the accuracy . \nAccording to the above classification, due to the length, the methods for time series imputation are too many to give a detailed introduction. \nSince among these methods, deep learning based ones are the latest and most powerful, we will discuss 3 latest deep learning methods \nfor time series imputation, find the connections and the differences among them.", "cites": [8109, 6177], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 38, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various time series imputation methods, including a classification into eight categories. While it references some papers and methods, it does so in a fragmented way without connecting the ideas cohesively. Limited critical evaluation is present, such as noting limitations of deletion methods at higher missing rates, but deeper analysis of strengths and weaknesses is missing. The section begins to generalize by grouping methods into categories, but the insights remain at a surface level without meta-level abstraction or synthesis across multiple sources."}}
{"id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "title": "Methods", "level": "section", "subsections": ["f5d95b6b-679b-44e5-9875-4783a472591c", "8dd26940-3b0d-46d3-894c-8b1610037640", "30efe478-aa4f-4ef2-b139-81afd7f7edbb", "7cb484d6-108e-4d13-828a-2423fa3cb1a0", "5dacb75b-98d3-41a7-8ff3-e151c45cb104", "5dc3a0b9-2bd3-472b-b459-73ad6f7d4982"], "parent_id": "9cc41caf-627e-4604-8046-5481f76b9b4f", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"]], "content": "In this section, we will first give an overall review of the relationships among the given approaches and comparisons of them and then discuss them individually with details.\nThe main deep learning methods we researched for time series imputation are GRU-D , GRUI-GAN ,  E$^2$GAN , \nBRITS  and NAOMI . All of them are deep learning approaches published recently for time series imputation tasks.\nAmong these methods, recurrent neural network (RNN) and generative adversarial network (GAN) are main architectures that are adopted. \nThe reason is that RNN and its variations (e.g., LSTM, GRU) have been proven powerful in modeling sequence data, while GAN has been successfully applied to generation and imputation tasks.\nTo describe the relationships among these methods, we illustrate the dependencies and common structures of them in Figure~\\ref{fig:relationships}.\nIn Figure~\\ref{fig:relationships}, we use arrows to describe the dependencies, for example GRUI-GAN improves the work by using GAN while \nE$^2$GAN is the updated version of GRUI-GAN. And we use boxes to describe the common structures among the methods, for example GRU-D and BRITS are both pure RNN models and \nBRITS and NAOMI both adopt bidirectional RNN structures. This can help us to understand how the time series imputation task is systematically modeled, how the solutions are developed and what progress people \nmake in this process. In the following sections,  we will take a progressive order to review them.\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.7\\expwidths]{fig-relationships}\\\\\n  \\caption{The relationships among methods we mainly surveyed.}\n  \\label{fig:relationships}\n\\end{figure}    \n\\begin{table*}[t]\n  \\centering\n  \\caption{Characteristics of the chosen methods}\n  \\label{tab:characteristics}\n    \\begin{tabular}{|l|l|l|l|l|l|}\n    \\hline\n    Methodologies & Model Prototype & Specific Models & \\begin{tabular}[c]{@{}l@{}}Auto-Encoder \\\\ Enhanced\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Adversarial Training \\\\ Enhancednced\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Bidirectional \\\\ Enhanced\\end{tabular} \\\\ \\hline\n    GRU-D         & RNN             & GRU                                                        & --                                                               & --                                                                           & --                                                                \\\\ \\hline\n    GRUI-GAN      & Hybrid          & GRU+GAN                                                    & --                                                               & yes                                                                          & --                                                                \\\\ \\hline\n    E2GAN         & Hybrid          & GRU+GAN                                                    & yes                                                              & yes                                                                          & --                                                                \\\\ \\hline\n    BRITS         & RNN             & Bidirectional RNN                                          & --                                                               & --                                                                           & yes                                                               \\\\ \\hline\n    NAOMI         & Hybrid          & RNN+GAN                                                    & --                                                               & yes                                                                          & yes                                                               \\\\ \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8109, 1005, 6177], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by grouping them into common architectures and identifying structural similarities (e.g., bidirectional RNNs). However, it lacks deeper critical evaluation or analysis of the methods' strengths and weaknesses. The abstraction is limited to categorizing the models based on their components and structures without broader conceptual or theoretical generalization."}}
{"id": "f5d95b6b-679b-44e5-9875-4783a472591c", "title": "Characteristics of Chosen Methods", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "Characteristics of Chosen Methods"]], "content": "\\label{sect:characteristics}\nIn this section, we give the characteristics of the chosen methods in Table~\\ref{tab:characteristics} to give a brief introduction and \na taxonomy of the chosen methods we reviewed. We consider the following criteria:\n\\begin{itemize}\n  \\item \\emph{Irregular Time Series Awareness}: time series including regular time series with fixed time interval and irregular time series. Both of them are common kinds  \n  which are important for classifying the using condition of the methods~. \n  \\item \\emph{Model Prototype}: model prototype concludes the overall kind of model in the methods, e.g., RNN, GAN and CNN. It is a basic information to classify the model type. If the \n  model prototype is hybrid, it means more than 1 kind of prototype is employed.\n  \\item \\emph{Specific Models}: specific models introduce the specific kinds of model adopted in the methods. The specific models may relate to the basic idea of the methods. \n  \\item \\emph{Auto-Encoder Enhanced}: auto-encoder structure is an approach that can be applied in the imputation of the data. With the structure of encoder and decoder, \n  it extracts the features from low-dimensional layers and recovery missing values by decoder. Therefore, it can serve as a feature of methods.\n  \\item \\emph{Adversarial Training Enhanced}: adversarial training adopts adversarial structure (e.g., GAN  and CGAN )\n  to enhance the model. It takes the idea of generative adversarial structure with generator and discriminator. Large amount of models can be enhanced with such idea.\n  \\item \\emph{Bidirectional Enhanced}:  Bidirectional RNN trains 2 models in forward direction and backward direction respectively with RNN and then combines them into the same loss function .\n  This idea is vital in data imputation tasks since both previous series and future series of missing values are known. Therefore, bidirectional structure benefits from both backward and forward training processing. Such idea is \n  adopted in .\n\\end{itemize}", "cites": [529, 8109, 1005], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the characteristics of deep learning methods for time series imputation, organizing them into categories like model prototype and training strategy. While it references relevant papers (e.g., BRITS for bidirectional RNNs, NAOMI for non-autoregressive imputation), it does not deeply synthesize or connect their ideas. The analysis remains shallow, with no substantial critique or comparison of the methods' strengths and weaknesses."}}
{"id": "8dd26940-3b0d-46d3-894c-8b1610037640", "title": "GRU-D", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "GRU-D"]], "content": "\\label{subsect:GRUD}\nGRU-D is proposed by  as one of the early attempts to impute time series with deep learning models.\nIt is the first one among the 5 researched paper to systematically model missing patterns into RNN for time series \nclassification problems. It is also the first research to exploit that, RNN can model multivariable time series with the informativeness\nfrom the time series. \nFormer works like  attempted to impute missing values with RNN by concatenating timestamps and raw data, i.e., regard \ntimestamps as one attribute of raw data. But in , the concept \\textbf{time lag} is first proposed. \nIn this paper, Gated Recurrent Unit (GRU) is first adopted to generate missing values. In each layer of GRU, since the input can contain missing values, \nthey replace the input $x_{t_i}^j$ with a combination of the existing values $x_{t_i}^j$ and statistical values, element-wise multiplied with $\\mathbf{M}$ \nand $\\mathbf{1}-\\mathbf{M}$ respectively.\n$$\nx_{t_i}^{j} \\leftarrow m_{t_i}^{j} x_{t_i}^{j}+\\left(1-m_{t_i}^{j}\\right) \\tilde{x}^{j}\n$$\nwhere $\\tilde{x}$ can be one of the mean value, last observed value or concatenation of $\\left[\\mathbf{x_i}; \\mathbf{m_i};\\delta_i\\right]$. \nThe main contribution of this paper is the GRU based model GRU-D and the proposition of \\textbf{decay rate}.\nTo address the imputation of the missing values, they discover that \n\\begin{itemize}\n  \\item The missing variables tend to be close to some default value if its last observation happens a long time ago. \n  \\item The influence of the input variables will fade away over time if the variable has been missing for a while.\n  \\end{itemize}\nAnd then they propose \\textbf{decay rate} $\\gamma$, which is defined as below \n$$\n\\gamma_{t_i} = \\exp({-\\max{(\\mathbf{0},\\mathbf{W}_{\\gamma}\\mathbf{\\delta_{t_i}})}})\n$$\nThe decay rate tries to model the impact of the other values have on the missing values. In brief, it guarantees that the larger the time intervals are, \nthe less their influence on imputing the missing values. And then they replace the input variable as  \n$$\nx_{t_i}^{j} \\leftarrow m_{t_i}^{j} x_{t_j}^{j}+\\left(1-m_{t_i}^{j}\\right) \\gamma_{\\boldsymbol{x}_{t_i}}^{j} x_{t_i^{\\prime}}^{j}+\\left(1-m_{t_i}^{j}\\right)\\left(1-\\gamma_{\\boldsymbol{x}_{t_i}}^{j}\\right) \\tilde{x}^{j}\n$$ \nTherefore, as illustrated in Figure~\\ref{fig:GRUD}, the GRU-D model is proposed with 2 different trainable decays $\\gamma_{\\boldsymbol{x}}$ and $\\gamma_{\\boldsymbol{h}}$, where \n$\\gamma_{\\boldsymbol{x}}$ is the input decay rate and the $\\gamma_{\\boldsymbol{h}}$ is the decay rate for the hidden state.\n\\begin{figure}[t]\n  \\subfigure[GRU]{\n    \\label{fig:GRUD1}\n    \\includegraphics[width=0.45\\figwidths]{./fig-GRUD-1}\n  }\n  \\subfigure[GRU-D]{\n    \\label{fig:GRUD2}\n    \\includegraphics[width=0.45\\figwidths]{./fig-GRUD-2}\n  }\n  \\caption{Model of GRU and GRU-D. Images extracted from .}\n  \\label{fig:GRUD}\n\\end{figure}", "cites": [6178, 6177], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section insightfully synthesizes the core ideas of GRU-D by integrating concepts from the cited papers, particularly highlighting the introduction of time lag and decay rates as novel contributions. It critically evaluates prior approaches by pointing out their limitations, such as treating timestamps as mere attributes. While it identifies some broader principles (e.g., the fading influence of missing data over time), the abstraction remains somewhat constrained to the GRU-D model itself without fully placing it in a larger methodological context."}}
{"id": "30efe478-aa4f-4ef2-b139-81afd7f7edbb", "title": "GRUI-GAN", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "GRUI-GAN"]], "content": "\\label{subsect:GRUI}\nIn , GRU-I is proposed as the recurrent unit to capture the time information. As Figure~\\ref{fig:GRUI} illustrates, \nit follows the structure of GRU-D in Section~\\ref{subsect:GRUD} with the removal of the input decay. Therefore, there is no innovation in the \nRNN part as well as the decay rate.\nThe main contribution of this paper locates in the GAN structure. Figure~\\ref{fig:GRUI-GAN} shows the structure. \nThe Generative Adversarial Network (GAN) structure is made up of a generator (G) and a discriminator (D). \nThe G learns a mapping $G(z)$ that tries to map the random noise vector $z$ to realistic time series. \nThe D tries to find a mapping $D(.)$ that tells us the input data's probability of being real.\nTherefore, in this paper, the model takes a random noise as the input of the GAN model, which means the generating is a \nrandom process. Both G and D are based on GRU-I, and it takes lots of time to train the model to get the data imputed.\nThe GRUI-GAN takes advantage of the ability of GAN in imputation, which has been proven powerful in image imputation such as .\nAnd the adversarial structure improves accuracy. Moreover, the paper adopts a WGAN structure, which improves the stability of the \nlearning stage, get out of the problem of mode collapse and makes it easy for the optimization of the GAN model.\nHowever, this model is not practical since the accuracy of the generative model seems not stable with a random noise input. And it also \nmakes the model hard to converge.\n\\begin{figure}[t]\n  \\subfigure[GRU]{\n    \\label{fig:GRUI1}\n    \\includegraphics[width=0.45\\figwidths]{./fig-GRUI-1}\n  }\n  \\subfigure[GRU-I]{\n    \\label{fig:GRUI2}\n    \\includegraphics[width=0.45\\figwidths]{./fig-GRUI-2}\n  }\n  \\caption{Model of GRU and GRU-I. Images extracted from .}\n  \\label{fig:GRUI}\n\\end{figure} \n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=1.2\\expwidths]{fig-GRUI-GAN}\\\\\n  \\caption{The structure of the GRUI-GAN. Image extracted from .}\n  \\label{fig:GRUI-GAN}\n\\end{figure*}    \n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\expwidths]{fig-BRIT}\\\\\n  \\caption{The structure of the BRITS. Image extracted from .}\n  \\label{fig:BRITS}\n\\end{figure}", "cites": [8109, 1254], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of the GRUI-GAN model, explaining its generator and discriminator components and referencing related work, such as image inpainting with GANs. It makes minimal connections between GRUI-GAN and other methods like BRITS, and only superficially notes issues such as instability and convergence problems. While some critical evaluation is present, the analysis lacks depth and the section does not abstract broader patterns or insights from the cited works."}}
{"id": "7cb484d6-108e-4d13-828a-2423fa3cb1a0", "title": "BRITS", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "BRITS"]], "content": "\\label{sect:BRITS}\nUnlike former methods, BRITS  is totally based on RNN structure and proposes imputation with unidirectional dynamics. \nTime lag (corresponding to \"time gaps\" in ) is also employed since the time series may be irregular.\nSimilar to the idea of decay rate $\\gamma$ from GRU-D introduced in Section~\\ref{subsect:GRUD}, they propose \\textbf{temporal decay factor} \n$\\gamma_t = \\exp{(-max\\left(0,\\mathbf{W}_{\\gamma}\\delta_t + \\mathbf{b}_{\\gamma}\\right))}$. Compared to GRU-D where the time lags are considered in input and serve as \nthe decay rate, in BRITS the hidden states update with the decay rate $\\gamma$.\nIt means when updating the hidden state, the old hidden state decays according to the time duration recorded in the time lags. Hence, the model is updated by:\n\\begin{equation}\n\\begin{aligned} \n\\hat{\\mathbf{x}}_{t} &=\\mathbf{W}_{x} \\mathbf{h}_{t-1}+\\mathbf{b}_{x} \\\\ \n\\mathbf{x}_{t}^{c} &=\\mathbf{m}_{t} \\odot \\mathbf{x}_{t}+\\left(1-\\mathbf{m}_{t}\\right) \\odot \\hat{\\mathbf{x}}_{t} \\\\\n \\gamma_{t} &=\\exp \\left\\{-\\max \\left(0, \\mathbf{W}_{\\gamma} \\delta_{t}+\\mathbf{b}_{\\gamma}\\right)\\right\\} \\\\\n \\mathbf{h}_{t} &=\\sigma\\left(\\mathbf{W}_{h}\\left[\\mathbf{h}_{t-1} \\odot \\gamma_{t}\\right]+\\mathbf{U}_{h}\\left[\\mathbf{x}_{t}^{c} \\circ \\mathbf{m}_{t}\\right]+\\mathbf{b}_{h}\\right) \\\\ \n\\ell_{t} &=\\left\\langle\\mathbf{m}_{t}, \\mathcal{L}_{e}\\left(\\mathbf{x}_{t}, \\hat{\\mathbf{x}}_{t}\\right)\\right\\rangle \n\\end{aligned}\n\\end{equation}\nThe former model named RITS is the unidirectional version of the proposed methods in . \nAs the bidirectional version, BRITS employs bidirectional RNN by utilizing the bidirectional recurrent dynamics, i.e., they train 2 models in forward direction and \nbackward direction respectively . \nThus consistency loss is introduced to take the losses of both directions into consideration.\nTo conclude, in BRITS, time lags are still adopted to deal with irregular time series. Only RNN is used to model the time series. We can also conclude from the model and the experiments that \nbidirectional RNN contributes to a higher performance since the unidirectional model may suffer from bias exploding problem .\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=1.1\\expwidths]{fig-E2GAN}\\\\\n  \\caption{The structure of the E$^2$GAN. Image extracted from .}\n  \\label{fig:E2GAN}\n\\end{figure*}", "cites": [8109, 1106], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section describes the BRITS model in a straightforward manner, incorporating equations and mentioning its relation to RITS and GRU-D. While it makes basic connections between the cited papers, particularly in terms of temporal decay and RNN structures, it lacks deeper critical evaluation or abstraction into broader principles. The explanation remains focused on the model's structure and performance without situating it in a larger context or identifying limitations."}}
{"id": "5dacb75b-98d3-41a7-8ff3-e151c45cb104", "title": "E$^2$GAN", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "E$^2$GAN"]], "content": "E$^2$GAN  is another work based on GAN. While the GRUI-GAN in Section~\\ref{subsect:GRUI} takes a random noise vector \nas input, which takes lots of time to train, E$^2$GAN adopts an auto-encoder structure based on GRUI to form the generator. The overall structure of their model \nis in Figure~\\ref{fig:E2GAN}.\nIn E$^2$GAN, concepts including mask, time lag, decay rate and GRUI are all reserved without improvement, thus there is no innovation in the GRUI structure.\nThe main contribution is the auto-encoder structure they adopt in the generator. This is a common strategy taken by image generation and imputation \nsuch as Context-Encoder , PixelGANs , but not a common strategy in RNN based GAN. \nSince the input of the model is the original time series, the model compresses the input incomplete time series $\\mathbf{X}$ into a low-dimensional vector $z$\nwith the help of the GRUI. And then the reconstructing part will reconstruct the complete time series $\\mathbf{X'}$ to fool the discriminator. \nAnd the discriminator of the method attempts to distinguish actual incomplete time series $\\mathbf{X}$ and\nthe fake but complete sample $\\mathbf{X'}$ through the adoption of recursive neural network. The framework of the discriminator is also an encoder.\nE$^2$GAN takes an encoder-decoder RNN based structure as the generator, which tackles the difficulty of training the model and the accuracy. \nSo far, according to the experiments in the paper, E$^2$GAN has achieved state-of-the-art and outperforms other existing methods.", "cites": [896, 1254], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of using an auto-encoder structure in the generator by referencing both E$^2$GAN and relevant image imputation works like Context-Encoder and PixelGANs. It offers a basic comparison by noting that this is a common strategy in image generation but less so in RNN-based GANs. However, the analysis remains somewhat superficial, lacking deeper evaluation of the methodâ€™s limitations or broader implications for time series imputation."}}
{"id": "5dc3a0b9-2bd3-472b-b459-73ad6f7d4982", "title": "NAOMI", "level": "subsection", "subsections": [], "parent_id": "07eea55f-9929-4104-93a2-cdf47d82dbdd", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Methods"], ["subsection", "NAOMI"]], "content": "NAOMI (\\textbf{N}on-\\textbf{A}ut\\textbf{O}regressive \\textbf{M}ultiresolution \\textbf{I}mputation )\nproposes a non-autoregressive model which conditions both previous values but also future values, i.e., equipped with bidirectional \nRNN like BRITS introduced in Section~\\ref{sect:BRITS}. Since in the imputation tasks, future values and historical values are both observed,\nthe intuition is to take advantage of both values and train bidirectional models for them. As illustrated in Figure~\\ref{fig:naomi}, $f_f$ and $f_b$ are \nforward and backward RNN respectively, thus the hidden state $h_t$ is a joint hidden state concatenated by $h^f_t$ and $h^b_t$.\nMoreover, a special predicting strategy is performed in this paper. They adopt a \\emph{divide and conquer strategy}. As it is shown in Figure~\\ref{fig:naomi}, \nwith 2 known values $x_1$ and $x_5$, they first predict the midpoint $x_3$ by $x_1$ and $x_5$ with proposed bidirectional RNN models, and then $x_3$ is updated and \nutilized to predict $x_2$ and $x_4$ respectively. Thus a fine-grained prediction is performed.\nFinally, adversarial training is taken to enhance the model. \nHowever, in NAOMI, time gaps are ignored and the data is injected into the RNN model without timestamps. It suggests the model is not aware of \nirregular time series although we can still take them as input by removing their timestamps directly.\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\expwidths]{fig-naomi}\\\\\n  \\caption{The structure of the NAOMI. Image extracted from .}\n  \\label{fig:naomi}\n\\end{figure}", "cites": [1005], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the NAOMI method, explaining its non-autoregressive structure and multiresolution prediction strategy. While it connects NAOMI to BRITS by mentioning bidirectional RNNs, the synthesis remains limited to a single cited paper. There is minimal abstraction or identification of broader patterns, and the critical analysis is basic, focusing only on one limitation (ignoring time gaps)."}}
{"id": "2abe7629-8198-4cf0-b369-ff20cc308612", "title": "Attention Mechanism Enhanced", "level": "subsection", "subsections": [], "parent_id": "01eb6746-47fc-45dd-ba0d-196c13dce670", "prefix_titles": [["title", "Time Series Data Imputation: A Survey on Deep Learning Approaches"], ["section", "Future Research Opportunities"], ["subsection", "Attention Mechanism Enhanced"]], "content": "In recent years, the attention mechanism has been shown successful in deep learning society, especially in NLP fields.\nWhen adopted in RNN, the attention mechanism allocates weights for each hidden state to draw information from the sequence.\nWith such mechanism, the model is improved to capture latent patterns in historical data, thus may benefit time series imputation.\nCompared to existing RNN models (e.g., LSTM and GRU) which already take long-term dependencies into consideration, the attention mechanism for instance temporal attention enables the model to \nsee features and status globally. However, LSTM and GRU will still lose long-term information due to the forget gate unit.\nRecently, pure attention models are proposed without RNN. The Transformer proposed in  is one of the popular frameworks.\nIn the proposed Transformer framework, it only adopts an attention layer called self-attention, which is computed as:\n$$\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n$$\nwhere $Q, K, V$ are queries, keys and values respectively, and $d_k$ is the dimension of the input.\nAccepting a single sequence as input, the self-attention mechanism relates different positions of the input and tries to compute a representation of the sequence.\nWithout applying RNN, the Transformer relies entirely on the self-attention layers to former an encoder-decoder structure, which is similar to the auto-encoder introduced in \nSection~\\ref{sect:characteristics}. Such a structure provides the ability to extract high-dimensional features for reconstructing, which benefits tasks like machine translation introduced in .\nFor improving the performance of data imputation, due to the effectiveness of the attention mechanisms, models based on attention mechanisms may also address the time series imputation problems.\nAnd two aforementioned categories of the attention mechanisms including temporal attention and self-attention are both potential techniques which may benefit the time series imputation.\nMoreover, with the idea of removing RNN and leveraging only attention mechanisms, structures like the Transformer may contribute to a new framework for the imputation tasks.\nTo summary, two categories of attention mechanisms including temporal attention and self-attention may bring future opportunities on time series imputation. And the pure attention frameworks are also \nnew directions to model time series.", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the role of attention mechanisms in time series imputation, integrating the Transformer paper to illustrate how attention can replace RNNs. It highlights the benefits of attention over RNN-based models like LSTM and GRU by pointing out their limitations with long-term dependencies. While it begins to abstract attention mechanisms into broader categories (temporal and self-attention), the analysis remains relatively surface-level and could have compared more existing approaches or deeper critiques."}}
