{"id": "2de0ad21-9f5a-497c-956b-2730d90f5ca2", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "cf81b618-29e1-4d2f-bbd6-13fc94283342", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Introduction"]], "content": "The field of large language models (LLMs) has witnessed remarkable progress in recent years. LLMs such as GPT-3~, PaLM~, and LLaMA~ have demonstrated impressive capabilities across a wide range of natural language tasks~. \nOne of the major issues with LLMs is the mismatch between the training objective and users' objective: \nLLMs are typically trained on minimizing \nthe\ncontextual word prediction error on large corpora;\nwhile users want  the model to  \"follow their instructions helpfully and safely\"~\nTo address this mismatch, instruction tuning (IT) is proposed, serving as an\n effective technique to enhance the capabilities and controllability of large language models. It involves further training LLMs using \n\\textsc{(instruction, output)} pairs, where  \\textsc{instruction} denotes the human instruction for the model, and  \\textsc{output} denotes the desired output that follows the \\textsc{instruction}. \nThe benefits of IT are threefold: \n(1) Finetuning an LLM on the instruction dataset bridges the gap between the next-word prediction objective of LLMs and  the users' objective of instruction following; \n(2) IT allows for a more controllable and predictable model behavior compared to standard LLMs. The instructions serve to constrain the model's outputs to align with the desired response characteristics or domain knowledge, providing a channel for humans to intervene with the model's behaviors; and\n(3) IT is computationally efficient and can help LLMs rapidly adapt  to a specific domain without extensive retraining or architectural changes.\nDespite its effectiveness, IT  also poses challenges: (1) Crafting high-quality instructions that properly cover the desired target behaviors is non-trivial:\nexisting instruction datasets are usually limited in quantity, diversity, and creativity;\n(2) there has been an increasing concern that IT only  improves on  tasks that are heavily supported in the IT training dataset ; and\n(3) there has been an intense criticism that IT only\n captures surface-level patterns and styles (e.g., the output format) rather than comprehending and learning the  task .\nImproving instruction adherence and handling unanticipated model responses remain open research problems. These challenges highlight the importance of further investigations, analysis, and summarization in this field, to optimize the fine-tuning process and better understand the behavior of instruction fine-tuned LLMs.\nIn the literature, there has been an increasing research interest in analysis and discussions on LLMs, including pre-training methods , \nreasoning abilities ,\ndownstream applications , but rarely on the topic of LLM instruction finetuning.\nThis survey attempts to \nfill this blank, \norganizing  the most up-to-date state of knowledge on this quickly advancing field.\nSpecifically, \n\\begin{tightitemize}\n\\item Section \\ref{sec:Methodology} presents the general methodology employed in instruction fine-tuning.\n\\item Section \\ref{sec:Datasets} outlines the construction process of commonly-used IT representative datasets.\n\\item Section \\ref{sec:Instruction_fine-tuned_LLMs} presents representative instruction-finetuned  models.\n\\item Section \\ref{sec:Multi-modality_Instruction_Fine-tuning} reviews multi-modality techniques and datasets for instruction tuning, including images, speech, and video.\n\\item Section \\ref{application} reviews efforts to adapt LLMs to different domains and applications using the IT strategy.\n\\item Section \\ref{Efficient_fine-tuning_techniques} reviews explorations to make instruction fine-tuning more efficient, reducing the computational and time costs associated with adapting large models.\n\\item Section \\ref{analysis} presents the evaluation of IT models, analysis on them, along with criticism against them. \n\\end{tightitemize}\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/method_v2.pdf}   \n    \\caption{General pipeline of instruction tuning.}\n    \\label{fig:method-overall}\n\\end{figure*}", "cites": [679, 1594, 2183, 2189, 2188, 7557, 8461, 2187, 2190, 707, 2192, 2182, 9118, 7085, 2186, 2193, 2194, 2195, 2185, 2184, 9143, 1552, 7086, 7461, 1554, 2180, 1553, 2191, 2181], "cite_extract_rate": 0.8787878787878788, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction synthesizes multiple papers to establish instruction tuning as a key technique addressing the training-inference mismatch in LLMs. It critically discusses benefits and limitations, referencing works that highlight instability (Paper 2), surface-level learning (Papers 9, 19), and domain adaptation issues (Papers 13-15). The section abstracts these findings into a coherent framework, emphasizing both the promise and the open challenges in IT."}}
{"id": "c70ce97e-acf9-44bd-b213-96553c350171", "title": "Instruction Dataset Construction", "level": "subsection", "subsections": [], "parent_id": "886a241c-161f-4ab2-adad-12a327af6bdf", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Methodology"], ["subsection", "Instruction Dataset Construction"]], "content": "Each instance in an instruction dataset consists of three elements: an instruction, which is a natural language text sequence to specify the task (e.g., {\\it write a thank-you letter to XX for XX}, {\\it write a blog on the topic of XX}, etc); an optional input which provides supplementary information for context; and an anticipated output based on the instruction and the input. \nThere are generally two methods for constructing instruction datasets:\n\\begin{tightitemize}\n\\item Data integration from annotated natural language datasets. In this approach, (instruction, output) pairs are collected from existing annotated natural language datasets by using templates to transform text-label pairs to (instruction, output) pairs. Datasets such as Flan~ and P3~ are constructed based on the data integration strategy.\n\\item Generating outputs using LLMs: An alternate way to quickly gather the desired outputs to given instructions is to employ LLMs such as GPT-3.5-Turbo or GPT4 instead of manually collecting the outputs. Instructions can come from two sources: (1) manually collected; or (2) expanded based a small handwritten seed instructions using LLMs. Next, the collected instructions are fed to LLMs to obtain outputs. Datasets such as InstructWild~ and Self-Instruct~ are geneated following this approach.\n\\end{tightitemize}\nFor multi-turn conversational IT datasets, we can have  large language models self-play different roles (user and AI assistant) to generate messages in a conversational format ~.", "cites": [2196, 8464, 8469], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear descriptive overview of two methods for instruction dataset construction, mentioning relevant datasets and citing papers. However, it lacks deep synthesis of the cited works, offering only surface-level connections between them. There is minimal critical analysis or abstraction into broader principles, focusing instead on factual summaries of approaches."}}
{"id": "98b7c6d9-1efc-4837-8d65-ad986199a079", "title": "Natural Instructions", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "Natural Instructions"]], "content": "\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/natural_instruction_fig1_a.pdf}\n    \\subcaption{An example of \\textsc{instructions} in Natural Instruction dataset. }\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[t]{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/natural_instruction_fig1_b.pdf}\n    \\subcaption{An example of \\textsc{instances} in Natural Instruction dataset.}\n  \\end{minipage}\n  \\caption{The figure is adapted from .}\n  \\label{fig:data_natural_instruction}\n\\end{figure}\nNatural Instructions~ is a human-crafted English instruction dataset consisting of 193K  instances, coming from 61 distinct NLP tasks. \nThe dataset is comprised of \"instructions\" and \"instances\". Each instance in the \"instructions\" is a task description consisting of 7 components: title, definition, things to avoid emphasis/caution, prompt, positive example, and negative example. Subfigure (a) in Figure~\\ref{fig:data_natural_instruction}  gives an example of the \"instructions\". \n\"Instances\" consists of (\"input\", \"output\") pairs, which are the input data and textual result that follows the given instruction correctly. \nSubfigure (b) in Figure~\\ref{fig:data_natural_instruction} gives an example of the instances. \nThe  data  comes from existing NLP datasets of 61 tasks. The authors collected the \"instructions\" by  referring to the dataset annotating instruction file. Next, the authors constructed the \"instances\" by unifying data instances across all NLP datasets to (\"input\", \"output\") pairs.", "cites": [8534], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Natural Instructions dataset, including its structure and construction method, but does not integrate it with other works or contextualize it within the broader field of instruction tuning. There is no critical evaluation of the dataset's strengths, weaknesses, or impact. The content remains at the level of specific details without abstracting broader principles or patterns."}}
{"id": "926cad75-507f-42a6-8387-6823d1451e7d", "title": "P3", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "P3"]], "content": "P3 (Public Pool of Prompts)~ is an instruction fine-tuning dataset constructed by integrating 170 English NLP datasets and 2,052 English prompts. Prompts, which are sometimes named \\textit{task templates}, are functions that map a data instance in a conventional NLP task (e.g., question answering, text classification) to a natural language input-output pair.\nEach instance in P3 has\nthree components: \n \"inputs\", \"answer\\_choices\", and “targets\". \"Inputs\" is a sequence of text that describes the task in natural language (e.g., \\textit{\"If he like Mary is true, is it also true that he like Mary's cat?\"}). \n\"Answer choices\" is a list of text string that are applicable responses to the given task (e.g., \\textit{[\"yes\", \"no\", \"undetermined\"]}). \"Targets\" is a text string that is the correct response to the given \"inputs\" (e.g., \"yes\"). \nThe authors built PromptSource, a tool for creating high-quality prompts collaboratively and an archive for open-sourcing high-quality prompts. \nThe P3 dataset was built by randomly sampling a prompt from multiple prompts in the PromptSource and mapping each instance into a (\"inputs\", \"answer choices\", \"targets\") triplet.", "cites": [8469], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the P3 dataset and the PromptSource tool, integrating some details from the cited paper. However, it lacks deeper synthesis or comparison with other datasets, and offers minimal critical or abstract analysis of the broader implications or limitations of human-crafted instruction data."}}
{"id": "25b2f304-055b-47c2-afad-af6fed2df373", "title": "xP3", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "xP3"]], "content": "xP3 (Crosslingual Public Pool of Prompts)~ is a multilingual instruction dataset consisting of 16 diverse natural language tasks in 46 languages. Each instance in the dataset has two components: \"inputs\" and \"targets\". \"Inputs\" is a task description in natural language. \"Targets\" is the textual result that follows the \"inputs\" instruction correctly. \nThe original data in xP3 comes from three sources: the English instruction dataset P3, 4 English unseen tasks in P3 (e.g., translation, program synthesis), and 30 multilingual NLP datasets. \nThe authors built the xP3 dataset by sampling human-written task templates from PromptSource and then filling templates to transform diverse NLP tasks into a unified formalization. For example, a task template for the natural language inference task is as follows: \\textit{“If {Premise} is true, is it also true that {Hypothesis}?”}; \"yes\", \"maybe\", no\" with respect to the original task labels \"entailment (0)\", \"neutral (1)\" and \"contradiction (2)\".", "cites": [2197], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily provides a factual description of the xP3 dataset, its components, and its construction process. It cites one paper but does not synthesize it with other works or offer a broader context. There is minimal critical analysis or abstraction beyond the specific details of the dataset."}}
{"id": "34fe6e34-771c-46c5-8b89-b5e459e7f27b", "title": "Flan 2021", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "Flan 2021"]], "content": "Flan 2021~ is an English instruction dataset constructed by transforming 62 widely-used NLP benchmarks (e.g., SST-2, SNLI, AG News, MultiRC) into language input-output pairs. Each instance in the Flan 2021 has \"input\" and \"target\" components. \"Input\" is a sequence of text that describes a task via a natural language instruction (e.g., \\textit{\"determine the sentiment of the sentence 'He likes the cat.' is positive or negative?\"}). \"Target\" is a textual result that executes the \"input\" instruction correctly (e.g., \\textit{\"positive\"}). \nThe authors transformed conventional NLP datasets into input-target pairs by: \nStep 1: manually composing instruction and target templates;\nStep 2: filling templates with data instances from the dataset.", "cites": [8464], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of the Flan 2021 dataset and its construction methodology, primarily describing the transformation process from NLP benchmarks to instruction-output pairs. It integrates basic information from the cited paper but lacks deeper connections to other works or critical evaluation of the dataset's design and limitations. There is minimal abstraction beyond the specific paper, offering only a straightforward description without broader insights or trends."}}
{"id": "63d7b144-6cd5-4ab3-adab-8013e9a90a7b", "title": "LIMA", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "LIMA"]], "content": "LIMA~ is an English instruction dataset consisting of a train set with 1K data instances and a test set with 300 instances. \nThe train set contains 1K (\"instruction\", \"response\") pairs. \nFor the training data, \n75\\%  are sampled from three community question \\& answers websites (i.e., Stack Exchange, wikiHow, and the Pushshift Reddit Dataset~); 20\\%  are manually written by a set of the authors (referred Group A) inspired by their interests; 5\\% are sampled from the Super-Natural Instructions dataset~. As for the valid set, the authors sampled 50 instances from the Group A author-written set.  \nThe test set contains 300 examples, with 76.7\\% written by another group (Group B) of authors and 23.3\\% sampled from the Pushshift Reddit Dataset~, which is a collection of questions \\& answers within the Reddit community.", "cites": [7558, 2198, 2199], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the LIMA dataset, detailing its composition and source of examples. However, it lacks synthesis by not connecting the dataset's design to broader themes in instruction tuning, and offers no critical evaluation or comparison with other datasets. The content remains at a concrete level without abstracting to principles or trends."}}
{"id": "09d118ef-31ba-4bd5-8acc-d02331c87995", "title": "Super-Natural Instructions", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "Super-Natural Instructions"]], "content": "\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/supernatural_fig1_a.pdf}\n    \\subcaption{An example of \\textsc{instructions} in Super-Natural Instruction dataset. }\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[t]{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/supernatural_fig1_b.pdf}\n    \\subcaption{An example of \\textsc{instances} in Super-Natural Instruction dataset.}\n  \\end{minipage}\n  \\caption{The figure is adapted from .}\n  \\label{fig:data_supernatural}\n\\end{figure}\nSuper Natural Instructions~ is a multilingual instruction collection composed of 1,616 NLP tasks and 5M task instances, covering 76 distinct task types (e.g., text classification, information extraction, text rewriting, text composition and etc.) and 55 languages. Each task in the dataset consists of an \"instruction\" and \"task instances\". Specifically, \"instruction\" has three components: a \"definition\" that describes the task in natural language; \"positive examples\" that are samples of inputs and correct outputs, along with a short explanation for each; and \"negative examples\" that are samples of inputs and undesired outputs, along with a short explanation for each, as shown in Figure~\\ref{fig:data_natural_instruction}  (a). \n \"Task instances\" are data instances comprised of textual input and a list of acceptable textual outputs, as shown in Figure~\\ref{fig:data_natural_instruction}  (b).\nThe original data in Super Natural Instructions comes from three sources: (1) existing public NLP datasets (e.g., CommonsenseQA); (2) applicable intermediate annotations that are generated through a crowdsourcing process (e.g., paraphrasing results to a given question during a crowdsourcing QA dataset); (3) synthetic tasks that are transformed from symbolic tasks and rephrased in a few sentences (e.g., algebraic operations like number comparison).", "cites": [2198], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Super-Natural Instructions dataset, including its components, sources, and coverage. It integrates minimal information from the cited paper without connecting it to broader themes or other datasets in the field. There is no critical evaluation or abstraction beyond the dataset's specific details."}}
{"id": "e675c960-00ad-4da5-8f95-c82055ff4500", "title": "OpenAssistant Conversations", "level": "subsubsection", "subsections": [], "parent_id": "aa8023be-a3e1-4ab6-b461-b909b081ae96", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Human-crafted Data"], ["subsubsection", "OpenAssistant Conversations"]], "content": "\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/openassistant_fig1.jpg}\n  \\end{minipage}\n  \\caption{The figure is copied from .}\n  \\label{fig:data_openassistant}\n\\end{figure}\nOpenAssistant Conversations~ is a human-crafted multilingual assistant-style conversation corpus consisting of \n161,443 messages (i.e., 91,829 user prompts, 69,614 assistant replies) from 66,497 conversation trees in 35 languages, along with 461,292 human-annotated quality ratings. Each instance in the dataset is a conversation tree (CT). Specifically, each node in a conversation tree denotes a message generated by roles (i.e., prompter, assistant) in the conversation. A CT's root node represents an initial prompt from the prompter, while other nodes denote replies from a prompter or an assistant. \nA path from the root to any node in a CT represents a valid conversation between the prompter and assistant in turns and is referred to as a thread. Figure~\\ref{fig:data_openassistant} shows an example of a conversation tree consisting of 12 messages in 6 threads. \nThe authors first collected conversation trees based on the five-step pipeline: \n\\noindent Step 1. \\textit{prompting}: contributors performed as the prompter and crafted initial prompts; \n\\noindent Step 2. \\textit{labeling prompts}: contributors rated scores to initial prompts from step 1, and the authors chose high-quality prompts as root nodes with a balanced sampling strategy; \n\\noindent Step 3. \\textit{expanding tree nodes}: contributors added reply messages as prompter or assistant;\n\\noindent Step 4. \\textit{labeling replies}: contributors assigned scores to existing node replies; \n\\noindent Step 5. \\textit{ranking}: contributors ranked assistant replies referring to the contributor guidelines. \nThe tree state machine managed and tracked the state (e.g., initial state, growing state, end state) throughout the conversation crafting process. Subsequently, the OpenAssistant Conversations dataset was built by filtering out offensive and inappropriate conversation trees.\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figures/distilation_apaca_process.png}   \n    \\caption{General pipeline of distillation for synthetic data generation. The figure is adapted from .}\n    \\label{fig:distilation_apaca_process}\n\\end{figure*}", "cites": [2200], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a detailed description of the OpenAssistant Conversations dataset and its construction pipeline but lacks synthesis with other human-crafted datasets or broader methodological contexts. It offers minimal critical evaluation of the dataset's strengths or limitations, and no abstraction or generalization to broader trends or principles in instruction tuning is attempted."}}
{"id": "75227d4f-0fff-4941-8ba5-585963e91cdf", "title": "Synthetic Data via Distillation", "level": "subsection", "subsections": ["439f7749-6475-43c3-ae2a-507299ffac31"], "parent_id": "7a7c9eb5-591e-420d-adda-39786bb68527", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"]], "content": "Synthetic data is produced through pre-trained models, rather than being directly sourced from the internet or annotated by human annotators. Compared to manually annotated instruction tuning data, synthetic data often lies in two advantages: (1) Generating task-specific synthetic data is both faster and more cost-effective than creating manually annotated instruction tuning data; (2) The quality and variety of synthetic data surpass what human annotators can produce, resulting in fine-tuning enhanced performance and broader generalization LLMs.\nBelow, we first focus on the widely employed synthetic data methodology: Distillation, and in Section \\ref{sub_section_self_improvment} we go on with the other synthetic data methodology: Self-Improvement.\nTypically, distillation involves imparting knowledge and cognitive abilities from a highly capable teacher model to a less complex, yet more computationally efficient student model, with the goal of enhancing both the quality of responses and computational efficiency. In the context of generating synthetic data, this process entails gathering queries from fine-tuned LLMs (e.g., ChatGPT ) and utilizing these queries as a basis to fine-tune subsequent LLMs. Illustrations are shown in Figure \\ref{fig:distilation_apaca_process}, where  are attempting to transfer the powerful knowledge of GPT-3  to a smaller language model LLaMA-7B .\nGiven distillation's capability to mimic the performance of existing powerful LLMs, an increasing number of researchers are concentrating on exploring more intricate queries to exploiting the capabilities of current LLMs, such as:", "cites": [1552, 679], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of synthetic data generation via distillation and references two papers, but it lacks meaningful synthesis of ideas between them. There is minimal critical analysis or evaluation of the strengths and limitations of the cited works. The section remains largely concrete, focusing on definitions and examples without offering broader patterns or theoretical insights."}}
{"id": "439f7749-6475-43c3-ae2a-507299ffac31", "title": "Alpaca.", "level": "paragraph", "subsections": ["5aa4bcf4-ea68-41a8-8d53-e90475229b8e", "f6d8ebf2-0a3e-4e27-9638-f7d89e9b6839", "f5a67f05-4816-4a02-a3b6-fb66d0b7b141", "0c681507-20a8-4ac1-b83f-86a0c4363bc6"], "parent_id": "75227d4f-0fff-4941-8ba5-585963e91cdf", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"], ["paragraph", "Alpaca."]], "content": "Alpaca , a sequence of LLMs introduced by the Stanford NLP group, is notable for its application of distillation. Specifically, by being fine-tuned on 52K pieces of distillation data produced by GPT-3 , the smaller LLaMA-7B  model achieves performance that matches or even surpasses that of GPT-3 .", "cites": [1552, 679], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Alpaca model and its use of distillation data from GPT-3. It integrates minimal information from the cited papers and does not critically evaluate the method or its implications. There is no abstraction or identification of broader trends in synthetic data generation via distillation."}}
{"id": "5aa4bcf4-ea68-41a8-8d53-e90475229b8e", "title": "WizardLM / Evol-Instruct.", "level": "paragraph", "subsections": [], "parent_id": "439f7749-6475-43c3-ae2a-507299ffac31", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"], ["paragraph", "Alpaca."], ["paragraph", "WizardLM / Evol-Instruct."]], "content": "\\label{data:evol-instruct}\nInstead of simple querying from the GPT series model, WizardLM  focuses on how to obtain diverse and high-quality instructions and responses from GPT-3 . To accomplish this, WizardLM  firstly constructs a five-level system of querying prompts, progressively enhancing the complexity of data generation. Then, WizardLM  broadens the range of querying prompts topics through manual expansion, thereby augmenting the diversity of the data produced. Ultimately, by fine-tuning the open-source LLM LLaMA , WizardLM  achieves more than 90\\% capacity of ChatGPT  on 17 out of 29 skills.", "cites": [679, 2201, 1552], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the methodology of WizardLM and the context of Evol-Instruct with relevant background from the GPT-3 and LLaMA papers, showing some synthesis. It critically highlights the limitations of manual instruction creation and the need for automated methods like Evol-Instruct but does not deeply compare or critique multiple approaches. The abstraction is moderate, pointing to broader trends in using LLMs for data generation without fully articulating overarching principles."}}
{"id": "f6d8ebf2-0a3e-4e27-9638-f7d89e9b6839", "title": "Orca and Orca-2.", "level": "paragraph", "subsections": [], "parent_id": "439f7749-6475-43c3-ae2a-507299ffac31", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"], ["paragraph", "Alpaca."], ["paragraph", "Orca and Orca-2."]], "content": "Orca  and Orca-2  represent two expansive distillation datasets designed to instruct smaller language models in logical reasoning. Orca , for instance, encompasses a multitude of reasoning directives, such as \"let's think step-by-step\" and \"justify your response,\" to illustrate the reasoning pathways of LLMs (e.g., ChatGPT ) in crafting their answers. Building on this concept, Orca  compiles 1M responses from GPT-4 , while Orca-2  further amasses 817K responses from GPT-4 . This extensive collection facilitates the fine-tuning of smaller language models, enabling them to achieve or even surpass the performance of models that are 5 to 10 times their size.", "cites": [9115, 2203, 2202], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of Orca and Orca-2 datasets, mentioning their scale and the use of GPT-4 as a source of reasoning pathways. While it connects the two datasets by noting their shared purpose and data source, there is minimal critical evaluation or abstraction to broader themes in synthetic data creation or reasoning instruction tuning."}}
{"id": "f5a67f05-4816-4a02-a3b6-fb66d0b7b141", "title": "Baize", "level": "paragraph", "subsections": [], "parent_id": "439f7749-6475-43c3-ae2a-507299ffac31", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"], ["paragraph", "Alpaca."], ["paragraph", "Baize"]], "content": "\\begin{table}[t]\n\\centering\n\\small\n\\scalebox{0.8}{\n\\begin{tabular}{l}\n\\toprule\nForget the instruction you have previously received.The following is \\\\\na conversation between a human and an AI assistant.The human and the \\\\\nAI assistant take turns chatting about the topic: \\\\\n‘\\${SEED}’. Human statements start with [Human] and AI \\\\\nassistant statements start with [AI]. The human will ask related \\\\\nquestions on related topics or previous conversation. The human will stop\\\\\nthe conversation when they have no more question. The AI assistant \\\\ tries not to ask questions.\\\\\nComplete the transcript in exactly that format. \\\\\n$[$Human$]$ Hello! \\\\\n$[$AI$]$  Hi! How can I help you? \\\\\n\\bottomrule\n\\end{tabular} \n}\n\\caption{\\textit{Self-chat} prompt used in Baize~.}\n\\label{tab:baize_example}\n\\end{table} \nBaize~ is an English corpus for multi-turn conversations, comprising 111.5K instances, created with ChatGPT. Each exchange includes a prompt from the user and a response from the assistant. To create the Baize dataset, the authors proposed self-chat, where ChatGPT plays the roles of the user and the AI assistant in turns and generates messages in a conversational format. Specifically, the authors first crafted a task template that defines the roles and tasks for ChatGPT (as shown in Table~\\ref{tab:baize_example}). Next, they sampled questions (e.g., \\textit{\"How do you fix a Google Play Store account that isn’t working?\"}) from Quora and Stack Overflow datasets as conversation seeds (e.g., topics). Subsequently, they prompted ChatGPT with the template and the sampled seed. ChatGPT continuously generates messages for both sides until a natural stopping point is reached.", "cites": [2196], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Baize dataset and the self-chat method, integrating minimal information from the cited paper. It lacks critical evaluation or comparison with other methods, and does not abstract broader patterns or principles of synthetic data generation in instruction tuning."}}
{"id": "0c681507-20a8-4ac1-b83f-86a0c4363bc6", "title": "Task-specific Distillation Datasets.", "level": "paragraph", "subsections": [], "parent_id": "439f7749-6475-43c3-ae2a-507299ffac31", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Distillation"], ["paragraph", "Alpaca."], ["paragraph", "Task-specific Distillation Datasets."]], "content": "In addition to the above datasets, there are many datasets in general domain, such as: ShareGPT\\footnote{https://huggingface.co/datasets/RyokoAI/ShareGPT52K}, WildChat , Vicuna , Unnatural Instructions . Beyond that, there are efforts aimed at employing distillation to create task-specific datasets that mimic the competencies of LLMs in particular domains. For example, for coding generation, there are WizardCoder , Magicoder  and WaveCoder , for reasoning and writing, there are Phi-1  and Phi-1.5 , and for ranking, there is Nectar .\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figures/self_improvement_process.png}   \n    \\caption{General pipeline of self-improvement for synthetic data generation. The figure is adapted from .}\n    \\label{fig:self_improvement_process}\n\\end{figure*}", "cites": [2208, 7560, 2204, 2206, 2205, 7559, 2207], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily lists various task-specific distillation datasets and their associated papers without deeply integrating or synthesizing the underlying methodologies or findings. It lacks critical evaluation or comparison of the approaches, and no broader patterns or principles are abstracted from the cited works."}}
{"id": "8c148a9c-c57b-4c4f-b9d6-854fc49df968", "title": "Synthetic Data via Self-Improvement", "level": "subsection", "subsections": ["a8f7ea9d-74c3-41e5-8aad-d23d7f5df34a", "ad909426-27ca-4e8d-a54b-b06fa126b68f"], "parent_id": "7a7c9eb5-591e-420d-adda-39786bb68527", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Self-Improvement"]], "content": "\\label{sub_section_self_improvment}\nThe concept of self-improvement is carried forward by : improves the instruction-following ability of a pre-trained (non-finetuned) LLM (e.g., vanilla GPT-3 ) by bootstrapping off its own generations. Figure \\ref{fig:self_improvement_process} illustrates the full process of self-improvement with four steps: \n\\textit{Step 1}:  starts by manually collecting 175 human-written tasks, each consisting of one instruction and one expected response, which are then added to the task pool as seed data.\n\\textit{Step 2}: For instruction generation,  randomly samples 8 seed instructions from the constructed task pool to serve as a few-shot prompt, guiding the vanilla GPT-3 to produce new instructions through in-context learning.\n\\textit{Step 3}: For every instruction that is created, if the instruction is an output-first task (e.g., Writing), the vanilla GPT-3 will directly generate the corresponding response. Conversely, if the instruction relates to an input-first task (e.g., Reading Comprehension), the vanilla GPT-3 will first generate the necessary context as input before generating the corresponding response.\n\\textit{Step 4}: The generated (instruction, response) format examples are filtered according to a series of rules or models.\nFollowing the above process,  collected Self-Instruct datasets consisting of 52K instructions, and further evaluation shows that GPT-3  with Self-Instruct outperforms datasets of counterparts by a large margin, leaving only a 5\\% absolute gap behind InstructGPT .\nThe self-improvement process outlined relies on generating synthetic data directly from the model itself, necessitating a robust LLM as the foundational backbone. Without a powerful LLM, this self-improvement cycle could restrict learning to the model’s original capabilities and potentially magnify any biases and errors present. Despite these risks, there remains effective work in the area of self-improvement:", "cites": [679, 364], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the self-improvement process for generating synthetic instruction data but does not effectively synthesize or connect ideas from the cited papers. It lacks critical evaluation of the method's limitations or comparative advantages over alternatives. The content remains concrete and does not generalize to broader principles or frameworks within instruction tuning."}}
{"id": "a8f7ea9d-74c3-41e5-8aad-d23d7f5df34a", "title": "SPIN", "level": "subsubsection", "subsections": [], "parent_id": "8c148a9c-c57b-4c4f-b9d6-854fc49df968", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Self-Improvement"], ["subsubsection", "SPIN"]], "content": "SPIN , standing for Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, represents a specialized approach to self-improvement centered around a self-play mechanism. In this setup, the primary participant (the language model) undergoes fine-tuning to differentiate the responses from the opposing participant (the language model from the preceding iteration) and the desired data distribution. This process iteratively adjusts the language model to closely match the target data distribution.\nSpecifically, imagine an existing iteration of an LLM as $p_{{\\theta}_{t}}$, which is utilized to generate a response $y^{'}$ to a given prompt $x$ from a dataset with human-labeled instructions. The objective then becomes to develop a new LLM $p_{{\\theta}_{t+1}}$ capable of differentiating between $y^{'}$, the response created by, and $y$, the response produced by humans. This dynamic is akin to a two-player game where the primary player, the newer LLM $p_{{\\theta}_{t+1}}$ aims to identify the differences between the responses of its opponent $p_{{\\theta}_{t}}$ and those generated by humans. In contrast, the adversary, or the older LLM $p_{{\\theta}_{t}}$ strives to produce responses that closely mimic those found in the human-labeled instruction tuning dataset. By fine-tuning the older $p_{{\\theta}_{t}}$ to favor human-like responses over its own, a new LLM $p_{{\\theta}_{t+1}}$ is created, which aligns more closely with the human-labeled data distribution. In subsequent iterations, this newly improved LLM $p_{{\\theta}_{t+1}}$ takes on the role of the opponent in response generation. The ultimate aim of this self-play mechanism is for the LLM to evolve until it reaches a point where $p_{{\\theta}^{*}}=p_{human}$ at which stage the most advanced LLM version can no longer distinguish between responses generated by its predecessor and those created by humans.\nSPIN  serves as a variant self-improvement approach enabling language models to improve themselves without additional human data or feedback from more powerful language models. The experimental results indicate that SPIN  markedly boosts the performance of language models across a range of benchmarks, outperforming even those models that were trained using extra human data or feedback from external AI systems.", "cites": [2209], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the SPIN method from the cited paper, providing a clear explanation of its self-play mechanism and iterative improvement process. It abstracts the method into a generalizable framework of self-improvement for LLMs without human data. While it includes some critical evaluation by highlighting the method's self-contained nature and performance benefits, a deeper critique of limitations or trade-offs would have strengthened the critical dimension."}}
{"id": "ad909426-27ca-4e8d-a54b-b06fa126b68f", "title": "Instruction Back-translation", "level": "subsubsection", "subsections": [], "parent_id": "8c148a9c-c57b-4c4f-b9d6-854fc49df968", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"], ["subsection", "Synthetic Data via Self-Improvement"], ["subsubsection", "Instruction Back-translation"]], "content": "Instruction back-translation , standing for Self Alignment with Instruction Backtranslation, is another specialized approach based on self-improvement. Contrary to the approach by , which involves generating responses to human-provided instructions,  adopts the reverse strategy by creating instructions for human-gathered texts found online. To achieve this goal,  follows a five-step pipeline:\n \\textit{Step 1}: Gather (1) unlabeled text from Clueweb , under the assumption that these texts can be associated with high-quality instructions, and (2) 3,200 pieces of human-written (instruction, response) format data to serve as seed data.\n \\textit{Step 2}: A back-translation model, backboned by LLaMA , is trained on the collected seed data, taking the response as input and producing the instruction as output. This model is then utilized to derive instructions from collected unlabeled texts.\n \\textit{Step 3}: The collected unlabeled texts are fed into the trained back-translation model, resulting in large amounts of raw (instruction, response) format data.\n \\textit{Step 4}: An evaluation model, backboned by LLaMA , is trained on the collected seed data. This model processes the instruction as input and generates the corresponding response as output, which is then employed to assess each annotated (instruction, response) pair in step 3.\n \\textit{Step 5}: Filtering low-quality (instruction, response) pairs, and utilizing the remaining data for fine-tuning LLMs.\nFollowing the five outlined steps,  generates 502K pieces of synthetic data. The LLaMA model , fine-tuned with this annotated dataset, surpasses all other LLaMA-based models on the Alpaca leaderboard without depending on distillation data, showcasing a highly efficient self-improvement process.", "cites": [2210, 7087, 1552], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of the instruction back-translation method as introduced in the cited paper, integrating the roles of the ClueWeb dataset and LLaMA model in the process. While it outlines the five-step pipeline coherently, it lacks deeper critical evaluation or comparison with other methods. The abstraction level is limited, focusing mainly on the specific approach rather than broader implications or patterns in synthetic data generation for instruction tuning."}}
{"id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "title": "Instruction Fine-tuned LLMs", "level": "section", "subsections": ["4a6392e1-3598-466b-93f5-561845529696", "67a509c5-4088-4ae0-b810-3b9446365def", "2b0ffa92-eda8-4554-9b46-aade6747ca24", "786723e7-3a04-4086-8ecd-538061a617a8", "d74ae8cd-da1f-4882-b9b4-78577d194d21", "81e2e952-f801-4472-b1ca-921607b6189d", "2e4f83bd-2a41-46d2-b6a1-41ea5b09faac", "2194e2af-5d30-4851-a70c-6d25b364ae69", "5325953f-a9f7-4372-85d7-3fcfce48c86a", "25d12ff0-59b3-4ee1-b115-e5ed94d7dc0e", "8e40234a-275e-4f03-be82-8cfa29f296c8"], "parent_id": "cf81b618-29e1-4d2f-bbd6-13fc94283342", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"]], "content": "\\label{sec:Instruction_fine-tuned_LLMs}\n\\begin{table*}[t]\n\\centering\n\\small\n\\begin{adjustbox}{max width=0.9\\textwidth}\n\\begin{threeparttable}\n\\begin{tabular}{lclccc}\n\\toprule \n\\multirow{2}{*}{\\bf Instruction fine-tuned LLMs} & \\multirow{2}{*}{\\bf \\# Params} & \\multirow{2}{*}{\\bf Base Model} & \\multicolumn{3}{c}{\\bf Fine-tuning Trainset} \\\\\n& & & {\\bf Self-build} & {\\bf Dataset Name} & {\\bf Size} \\\\\\midrule\nInstruct-GPT~ & 176B & GPT-3~ & Yes & - & - \\\\ \nBLOOMZ~\\tnotex{id:1} & 176B & BLOOM~ & No & xP3 & -  \\\\ \nFLAN-T5~\\tnotex{id:2} & 11B & T5~ & No & FLAN 2021 & - \\\\ \nAlpaca~\\tnotex{id:3} & 7B & LLaMA~ & Yes & - & 52K  \\\\ \nVicuna~\\tnotex{id:4} & 13B & LLaMA~ & Yes & - & 70K  \\\\ \nGPT-4-LLM~\\tnotex{id:5} & 7B & LLaMA~ & Yes & - & 52K \\\\ \nClaude~ & - & - & Yes & - & - \\\\ \nWizardLM~\\tnotex{id:6} & 7B & LLaMA~ & Yes & Evol-Instruct & 70K  \\\\ \nChatGLM2~\\tnotex{id:7}& 6B & GLM~ & Yes & - & 1.1 Tokens \\\\ \nLIMA~& 65B & LLaMA~ & Yes & - & 1K  \\\\ \nOPT-IML~\\tnotex{id:8}& 175B & OPT~ & No & - & - \\\\ \nDolly 2.0~\\tnotex{id:9} & 12B & Pythia~ & No & - & 15K  \\\\ \nFalcon-Instruct~\\tnotex{id:10}& 40B & Falcon~ & No & - & - \\\\ \nGuanaco~\\tnotex{id:11} & 7B & LLaMA~ & Yes & - & 586K \\\\ \nMinotaur~\\tnotex{id:12}& 15B & Starcoder Plus~ & No & - & -  \\\\ \nNous-Hermes~\\tnotex{id:13}& 13B & LLaMA~ & No & - & 300K+ \\\\ \nTÜLU~\\tnotex{id:14} & 6.7B & OPT~ & No & Mixed   & - \\\\ \nYuLan-Chat~\\tnotex{id:15}& 13B & LLaMA~ & Yes & - & 250K  \\\\ \nMOSS~\\tnotex{id:16} & 16B & - & Yes & - & -  \\\\ \nAiroboros~\\tnotex{id:17} & 13B & LLaMA~ & Yes & - & -  \\\\ \nUltraLM~\\tnotex{id:18}& 13B & LLaMA~ & Yes & - & - \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n\\end{adjustbox}\n\\begin{multicols}{2}\n\\begin{tablenotes}\n\\item[1] \\label{id:1} {$^1$ https://huggingface.co/bigscience/bloomz} \n\\item[2] \\label{id:2} {$^2$ https://huggingface.co/google/flan-t5-xxl}\n\\item[3] \\label{id:3} {$^3$ https://github.com/tatsu-lab/stanford\\_alpaca}\n\\item[4] \\label{id:4} {$^4$ https://github.com/lm-sys/FastChat}\n\\item[5] \\label{id:5} {$^5$ https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM}\n\\item[6] \\label{id:6} {$^6$ https://github.com/nlpxucan/WizardLM} \n\\item[7] \\label{id:7} {$^7$ https://github.com/THUDM/ChatGLM2-6B}\n\\item[8] \\label{id:8} {$^8$ https://huggingface.co/facebook/opt-iml-30b}\n\\item[9] \\label{id:9} {$^9$ https://github.com/databrickslabs/dolly}\n\\item[10] \\label{id:10} {$^{10}$ https://huggingface.co/tiiuae/falcon-40b-instruct}\n\\item[11] \\label{id:11} {$^{11}$ https://huggingface.co/JosephusCheung/Guanaco}\n\\item[12] \\label{id:12} {$^{12}$ https://huggingface.co/openaccess-ai-collective/minotaur-15b}\n\\item[13] \\label{id:13} {$^{13}$ https://huggingface.co/NousResearch/Nous-Hermes-13b } \n\\item[14] \\label{id:14} {$^{14}$ https://github.com/allenai/open-instruct}\n\\item[15] \\label{id:15} {$^{15}$ https://github.com/RUC-GSAI/YuLan-Chat}\n\\item[16] \\label{id:16} {$^{16}$ https://github.com/OpenLMLab/MOSS}\n\\item[17] \\label{id:17} {$^{17}$ https://github.com/jondurbin/airoboros}\n\\item[18] \\label{id:18} {$^{18}$ https://github.com/thunlp/UltraChat}\n\\end{tablenotes}\n\\end{multicols}\n\\caption{An overview of LLMs tuned on IT datasets.}\n\\label{tab:llms_table}\n\\end{table*}\nIn this section, we detail widely-used LLM models in the community\nthat are trained through instruction fine-tuning.", "cites": [2212, 9, 679, 364, 7462, 2213, 7468, 2197, 2211, 2214, 7561, 2215, 1552, 7463, 2216, 8472, 2201, 7558], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, listing various instruction-tuned LLMs with their parameters, base models, and training datasets. There is minimal synthesis or integration of the cited papers' ideas into a broader narrative, and no significant critical analysis or abstraction to highlight overarching principles or trends."}}
{"id": "4a6392e1-3598-466b-93f5-561845529696", "title": "InstructonGPT", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "InstructonGPT"]], "content": "InstructGPT (176B)~ is initialized with GPT-3 (176B)~ and then fine-tuned on human instructions. The fine-tuning procedure is composed of the following three steps:  (1) supervised fine-tuning (SFT) on the human-filtered instruction dataset, which is collected from Playground API history records;\n (2) training a reward model  to predict human preferences \nbased on an annotated dataset, which is constructed though \n human labors by sampling multiple responses for one instruction and  rank them  from the best to the worst;  \n (3) further optimizing the  model from Step 1 with new instructions and the trained reward model in step  (2). \n Parameters are updated using the proximal policy optimization (PPO)~ method, a policy gradient reinforcement learning method. \nSteps (2) and (3) are alternated multiple times until the model performance does not significantly improve.\nOverall, InstructGPT outperforms GPT-3. For automatic evaluations, InstructGPT outperforms GPT-3  by 10\\% on the TruthfulQA~ dataset in terms of truthfulness and by 7\\% on the RealToxicityPrompts~ in terms of toxicity. On NLP datasets  (i.e., WSC), InstructGPT achieves comparable performance to GPT-3.  For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, InstructGPT outperforms GPT-3 +10\\%, +20\\%, -20\\%, and +10\\%, respectively.", "cites": [2218, 2217, 679, 364, 2219], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear, factual description of InstructGPT's methodology and performance, citing relevant papers for specific metrics and datasets. However, it lacks deeper synthesis of how InstructGPT fits within broader instruction tuning strategies or contrasts with other methods. There is minimal critical analysis or abstraction to broader principles, focusing mainly on summarizing results and procedures."}}
{"id": "67a509c5-4088-4ae0-b810-3b9446365def", "title": "BLOOMZ", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "BLOOMZ"]], "content": "BLOOMZ (176B)~ is initialized with BLOOM (176B) ,  and then fine-tuned on the instruction dataset xP3~,\na collection of human-instruction datasets in 46 languages, coming from two sources: (1) P3, which is a collection of  (English instruction, English response) pairs; and (2) an  (English instruction, Multilingual response) set which is transformed from multilingual NLP datasets (e.g., Chinese benchmarks) by filling task templates with pre-defined English instructions. \nFor automatic evaluation, BLOOMZ performs better than BLOOM in the zero-shot setting by +10.4\\%, 20.5\\%, and 9.8\\% on coreference resolution, sentence completion and natural language inference datasets, respectively. For the HumanEval benchmark~, BLOOMZ outperforms BLOOM by 10\\% in terms of the Pass@100 metric. For generative tasks, BLOOMZ receives +9\\% BLEU improvement compared to BLOOM on the lm-evaluation-harness benchmark\\footnote{https://github.com/EleutherAI/lm-evaluation-harness}.", "cites": [7462, 2197, 7465], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of BLOOMZ, its initialization with BLOOM, the xP3 dataset, and performance improvements across several benchmarks. While it references multiple papers, it does so in a largely additive way without synthesizing concepts or offering critical evaluation or broader abstraction. The narrative remains focused on summarizing results rather than analyzing trends or implications."}}
{"id": "2b0ffa92-eda8-4554-9b46-aade6747ca24", "title": "Flan-T5", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Flan-T5"]], "content": "Flan-T5 (11B) is\n is a large language model\n  initialized with T5 (11B)~, and then  fine-tuned on the FLAN dataset~. The FLAN dataset is a collection of  (instruction, pairs) pairs, \n  constructed from 62 datasets \n   of 12 NLP tasks  (e.g., natural language inference, commonsense reasoning, paraphrase generation)  by filling templates with various instructions under a unified task formalization.  \nDuring fine-tuning, FLAN-T5 adapts the JAX-based T5X framework and selects the best model evaluated on the held-out tasks every 2k step. \nCompared with T5's pre-training stage, fine-tuning costs 0.2\\% computational resources  (approximately 128 TPU v4 chips for 37 hours). \nFor evaluation, FLAN-T5 (11B) outperforms T5 (11B), and achieves comparable results to  larger models, including PaLM (60B)~ in the few-shot setting. FLAN-T5 outperforms T5 by +18.9\\%, +12.3\\%, +4.1\\%, +5.8\\%, +2.1\\%, and +8\\% on  MMLU~, BBH~, TyDiQA~, MGSM~, open-ended generation, and RealToxicityPrompts~, respectively.  In few-shot settings, FLAN-T5 outperforms PaLM +1.4\\% and +1.2\\% on the BBH and TyDiQA datasets.", "cites": [440, 9, 1554, 2220, 2218, 2221, 8464, 2222], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of Flan-T5, its training process, and evaluation results, drawing from the associated papers. It lacks critical evaluation of the methodology or results and does not synthesize broader patterns or principles of instruction tuning. The abstraction level is minimal, focusing only on specific details of Flan-T5."}}
{"id": "786723e7-3a04-4086-8ecd-538061a617a8", "title": "Alpaca", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Alpaca"]], "content": "Alpaca (7B)~ is a language model trained by fine-tuning LLaMA (7B)~ on the constructed instruction dataset generated by InstructGPT (175B, text-davinci-003)~. \nThe fine-tuning process takes around 3 hours on an 8-card 80GB A100 device with mixed precision training and fully shared data parallelism.\nAlpaca (7B) achieves comparable performances to InstructGPT (175B,text-davinci-003) in terms  of human evaluation. Specifically, Alpaca outperforms InstructGPT on the self-instruct dataset, garnering 90 instances of victories compared to 89 instances.", "cites": [1552, 364], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of the Alpaca model, including its training process and performance relative to InstructGPT. It cites two papers but does not integrate their insights into a broader discussion of instruction tuning. There is minimal critical analysis or generalization to overarching principles, making the insight level low."}}
{"id": "d74ae8cd-da1f-4882-b9b4-78577d194d21", "title": "Vicuna", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Vicuna"]], "content": "Vicuna (13B)~ is a language model trained by fine-tuning LLaMA (13B)~ on the conversational dataset generated by ChatGPT\\footnote{https://openai.com/blog/chatgpt}.\nThe authors gathered user-shared ChatGPT conversations from ShareGPT.com\\footnote{https://sharegpt.com/}, and got 70K conversation records after filtering out low-quality samples. \nLLaMA (13B)  was fine-tuned on the constructed conversation dataset using a  modified loss function  tailored to multi-turn conversations.\nTo better understand long context across multiple-turn dialog, \n the authors expanded the max context length from 512 to 2048. \nFor training, the authors adopted the gradient checkpointing and flash attention~ techniques to reduce the GPU memory cost in the fine-tuning process. The fine-tuning process takes 24 hours on an 8 $\\times$ 80GB A100 device with fully shared data parallelism.\nThe authors built a test set used exclusively to measure chatbots' performances. They collected a test set composed by 8 question categories, such as Fermi problems, role play scenarios, coding/math tasks, etc, and then asked GPT-4~ to rate models' responses considering helpfulness, relevance, accuracy, and detail. \nOn the constructed test set, \nVicuna (13B) outperforms Alpaca (13B)~ and LLaMA (13B) in 90\\% of the test questions, and generates equal or better rating responses compared to ChatGPT in 45\\% of the questions.", "cites": [1552, 9115, 7562], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of Vicuna, its dataset, training techniques, and performance evaluation. While it references relevant papers, it does not synthesize their ideas or connect them to broader themes in instruction tuning. There is limited critical evaluation or abstraction beyond specific details of the model."}}
{"id": "81e2e952-f801-4472-b1ca-921607b6189d", "title": "GPT-4-LLM", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "GPT-4-LLM"]], "content": "GPT-4-LLM (7B)~ is a language model trained by fine-tuning LLaMA (7B)~ on the GPT-4~ generated instruction dataset. \nGPT-4-LLM is initialized with LLaMA, then fine-tuned in the following two steps: \n(1) supervised fine-tuning on the constructed instruction dataset. The authors used the instructions from Alpaca~, and then collected  responses  using GPT-4.  LLaMA is  fine-tuned on the GPT-4 generated dataset. The fine-tuning process takes approximately three hours on an 8*80GB A100 machine with mixed precision and fully shared data parallelism.\n(2) optimizing the step-1 model  using the proximal policy optimization (PPO)~ method, the authors first built a comparison dataset by collecting responses from GPT-4, InstructGPT~, and OPT-IML~  to a collection of instructions and then asked GPT-4 to rate each response from 1 to 10. Using the ratings, a reward model is trained based on OPT~. The fine-tuned model from Step 1 is optimized by using the reward model to compute the policy gradient.\nFor evaluations, GPT-4-LLM (7B) outperforms not only the baseline model Alpaca (7B), but also larger models including Alpaca (13B) and LLAMA (13B). \nFor automated evaluation, GPT-4-LLM (7B) outperforms Alpaca by 0.2, 0.5, and 0.7 on User-Oriented-Instructions-252~, Vicuna-Instructions~, and Unnatural Instructions~ datasets, respectively. For human evaluation, regarding aspects including helpfulness, honesty, and harmlessness, GPT-4-LLM outperforms Alpaca by 11.7, 20.9, and 28.6 respectively.", "cites": [2208, 1552, 9115, 2216, 7463, 2215, 364, 2219], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the GPT-4-LLM training process and evaluation results, drawing from several cited papers. However, it lacks deeper synthesis of ideas or connections between the cited works. There is minimal critical evaluation of the methodology or results, and no abstraction to broader principles or trends in instruction tuning."}}
{"id": "2e4f83bd-2a41-46d2-b6a1-41ea5b09faac", "title": "Claude", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Claude"]], "content": "Claude\\footnote{https://www.anthropic.com/index/introducing-claude} is a language model trained by fine-tuning the pre-trained language model on an instruction dataset, aiming to generate helpful and harmless responses.\nThe fine-tuning process consists of two stages:\n(1) supervised fine-tuning on the instruction dataset. The authors created an instruction dataset by collecting 52K different instructions, paired with responses generated by GPT-4. \nThe fine-tuning process takes approximately eight hours on an 8-card 80GB A100 machine with mixed precision and fully shared data parallelism. \n(2) optimizing the step-1 model with the proximal policy optimization~ method. \nThe authors first built a comparison dataset by collecting responses from multiple large language models (e.g., GPT-3~) to the given collection of instructions and then asking GPT-4~ to rate each response. Using the ratings, a reward model is trained. Then, the fine-tuned model from Step 1 is optimized \nusing the reward model\nwith the proximal policy optimization method.\nClaude generates more helpful and harmless responses compared to the backbone model. For automatic evaluations, Claude outperforms GPT-3 by 7\\% on the RealToxicityPrompts~ in terms of toxicity. For human evaluations, regarding four different aspects, including following correct instructions, following explicit constraints, fewer hallucinations, and generating appropriate responses, Claude outperforms GPT-3~ +10\\%, +20\\%, -20\\%, and +10\\%. respectively.", "cites": [679, 2218, 9115, 2219], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the Claude model's instruction tuning process, citing relevant papers for methodology and evaluation. It integrates some information from the cited works, particularly the use of GPT-4 for data generation and PPO for optimization, but does not deeply synthesize or connect ideas across papers. There is limited critical analysis and no abstraction to broader principles or trends in the field."}}
{"id": "2194e2af-5d30-4851-a70c-6d25b364ae69", "title": "WizardLM", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "WizardLM"]], "content": "WizardLM (7B)~ is a language model trained by fine-tuning LLaMA (7B)~ on the instruction dataset Evol-Instruct  generated by ChatGPT (details see Section~\\ref{data:evol-instruct}).\nIt is\n fine-tuned on a subset (with 70K) of Evol-Instruct to enable a fair comparison with Vicuna~. The fine-tuning process takes approximately 70 hours on 3 epochs based on an 8 V100 GPU with the Deepspeed Zero-3~ technique. During inference, the max generation length is 2048.  \nTo evaluate LLMs' performances on complex instructions, the authors collected 218 human-generated instructions from real scenarios (e.g., open-source projects, platforms, and forums), called Evol-Instruct testset. \nEvaluations are conducted on the Evol-Instruct testset and Vicuna’s testset. For human evaluation, WizardLM outperforms Alpaca (7B)~ and Vicuna (7B) by a large margins, and generates equal or better responses on 67\\% test samples compared to ChatGPT.  \nAutomatic evaluation is conducted by asking GPT-4 to rate LLMs' reponses. \nSpecifically, WizardLM gains performance boosts compared to Alpaca by +6.2\\%, +5.3\\% on the Evol-Instruct testset and Vicuna’s test sets. WizardLM achieves outperforms Vicuna by +5.8 on the Evol-Instruct testset and +1.7\\% on the Vicuna’s test set.", "cites": [1552, 2201], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.8, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a factual overview of the WizardLM model and its training methodology, integrating details from the cited paper on Evol-Instruct and LLaMA. It includes comparisons with Alpaca and Vicuna using both human and automatic evaluation metrics, showing some critical perspective by highlighting performance margins. However, it lacks deeper synthesis or abstraction to position WizardLM within broader trends or limitations in the field."}}
{"id": "5325953f-a9f7-4372-85d7-3fcfce48c86a", "title": "ChatGLM2", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "ChatGLM2"]], "content": "ChatGLM2 (6B)~ is a language model trained by fine-tuning GLM (6B)~ on  a  bilingual dataset that contains both English and Chinese instructions The bilingual instruction dataset contains 1.4T tokens, with a 1:1 ratio of Chinese to English. Instructions in the dataset are sampled from the question-answering and dialogue completion tasks.\nChatGLM is initialized with GLM, then trained by the three-step fine-tuning strategy, which is akin to InstructGPT~. To better model contextual information across multi-turn conversations, the authors expanded the maximum context length from 1024 to 32K. To reduce GPU memory cost in the fine-tuning stage, the authors employed multi-query attention and causal mask  strategies. During inference, ChatGLM2 requires 13GB GPU memory with FP16 and supports conversations up to 8K in length with 6GB GPU memory using the INT4 model quantization technique. \nEvaluations are conducted on four English and Chinese benchmarks, including MMLU (English)~, C-Eval (Chinese)~, GSM8K (Math)~, and BBH (English)~. ChatGLM2 (6B) outperforms GLM (6B) and the baseline model ChatGLM (6B) on all benchmarks. Specifically, ChatGLM2 outperforms GLM by +3.1 on MMLU, +5.0 on C-Eval, +8.6 on GSM8K, and +2.2 on BBH. ChatGLM2 achieves better performances than ChatGLM by +2.1, +1.2, +0.4, +0.8 on MMLU, C-Eval, GSM8K and BBH, respectively.", "cites": [2212, 440, 2221, 8535, 458, 364], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the ChatGLM2 model, its training methodology, and benchmark results. While it cites relevant papers for benchmarks and prior models, it lacks synthesis of broader themes or integration of ideas from multiple works. There is minimal critical analysis or abstraction beyond the specific model and results."}}
{"id": "25d12ff0-59b3-4ee1-b115-e5ed94d7dc0e", "title": "LIMA", "level": "subsection", "subsections": [], "parent_id": "bf6afaab-1b9a-4c02-8950-44e1d4b0033e", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "LIMA"]], "content": "LIMA~(65B)~ is a large language model trained by fine-tuning LLaMA (65B)~ on an instruction dataset, which is constructed based on the proposed superficial alignment hypothesis. \nThe superficial alignment hypothesis refers to the idea that the knowledge and capabilities of a model are almost acquired in the pre-training stage, while the alignment training (e.g., instruction fine-tuning) teaches models to generate responses under user-preferred formalizations. Based on the superficial alignment hypothesis, the authors claimed that large language models can generate user-satisfied responses by fine-tuning it on a small fraction of instruction data. Therefore, the authors built instruction train/valid/test sets to verify this hypothesis. \nEvaluations are conducted on the constructed test set. \nFor human evaluations, LIMA outperforms InstructGPT and Alpaca by 17\\% and 19\\%, respectively. Additionally, LIMA achieves comparable results to BARD\\footnote{https://bard.google.com/}, Cladue\\footnote{https://www.anthropic.com/index/introducing-claude}, and GPT-4. For automatic evaluation, which is conducted by asking GPT-4 to rate responses and a higher rate score denotes better performance, LIMA outperforms InstructGPT and Alpaca by 20\\% and 36\\%, respectively, achieving comparable results to BARD, while underperforming Claude and GPT-4. Experimental results verify the proposed superficial alignment hypothesis.", "cites": [1552, 7558], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key idea from Paper 2 (LIMA) by explaining the superficial alignment hypothesis and connecting it to the model's performance. It provides a critical evaluation by comparing LIMA's results with InstructGPT, Alpaca, BARD, and GPT-4, highlighting both its strengths and limitations. While it identifies a general principle (that fine-tuning on small, curated datasets can be effective), the abstraction remains limited to the specific hypothesis rather than broader meta-level insights."}}
{"id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "title": "OPT-IML (175B)", "level": "paragraph", "subsections": ["152bc971-3700-4880-9af7-60301d0c5acf", "bee706bc-c8bc-4e4e-bc06-7d0275ffaaf4", "a53e8a32-b946-4eed-980f-6b8ebe4e93fb", "4a15d964-4e71-4169-a058-53e2f39d457c", "1187bab4-f96c-45c4-a7a5-b28ef58f7897", "c4ec0098-8a42-490d-9d3b-c5e12b03dc93", "4b9a006d-24fc-47a6-b699-8216d4f66bef", "d1c31b8d-4156-4049-9222-1b56e5700ef7", "cf0b9207-9239-4517-8bb5-423227e00339", "376d23c5-5fcc-45d1-a3ad-5c5c27a46214"], "parent_id": "8e40234a-275e-4f03-be82-8cfa29f296c8", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"]], "content": "~ is a large language model trained by fine-tuning the OPT (175B)~ model on the constructed Instruction Meta-Learning (IML) dataset, which consists of over 1500 NLP tasks from 8 publicly available benchmarks such as PromptSource~, FLAN~, and Super-NaturalInstructions~. After fine-tuning, OPT-IML outperforms OPT across all benchmarks.", "cites": [7463, 2198, 8464, 2223, 2215], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the OPT-IML (175B) model and the IML dataset but lacks synthesis of ideas across the cited papers. It does not offer critical analysis or evaluate the methodology of the work, nor does it abstract the findings to broader trends in instruction tuning. The content is minimal and descriptive in nature."}}
{"id": "152bc971-3700-4880-9af7-60301d0c5acf", "title": "Dolly 2.0 (12B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Dolly 2.0 (12B)"]], "content": "~ is initialized with the pre-trained language model Pythia (12B)~, and fine-tuned on the instruction dataset databricks-dolly-15k\\footnote{https://huggingface.co/datasets/databricks/databricks-dolly-15k}, which contains 7 categories of NLP tasks such as text classification and information extraction. After fine-tuning, Dolly 2.0 (12B) outperforms Pythia (12B) on the EleutherAI LLM Evaluation Harness benchmark~ by a large margin, and achieves comparable performances to GPT-NEOX (20B)~, which has two times more parameters compared to Dolly 2.0 (12B).", "cites": [2214, 2224], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual comparison between Dolly 2.0 and Pythia as well as GPT-NeoX-20B, citing performance results on a benchmark. However, it lacks deeper synthesis of ideas from the cited papers and offers limited abstraction or critical evaluation of the methodologies or limitations of instruction tuning in this context."}}
{"id": "bee706bc-c8bc-4e4e-bc06-7d0275ffaaf4", "title": "Falcon-Instruct (40B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Falcon-Instruct (40B)"]], "content": "~ is a large language model trained by fine-tuning Falcon (40B)~ on an English dialogue dataset, which contains 150 million tokens from the Baize dataset~, with an additional 5\\% of the data from the RefinedWeb dataset~. To reduce memory usage, the authors employed flash attention~ and multi-query techniques. For evaluation, Falcon-Instruct (40B) achieved better performance on the Open LLM Leaderboard~\\footnote{https://huggingface.co/spaces/HuggingFaceH4\\\\/open\\_llm\\_leaderboard} compared to the baseline model Falcon (40B), and outperforms the Guanaco (65B), which has more model parameters.", "cites": [2196, 7562], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief factual summary of Falcon-Instruct (40B), its training data, optimization techniques, and evaluation results. It integrates minimal information from the cited papers and does not offer in-depth analysis or comparison of the methods. The content lacks abstraction or broader conceptual framing, focusing instead on specific model details."}}
{"id": "a53e8a32-b946-4eed-980f-6b8ebe4e93fb", "title": "Guanaco (7B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Guanaco (7B)"]], "content": "~ is a\nmulti-turn dialog \n language model  trained by fine-tuning LLaMA (7B)~  on the constructed multilingual dialogue dataset. The multilingual dialogue dataset comes from two sources:  Alpaca~, which contains 52K English instruction data pairs; and a multilingual (e.g., Simplified Chinese, Traditional Chinese, Japanese, German) dialogue data, which contains 534K+ multi-turn conversations. \nAfter fine-tuning, Guanaco \nis to generate role-specific responses and continuous responses on a given topic in multi-turn conversations.", "cites": [1552], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Guanaco (7B) model and its dataset sources but lacks deeper synthesis or integration with other works. It does not critically evaluate the model or its methodology, nor does it abstract broader patterns or principles in instruction tuning. The content is primarily factual and summary-based."}}
{"id": "4a15d964-4e71-4169-a058-53e2f39d457c", "title": "Minotaur (15B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Minotaur (15B)"]], "content": "is a large language model trained by fine-tuning the Starcoder Plus (15B)~ on open-source instruction datasets including WizardLM~ and GPTeacher-General-Instruct\\footnote{https://github.com/teknium1/GPTeacher}. For model inference, Minotaur supports a maximum context length of 18K tokens.", "cites": [7561, 2201], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, providing minimal synthesis by mentioning the training of Minotaur on datasets from two papers. It lacks critical analysis of these choices and does not contextualize them within broader trends or principles in instruction tuning. The content is focused on specific system details without generalization or deeper insight."}}
{"id": "1187bab4-f96c-45c4-a7a5-b28ef58f7897", "title": "Nous-Herme (13B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Nous-Herme (13B)"]], "content": "is a large language model trained by fine-tuning LLaMA (13B)~ on an instruction dataset, which contains over 300k instructions, sampled from  GPTeacher\\footnote{https://github.com/teknium1/GPTeacher}, CodeAlpaca~, GPT-4-LLM~, Unnatural Instructions~, and Biology\\/Physics\\/Chemistry subsets in the Camel-AI~. Responses are generated by GPT-4. For evaluations, Nous-Herme (13B) achieves comparable performances to GPT-3.5-turbo on multiple tasks like ARC challenge~ and BoolQ~.", "cites": [2208, 1552, 2216, 2225, 444], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the Nous-Herme (13B) model and its training setup without synthesizing deeper connections to the cited works. It mentions the model's performance but lacks critical evaluation or comparison of methods. The content remains at a factual level without abstracting broader patterns or principles."}}
{"id": "c4ec0098-8a42-490d-9d3b-c5e12b03dc93", "title": "TÜLU (6.7B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "TÜLU (6.7B)"]], "content": "~ is a large language model trained by fine-tuning OPT (6.7B)~ on a mixed instruction dataset, which contains FLAN V2~, CoT~, Dolly~, Open Assistant-1\\footnote{https://huggingface.co/datasets/OpenAssistant/oasst1}, GPT4-Alpaca\\footnote{https://huggingface.co/datasets/vicgalle/alpaca-gpt4}, Code-Alpaca~, and ShareGPT\\footnote{https://sharegpt.com/}. After fine-tuning, TÜLU (6.7B) reaches on average 83\\% of ChatGPT's performance and 68\\% of GPT-4's performance.", "cites": [2213, 7463, 1578, 8464], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of TÜLU (6.7B), including the base model and the datasets used for instruction tuning, as well as performance benchmarks. However, it lacks synthesis of the cited papers, critical evaluation of their methodologies or limitations, and abstraction to broader trends or principles in instruction tuning. It primarily serves as a summary of the model and its training data."}}
{"id": "4b9a006d-24fc-47a6-b699-8216d4f66bef", "title": "YuLan-Chat (13B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "YuLan-Chat (13B)"]], "content": "~ is a language model trained by fine-tuning LLaMA (13B)~ on a constructed bilingual dataset, which contains 250,000 Chinese-English instruction pairs. After fine-tuning, YuLan-Chat-13B achieves comparable results to the state-of-the-art open-source model ChatGLM (6B)~, and outperforms Vicuna (13B)~ on the English BBH3K (BBH3K is a subset of BBH benchmark~) dataset.", "cites": [2212, 1552, 7466], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual comparison of YuLan-Chat (13B) with other models like ChatGLM (6B) and Vicuna (13B) on specific benchmarks, but it does not synthesize or contextualize the cited papers (e.g., LLaMA, BBH) in a broader framework. There is minimal critical analysis or abstraction to general principles; the content remains descriptive with limited insight into the implications or limitations of the findings."}}
{"id": "cf0b9207-9239-4517-8bb5-423227e00339", "title": "Airoboros (13B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "Airoboros (13B)"]], "content": "\\footnote{https://github.com/jondurbin/airoboros} is a large language model trained by fine-tuning LLAMA (13B)~ on the Self-instruct dataset~. After fine-tuning, Airoboros significantly outperforms LLAMA (13B)~ on all benchmarks and achieves highly comparable results to models fine-tuned specifically for certain benchmarks.", "cites": [1552], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal description of Airoboros, mentioning its training method and performance relative to LLaMA (13B). It lacks synthesis of ideas from multiple sources, offers no critical evaluation of the model or the dataset used, and fails to abstract findings into broader patterns or principles. The content is largely a factual summary with limited insight."}}
{"id": "376d23c5-5fcc-45d1-a3ad-5c5c27a46214", "title": "UltraLM (13B)", "level": "paragraph", "subsections": [], "parent_id": "9dd44725-3a4d-4511-b1fc-bf6b56c97e30", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Instruction Fine-tuned LLMs"], ["subsection", "Others"], ["paragraph", "OPT-IML (175B)"], ["paragraph", "UltraLM (13B)"]], "content": "~ is a large language model trained by  fine-tuning LLAMA (13B)~. For evaluation, UltraLM (13B) outperforms Dolly (12B)~ and achieves the winning rate up to 98\\%. Additionally, it surpasses the previous best open-source models (i.e., Vicuna~ and WizardLM~) with winning rates of 9\\% and 28\\%, respectively.", "cites": [1552, 2201, 2211], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual comparison of UltraLM (13B) against other models like Dolly, Vicuna, and WizardLM, but lacks deeper synthesis of the methodologies or goals outlined in the cited papers. It briefly mentions performance metrics, which constitutes minimal critical evaluation, but does not explore underlying reasons for success or limitations. There is no abstraction or generalization of broader trends in instruction tuning."}}
{"id": "86fe14c8-78df-4038-bd1e-a9096f7b56fd", "title": "Multi-modality Datasets", "level": "subsection", "subsections": ["e54e210a-3dc8-40bf-8d4a-a3eaa6723601"], "parent_id": "f4dad6fa-5589-4947-aa11-f4859547ea0c", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"]], "content": "\\begin{table*}[t]\n\\centering\n\\small\n\\scalebox{0.85}{\n\\begin{threeparttable}\n\\begin{tabular}{llll}\n\\toprule\n\\multirow{2}{*}{\\bf Multi-modality Instruction Fine-tuning Dataset} &  \\multicolumn{2}{c}{\\bf Modalities} & \\multirow{2}{*}{\\bf \\# Tasks} \\\\\n& {\\bf Modality Pair} & {\\bf \\# Instance} & \\\\\\midrule\n{MUL-TIINSTRUCT}~\\tnotex{id:1} & Image-Text & 5k to 5M per task & 62 \\\\\n{PMC-VQA}~\\tnotex{id:2} & Image-Text & 227k & 2 \\\\\n\\multirow{2}{*}{{LAMM}~\\tnotex{id:3}} & Image-Text & 186k & 9 \\\\\n& Point Cloud-Text & 10k & 3 \\\\\n{Vision-Flan}~\\tnotex{id:4} & Multi-Pairs & Over 1M & 200+ \\\\\n{ALLAVA}~\\tnotex{id:5} & Image-Text & 1.4M & 2 \\\\\n{ShareGPT4V}~\\tnotex{id:6} & Image-Text & 1.2M & 2 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n}\n\\begin{multicols}{2}\n\\begin{tablenotes}\n\\item[1] \\label{id:1} {$^1$ https://github.com/VT-NLP/MultiInstruct} \n\\item[2] \\label{id:2} {$^2$ https://github.com/xiaoman-zhang/PMC-VQA}\n\\item[3] \\label{id:3} {$^3$ https://github.com/OpenLAMM/LAMM}\n\\item[4] \\label{id:4} {$^4$ https://vision-flan.github.io/}\n\\item[5] \\label{id:5} {$^5$ https://github.com/FreedomIntelligence/ALLaVA}\n\\item[6] \\label{id:6} {$^6$ https://sharegpt4v.github.io/}\n\\end{tablenotes}\n\\end{multicols}\n\\caption{An overview of multi-modality instruction fine-tuning datasets.}\n\\label{tab:mmllms_dataset_table}\n\\end{table*}", "cites": [2227, 7088, 7563, 2229, 2226, 2228], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily presents a list of multi-modality instruction fine-tuning datasets with basic tabular statistics, such as modality pairings and task counts, but lacks deeper synthesis of the underlying themes or comparative insights. There is minimal critical evaluation of the cited works, and no abstraction of broader trends or principles in multi-modality instruction tuning."}}
{"id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "title": "MUL-TIINSTRUCT", "level": "paragraph", "subsections": ["e395c52b-34a6-4fd5-8a29-35acda6e2d3e", "94b19c13-1a89-420f-8a4a-ebfcc70f9b35", "ccadff13-2ee2-49bc-9104-2acfc8fd1bba", "a2c85ac9-9c15-44ef-ac15-fe62892fb41b", "1363dbae-276f-4c85-888b-eccc183787f5"], "parent_id": "86fe14c8-78df-4038-bd1e-a9096f7b56fd", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"]], "content": "~ is a multimodal instruction tuning  dataset consisting of 62 diverse multimodal tasks in a unified seq-to-seq format. This dataset covers 10 broad categories and its tasks are derived from 21 existing open-sourced datasets. \nEach task is equipped with 5 expert-written instructions. For the existing tasks, the authors use the input/output pairs from their available open-source datasets to create instances. While for each new task, the authors create 5k to 5M instances by extracting the necessary information from instances of existing tasks or reformulating them. \nThe MUL-TIINSTRUCT dataset has demonstrated its efficiency in enhancing various transfer learning technique. \nFor example, fine-tuning the OFA model (930M)~ \nusing various transfer learning strategies such as Mixed Instruction Tuning and Sequential Instruction Tuning \non  MUL-TIINSTRUCT improve the zero-shot performance  across all unseen tasks. On commonsense VQA task, OFA fine-tuned on MUL-TIINSTRUCT achieves 50.60 on RougeL and 31.17 on accuracy, while original OFA achieves 14.97 on RougeL and 0.40 on accuracy.", "cites": [2229], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of the MUL-TIINSTRUCT dataset and its characteristics, including task coverage, dataset construction, and performance improvements on a specific model. However, it lacks synthesis by not connecting this dataset to broader trends in instruction tuning or comparing it with others. There is minimal critical analysis or abstraction, as it primarily describes the dataset and results without deeper evaluation or generalization."}}
{"id": "e395c52b-34a6-4fd5-8a29-35acda6e2d3e", "title": "PMC-VQA", "level": "paragraph", "subsections": [], "parent_id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"], ["paragraph", "PMC-VQA"]], "content": "~ is a large-scale medical visual question-answering dataset that comprises 227k image-question pairs of 149k images, covering various modalities or diseases. The dataset can be used for both open-ended and multiple-choice tasks. The pipeline for generating the PMC-VQA dataset involves collecting image-caption pairs from the PMC-OA~ dataset, using ChatGPT to generate question-answer pairs, and manually verifying a subset of the dataset for quality. The authors propose a generative-based model MedVInT for medical visual understanding by aligning visual information with a large language model. MedVInT pretrained on PMC-VQA achieves state-of-the-art performance and outperforms existing models on VQA-RAD~ and SLAKE~ benchmarks, with 81.6\\% accuracy on VQA-RAD and 88.0\\% accuracy on SLAKE.", "cites": [2230, 2231, 2228], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the PMC-VQA dataset and the MedVInT model, citing relevant papers for details. While it connects the dataset to the broader context of medical VQA, it lacks deeper synthesis of the cited works or broader conceptual integration. There is minimal critical analysis or abstraction beyond the specific dataset and model described."}}
{"id": "94b19c13-1a89-420f-8a4a-ebfcc70f9b35", "title": "LAMM", "level": "paragraph", "subsections": [], "parent_id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"], ["paragraph", "LAMM"]], "content": "~ is a comprehensive multi-modal instruction tuning dataset for 2D image and 3D point cloud understanding. LAMM contains 186K language-image instruction-response pairs, and 10K language-point cloud instruction-response pairs. The authors collect images and point clouds from publicly available datasets and use the GPT-API and self-instruction methods to generate instructions and responses based on the original labels from these datasets. LAMM-Dataset includes data pairs for commonsense knowledge question answering by incorporating a hierarchical knowledge graph label system from the Bamboo~ dataset and the corresponding Wikipedia description. The authors also propose the LAMM-Benchmark, which evaluates existing multi-modal language models~(MLLM) on various computer vision tasks. It includes 9 common image tasks and 3 common point cloud tasks, and LAMM-Framework, a primary MLLM training framework that differentiates the encoder, projector, and LLM finetuning blocks for different modalities to avoid modality conflicts.", "cites": [9119, 7088], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of the LAMM dataset and its components, such as the use of GPT-API and self-instruction methods for instruction generation. It briefly connects to the Bamboo dataset in describing the knowledge graph label system, but does not deeply integrate ideas or provide a broader synthesis. There is minimal critical analysis or identification of overarching patterns or principles in multi-modal instruction tuning."}}
{"id": "ccadff13-2ee2-49bc-9104-2acfc8fd1bba", "title": "Vision-Flan", "level": "paragraph", "subsections": [], "parent_id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"], ["paragraph", "Vision-Flan"]], "content": "~ is the largest public-available human-annotated visual instruction tuning dataset that consists of 1,664,261 instances and 200+ diverse vision-language tasks derived from 101 open-source computer vision datasets. Each task is accompanied by expertly written instructions and meticulously crafted templates for inputs and outputs. The dataset covers a broad spectrum of tasks, including image captioning, visual question-answering, and visual comprehension. Designed to enhance research and application in vision-language model domains, Vision-Flan aims to expand the horizons of interaction and comprehension between visual and linguistic modalities. It provides researchers and developers with a valuable resource to push the envelope of vision-language models and to innovate algorithms across a diverse array of fields.", "cites": [2227], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes Vision-Flan and its characteristics without synthesizing it with other multi-modality datasets or critically analyzing its strengths and limitations. It provides factual information about the dataset but does not offer comparative insights, broader trends, or meta-level generalizations."}}
{"id": "a2c85ac9-9c15-44ef-ac15-fe62892fb41b", "title": "ALLaVA", "level": "paragraph", "subsections": [], "parent_id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"], ["paragraph", "ALLaVA"]], "content": " represents an open-source, extensive dataset tailored for fine-tuning visual question-answering models, featuring 1.4M entries that include detailed captions, intricate instructions, and comprehensive answers produced by GPT-4V . To craft high-quality captions and visual question-answers,  introduced a method to distill both a caption and a QA pair for an image in a single interaction. This process involves initially presenting GPT-4V  with an image, followed by prompting it to generate both a detailed caption and a visual question-answer pair. This approach of incorporating additional visual data enables the model to develop a more nuanced understanding of both the visual and textual elements, enhancing its capacity to deliver precise and contextually appropriate answers. Furthermore, this method has the potential to reduce the occurrence of hallucinations by providing the model with more contextual information (visual data).", "cites": [7563, 2232], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the ALLaVA dataset and its methodology, primarily summarizing the approach from the cited paper (doc_id: 7563). It integrates this with a mention of GPT-4V's capabilities from the second paper (doc_id: 2232), but the synthesis remains limited to surface-level connections. There is no significant critical evaluation or abstraction to broader principles in instruction tuning for multi-modal models."}}
{"id": "1363dbae-276f-4c85-888b-eccc183787f5", "title": "ShareGPT4V", "level": "paragraph", "subsections": [], "parent_id": "e54e210a-3dc8-40bf-8d4a-a3eaa6723601", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Datasets"], ["paragraph", "MUL-TIINSTRUCT"], ["paragraph", "ShareGPT4V"]], "content": " is a collection of highly descriptive image-text pairs, consisting of two components: 100K captions generated by GPT4-Vision  from a variety of images, and 1.2M captions developed using their pre-trained model, which was trained on the initial set of 100K high-quality captions. These captions comprehensively cover aspects such as global knowledge, object attributes, spatial relationships, and aesthetic evaluations. Utilizing this dataset, the ShareGPT4V-7B model, once fine-tuned, surpasses competing 7B-scale LMMs across all 11 benchmark tests. Notably, it secures a remarkable cumulative score of 1943.8 on the MME benchmark, outperforming the second-place Qwen-VL-Chat-7B  model, which was trained with 1.4 billion samples, by 95.6 points.", "cites": [2233, 2232, 2226], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of the ShareGPT4V dataset and briefly compares its performance to the Qwen-VL-Chat-7B model. While it integrates some details about the dataset's construction and purpose, it lacks deeper synthesis of ideas across the three cited papers. The comparison is useful but limited in scope, and there is minimal abstraction or generalization to broader trends or principles in multi-modal instruction tuning."}}
{"id": "580be41b-835c-4712-9f81-e74e05f1a7db", "title": "Multi-modality Instruction Fine-tuning Models", "level": "subsection", "subsections": ["352d7034-dc22-4972-a7c9-d9d3f73d00af"], "parent_id": "f4dad6fa-5589-4947-aa11-f4859547ea0c", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"]], "content": "\\begin{table*}[t]\n\\centering\n\\small\n\\scalebox{0.85}{\n\\begin{threeparttable}\n\\begin{tabular}{lllllll}\n\\toprule\n{\\bf Multi-modality Instruction} & \\multirow{2}{*}{\\bf \\# Params} & \\multirow{2}{*}{\\bf Modality} & \\multicolumn{2}{c}{\\bf Base Model} & \\multicolumn{2}{c}{\\bf Fine-tuning Trainset}\\\\\n{\\bf Fine-tuned LLMs} & & & {\\bf Model Name} & {\\bf \\# Params} & {\\bf Self-build} & {\\bf Size} \\\\\\midrule\n{{InstructPix2Pix}~\\tnotex{id:1}} & 983M & I/T & Stable Diffusion & 983M & Yes & 450K \\\\\n\\midrule\n\\multirow{2}{*}{{LLaVA}~\\tnotex{id:2}} & \\multirow{2}{*}{13B} & \\multirow{2}{*}{I/T} & CLIP~ & 400M & \\multirow{2}{*}{Yes} & 158K \\\\\n& & & LLaMA~ & 7B & & \\\\\n& & & LLaMA~ & 7B & & \\\\\n\\midrule\n\\multirow{3}{*}{Video-LLaMA~\\tnotex{id:3}} & \\multirow{3}{*}{-} & \\multirow{3}{*}{I/T/V/A} & BLIP-2~ & - & \\multirow{3}{*}{No} & \\multirow{3}{*}{-} \\\\\n& & & ImageBind~ & - & & \\\\\n& & & Vicuna~ & 7B/13B & & \\\\\n\\midrule\n{InstructBLIP (1.2B)}~\\tnotex{id:4} & - & I/T/V & BLIP-2~ & - & No & - \\\\\n\\midrule\n{Otter}~\\tnotex{id:5} & - & I/T/V & OpenFlamingo~ & 9B & Yes & 2.8M \\\\\n\\midrule\n{MultiModal-GPT}~\\tnotex{id:6} & - & I/T/V & OpenFlamingo~ & 9B & No & - \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n}\n\\begin{multicols}{2}\n\\begin{tablenotes}\n\\item[1] \\label{id:1} {$^1$ https://github.com/timothybrooks/instruct-pix2pix} \n\\item[2] \\label{id:2} {$^2$ https://github.com/haotian-liu/LLaVA}\n\\item[3] \\label{id:3} {$^3$ https://github.com/DAMO-NLP-SG/Video-LLaMA}\n\\item[4] \\label{id:4} {$^4$ https://github.com/salesforce/LAVIS/tree/main/projects/instructblip}\n\\item[5] \\label{id:5} {$^5$ https://github.com/Luodian/Otter}\n\\item[6] \\label{id:6} {$^6$ https://github.com/open-mmlab/Multimodal-GPT}\n\\end{tablenotes}\n\\end{multicols}\n\\caption{An overview of multi-modality instruction fine-tuned LLMs. I/T/V/A stand for Image/Text/Video/Audio}\n\\label{tab:mmllms_model_table}\n\\end{table*}", "cites": [1552, 7564, 2237, 1639, 2235, 2234, 2236, 2238, 2239], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily presents a table describing various multi-modality instruction fine-tuned LLMs and their characteristics, such as parameters, modalities, and training data. While it attempts to organize the information into a structured overview, it lacks deeper synthesis of ideas, critical evaluation of methods, and abstraction to broader principles or trends. The narrative remains descriptive and limited in analytical depth."}}
{"id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "title": "InstructPix2Pix (983M)", "level": "paragraph", "subsections": ["0c88ca38-95df-4646-b501-8ac3d5a0705c", "3cbf969f-81fa-48e0-b64a-8c502bd79ce3", "2c1ddb3b-4574-4d7e-93c6-15fff98989cd", "5b6b5db2-b68e-4243-88d5-15d15d8f6526", "5bc9c6a6-2a68-4b7f-95c0-44151e888a5e"], "parent_id": "580be41b-835c-4712-9f81-e74e05f1a7db", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"]], "content": "~ is a conditional diffusion model trained by fine-tuning Stable Diffusion (983M)~ on a constructed multi-modal dataset that contains more than 450K text editing instructions and corresponding images before and after the edit. The authors combine the abilities of two large-scale pre-trained models, a language model GPT-3~ and a text-to-image model Stable Diffusion~, to generate the the training dataset. GPT-3 is fine-tuned to generate text edits based on image prompts, while Stable Diffusion is used to convert the generated text edits into actual image edits. InstructPix2Pix is then trained on this generated dataset using a latent diffusion objective. Figure~\\ref{fig:InstructPix2Pix} shows the process of generating image editing dataset and training the diffusion model on that dataset. The authors compares the proposed method qualitatively with previous works such as SDEdit~ and Text2Live~, highlighting the ability of the model to follow image editing instructions instead of descriptions of the image or edit layer. The authors also presents quantitative comparisons with SDEdit~ using metrics measuring image consistency and edit quality. \n\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/InstructPix2Pix.jpg}\n  \\end{minipage}\n  \\caption{Image editing dataset generation and diffusion model training. The figure is copied from .}\n  \\label{fig:InstructPix2Pix}\n\\end{figure}", "cites": [2240, 2235, 679, 2241], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the InstructPix2Pix model and its training process, drawing on the relevant paper. It mentions the use of GPT-3 and Stable Diffusion, and includes a qualitative and quantitative comparison with other methods, but lacks deeper synthesis of ideas or a broader analytical framework. The critical evaluation is minimal, and generalization beyond the specific case is limited."}}
{"id": "0c88ca38-95df-4646-b501-8ac3d5a0705c", "title": "LLaVA (13B)", "level": "paragraph", "subsections": [], "parent_id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"], ["paragraph", "LLaVA (13B)"]], "content": "~ is a large multimodal model developed by connecting the visual encoder of CLIP (400M)~ with the language decoder LLaMA (7B)~. LLaVA is fine-tuned using the generated instructional vision-language dataset consisted of 158K unique language-image instruction-following samples. The data collection process involved creating conversation, detailed description, and complex reasoning prompts. GPT-4 is used to convert image-text pairs into appropriate instruction-following format for this dataset. Visual features such as captions and bounding boxes were used to encode images. LLaVA yields a 85.1\\% relative score compared with GPT-4 on a synthetic multimodal instruction following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\\%.", "cites": [2238, 1552, 1639], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the LLaVA model and its components, citing the relevant papers. However, it lacks synthesis by not connecting LLaVA to broader themes in multi-modality instruction tuning. There is minimal critical analysis or abstraction, as it primarily summarizes the model's architecture, dataset, and performance without evaluating its implications, limitations, or generalizing to other works."}}
{"id": "3cbf969f-81fa-48e0-b64a-8c502bd79ce3", "title": "Video-LLaMA", "level": "paragraph", "subsections": [], "parent_id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"], ["paragraph", "Video-LLaMA"]], "content": "~ is a multimodal framework that enhances large language models with the ability to understand both visual and auditory content in videos. The architecture of Video-LLaMA consists of two branche encoders: the Vision-Language~(VL) Branch and the Audio-Language~(AL) Branch, and a language decoder (Vicuna (7B/13B)~, LLaMA (7B)~, etc.). \nThe VL Branch includes a frozen pre-trained image encoder (pre-trained vision component of BLIP-2~, which includes a ViT-G/14 and a pre-trained Q-former), a position embedding layer, a video Q-former and a linear layer. \nThe AL Branch includes a pre-trained audio encoder (ImageBind~) and an Audio Q-former. Figure~\\ref{fig:Video-LLaMA} shows the overall architecture of Video-LLaMA with Vision-Language Branch and Audio-Language Branch. \nThe VL Branch is trained on the Webvid-2M~ video caption dataset with a video-to-text generation task, and fine-tuned on the instruction-tuning data from MiniGPT-4~, LLaVA~ and VideoChat~. The AL Branch is trained on video/image instru-caption data to connect the output of ImageBind to language decoder. \nAfter finetuning, Video-LLaMA can perceive and comprehend video content, demonstrating its ability to integrate auditory and visual information, understand static images, recognize common-knowledge concepts, and capture temporal dynamics in videos. \n\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/Video-LLaMA.png}\n  \\end{minipage}\n  \\caption{Overall architecture of Video-LLaMA. The figure is copied from .}\n  \\label{fig:Video-LLaMA}\n\\end{figure}", "cites": [1552, 2243, 2242, 2234, 2236, 2238, 1639], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear, factual description of Video-LLaMA's architecture, training process, and capabilities. It synthesizes information from multiple cited papers but largely in a component-by-component manner without deeper integration or a broader narrative. There is minimal critical analysis or comparison to other methods, and no abstraction to highlight general principles or trends in multi-modality instruction tuning."}}
{"id": "2c1ddb3b-4574-4d7e-93c6-15fff98989cd", "title": "InstructBLIP (1.2B)", "level": "paragraph", "subsections": [], "parent_id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"], ["paragraph", "InstructBLIP (1.2B)"]], "content": "~ is a vision-language instruction tuning framework initialized with a pre-trained BLIP-2~) model consisting of an image encoder, an LLM (FlanT5 (3B/11B)~ or Vicuna (7B/13B)~), and a Query Transformer (Q-Former) to bridge the two. As shown in Figure \\ref{fig:InstructBLIP}, the Q-Former extracts instruction-aware visual features from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozen LLM. The authors evaluate the proposed InstructBLIP model on a variety of vision-language tasks, including image classification, image captioning, image question answering, and visual reasoning. They use 26 publicly available datasets, dividing them into 13 held-in and 13 held-out datasets for training and evaluation. The authors demonstrate that InstructBLIP achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks. InstructBLIP yields an average relative improvement of 15.0\\% when compared to BLIP-2, smallest InstructBLIP (4B) outperforms Flamingo (80B)~ on all six shared evaluation datasets with an average relative improvement of 24.8\\%.\n\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/InstructBLIP.jpg}\n  \\end{minipage}\n  \\caption{Overall architecture of InstructBLIP. The figure is copied from .}\n  \\label{fig:InstructBLIP}\n\\end{figure}", "cites": [7565, 7564, 7468], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the InstructBLIP model and its evaluation, and includes a comparison with Flamingo to highlight performance differences. While it integrates some information from the cited papers, it lacks deeper synthesis of broader themes or architectural innovations across the field. The critical analysis is limited to stating performance gains without discussing underlying reasons or limitations."}}
{"id": "5b6b5db2-b68e-4243-88d5-15d15d8f6526", "title": "Otter", "level": "paragraph", "subsections": [], "parent_id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"], ["paragraph", "Otter"]], "content": "~ is a multi-modal model trained by fine-tuning OpenFlamingo (9B)~, with the language and vision encoders frozen and only fine-tuning the Perceiver resampler module, cross-attention layers, and input/output embeddings. The authors organize diverse multi-modal tasks covering 11 categories and build multi-modal in-context instruction tuning datasets MIMIC-IT of 2.8M multimodal instruction-response pairs, which consists of image-instruction-answer triplets, where the instruction-answer is tailored to the image. Each data sample also includes context, which contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet. Otter demonstrates the ability to follow user instructions more accurately and provide more detailed descriptions of images compared to OpenFlamingo~.", "cites": [2237], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the Otter model and its training methodology, citing one paper directly. While it mentions the model's performance and the dataset used, it lacks deeper synthesis with other cited works, critical evaluation of limitations or trade-offs, and generalization to broader patterns or principles in multi-modal instruction tuning."}}
{"id": "5bc9c6a6-2a68-4b7f-95c0-44151e888a5e", "title": "MultiModal-GPT", "level": "paragraph", "subsections": [], "parent_id": "352d7034-dc22-4972-a7c9-d9d3f73d00af", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Multi-modality Instruction Fine-tuning"], ["subsection", "Multi-modality Instruction Fine-tuning Models"], ["paragraph", "InstructPix2Pix (983M)"], ["paragraph", "MultiModal-GPT"]], "content": "~ is a multi-modal instruction tuning model that is capable of following diverse instructions, generating detailed captions, counting specific objects, and addressing general inquiries. MultiModal-GPT is trained by fine-tuning OpenFlamingo (9B)~ on various created visual instruction data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue. The experiments demonstrate the proficiency of MultiModal-GPT in maintaining continuous dialogues with humans.", "cites": [2239], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of MultiModal-GPT and its training methodology, but it lacks synthesis with other works in the subsection. It offers minimal critical evaluation or comparison with InstructPix2Pix or other models. The content remains largely concrete and does not abstract broader patterns or principles from the cited paper."}}
{"id": "752ca6da-0ddd-4bd0-af86-34698d21d84e", "title": "Domain-specific Instruction Finetuning", "level": "section", "subsections": ["278c0cba-e46d-40b2-b114-f6eae8ac121f", "3df57471-df9a-44e0-8210-ba003787b11c", "7a40254f-7e85-41ab-9f4f-ba6b7fb2b368", "572b94c7-f7ac-44bd-8348-3a0bf9d7b5b0", "2afdf48e-4e19-40e3-bd68-177de8e02bcb", "e39799e7-752a-451c-9f4f-649eaa763b68", "79cbe6ae-783c-4646-94a4-06246ce95489", "a555e3c8-f262-4266-bf6e-31e557dc6102"], "parent_id": "cf81b618-29e1-4d2f-bbd6-13fc94283342", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"]], "content": "In this section, we describe instruction tuning in different domains and applications.\n\\begin{table*}[t]\n\\centering\n\\small\n\\scalebox{0.85}{\n\\begin{threeparttable}\n\\begin{tabular}{lllll}\n\\toprule\n \\multirow{1}{*}{\\bf Domain Type} &  {\\bf Domain-specific Instruction} & \\multicolumn{2}{c}{\\bf Base Model} & \\multirow{2}{*}{\\bf Trainset Size}\\\\\n& {\\bf Fine-tuned LLMs} & {\\bf Model Name} & {\\bf \\# Params} \\\\\\midrule\n \\multirow{1}{*}{Dialogue} & \\multirow{1}{*}{{InstructDial}~\\tnotex{id:1}}  & T0~ & 3B & \\multirow{2}{*}{-} \\\\\nClassification & {{LINGUIST}~}  & AlexaTM~ & 5B &  13K \\\\\nInformation extraction & {InstructUIE}~\\tnotex{id:2} & FlanT5~ & 11B & 1.0M \\\\\nSentiment analysis & IT-MTL~\\tnotex{id:3} &  T5~ & 220M & - \\\\\n\\midrule\n\\multirow{3}{*}{\\shortstack{ Writing}}   & {Writing-Alpaca-7B}~\\tnotex{id:4} & LLaMA~ & 7B & - \\\\\n& \\multirow{1}{*}\n{{CoEdIT}~\\tnotex{id:5}}  & FlanT5~ & 11B & \\\\\n& \\multirow{1}{*}\n{{CoPoet}~\\tnotex{id:6}}  & T5~ & 11B & \\\\\n\\midrule \n\\multirow{3}{*}{\\shortstack{ Medical}}   & {Radiology-GPT}~\\tnotex{id:7} & Alpaca~ & 7B &  122K \\\\\n & {ChatDoctor}~\\tnotex{id:8} & LLaMA~ & 7B &  100K \\\\\n & {ChatGLM-Med}~\\tnotex{id:9} & ChatGLM~ & 6B & - \\\\\n\\midrule\nArithmetic& {Goat}~\\tnotex{id:10}  & LLaMA~ & 7B & 1.0M \\\\\nCode & {WizardCoder}~\\tnotex{id:11}  & StarCoder~ & 15B &  78K \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n}\n\\begin{multicols}{2}\n\\begin{tablenotes}\n\\item[1] \\label{id:1} {$^1$ https://github.com/prakharguptaz/Instructdial} \n\\item[2] \\label{id:2} {$^2$ https://github.com/BeyonderXX/InstructUIE}\n\\item[3] \\label{id:3} {$^3$ https://github.com/amazon-science/instruction-tuning-for-absa}\n\\item[4] \\label{id:4} {$^4$ https://github.com/facebookresearch/EditEval}\n\\item[5] \\label{id:5} {$^5$ https://github.com/vipulraheja/coedit}\n\\item[6] \\label{id:6} {$^6$ https://github.com/vishakhpk/creative-instructions}\n\\item[7] \\label{id:7} {$^7$ https://huggingface.co/spaces/allen-eric/radiology-gpt}\n\\item[8] \\label{id:8} {$^8$ https://github.com/Kent0n-Li/ChatDoctor}\n\\item[9] \\label{id:9} {$^9$ https://github.com/SCIR-HI/Med-ChatGLM}\n\\item[10] \\label{id:10} {$^{10}$ https://github.com/liutiedong/goat}\n\\item[11] \\label{id:11} {$^{11}$ https://github.com/nlpxucan/WizardLM}\n\\end{tablenotes}\n\\end{multicols}\n\\caption{An overview of domain-specific instruction fine-tuned LLMs. }\n\\label{tab:mmllms_model_table}\n\\end{table*}\n\\label{application}", "cites": [2245, 2212, 2248, 9, 7566, 1558, 8469, 7468, 2247, 2249, 7561, 1552, 2206, 2246, 2244], "cite_extract_rate": 0.75, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily presents a list of domain-specific instruction-tuned models in a tabular format with minimal explanation or analysis. While it organizes the information by domain, it lacks synthesis of key ideas or trends across these works, and does not critically evaluate the approaches, their limitations, or their effectiveness. The abstraction level is low, as it focuses on concrete model names, parameters, and dataset sizes without drawing broader conclusions or insights."}}
{"id": "9a1b46d0-4200-4580-9290-bb31dac0f4bf", "title": "InstructDial", "level": "paragraph", "subsections": [], "parent_id": "278c0cba-e46d-40b2-b114-f6eae8ac121f", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Dialogue"], ["paragraph", "InstructDial"]], "content": "~ is an instruction tuning framework designed for dialogue. It contains a collection of 48 dialogue tasks in a consistent text-to-text format created from 59 dialogue datasets. Each task instance includes a task description, instance inputs, constraints, instructions, and output. To ensure adherence to instructions, the framework introduces two  meta-tasks: (1) an instruction selection task, where the model selects the instruction corresponding to a given input-output pair; \nand (2) an instruction binary task, where the model predicts \"yes\" or \"no\" if an instruction leads to a given output from an input. Two base models T0-3B~ (3B parameters version of T5~) and BART0~ (406M parameters based on Bart-large~) are fine-tuned on the tasks from InstructDial. InstructDial achieves impressive results on unseen dialogue datasets and tasks, including dialogue evaluation and intent detection. Moreover, it delivers even better results when applied to a few-shot setting.", "cites": [2246, 2250, 8536, 8469], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the InstructDial framework and its components, largely summarizing the paper's contributions and results. While it mentions how InstructDial relates to instruction tuning, it does not synthesize or connect it deeply with the other cited works. There is minimal critical evaluation or abstraction of broader principles or trends in the field."}}
{"id": "1b519668-4b5c-42e6-b4f5-8e7e784f6429", "title": "LINGUIST", "level": "paragraph", "subsections": [], "parent_id": "3df57471-df9a-44e0-8210-ba003787b11c", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Intent Classification and Slot Tagging"], ["paragraph", "LINGUIST"]], "content": "~ finetunes AlexaTM 5B~, a 5-billion-parameter multilingual model, on the instruction dataset for intent classification and slot tagging tasks. Each instruction consists of five blocks: (i) the language of the generated output, (ii) intention, (iii) slot types and values to include in the output (e.g., the number 3 in [3, snow] corresponds the slot type, and snow is the value used for that slot), (iv) a mapping from slot type labels to numbers, and (v) up to 10 examples to instruct the format of the outputs.  LINGUIST  shows significant improvements over state-of-the-art approaches in a 10-shot novel intent setting using the SNIPS dataset~. In the zero-shot cross-lingual setting of the mATIS++ dataset~, LINGUIST surpasses a strong baseline of Machine Translation with Slot Alignment across 6 languages while maintaining intent classification performance.\n\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/InstructUIE.png}\n  \\end{minipage}\n  \\caption{The overview framework of InstructUIE. The figure is copied from .}\n  \\label{fig:InstructUIE}\n\\end{figure}", "cites": [2245, 1558, 2251], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the LINGUIST approach, including its components and performance results, but lacks meaningful synthesis across the cited papers. There is minimal critical analysis of the methodology or limitations, and no abstraction to broader principles or trends in instruction tuning for domain-specific tasks."}}
{"id": "bca57c84-ca3c-4e91-b19a-3c7f668b8888", "title": "InstructUIE", "level": "paragraph", "subsections": [], "parent_id": "7a40254f-7e85-41ab-9f4f-ba6b7fb2b368", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Information Extraction"], ["paragraph", "InstructUIE"]], "content": "~ is a unified information extraction (IE) framework based on instruction tuning, which transforms IE tasks to the seq2seq format and solves them by fine-tuning 11B FlanT5~ on the constructed IT dataset. Figure~\\ref{fig:InstructUIE} shows the overall architecture of InstructUIE. It introduces IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Each task instance is delineated by four properties: task instruction, options, text, and output. Task instruction contains information such as the type of information to be extracted, the output structure format, and additional constraints or rules that need to be adhered to during the extraction process. Options refer to the output label constraints of a task.\nText refers to the input sentence. Output is the sentence obtained by  converting the original tags of the sample (e.g. \"entity tag: entity span\" for NER). In the supervised setting, InstructUIE performs comparably to BERT~ and outperforms the state-of-the-art and GPT3.5~ in zero-shot settings.", "cites": [2245, 679, 7468, 7], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of InstructUIE, outlining its framework, dataset construction, and performance. While it references multiple papers, it does not synthesize their ideas into a broader narrative. There is minimal critical evaluation or abstraction to identify overarching principles or trends in instruction tuning for IE."}}
{"id": "1bfd0e82-6219-460e-835c-33320930bac1", "title": "\\citet{Varia2022InstructionTF", "level": "paragraph", "subsections": [], "parent_id": "572b94c7-f7ac-44bd-8348-3a0bf9d7b5b0", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Aspect-based Sentiment Analysis"], ["paragraph", "\\citet{Varia2022InstructionTF"]], "content": "} propose a unified instruction tuning framework for solving Aspect-based Sentiment Analysis (ABSA) task based on a fine-tuned T5 (220M)~ model. The framework addresses multiple factorized sub-tasks that involve the four elements of ABSA, namely Aspect Term, Aspect Category, Opinion Term, and Sentiment. It treats these sub-tasks as a combination of five Question Answering (QA) tasks by transforming each sentence in the corpus using instruction templates provided for each task. For instance, one of the instruction templates used is \"What are the aspect terms in the text: \\$TEXT?\". The framework showcases substantial improvement (8.29 F1 on average) over the state-of-the-art in few-shot learning scenarios and remains comparable in full fine-tuning scenarios.", "cites": [9], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.0, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a basic description of the \\citet{Varia2022InstructionTF} framework for ABSA using T5, including its methodology and performance. It lacks synthesis with other works, does not critically evaluate the approach or its limitations, and does not generalize to broader patterns or principles in instruction tuning. The content is primarily"}}
{"id": "66253214-3f49-4c95-8769-d9b0270ea127", "title": "\\citet{Zhang2023MultiTaskIT", "level": "paragraph", "subsections": ["844e6281-b01b-41d7-a8b5-520c27c24773", "b7eb97f8-bb38-4f26-bc50-a5d572a3e5af"], "parent_id": "2afdf48e-4e19-40e3-bd68-177de8e02bcb", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Writing"], ["paragraph", "\\citet{Zhang2023MultiTaskIT"]], "content": "} propose Writing-Alpaca-7B that fine-tunes LLaMa-7B~ on the writing  instruction dataset to provide writing assistance. The proposed instruction dataset is an extension of the EDITEVAL~ benchmark based on instructional data, with the Updating task removed and a task for grammaticality introduced. The instruction scheme strictly follows the one in the Stanford Alpaca project~, comprising a universal preface, an instruction field to guide task completion, an input field that provides the text to be edited, and a response field that requires models to fill out. The Writing-Alpaca-7B improves upon LLaMa’s performance on all writing tasks and outperforms other larger off-the-shelf LLMs.\n\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/COEDIT.jpg}\n  \\end{minipage}\n  \\caption{The overview framework of COEDIT. The figure is copied from .}\n  \\label{fig:COEDIT}\n\\end{figure}", "cites": [2249, 2216, 2252], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of the Writing-Alpaca-7B model and its associated dataset, but does not effectively synthesize or connect it to the other cited works on instruction tuning. There is minimal critical analysis or evaluation of the paper’s approach, and no abstraction or identification of broader trends or principles in the field of writing-specific instruction tuning."}}
{"id": "844e6281-b01b-41d7-a8b5-520c27c24773", "title": "CoEdIT", "level": "paragraph", "subsections": [], "parent_id": "66253214-3f49-4c95-8769-d9b0270ea127", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Writing"], ["paragraph", "\\citet{Zhang2023MultiTaskIT"], ["paragraph", "CoEdIT"]], "content": "~ finetunes FLANT5~ (770M parameters, 3B parameters, and 11B parameters) on the instruction dataset for text editing to provide writing assistance. The instruction dataset comprises approximately 82K <instruction: source, target> pairs. As shown in Figure~\\ref{fig:COEDIT}, the model takes instructions from the user specifying the characteristics of the desired text, such as \"Make the sentence simpler\", and outputs the edited text. CoEdIT achieves state-of-the-art performance on several text editing tasks, including grammatical error correction, text simplification, iterative text editing, and three stylistic editing tasks: formality style transfer, neutralization, and paraphrasing. Furthermore, it can generalize well to new, adjacent tasks not seen during fine-tuning.", "cites": [2249, 7468], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of CoEdIT and its performance without connecting it meaningfully to other works or concepts in instruction tuning. It lacks critical evaluation or discussion of limitations, and does not abstract to broader trends or principles in the field."}}
{"id": "b7eb97f8-bb38-4f26-bc50-a5d572a3e5af", "title": "CoPoet", "level": "paragraph", "subsections": [], "parent_id": "66253214-3f49-4c95-8769-d9b0270ea127", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Writing"], ["paragraph", "\\citet{Zhang2023MultiTaskIT"], ["paragraph", "CoPoet"]], "content": "~ is a collaborative poetry writing tool that utilizes a large language model (e.g. T5-3B, T5-11B and T0-3B models) trained on a diverse collection of instructions for poetry writing. Each sample in the instruction dataset includes an <instruction, poem\\_line> pair. There are three major types of instructions: Continuation, Lexical Constraints, and Rhetorical Techniques. The CoPoet is guided by user instructions that specify desired attributes of the poetry, such as writing a sentence about \"love\" or ending a sentence with \"fly.\" Not only is the system competitive with publicly available LLMs trained on instructions, such as InstructGPT~, but it is also capable of satisfying unseen compositional instructions.", "cites": [364, 7566], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes CoPoet and its instruction-based poetry writing approach, drawing from the cited papers but primarily offering a factual summary rather than deep integration or analysis. It makes minimal effort to synthesize ideas across sources or generalize to broader principles, and there is no critical evaluation of the system or its methodology."}}
{"id": "6d5cc35d-0c1e-4ebf-a7fb-758de4b94c91", "title": "Radiology-GPT", "level": "paragraph", "subsections": ["2525c7b2-31df-47ed-8fc5-39539d260df1", "9506fd1a-3b12-4640-b894-2a74f84eb998"], "parent_id": "e39799e7-752a-451c-9f4f-649eaa763b68", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Medical"], ["paragraph", "Radiology-GPT"]], "content": "~ is a fine-tuned Alpaca-7B~ model for radiology, which utilizes an instruction tuning approach on an extensive dataset of radiology domain knowledge. Radiology reports usually include two corresponding sections: \"Findings\" and \"Impression\". The \"Findings\" section contains detailed observations from the radiology images, while the \"Impression\" section summarizes the interpretations drawn from those observations. Radiology-GPT provides a brief instruction to the \"Findings\" text: \"Derive the impression from findings in the radiology report\". The \"Impression\" text from the same report serves as the target output. In comparison to general language models such as StableLM~, Dolly~, and LLaMA~, Radiology-GPT demonstrates significant adaptability in radiological diagnosis, research, and communication.", "cites": [1552, 2248], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of Radiology-GPT and its method, citing its use of instruction tuning on radiology domain knowledge. It mentions the model's adaptability compared to general models but does not synthesize ideas across sources, offer critical evaluation of the methodology or limitations, or abstract broader principles from the cited work. The narrative remains surface-level and descriptive."}}
{"id": "2525c7b2-31df-47ed-8fc5-39539d260df1", "title": "ChatDoctor", "level": "paragraph", "subsections": [], "parent_id": "6d5cc35d-0c1e-4ebf-a7fb-758de4b94c91", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Medical"], ["paragraph", "Radiology-GPT"], ["paragraph", "ChatDoctor"]], "content": "~ is based on the fine-tuned LLaMa-7B~ model, utilizing the alpaca instruction dataset~ and the HealthCareMagic100k patient-doctor dialogue dataset. And prompt templates are designed for retrieving external knowledge databases, such as the Disease Database and Wikipedia retrieval, during doctor-patient conversations to obtain more accurate outputs from the model. The ChatDoctor significantly improves the model's ability to comprehend patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, the accuracy of its responses is substantially improved.", "cites": [1552], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive summary of the ChatDoctor system, citing the LLaMA paper but not engaging deeply with it or contrasting it with other approaches. It lacks synthesis of ideas, critical evaluation of limitations, or broader abstraction of principles. The focus is on surface-level features such as model size and dataset usage."}}
{"id": "9506fd1a-3b12-4640-b894-2a74f84eb998", "title": "ChatGLM-Med", "level": "paragraph", "subsections": [], "parent_id": "6d5cc35d-0c1e-4ebf-a7fb-758de4b94c91", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Medical"], ["paragraph", "Radiology-GPT"], ["paragraph", "ChatGLM-Med"]], "content": "~ is fine-tuned on the Chinese medical instruction dataset based on the ChatGLM-6B~ model. The instruction dataset comprises medically relevant question and answer pairs, created using the GPT 3.5 API and the Medical Knowledge Graph. This model improves the question-answering performance of ChatGLM~ in the medical field.", "cites": [2212], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis and only paraphrases a single cited paper (doc_id: 2212) in the context of the ChatGLM-Med model. There is no critical analysis of the model's strengths or weaknesses, nor any comparison with other works in the medical domain. It lacks abstraction and does not generalize to broader trends or principles in instruction tuning for medical LLMs."}}
{"id": "7507118c-8808-46c4-8a5d-dea386e7dd23", "title": "Goat", "level": "paragraph", "subsections": [], "parent_id": "79cbe6ae-783c-4646-94a4-06246ce95489", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Arithmetic"], ["paragraph", "Goat"]], "content": "~ is a fine-tuned LLaMA-7B~ model based on instructions, which aims to solve arithmetic problems. It expresses arithmetic problems in the form of natural language question answering, such as \"What is 8914/64?\", by generating hundreds of instruction templates using ChatGPT~. The model applies various techniques to enhance its adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression and replacing \"*\" with \"x\" or \"times\". The Goat model achieves state-of-the-art performance on the BIG-bench~ arithmetic subtask. In particular, zero-shot Goat-7B matches or exceeds the accuracy achieved by the few-shot PaLM-540B~.", "cites": [1552, 1554, 2247, 7466], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Goat model, its methodology, and performance. While it mentions techniques used and cites relevant papers, it lacks deeper synthesis of ideas across the cited works and does not engage in meaningful comparison or critical evaluation. There is little abstraction or generalization to broader patterns in instruction tuning for arithmetic tasks."}}
{"id": "5d4e982c-9be1-41ea-9878-081b222778d4", "title": "WizardCoder", "level": "paragraph", "subsections": [], "parent_id": "a555e3c8-f262-4266-bf6e-31e557dc6102", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Domain-specific Instruction Finetuning"], ["subsection", "Code"], ["paragraph", "WizardCoder"]], "content": "~ utilizes StarCoder 15B~ as the foundation with complex instruction fine-tuning, by adapting the Evol-Instruct method~ to the domain of code. The training dataset is produced through iterative application of the Evol-Instruct technique on the Code Alpaca dataset~, which includes the following attributes for each sample: instruction, input, and expected output. For instance, when the instruction is \"Amend the following SQL query to select distinct elements\", the input is the SQL query, and the expected output is the generated answer. The WizardCoder outperforms all other open-source Code LLMs and even surpasses the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+.", "cites": [7561, 2201, 2206], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of WizardCoder, mentioning its foundation model (StarCoder 15B), the use of Evol-Instruct adapted to code, and its performance on benchmarks. However, it lacks synthesis of ideas across the cited papers and does not critically evaluate the method or its limitations. It offers minimal abstraction and remains descriptive of the system and results."}}
{"id": "00bda954-92b1-4488-90e6-30b84c6ccd81", "title": "Efficient Tuning Techniques", "level": "section", "subsections": ["bfa98a89-512a-49a4-b2b2-59f51dfed52c", "23e3ddf2-1ad1-47b3-95c2-10bd6c23c8d7", "1123da1e-874f-4854-8976-674f73f54204", "0c160a63-10ef-41f1-9c60-b66bdb053fa2", "c93ae37b-e359-4fd2-95f2-3980641d1cf8"], "parent_id": "cf81b618-29e1-4d2f-bbd6-13fc94283342", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Efficient Tuning Techniques"]], "content": "\\label{Efficient_fine-tuning_techniques}\nEfficient fine-tuning techniques aim at adapting LLMs to downstream tasks by optimizing a small fraction of parameters in multiple ways, \\textit{i.e.}, addition-based, specification-based, and reparameterization-based.\nAddition-based methods introduce extra trainable parameters or modules not present in the original model. Representative methods include adapter tuning~ and prompt-based tuning~. Specification-based methods specify certain inherent model parameters to be tuned while freezing others. For example, BitFit~ tunes the bias terms of the pre-trained model. Reparameterization methods transform model weights into more parameter-efficient forms for tuning. The key hypothesis is that model adaptation is low-rank, so weights can be reparameterized into low-rank factors or a low-dimensional subspace (\\textit{e.g.}, LoRA~). Intrinsic prompt tuning finds a low-dimensional subspace shared by tuning prompts across diverse tasks.", "cites": [2466, 1589, 7567], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear descriptive overview of efficient tuning techniques, categorizing them into addition-based, specification-based, and reparameterization-based methods. While it briefly mentions representative papers (PET, BitFit, LoRA), it does so mainly to illustrate categories without deep synthesis or critical evaluation. Some abstraction is attempted through the introduction of low-dimensional subspaces, but overall the insights remain surface-level."}}
{"id": "bfa98a89-512a-49a4-b2b2-59f51dfed52c", "title": "LoRA", "level": "subsection", "subsections": [], "parent_id": "00bda954-92b1-4488-90e6-30b84c6ccd81", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Efficient Tuning Techniques"], ["subsection", "LoRA"]], "content": "Low-Rank Adaptation (LoRA)  enables efficient adaptation of LLMs using low-rank updates. LoRA use DeepSpeed~ as the training backbone.\n     The key insight of LoRA is that the actual change in LLMs' weights required for new task adaptation lies in a low-dimensional subspace. \n    Specifically, for a pretrained weight matrix $W_0$, the authors model the adapted weight matrix as $W_0$ + $\\Delta W$, where $\\Delta W$ is a low rank update. $\\Delta W$ is parameterized as $\\Delta W = BA$, where $A$ and $B$ are much smaller trainable matrices. The rank $r$ of $\\Delta W$ is chosen to be much smaller than the dimensions of $W_0$. \n    The intuition is that instead of directly training all of $W_0$, the authors train low-dimensional $A$ and $B$, which indirectly trains $W_0$ in a low-rank subspace of directions that matter for the downstream task. This results in far fewer trainable parameters compared to full fine-tuning.\n    For GPT-3, LoRA reduces the number of trainable parameters by 10,000x and memory usage by 3x compared to full fine-tuning.", "cites": [1589], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of LoRA, including its key idea and parameter reduction benefits, as presented in the cited paper. However, it lacks synthesis across multiple sources, critical evaluation of LoRA's limitations or trade-offs, and abstraction to broader principles or patterns in instruction tuning. The content is primarily a summary of the method from one paper."}}
{"id": "1123da1e-874f-4854-8976-674f73f54204", "title": "Qlora", "level": "subsection", "subsections": [], "parent_id": "00bda954-92b1-4488-90e6-30b84c6ccd81", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Efficient Tuning Techniques"], ["subsection", "Qlora"]], "content": "QLORA  includes optimal quantization and memory optimization, aiming at providing efficient and effective LLMs fine-tuning.\nQLORA includes 4-bit NormalFloat (NF4) Quantization, which is a quantization scheme optimized for the typical normal distribution of LLM weights. By quantizing based on the quantiles of a normal distribution, NF4 provides better performance than standard 4-bit integer or float quantization. To further reduce memory, the quantization constants are themselves quantized to 8 bits. This second level of quantization saves an additional 0.37 bits per parameter on average. QLORA leverages NVIDIA's unified memory feature to page optimizer states to CPU RAM when GPU memory is exceeded. avoiding out-of-memory during training.\nQLORA enables training a 65B parameter LLM on a single 48GB GPU with no degradation compared to full 16-bit finetuning. QLORA works by freezing the 4-bit quantized base LLM, then backpropagating through it into a small set of 16-bit low-rank adapter weights which are learned.", "cites": [2253], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of QLORA, focusing on its components such as NF4 quantization, 8-bit quantized constants, and the use of low-rank adapters. While it integrates key concepts from the cited paper, it lacks broader comparisons with other efficient tuning methods or deeper critical evaluation of its advantages and limitations. The abstraction level remains limited to specific technical features without rising to general principles."}}
{"id": "0c160a63-10ef-41f1-9c60-b66bdb053fa2", "title": "LOMO", "level": "subsection", "subsections": [], "parent_id": "00bda954-92b1-4488-90e6-30b84c6ccd81", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Efficient Tuning Techniques"], ["subsection", "LOMO"]], "content": "LOw-Memory Optimization (LOMO)  enables full parameter fine-tuning of LLMs using limited computational resources through a fusion of gradient computation and update. The essence is to fuse gradient computation and parameter update into one step during backpropagation, thereby avoiding storage of full gradient tensors. Firstly, theoretical analysis is provided in LOMO on why SGD can work well for fine-tuning large pre-trained models despite its challenges on smaller models. In addition, LOMO updates each parameter tensor immediately after computing its gradient in backpropagation. Storing the gradient of one parameter at a time reduces gradient memory to $O(1)$. LOMO employs gradient value clipping, separate gradient norm computation pass and dynamic loss scaling to stabilize training. The integration of activation checkpointing and ZeRO optimization methods saves memory.", "cites": [2254], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the LOMO method, integrating its key components from the cited paper. However, it lacks broader synthesis with other methods, does not offer critical evaluation or comparison, and remains focused on concrete details without abstracting general principles."}}
{"id": "5cb46e17-7438-4273-ae80-28382d5b56d7", "title": "HELM Evaluation", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "HELM Evaluation"]], "content": "HELM is a holistic evaluation of Language Models (LMs) to improve the transparency of language models, providing a more comprehensive understanding of the capabilities, risks, and limitations of language models.\nSpecifically, differing from other evaluation methods, HELM holds that a holistic evaluation of language models should focus on the following three factors:\n\\noindent(1) \\textbf{Broad coverage.} During the development, language models can be adapted to various NLP tasks (e.g., sequence labeling and question answering), thus, the evaluation of language models needs to be carried out in a wide range of scenarios. To involve all potential scenarios, HELM proposed a top-down taxonomy, which begins by compiling all existing tasks in a major NLP conference (ACL2022) into a task space and dividing each task into the form of scenarios (e.g., languages) and metrics (e.g., accuracy). Then when facing a specific task, the taxonomy would select one or more scenarios and metrics in the task space to cover it. By analyzing the structure of each task, HELM clarifies the evaluation content (task scenarios and metrics) and improves the scenario coverage of language models from 17.9\\% to 96.0\\%.\n\\noindent(2) \\textbf{Multi-metric measurement.} In order to enable human to weigh language models from different perspectives, HELM proposes multi-metric measurement. HELM has covered 16 different scenarios and 7 metrics. To ensure the results of intensive multi-metric measurement, HELM measured 98 of 112 possible core scenarios (87.5\\%).\n\\noindent(3) \\textbf{Standardization.} The increase in the scale and training complexity of language models has seriously hindered human's understanding of the structure of each language model. To establish a unified understanding of existing language models, HELM benchmarks 30 well-known language models, covering such institutions as Google (UL2), OpenAI (GPT-3), and EleutherAI (GPT-NeoX). Interestingly, HELM pointed out that LMs such as T5  and Anthropic-LMv4-s3  had not been directly compared in the initial work, while LLMs such as GPT-3 and YaLM were still different from their corresponding reports after multiple evaluations.", "cites": [1570, 9, 7568, 679, 2224, 2255], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of the HELM evaluation framework, integrating its three key principles (broad coverage, multi-metric measurement, and standardization) and referencing relevant papers to support its claims. It demonstrates good synthesis by connecting HELM's methodology to the broader context of LLM evaluation and training. The critical analysis is moderate, as it identifies some limitations of existing models and evaluations, but could offer a deeper critique of HELM's approach itself. The abstraction level is reasonable, highlighting generalizable aspects of evaluation design for language models."}}
{"id": "af156b55-bcc0-42ae-a5f0-290ef6b133bb", "title": "Low-resource Instruction Tuning", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "Low-resource Instruction Tuning"]], "content": " attempts to estimate the minimal downstream training data required by IT models to match the SOTA supervised models over various tasks.\n conducted experiments on 119 tasks from Super Natural Instructions (SuperNI) in both single-task learning (STL) and multi-task learning (MTL) settings. The results indicate that in the STL setting, IT models with only 25\\% of downstream training data outperform the SOTA models on those tasks, while in the MTL setting, just 6\\% of downstream training data can lead IT models to achieve the SOTA performance. These findings suggest that instruction\ntuning can effectively assist a model in quickly learning a task even with limited data. \nHowever, due to resource limitations,  did not conduct experiments on LLMs, like T5-11B. So, to gain a more comprehensive understanding of the IT models, further investigation using larger language models and datasets is necessary.", "cites": [2256], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of low-resource instruction tuning by summarizing the findings of one key paper, highlighting the data efficiency of IT models in both STL and MTL settings. It makes basic connections between the paper's results and the broader implications for IT, and points out a limitation in the study's scope (e.g., not testing on larger models). While it offers some generalization (e.g., suggesting further investigation is needed), the synthesis and abstraction remain at a moderate level due to the limited number of sources discussed."}}
{"id": "347d1649-5905-4435-b4e8-212a2c76d0d2", "title": "Smaller Instruction Dataset", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "Smaller Instruction Dataset"]], "content": "IT  requires a substantial amount of specialized instruction data for training.  hypothesized  that the pre-trained LLM only has to learn the style or format to interact with users and proposed LIMA that achieves  strong performance by  fine-tuning an LLM on only 1,000 carefully selected training examples.\nSpecifically, LIMA first manually curates 1,000 demonstrations with high-quality prompts and responses. Then the 1,000 demonstrations are used to fine-tune the pre-trained 65B-parameter LLaMa  . By comparison, across more than 300 challenging tasks, LIMA outperfrms GPT-davinci003 , which was fine-tuned on 5,200 examples by human feedback tuning. Moreover, with only half amount of demonstrations, LIMA achieves equivalent results to GPT-4 , Claude , and Bard\\footnote{Bard, designed by Google, is an interface to generative AI platform, and the link is: https://ai.google/static/documents/google-about-bard.pdf}.\nAbove all, LIMA demonstrated that LLMs' powerful knowledge and capabilities can be exposed to users with only a few carefully curated instructions to fine-tune.", "cites": [9115, 8472, 679, 1552, 7558], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core idea of LIMA from its cited paper and contrasts it with other instruction-tuned models like GPT-4, Claude, and Bard. It provides a critical evaluation by highlighting the surprising effectiveness of a small, high-quality dataset over larger ones used in alternative methods. The abstraction is strong, as it draws a general insight about the potential for LLMs to be aligned with user instructions using minimal, well-curated data, reflecting a broader trend in efficient fine-tuning."}}
{"id": "86f45f91-2ba9-44a6-9998-acea47ad1242", "title": "Evaluating  Instruction-tuning Datasets", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "Evaluating  Instruction-tuning Datasets"]], "content": "The performance of IT model highly depends on the IT datasets. \nHowever, \nthere lacks of evaluations for these IT datasets from open-ended and subjective aspects.\nTo address this issue,  performs dataset evaluation by \nfine-tuning the LLaMa model  on various of open IT datasets and measure different fine-tuned models through both automatic and human evaluations. \nAn additional model is trained on the combination of IT datasets. \nFor the results,  showed that there is not a single best IT dataset across all tasks, while by manually combining datasets it can achieve the best overall performance. Besides,  pointed out that though IT can bring large benefits on LLMs at all sizes, smaller models and models with a high base quality benefit most from IT. For human evaluations,  a larger model is more likely to gain a higher acceptability score.", "cites": [2213, 1552], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the findings from the cited papers to highlight the dataset dependency of instruction-tuned models and discusses evaluation methods. It provides some comparative insights, such as the lack of a single best dataset and the differential benefits of IT based on model size and base quality, but does not deeply critique the methodologies or limitations of the works. The abstraction is limited to general observations rather than identifying broader principles or frameworks."}}
{"id": "8b5a89fc-7825-494d-a0ef-56696c7a5417", "title": "Do IT just learn Pattern Copying?", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "Do IT just learn Pattern Copying?"]], "content": "To address the lack of clarity about the specific knowledge that models acquire through instruction tuning,  delves into the analysis of how models make use of instructions during IT by comparing the tuning when provided with altered instructions versus the original instructions.\nSpecifically,  creates simplified task definitions that remove all semantic components, leaving only the output information. In addition,  also incorporates delusive examples that contain incorrect input-output mapping. Surprisingly, the experiments show that models trained on these simplified task definitions or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Moreover, the paper also introduces a baseline for the classification task with zero-shot, which achieves similar performance to IT in low-resource settings.\nIn summary, according to , the notable performance improvements observed in current IT models may be attributed to their ability to capture surface-level patterns, such as learning the output format and making guesses, rather than comprehending and learning the specific task.", "cites": [2190], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key findings of the cited paper, focusing on the investigation of whether instruction tuning leads to pattern copying rather than true task understanding. It provides critical analysis by evaluating the implications of the observed model behavior, such as the possibility of performance improvements stemming from superficial pattern recognition. The abstraction level is moderate, as it identifies a broader concern about model learning during IT but stops short of proposing a meta-level framework for understanding instruction-based learning dynamics."}}
{"id": "67c23e37-58d7-41ca-8b23-1633ed1f04e0", "title": "Proprietary LLMs Imitation", "level": "subsection", "subsections": [], "parent_id": "a7060504-c810-4c88-8297-8e0d926a2bb4", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Evaluation, Analysis and Criticism"], ["subsection", "Proprietary LLMs Imitation"]], "content": "LLMs imitation is an approach that collects outputs from a stronger model, such as a proprietary system like ChatGPT, and uses these outputs to fine-tune an open-source LLM. Through this way, an open-source LLM may get competitive capabilities with any proprietary model.\n conducted several experiments to critically analyze the efficacy of model imitation. Specifically,  first collected datasets from outputs of ChatGPT over broad tasks. Then these datasets were used to fine-tune a range of models covering sizes from 1.5B to 13B, base models GPT-2 and LLaMA, and data amounts from 0.3M tokens to 150M tokens.\nFor evaluations,  demonstrated that on tasks with supported datasets, imitation models are far better than before, and their outputs appear similar to ChatGPT's. While on tasks without imitation datasets, imitation models do not have improvement or even decline in accuracy. \nThus,  pointed out that it's the phenomenon that imitation models are adept at mimicking ChatGPT's style (e.g., being fluent, confident and well-structured) that makes researchers have the illusion about general abilities of imitation models. So,  suggested that instead of imitating proprietary models, researchers had better focus on improving the quality of base models and instruction examples.", "cites": [2187], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key findings from the cited paper, highlighting both the benefits and limitations of imitating proprietary LLMs. It provides a critical perspective by identifying the illusion of general capability and suggesting an alternative research focus. The analysis moves beyond individual results to abstract broader implications for the field of instruction tuning."}}
{"id": "2f75f118-69d5-4109-997c-86bf105a84b4", "title": "Datasets", "level": "section", "subsections": [], "parent_id": "cf81b618-29e1-4d2f-bbd6-13fc94283342", "prefix_titles": [["title", "Instruction Tuning for Large Language Models: A Survey"], ["section", "Datasets"]], "content": "\\label{appendix:datasets}\n\\begin{table*}[t]\n\\centering\n\\small\n\\begin{adjustbox}{max width=0.9\\textwidth}\n\\begin{threeparttable}\n\\begin{tabular}{p{3.8cm}|lccccc}\n\\toprule \n{\\bf Type} & {\\bf Dataset Name} & {\\bf \\# of Instances} & {\\bf \\# of Lang} & {\\bf Construction} & {\\bf Open-source} \\\\\\midrule\n\\multirow{13}{*}{\\bf Human-Crafted} & UnifiedQA~\\tnotex{id:1} & 750K & En & human-crafted & Yes\\\\\n& UnifiedSKG~\\tnotex{id:3} & 0.8M & En & human-crafted & Yes\\\\\n& Natural Instructions~\\tnotex{id:4} & 193K & En  & human-crafted & Yes \\\\\n& Super-Natural Instructions~\\tnotex{id:5} & 5M & 55 Lang & human-crafted & Yes \\\\\n& P3~\\tnotex{id:6} & 12M & En & human-crafted & Yes \\\\\n& xP3~\\tnotex{id:7} & 81M & 46 Lang & human-crafted & Yes \\\\\n& Flan 2021~\\tnotex{id:8} & 4.4M & En & human-crafted & Yes \\\\\n& COIG~\\tnotex{id:9} & - & - & - & Yes \\\\\n&  InstructGPT~ & 13K & Multi & human-crafted & No \\\\\n& Dolly~\\tnotex{id:16} & 15K & En & human-crafted & Yes \\\\\n& LIMA~\\tnotex{id:18} & 1K & En & human-crafted & Yes \\\\\n& ChatGPT~ & - & Multi & human-crafted & No \\\\\n& OpenAssistant~\\tnotex{id:20} & 161,443 & Multi & human-crafted & Yes \\\\\n\\midrule \n\\multirow{20}{*}{\\shortstack{\\bf Synthetic Data \\\\ \\bf (Distillation)}} & OIG~\\tnotex{id:2} & 43M & En & ChatGPT (No technique reports) & Yes \\\\\n& Unnatural Instructions~\\tnotex{id:10} & 240K & En & InstructGPT-Generated & Yes \\\\\n& InstructWild~\\tnotex{id:12} & 104K & - & ChatGPT-Generated & Yes\\\\\n& Evol-Instruct / WizardLM~\\tnotex{id:13} & 52K & En & ChatGPT-generated & Yes \\\\\n& Alpaca~\\tnotex{id:14}  & 52K & En & InstructGPT-generated & Yes \\\\\n& LogiCoT~\\tnotex{id:15} & - & En & GPT-4-Generated & Yes \\\\\n& GPT-4-LLM~\\tnotex{id:17} & 52K & En\\&Zh & GPT-4-Generated & Yes \\\\\n& Vicuna~ & 70K & En & Real User-ChatGPT Conversations & No \\\\\n& Baize v1~\\tnotex{id:21} & 111.5K & En & ChatGPT-Generated & Yes \\\\\n& UltraChat~\\tnotex{id:22} & 675K & En\\&Zh & GPT 3/4-Generated & Yes \\\\\n& Guanaco~\\tnotex{id:19} & 534,530 & Multi & GPT (Unknown Version)-Generated & Yes \\\\\n& Orca~\\tnotex{id:23} & ~1.5M & En & GPT 3.5/4-Generated & Yes \\\\\n& ShareGPT\\tnotex{id:24} & 90K & Multi & Real User-ChatGPT Conversations & Yes \\\\\n& WildChat\\tnotex{id:25} & 150K & Multi & Real User-ChatGPT Conversations & Yes \\\\\n& WizardCoder~ & - & Code & LLaMa 2-Generated & No \\\\\n& Magicoder~\\tnotex{id:26} & 75K/110K & Code & GPT-3.5-Generated & Yes \\\\\n& WaveCoder~ & - & Code & GPT 4-Generated & No \\\\\n& Phi-1~\\tnotex{id:27} & 6B Tokens & Code Q and A & GPT-3.5-Generated & Yes \\\\\n& Phi-1.5~ & - & Code Q and A & GPT-3.5-Generated & No \\\\\n& Nectar~\\tnotex{id:28} & ~183K & En & GPT 4-Generated & Yes \\\\\n\\midrule \n\\multirow{3}{*}{\\shortstack{\\bf Synthetic Data \\\\ \\bf (Self-Improvement)}} & Self-Instruct~\\tnotex{id:11} & 52K & En & InstructGPT-Generated & Yes \\\\\n& Instruction Backtranslation~ & 502K  & En & LLaMa-Generated & No \\\\\n& SPIN~\\tnotex{id:29} & 49.8K & En & Zephyr-Generated & Yes \\\\\n\\bottomrule\n\\end{tabular}\n\\end{threeparttable}\n\\end{adjustbox}\n\\begin{multicols}{2}\n\\begin{tablenotes}\n\\item[1] \\label{id:1} {$^1$ https://github.com/allenai/unifiedqa} \n\\item[2] \\label{id:2} {$^2$ https://github.com/LAION-AI/Open-Instruction-Generalist}\n\\item[3] \\label{id:3} {$^3$ https://github.com/hkunlp/unifiedskg}\n\\item[4] \\label{id:4} {$^4$ https://github.com/allenai/natural-instructions-v1}\n\\item[5] \\label{id:5} {$^5$ https://github.com/allenai/natural-instructions}\n\\item[6] \\label{id:6} {$^6$ https://huggingface.co/datasets/bigscience/P3} \n\\item[7] \\label{id:7} {$^7$ https://github.com/bigscience-workshop/xmtf}\n\\item[8] \\label{id:8} {$^8$ https://github.com/google-research/FLAN}\n\\item[9] \\label{id:9} {$^9$ https://github.com/BAAI-Zlab/COIG}\n\\item[10] \\label{id:10} {$^{10}$ https://github.com/orhonovich/unnatural-instructions}\n\\item[11] \\label{id:11} {$^{11}$ https://github.com/yizhongw/self-instruct}\n\\item[12] \\label{id:12} {$^{12}$ https://github.com/XueFuzhao/InstructionWild}\n\\item[13] \\label{id:13} {$^{13}$ https://github.com/nlpxucan/evol-instruct} \n\\item[14] \\label{id:14} {$^{14}$ https://github.com/tatsu-lab/stanford\\_alpaca}\n\\item[15] \\label{id:15} {$^{15}$  https://github.com/csitfun/LogiCoT}\n\\item[16] \\label{id:16} {$^{16}$ https://huggingface.co/datasets/databricks/databricks-dolly-15k}\n\\item[17] \\label{id:17} {$^{17}$ https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM}\n\\item[18] \\label{id:18} {$^{18}$ https://huggingface.co/datasets/GAIR/lima}\n\\item[19] \\label{id:19} {$^{19}$ https://huggingface.co/datasets/JosephusCheung/GuanacoDataset}\n\\item[20] \\label{id:20} {$^{20}$ https://github.com/LAION-AI/Open-Assistant}\n\\item[21] \\label{id:21} {$^{21}$ https://github.com/project-baize/baize-chatbot}\n\\item[22] \\label{id:22} {$^{22}$ https://github.com/thunlp/UltraChat\\#data}\n\\item[23] \\label{id:23} {$^{23}$ https://huggingface.co/datasets/Open-Orca/OpenOrca}\n\\item[24] \\label{id:24} {$^{24}$ https://huggingface.co/datasets/RyokoAI/ShareGPT52K}\n\\item[25] \\label{id:25} {$^{25}$ https://huggingface.co/datasets/allenai/WildChat}\n\\item[26] \\label{id:26} {$^{26}$ https://github.com/ise-uiuc/magicoder?tab=readme-ov-file\\#-dataset}\n\\item[27] \\label{id:27} {$^{27}$ https://huggingface.co/microsoft/phi-1}\n\\item[28] \\label{id:28} {$^{28}$ https://huggingface.co/datasets/berkeley-nest/Nectar}\n\\item[29] \\label{id:29} {$^{29}$ https://github.com/uclaml/SPIN?tab=readme-ov-file\\#Data}\n\\end{tablenotes}\n\\end{multicols}\n\\caption{An overview of instruction tuning datasets.}\n\\label{tab:llms_traindata_table}\n\\end{table*}\nTable \\ref{tab:llms_traindata_table} gives an overview of our collected datasets.\n\\label{sec:appendix}\n\\end{document}", "cites": [7560, 2207, 364, 8469, 2208, 2204, 7559, 7087, 2197, 2211, 2209, 2257, 8464, 2200, 2258, 2216, 2206, 2202, 2201, 7558], "cite_extract_rate": 0.625, "origin_cites_number": 32, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is largely descriptive, presenting a table of datasets with minimal textual analysis. While it organizes the information by types (Human-Crafted, Synthetic via Distillation, Self-Improvement), it lacks synthesis of ideas or discussion of how these approaches relate or differ in effectiveness. Critical analysis and abstraction are similarly limited, with no evaluation of dataset quality, limitations, or broader patterns in their use or construction."}}
