{"id": "f03284dc-2f90-48a6-be66-97100b110de0", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "91cd4abf-5403-4058-9b30-84529d4887a4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nData augmentation (\\textsc{DA}) refers to strategies for increasing the diversity of training examples without explicitly collecting new data. It has received active attention in recent machine learning (ML) research in the form of well-received, general-purpose techniques such as \\textsc{UDA}  (\\ref{subsec:rule-based}), which used \\textit{backtranslation} , \\textit{AutoAugment} , and \\textit{RandAugment} , and \\textsc{MixUp}  (\\ref{subsec:example_interpolation}). These are often first explored in computer vision (CV), and DA's adaptation for natural language processing (NLP) seems secondary and comparatively underexplored, perhaps due to challenges presented by the discrete nature of language, which rules out continuous noising and makes it more difficult to maintain invariance. \nDespite these challenges, there has been increased interest and demand for \\textsc{DA} for NLP. As NLP grows due to off-the-shelf availability of large pretrained models, there are increasingly more tasks and domains to explore. Many of these are low-resource, and have a paucity of training examples, creating many use-cases for which \\textsc{DA} can play an important role. Particularly, for many non-classification NLP tasks such as span-based tasks and generation, \\textsc{DA} research is relatively sparse despite their ubiquity in real-world settings.\nOur paper aims to sensitize the NLP community towards this growing area of work, \nwhich has also seen increasing interest in ML overall (as seen in Figure \\ref{tab:googleTrends}). As interest and work on this topic continue to increase, this is an opportune time for a paper of our kind to (i) give a bird's eye view of DA for NLP, and (ii) identify key challenges to effectively motivate and orient interest in this area. To the best of our knowledge, this is the first survey to take a detailed look at \\textsc{DA} methods for NLP.\\footnote{ present a smaller-scale text data augmentation survey that is concise and focused. Our work serves as a more comprehensive survey with larger coverage and is more up-to-date.}\n\\begin{figure}\n\\begin{tabular}{@{}ll@{}}\n\\includegraphics[width=0.48\\textwidth]{Images/Google_Trends.png}\\\\\n\\end{tabular}\n\\vspace{-0.8\\abovedisplayskip}\n  \\caption{\\label{tab:googleTrends} Weekly Google Trends scores for the search term \\emph{\"data augmentation\"}, with a control, uneventful ML search term (\\emph{\"minibatch\"}) for comparison.}\n\\vspace{-2ex}\n\\end{figure}\n\\begin{comment}\n\\begin{figure}\n\\begin{tabular}{@{}ll@{}}\n\\includegraphics[width=0.48\\textwidth]{Images/NLP_Paper_Trends.png}\\\\\n\\end{tabular}\n\\vspace{-0.8\\abovedisplayskip}\n  \\caption{\\label{tab:nlpPaperTrends} \\%  of conference papers  published at NLP Venues (EMNLP, COLING, TACL and *CL), with \\textit{\"augment\"} in their title, graphed over the 2010s decade.}\n\\vspace{-3ex}\n\\end{figure}\n\\end{comment}\nThis paper is structured as follows. Section \\ref{sec:background} discusses what DA is, \nits goals and trade-offs, and why it works. Section \\ref{sec:techniquesandmethods} describes popular methodologically representative DA techniques for NLP\n---which we categorize into rule-based (\\ref{subsec:rule-based}), example interpolation-based (\\ref{subsec:example_interpolation}), or model-based (\\ref{subsec:model_based}). Section \\ref{sec:applications} discusses useful NLP applications for DA, including low-resource languages (\\ref{applications:1}), mitigating bias (\\ref{applications:2}), fixing class imbalance (\\ref{applications:3}), few-shot learning (\\ref{applications:4}), and adversarial examples (\\ref{applications:5}). Section \\ref{sec:tasks} describes DA methods for common NLP tasks including \nsummarization (\\ref{tasks:3}), question answering (\\ref{tasks:5}), sequence tagging tasks (\\ref{tasks:4}), parsing tasks (\\ref{tasks:7}), grammatical error correction (\\ref{tasks:9}), neural machine translation (\\ref{tasks:10}), data-to-text NLG (\\ref{tasks:8}), open-ended and conditional text generation (\\ref{tasks:1}), dialogue (\\ref{tasks:2}), and multimodal tasks (\\ref{tasks:11}). Finally, Section \\ref{sec:currentchallenges} discusses challenges and future directions in DA for NLP. Appendix \\ref{sec:appendix_blog_posts} lists useful blog posts and code repositories.\nThrough this work, we hope to emulate past papers which have surveyed \\textsc{DA} methods for other types of data, such as images , faces , and time series . We hope to draw further attention, elicit broader interest, and motivate additional work in \\textsc{DA}, particularly for NLP.\n\\begin{comment}\n\\tikzset{edge from parent/.style=\n{draw, edge from parent path={(\\tikzparentnode.south)\n-- +(0,-8pt)\n-| (\\tikzchildnode)}},\nblank/.style={draw=none}}\n\\begin{tikzpicture}\n\\matrix\n{\n\\node{\\Tree \n [.{$\\gamma$-proteobacteria} \n    [.Alteromonadales \n        [.Alteromonadaceae  {Glaciecola}  Alteromonas Agarivorans ] ]\n    [.Vibrionales [.Vibrionacae Vibrio ]]]};\\\\\n};           \n\\end{tikzpicture}\n\\steven{I assume the graph above is for the \"taxonomy\" which we will fill in at the end. Will likely be based on techniques and methods (e.g. sections first-level of tree, subsections second-level of tree, etc.). Note that it is formatted incorrectly and overlaps text on the second column.}\n\\end{comment}", "cites": [7191, 5475, 8393, 115, 5474, 8922], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of data augmentation in NLP, references key methods like backtranslation, AutoAugment, RandAugment, and MixUp, and situates them in the broader context of ML. It mentions some challenges, such as the discrete nature of language, but lacks deeper critical evaluation or synthesis of the cited works into a cohesive framework. The structure is outlined clearly, but the analysis remains largely descriptive with limited abstraction or comparative insight."}}
{"id": "c06f6e29-f4fb-4672-ac0e-c9698362b475", "title": "Background", "level": "section", "subsections": ["7ec808ea-23df-4942-b00c-3750b49187f5"], "parent_id": "91cd4abf-5403-4058-9b30-84529d4887a4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Background"]], "content": "\\label{sec:background}\n\\noindent \\textbf{What is data augmentation?} \\label{whatis}\nData augmentation (\\textsc{DA}) encompasses methods of increasing training data diversity without directly collecting more data. Most strategies either add slightly modified copies of existing data or create synthetic data, aiming for the augmented data to act as a regularizer and reduce overfitting when training ML models . DA has been commonly used in CV, where techniques like \\textit{cropping}, \\textit{flipping}, and \\textit{color jittering} are a standard component of model training. \nIn NLP, where the input space is discrete, how to generate effective augmented examples that capture the desired invariances is less obvious.\n\\noindent", "cites": [5476], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition of data augmentation and mentions its role in regularization, citing one paper briefly. However, it lacks deeper synthesis of ideas, critical evaluation of the cited work, or abstraction to broader principles. The narrative remains descriptive and does not offer a cohesive or insightful integration of the literature."}}
{"id": "7ec808ea-23df-4942-b00c-3750b49187f5", "title": "What are the goals and trade-offs?", "level": "paragraph", "subsections": ["a92d7d51-8f95-48e4-8f82-91b1ec71f619"], "parent_id": "c06f6e29-f4fb-4672-ac0e-c9698362b475", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Background"], ["paragraph", "What are the goals and trade-offs?"]], "content": "\\label{goals}\nDespite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations  to more complicated generative approaches . As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. \nMost offer trade-offs between these two. \nRule-based techniques are easy-to-implement but usually offer incremental performance improvements . Techniques leveraging trained models \nmay be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong  \neffects on performance but be difficult to develop and utilize. \nFurther, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance.\n devise a KL-Divergence-based unsupervised procedure to \\emph{preemptively} choose among \\textsc{DA} heuristics, rather than a typical \"run-all-heuristics\" comparison, which can be very time and cost intensive.\n\\noindent", "cites": [8466, 5479, 5478, 1096, 5477], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section offers a concise analytical overview of the goals and trade-offs in data augmentation, connecting rule-based and model-based methods across multiple papers. It integrates concepts from the cited works to highlight the balance between implementation cost and performance gain, as well as data distribution concerns. While it provides some critical analysis (e.g., noting limitations of rule-based methods), it does not deeply evaluate or compare the approaches in a systematic or novel way."}}
{"id": "a92d7d51-8f95-48e4-8f82-91b1ec71f619", "title": "Interpretation of DA", "level": "paragraph", "subsections": [], "parent_id": "7ec808ea-23df-4942-b00c-3750b49187f5", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Background"], ["paragraph", "What are the goals and trade-offs?"], ["paragraph", "Interpretation of DA"]], "content": "\\label{why}\n note that \\textit{\"data augmentation is typically performed in an ad-hoc manner with little understanding of the underlying theoretical principles\"}, and claim the typical explanation of \\textsc{DA} as \\textit{regularization} to be insufficient. Overall, there indeed appears to be a lack of research on \\textit{why} exactly DA works. Existing work on this topic is mainly surface-level, and rarely investigates the theoretical underpinnings and principles. We discuss this challenge more in \\S \\ref{sec:currentchallenges}, and highlight some of the existing work below.\n show training with noised examples is reducible to Tikhonov regularization (subsumes L2).  \nshow that DA can increase the positive margin for classifiers, but only when augmenting exponentially many examples for common DA methods.\n think of DA transformations as kernels, and find two ways DA helps: averaging of features and variance regularization.  show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant.", "cites": [5482, 5480, 5481], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section analytically connects three distinct theoretical perspectives on data augmentation, showing how they relate to regularization, margin expansion, and kernel-based averaging. It moves beyond mere description by highlighting limitations in current understanding and identifying recurring themes, such as invariance and variance reduction. The synthesis across papers is strong, and the abstraction level reveals broader theoretical implications."}}
{"id": "22dfe916-c9dd-47d9-91c8-aba04e2f745b", "title": "Rule-Based Techniques", "level": "subsection", "subsections": [], "parent_id": "a72717e3-eb8c-4dac-ac7b-dc65aebe8bf4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Techniques \\& Methods"], ["subsection", "Rule-Based Techniques"]], "content": "\\label{subsec:rule-based}\nHere, we cover \\textsc{DA} primitives which use easy-to-compute, predetermined transforms sans model components. \\textit{Feature space DA} approaches generate augmented examples in the model's feature space rather than input data. Many few-shot learning approaches  leverage estimated feature space \"analogy\" transformations between examples of known classes to augment for novel classes (see \\S\\ref{applications:4}).  use iterative affine transformations and projections to maximally \"stretch\" an example along the class-manifold.\n propose \\textsc{Easy Data Augmentation (EDA)}, a set of token-level random perturbation operations including \n\\textit{random insertion, deletion}, and \\textit{swap}.\nThey show improved performance on many text classification tasks. UDA  show how supervised \\textsc{DA} methods can be exploited for unsupervised data through consistency training on $(x,DA(x))$ pairs.\nFor paraphrase identification,  construct a signed graph over the data, with individual sentences as nodes and pair labels as signed edges. They use balance theory and transitivity to infer augmented sentence pairs from this graph. Motivated by image cropping and rotation,  propose \\emph{dependency tree morphing}. For dependency-annotated sentences, children of the same parent are swapped (à la rotation) or some deleted (à la cropping), as seen in Figure \\ref{img-deptreemorph}. This is most beneficial for language families with rich case marking systems (e.g. \\textit{Baltic} and \\textit{Slavic}).\n\\begin{figure}\n\\centering\n\\begin{tabular}{@{}ll@{}}\n\\includegraphics[width=0.40\\textwidth]{Images/SahinSteedman_edited.png}\\\\\n\\end{tabular}\n\\vspace{-0.8\\abovedisplayskip}\n  \\caption{\\label{img-deptreemorph} \\emph{Dependency tree morphing} \\textsc{DA} applied to a Turkish sentence, }\n\\vspace{-2ex}\n\\end{figure}", "cites": [5484, 5483, 1096, 115, 3637], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.2, "critical": 1.6, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of rule-based data augmentation techniques and references several papers, but it lacks a deep synthesis of the ideas or a unifying framework. It does not critically evaluate the cited works or identify broader patterns or principles beyond specific methods. The narrative is largely additive, offering minimal analytical depth or comparative perspective."}}
{"id": "20861733-c4ec-440e-904e-49a82d8348f7", "title": "Example Interpolation Techniques", "level": "subsection", "subsections": [], "parent_id": "a72717e3-eb8c-4dac-ac7b-dc65aebe8bf4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Techniques \\& Methods"], ["subsection", "Example Interpolation Techniques"]], "content": "\\label{subsec:example_interpolation}\nAnother class of \\textsc{DA} techniques, pioneered by \\textsc{MixUp} , interpolates the inputs and labels of two or more real examples. \nThis class of techniques is also sometimes referred to as \\emph{Mixed Sample Data Augmentation} (MSDA). Ensuing work has explored interpolating inner components , more general mixing schemes , and adding adversaries .\nAnother class of extensions of \\textsc{MixUp} which has been growing in the vision community attempts to fuse raw input image pairs together into a single input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm include \\textsc{CutMix} , \\textsc{CutOut}  and \\textsc{Copy-Paste} . For instance, \\textsc{CutMix} replaces a small sub-region of Image A with a patch sampled from Image B, with the labels mixed in proportion to sub-region sizes. There is potential to borrow ideas and inspiration from these works for NLP, e.g. for multimodal work involving both images and text (see \\textit{\"Multimodal challenges\"} in \\S\\ref{sec:currentchallenges}).\nA bottleneck to using \\textsc{MixUp} for NLP tasks was the requirement of continuous inputs. This has been overcome by \nmixing embeddings or higher hidden layers . Later variants propose speech-tailored mixing schemes  and interpolation with adversarial examples , among others.\n\\textsc{Seq2MixUp}  generalizes \\textsc{MixUp} for sequence transduction tasks in two ways - the \"hard\" version samples a binary mask (from a Bernoulli with a $\\beta(\\alpha,\\alpha)$ prior) and picks from one of two sequences at each token position, while the \"soft\" version softly interpolates between sequences based on a coefficient sampled from $\\beta(\\alpha,\\alpha)$. The \"soft\" version is found to outperform the \"hard\" version and earlier interpolation-based techniques like \\textsc{SwitchOut} .", "cites": [5487, 7191, 107, 5488, 8319, 5485, 5486], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes several papers, connecting \textsc{MixUp} with its extensions such as \textsc{CutMix}, \textsc{CutOut}, and \textsc{Copy-Paste}, and discusses their adaptation to NLP. It provides a critical comparison between 'hard' and 'soft' versions of \textsc{Seq2MixUp}, noting the 'soft' version's superior performance over earlier methods like \textsc{SwitchOut}. The abstraction is strong, as it generalizes the concept of interpolation techniques and outlines a broader framework for how they might be applied in different modalities."}}
{"id": "dd8a1500-d5e4-422e-8a50-039e0d8d562e", "title": "Model-Based Techniques", "level": "subsection", "subsections": [], "parent_id": "a72717e3-eb8c-4dac-ac7b-dc65aebe8bf4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Techniques \\& Methods"], ["subsection", "Model-Based Techniques"]], "content": "\\label{subsec:model_based}\nSeq2seq and language models have also been used for \\textsc{DA}. The popular \\textsc{backtranslation} method  translates a sequence into another language and then back into the original language.  train seq2seq models with their proposed method DiPS which learns to generate diverse paraphrases of input text using a modified decoder with a submodular objective, and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs  and transformers  have also been used for augmentation.\n generate augmented examples by replacing words with others randomly drawn according to the recurrent language model's distribution based on the current context (illustration in Figure \\ref{tab:qualitative_1}).  propose \\textsc{G-DAug}$^c$ which generates synthetic examples using pretrained transformer language models, and selects the most informative and diverse set for augmentation.  advocate retaining the full distribution through \"soft\" augmented examples, showing gains on machine translation.\n\\begin{figure}\n\\centering\n\\begin{tabular}{@{}ll@{}}\n\\includegraphics[width=0.36\\textwidth]{Images/ContextualKobayashi.png}\\\\\n\\end{tabular}\n\\vspace{-0.8\\abovedisplayskip}\n  \\caption{\\label{tab:qualitative_1} \\emph{Contextual Augmentation}, }\n\\vspace{-2ex}\n\\end{figure} \n augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders,  use a corrupt-and-reconstruct approach, with the corruption function $q(x'|x)$ masking an arbitrary number of word positions and the reconstruction function $r(x|x')$ unmasking them using BERT . Their approach works well on domain-shifted \ntest sets across 9 datasets on sentiment, NLI, and NMT.\n propose a task called \\textsc{Semantic Text Exchange} (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the \\textit{replacement entity (RE)}. They do so by using a system called SMERTI and a masked LM approach. \nWhile not proposed directly for DA, it can be used as such, as investigated in .\nRather than starting from an existing example and modifying it, some model-based \\textsc{DA} approaches directly estimate a generative process from the training set and sample from it.  learn a label-conditioned generator by finetuning GPT-2  on the training data, using this to generate candidate examples per class. A classifier trained on the original training set is then used to select top $k$ candidate examples which confidently belong to the respective class for augmentation.  use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness as a \\textsc{DA} method in an active learning setup.\nOther approaches include syntactic or controlled paraphrasing , document-level paraphrasing , augmenting misclassified examples , BERT cross-encoder labeling of new inputs , guided generation using large-scale generative language models , and \\textit{automated text augmentation} . Models can also learn to combine together simpler DA primitives  or add human-in-the-loop .\n\\begin{comment}\n\\begin{table*}[t!]\n\\caption{Timeline of something. \\steven{Note: this needs to be modified for DA and completed. It is also too large (font is huge) and can probably be shrunk to save space. Can also remove this in the end if unnecessary/incoherent.}}\n\\centering\n\\begin{minipage}[t]{\\linewidth}\n\\color{gray}\n\\rule{\\linewidth}{1pt}\n\\ytl{1947}{AT and T Bell Labs develop the idea of cellular phones}\n\\ytl{1968}{First appearance of the term \\emph{`data augmentation'} in print.}\n\\ytl{1971}{Busicom 'Handy-LE' Calculator}\n\\ytl{2001}{VRM proposed }\n\\ytl{1978}{Parker Bros. Merlin Computer Toy}\n\\ytl{1981}{Osborne 1 Portable Computer}\n\\ytl{1982}{Grid Compass 1100 Clamshell Laptop}\n\\ytl{2019}{EDA , UDA  repos both cross 500+ stars}\n\\ytl{1984}{Psion Organiser Handheld Computer}\n\\ytl{1991}{Psion Series 3 Minicomputer}\n\\bigskip\n\\rule{\\linewidth}{1pt}\n\\end{minipage}\n\\end{table*}\n\\end{comment}\n\\setlength{\\tabcolsep}{2.4pt}\n\\begin{table*}[!ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n    \\textbf{\\textsc{DA} Method} & Ext.Know & Pretrained & Preprocess & Level  & Task-Agnostic    \\\\ \n    \\midrule\n    \\textsc{Synonym Replacement}  & \\ding{51} & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{Random Deletion}  & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{Random Swap}  & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n        \\textsc{Backtranslation}  & $\\times$ & \\ding{51} & Depends & Input  & \\ding{51}   \\\\\n        \\textsc{SCPN}  & $\\times$ & \\ding{51} & const & Input  & \\ding{51}   \\\\\n    \\textsc{Semantic Text Exchange}  & $\\times$ & \\ding{51} & const & Input  & \\ding{51}   \\\\\n    \\textsc{ContextualAug}  & $\\times$ & \\ding{51} & - & Input  & \\ding{51}   \\\\\n    \\textsc{LAMBADA}  & $\\times$ & \\ding{51} & - & Input & $\\times$   \\\\\n    \\textsc{GECA}  & $\\times$  & $\\times$ & tok & Input  & $\\times$ \\\\\n       \\textsc{SeqMixUp}   & $\\times$ & $\\times$ & tok & Input & $\\times$ \\\\\n    \\textsc{SwitchOut}   & $\\times$  & $\\times$ & tok & Input  & $\\times$ \\\\   \n    \\textsc{Emix}   & $\\times$  & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n    \\textsc{SpeechMix}   & $\\times$  & $\\times$ & - & Emb/Hidden  & Speech/Audio \\\\\n        \\textsc{MixText}   & $\\times$  & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n   \\textsc{SignedGraph}   & $\\times$  & $\\times$ & - & Input  & $\\times$ \\\\\n      \\textsc{DTreeMorph}   & $\\times$  & $\\times$ & dep & Input  & \\ding{51} \\\\\n            \\textsc{$Sub^2$}   & $\\times$  & $\\times$ & dep & Input  & Substructural \\\\\n\\textsc{DAGA}   & $\\times$  & $\\times$ & tok & Input+Label  & $\\times$ \\\\\n\\textsc{WN-Hypers}   & \\ding{51}  & $\\times$ & const+KWE & Input  & \\ding{51} \\\\\n\\textsc{Synthetic Noise}   & $\\times$  & $\\times$ & tok & Input  & \\ding{51} \\\\\n\\textsc{UEdin-MS} (DA part)  & \\ding{51}  & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{Nonce}  & \\ding{51} & $\\times$ & const & Input & \\ding{51} \\\\\n\\textsc{XLDA}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n\\textsc{SeqMix}  & $\\times$ & \\ding{51} & tok & Input+Label & $\\times$ \\\\\n\\textsc{Slot-Sub-LM}  & $\\times$ & \\ding{51} & tok & Input & \\ding{51} \\\\\n\\textsc{UBT \\& TBT}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n\\textsc{Soft Contextual DA}  & $\\times$ & \\ding{51} & tok & Emb/Hidden & \\ding{51} \\\\\n\\textsc{Data Diversification}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n\\textsc{DiPS}  & $\\times$ & \\ding{51} & tok & Input & \\ding{51} \\\\\n\\textsc{Augmented SBERT}  & $\\times$ & \\ding{51} & - & Input+Label & Sentence Pairs \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparing a selection of \\textsc{DA} methods by various aspects relating to their applicability, dependencies, and requirements. \\emph{Ext.Know}, \\emph{KWE}, \\emph{tok}, \\emph{const}, and \\emph{dep} stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. \\emph{Ext.Know} refers to whether the DA method requires external knowledge (e.g. WordNet) and \\emph{Pretrained} if it requires a pretrained model (e.g. BERT). \\emph{Preprocess} denotes preprocessing required, \\emph{Level} denotes the depth at which data is modified by the DA, and \\emph{Task-Agnostic} refers to whether the DA method can be applied to different tasks. See Appendix \\ref{sec:appendix_table_typology} for further explanation.}\n    \\textbf{\\label{table:comparison}}\n    \\vspace{-5ex}\n\\end{table*}", "cites": [5483, 1096, 5474, 115, 7, 7937, 5493, 5492, 5479, 5491, 4215, 7936, 5489, 5477, 5490, 5476], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 39, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured comparison of model-based data augmentation techniques and references multiple papers to support the discussion, showing basic synthesis. It organizes these methods in a table with key characteristics, which contributes to its comparative nature. However, it lacks deeper critical evaluation or abstraction to broader principles, focusing more on summarizing and categorizing methods."}}
{"id": "b43c1a07-b9e4-4bf2-be1a-6251b0622b9b", "title": "Low-Resource Languages", "level": "subsection", "subsections": [], "parent_id": "2a5ee271-1f92-4c75-bf18-559647dea32f", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Applications"], ["subsection", "Low-Resource Languages"]], "content": "\\label{applications:1} Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet  may be difficult to use effectively here.\\footnote{Low-resource language challenges discussed more in \\S\\ref{sec:currentchallenges}.} There are ways to leverage high-resource languages for low-resource languages, particularly if they have similar linguistic properties.  use this approach to improve low-resource NMT.\n use \\textit{backtranslation} and self-learning to generate augmented training data. Inspired by work in CV,  generate additional training examples that contain low-frequency (rare) words in synthetically created contexts.  present a DA framework to generate multi-lingual code-switching data to finetune multilingual-BERT. It encourages the alignment of representations from source and multiple target languages once by mixing their context information. They see improved performance across 5 tasks with 19 languages.", "cites": [4976, 7939, 7938, 5494], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured overview of data augmentation for low-resource languages, connecting the use of backtranslation, code-switching, and rare word generation to the broader NLP task of improving NMT. While it integrates some methods across cited papers, the synthesis is limited by brief mentions and lack of deeper connections. There is some critical insight, such as noting the limitations of external knowledge sources like WordNet, but fewer evaluations of specific techniques. The section identifies patterns, such as the importance of alignment in multilingual models, offering a moderate level of abstraction."}}
{"id": "06a3d877-f59d-402c-8390-c603de43cf06", "title": "Mitigating Bias", "level": "subsection", "subsections": [], "parent_id": "2a5ee271-1f92-4c75-bf18-559647dea32f", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Applications"], ["subsection", "Mitigating Bias"]], "content": "\\label{applications:2} \n attempt to mitigate gender bias in coreference resolution by creating an augmented dataset identical to the original but biased towards the underrepresented gender (using gender swapping of entities such as replacing \\textit{\"he\"} with \\textit{\"she\"}) and train on the union of the two datasets.  formally propose \\textsc{counterfactual DA} (CDA) for gender bias mitigation, which involves causal interventions that break associations between gendered and gender-neutral words.  and  propose further improvements to \\textsc{CDA}.  augment training sentences with their corresponding predicate-argument structures, improving the robustness of transformer models against various types of biases.", "cites": [7939, 5495], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of two cited papers related to bias mitigation in NLP through data augmentation. It lacks synthesis by not connecting these works to broader themes or trends in bias mitigation, and offers minimal critical analysis or abstraction beyond the specific methods described."}}
{"id": "72ec0e5f-41a4-4de2-8d2a-07fe478ab4c6", "title": "Fixing Class Imbalance", "level": "subsection", "subsections": [], "parent_id": "2a5ee271-1f92-4c75-bf18-559647dea32f", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Applications"], ["subsection", "Fixing Class Imbalance"]], "content": "\\label{applications:3} Fixing class imbalance typically involves a combination of undersampling and oversampling. \n\\textsc{Synthetic Minority Oversampling Technique} (SMOTE) , which generates augmented minority class examples through interpolation, still remains popular . \\textsc{Multilabel SMOTE} (MLSMOTE)  modifies SMOTE to balance classes for multi-label classification, where classifiers predict more than one class at the same time. Other techniques such as EDA  can possibly be used for oversampling as well.", "cites": [8392, 1096], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief description of class imbalance solutions like SMOTE and MLSMOTE but lacks meaningful synthesis or comparison of the cited works. It does not critically analyze the effectiveness, limitations, or trade-offs of these methods, nor does it generalize to broader principles or trends in data augmentation for class imbalance in NLP."}}
{"id": "c2675739-f8d3-41b4-bd7c-921f3bdddcc3", "title": "Few-Shot Learning", "level": "subsection", "subsections": [], "parent_id": "2a5ee271-1f92-4c75-bf18-559647dea32f", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Applications"], ["subsection", "Few-Shot Learning"]], "content": "\\label{applications:4} DA methods can ease few-shot learning by adding more examples for novel classes introduced in the few-shot phase.  use learned analogy transformations $\\phi(z_1,z_2,x)$ between example pairs from a non-novel class $z_{1} \\rightarrow z_{2}$ to generate augmented examples $x \\rightarrow x'$ for novel classes. \n generalize this to beyond just linear offsets, through their \"$\\Delta$-network\" autoencoder which learns the distribution $P(z_2|z_1,C)$ from all $y^{*}_{z_1} = y^{*}_{z_2} = C$ pairs, where $C$ is a class and $y$ is the ground-truth labelling function. Both these methods are applied only on image tasks, but their theoretical formulations are generally applicable, and hence we discuss them.\n apply these and other DA methods for few-shot learning of novel intent classes in task-oriented dialog.\n show that data augmentation facilitates curriculum learning for training triplet networks for few-shot text classification.  use T5 to generate additional examples for data-scarce classes.", "cites": [5484, 5478, 3637, 5496], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how data augmentation is applied in few-shot learning but lacks deeper synthesis of the cited works. It mentions different methods and their applications without drawing connections between them or offering a unified framework. There is minimal critical analysis or abstraction, as it focuses on summarizing specific techniques rather than evaluating their strengths, weaknesses, or broader implications."}}
{"id": "177e7b17-51d4-4dd0-839b-2014c6b34d93", "title": "Adversarial Examples (AVEs)", "level": "subsection", "subsections": [], "parent_id": "2a5ee271-1f92-4c75-bf18-559647dea32f", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Applications"], ["subsection", "Adversarial Examples (AVEs)"]], "content": "\\label{applications:5} Adversarial examples can be generated using innocuous label-preserving transformations (e.g. paraphrasing) that fool state-of-the-art NLP models, as shown in . Specifically, they add sentences with distractor spans to passages to construct AVEs for span-based QA.  construct AVEs for paraphrase detection using word swapping.  and  create AVEs for textual entailment using WordNet relations.", "cites": [7939, 5494], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes specific adversarial example generation methods from cited papers but lacks synthesis, critical evaluation, or abstraction. It provides a minimal narrative by listing techniques without connecting them to broader themes or analyzing their strengths and limitations."}}
{"id": "e7641332-e15f-4d23-a519-feaf23438a46", "title": "Tasks", "level": "section", "subsections": ["0730c27d-0992-4090-904e-65e8c56e082a", "92b0698d-9d2b-4744-90d0-4928f299c928", "01b3ecd7-108b-4e3a-9a51-fff419da00aa", "b0f25cc9-18d2-4353-a922-ffe5b8720f81", "0d7214f1-84e5-420e-bfb6-fd2b699bb5b6", "9d59ef90-677c-4433-b821-bc78169667f8", "11cd3814-627e-402b-8082-9d878a760c31", "9a7295dc-9668-469e-91d8-f550aa38eee6", "c0014d4c-5fe0-4aeb-b642-f63c8497a35c", "b05ac4a2-a37e-47ef-83e7-ce31a948fd22"], "parent_id": "91cd4abf-5403-4058-9b30-84529d4887a4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"]], "content": "\\label{sec:tasks}\nIn this section, we discuss several DA works for common NLP tasks.\\footnoteref{note1} We focus on non-classification tasks as classification is worked on by default, and well covered in earlier sections (e.g. \\S\\ref{sec:techniquesandmethods} and \\S\\ref{sec:applications}). Numerous previously mentioned DA techniques, e.g. , have been used or can be used for text classification tasks.", "cites": [1096], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides minimal synthesis, critical analysis, and abstraction. It merely mentions that several DA techniques can be used for text classification without elaborating on how or why, and it does not engage with the cited paper in any meaningful way. There is no effort to connect ideas, evaluate methods, or generalize findings."}}
{"id": "0730c27d-0992-4090-904e-65e8c56e082a", "title": "Summarization", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Summarization"]], "content": "\\label{tasks:3}\n investigate \\textit{backtranslation} as a DA method for few-shot abstractive summarization with the use of a consistency loss inspired by UDA.  propose an iterative DA approach for abstractive summarization that uses a mix of synthetic and real data, where the former is generated from Common Crawl.  introduce a query-focused summarization  dataset collected using Wikipedia called \\textsc{WikiRef} which can be used for DA.  use DA methods to construct two training datasets for Query-focused Multi-Document Summarization (QMDS) called \\textsc{QmdsCnn} and \\textsc{QmdsIr} by modifying CNN/DM  and mining search-query logs, respectively.", "cites": [5498, 5497, 1140], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of data augmentation methods applied to summarization tasks, citing specific papers and their contributions. However, it lacks synthesis by not connecting these approaches in a structured way or highlighting their relationships. There is minimal critical analysis or abstraction, as it does not evaluate the methods, compare their effectiveness, or generalize to broader principles."}}
{"id": "92b0698d-9d2b-4744-90d0-4928f299c928", "title": "Question Answering (QA)", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Question Answering (QA)"]], "content": "\\label{tasks:5}\n investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by \\textit{backtranslation}.  propose a DA method using distant supervision to improve BERT finetuning for open-domain QA.  leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA.  propose \\textsc{XLDA}, or \\textsc{cross-lingual DA}, which substitutes a portion of the input text with its translation in another language, improving performance across multiple languages on NLI tasks including the SQuAD QA task.  use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models.  introduce a new QA architecture called QANet that shows improved performance on SQuAD when combined with augmented data generated using backtranslation.", "cites": [5492, 7940, 7941], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lists several data augmentation techniques applied to question answering but lacks synthesis, integration, or comparison of the methods. It does not provide a deeper analysis or abstract patterns across the cited works, and the presentation remains largely descriptive with minimal critical evaluation."}}
{"id": "01b3ecd7-108b-4e3a-9a51-fff419da00aa", "title": "Sequence Tagging Tasks", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Sequence Tagging Tasks"]], "content": "\\label{tasks:4}\n propose \\textsc{DAGA}, a two-step \\textsc{DA} process. First, a language model over sequences of tags and words linearized as per a certain scheme \nis learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. , discussed in \\S\\ref{subsec:rule-based}, use \\emph{dependency tree morphing} (Figure \\ref{img-deptreemorph}) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. \n modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models.  propose a DA method based on \\textsc{MixUp} called \\textsc{SeqMix} for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection.", "cites": [5483], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes data augmentation methods for sequence tagging tasks, mentioning DAGA, dependency tree morphing, and SeqMix without providing a deeper synthesis of their relationships or contributions. There is minimal critical analysis or abstraction to broader principles, focusing instead on individual techniques and their applications."}}
{"id": "b0f25cc9-18d2-4353-a922-ffe5b8720f81", "title": "Parsing Tasks", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Parsing Tasks"]], "content": "\\label{tasks:7}\n propose \\textsc{data recombination} for injecting task-specific priors to neural semantic parsers. A \nsynchronous context-free grammar (SCFG) is induced from training data, and new \"recombinant\" examples are sampled.  introduce \\textsc{Grappa}, a pretraining approach for table semantic parsing, and generate synthetic question-SQL pairs via an SCFG. \n use \n\\textit{compositionality} to construct synthetic examples for downstream tasks like semantic parsing. Fragments of original examples are replaced with fragments from other examples in similar contexts. \n investigate DA for low-resource dependency parsing including \\emph{dependency tree morphing} from  (Figure \\ref{img-deptreemorph}) and modified \\textit{nonce} sentence generation from , which replaces content words with other words of the same POS, morphological features, and dependency labels.", "cites": [7942, 5483], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes specific data augmentation methods applied to parsing tasks, such as data recombination, SCFG-based synthetic example generation, and dependency tree morphing. However, it lacks synthesis of these methods into a broader framework, offers no critical evaluation or comparison of their strengths and weaknesses, and does not abstract to general principles or trends. The presentation remains largely descriptive."}}
{"id": "0d7214f1-84e5-420e-bfb6-fd2b699bb5b6", "title": "Grammatical Error Correction (GEC)", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Grammatical Error Correction (GEC)"]], "content": "\\label{tasks:9}\nLack of parallel data is typically a barrier for GEC. Various works have thus looked at DA methods for GEC. \nWe discuss some here, and more can be found in Table \\ref{table:additional_DA_works_GEC} in Appendix \\ref{section:appendix_additional_DA_works}. \nThere is work that makes use of additional resources.  use German edits from Wikipedia revision history and use those relating to GEC as augmented training data.  \n explore multi-task transfer, or the use of annotated data from other tasks.\nThere is also work that adds synthetic errors to noise the text. \n investigate two approaches: token-level perturbations \nand training error generation models with a filtering strategy to keep generations with sufficient errors.  use \\textit{confusion sets} generated by a spellchecker for noising.  learn error patterns from small annotated samples along with \\textit{POS-specific noising}.\nThere have also been approaches to improve the diversity of generated errors.  investigate noising through editing the latent representations of grammatical sentences, and  use a neural sequence transduction model and beam search noising procedures.", "cites": [7944, 7943, 5499], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of data augmentation methods used in Grammatical Error Correction (GEC) but lacks in-depth synthesis and critical evaluation. It mentions different approaches (e.g., using Wikipedia edits, synthetic errors, confusion sets, and latent representation editing) and attributes them to various papers, but it does not connect these ideas in a meaningful way or evaluate their effectiveness or limitations. Pattern recognition and abstraction are minimal."}}
{"id": "9d59ef90-677c-4433-b821-bc78169667f8", "title": "Neural Machine Translation (NMT)", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Neural Machine Translation (NMT)"]], "content": "\\label{tasks:10}\nThere are many works which have investigated DA for NMT. We highlighted some in \\S\\ref{sec:techniquesandmethods} and \\S\\ref{applications:1}, e.g. . \nWe discuss some further ones here, and more can be found in Table \\ref{table:additional_DA_works_MT} in Appendix \\ref{section:appendix_additional_DA_works}.\n propose \\textsc{SwitchOut}, a DA method that randomly replaces words in both source and target sentences with other random words from their corresponding vocabularies.  introduce \\textsc{Soft Contextual DA} that softly augments randomly chosen words in a sentence using a contextual mixture of multiple related words over the vocabulary.  propose \\textsc{Data Diversification} which merges original training data with the predictions of several forward and backward models.", "cites": [7939, 5494, 5490], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of three data augmentation methods for NMT but lacks meaningful synthesis of the ideas or connection between them. It does not critically evaluate the methods, their strengths, or limitations. Additionally, it remains at the level of specific techniques without abstracting broader patterns or principles."}}
{"id": "11cd3814-627e-402b-8082-9d878a760c31", "title": "Data-to-Text NLG", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Data-to-Text NLG"]], "content": "\\label{tasks:8}\n\\textit{Data-to-text NLG} refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables . Randomly perturbing game score values without invalidating overall game outcome is one \\textsc{DA} strategy explored in game summary generation .\nTwo popular recent benchmarks are E2E-NLG  and WebNLG . Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively.  show performance gains on WebNLG by \\textsc{DA} using Wikipedia sentences as targets and parsed OpenIE triples as inputs.  propose \\textsc{DA} for E2E-NLG based on permuting the input MR sequence.  inject Gaussian noise into a trained decoder's hidden states and sample diverse augmented examples from it. This \nsample-augment-retrain loop helps performance on E2E-NLG.", "cites": [7533, 2345], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of data-to-text NLG and references two papers (E2E-NLG and WebNLG), but the synthesis is limited to listing methods rather than connecting them conceptually. There is minimal critical analysis or identification of broader patterns; the content primarily describes specific DA techniques used in these tasks."}}
{"id": "c0014d4c-5fe0-4aeb-b642-f63c8497a35c", "title": "Dialogue", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Dialogue"]], "content": "\\label{tasks:2}\nMost DA approaches for dialogue focus on task-oriented dialogue. We outline some below, and more can be found in Table \\ref{table:additional_DA_works_dialogue} in Appendix \\ref{section:appendix_additional_DA_works}.\n present sentence and word-level DA approaches for end-to-end task-oriented dialogue.  propose \\textsc{lightweight augmentation}, a set of word-span and sentence-level DA methods for low-resource slot filling and intent classification. \n present a seq2seq DA framework to augment dialogue utterances for dialogue language understanding , \nincluding a \\textit{diversity rank} to produce diverse utterances. \n propose \n\\textsc{MADA} to generate diverse responses using the property that several valid responses exist for a dialogue context. \nThere is also DA work for spoken dialogue. , , , and  investigate DA methods for dialogue and \\emph{spoken language understanding} (SLU), including generative latent variable models.", "cites": [5500, 7945, 5499], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a straightforward listing of data augmentation methods applied to dialogue systems without synthesizing or connecting the approaches in a deeper framework. It lacks critical evaluation of the cited papers and does not abstract beyond the specific methods to identify broader patterns or principles. The narrative is minimal and primarily descriptive."}}
{"id": "b05ac4a2-a37e-47ef-83e7-ce31a948fd22", "title": "Multimodal Tasks", "level": "subsection", "subsections": [], "parent_id": "e7641332-e15f-4d23-a519-feaf23438a46", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Tasks"], ["subsection", "Multimodal Tasks"]], "content": "\\label{tasks:11}\nDA techniques have also been proposed for multimodal tasks where aligned data for multiple modalities is required. We look at ones that involve language or text. Some are discussed below, and more can be found in Table \\ref{table:additional_DA_works_multimodal} in Appendix \\ref{section:appendix_additional_DA_works}.\nBeginning with speech,  propose a DA method to improve the robustness of downstream dialogue models to speech recognition errors.  and  propose DA methods for end-to-end \\emph{automatic speech recognition} (ASR). \nLooking at images or video,  learn a cross-modality matching network to produce synthetic image-text pairs for multimodal classifiers. \n explore DA methods such as synonym replacement and contextualized word embeddings augmentation using BERT for \\emph{image captioning}. , , and  propose methods for \\emph{visual QA} including question generation and adversarial examples.", "cites": [5502, 5501, 7946], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of data augmentation methods applied to speech and vision-based NLP tasks but lacks synthesis, critical evaluation, or abstraction. It lists approaches and papers without connecting them to broader themes, comparing their effectiveness, or identifying underlying principles or limitations."}}
{"id": "0b817777-283f-4c05-a011-2f70634ff3c9", "title": "Minimal benefit for pretrained models on in-domain data:", "level": "paragraph", "subsections": ["eb2be4a4-51ba-43fd-a731-16dfe5d21f0f", "9da6575c-ebe0-4349-ac22-aeadcf8d1a9e", "30114ec6-d641-41ac-9e17-540b1a338144", "a6571abe-a5eb-44a5-9f62-4b0545353eea", "4b067f70-02a9-477d-a457-1eedd2067b73", "13341726-92bb-440d-bb06-a01dd4fbb98b", "f473b5b1-ad92-4ce0-9490-f41280863ebc", "f106c1cf-ad71-4cc4-ad08-a71c4cd6dbaa", "e601ade3-b53b-4620-a3d7-b7879626f88a"], "parent_id": "19740f4b-91d2-45a0-9b47-958d3dd47355", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Challenges \\& Future Directions"], ["paragraph", "Dissonance between empirical novelties and theoretical narrative:"], ["paragraph", "Minimal benefit for pretrained models on in-domain data:"]], "content": "With the popularization of large pretrained language models, it has come to light that a couple previously effective \\textsc{DA} techniques for certain English text classification tasks  provide little benefit for models like BERT and RoBERTa, which already achieve high performance on in-domain text classification . One hypothesis is that using simple DA techniques provides little benefit when finetuning large pretrained transformers on tasks for which examples are well-represented in the pretraining data, but DA methods could still be effective when finetuning on tasks for which examples are scarce or out-of-domain compared with the training data. Further work could study under which scenarios data augmentation for large pretrained models is likely to be effective.", "cites": [1096, 5477], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from two cited papers to highlight a broader trend regarding the diminishing returns of data augmentation for large pretrained models on in-domain data. It provides a critical perspective by questioning the effectiveness of DA in specific scenarios and proposes a hypothesis grounded in the pretraining data distribution. The abstraction level is strong as it generalizes beyond the cited works to identify a potential principle about when DA is likely to be beneficial for NLP models."}}
{"id": "eb2be4a4-51ba-43fd-a731-16dfe5d21f0f", "title": "Multimodal challenges:", "level": "paragraph", "subsections": [], "parent_id": "0b817777-283f-4c05-a011-2f70634ff3c9", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Challenges \\& Future Directions"], ["paragraph", "Dissonance between empirical novelties and theoretical narrative:"], ["paragraph", "Minimal benefit for pretrained models on in-domain data:"], ["paragraph", "Multimodal challenges:"]], "content": "While there has been increased work in multimodal DA, as discussed in \\S\\ref{tasks:11}, effective DA methods for multiple modalities has been challenging. Many works focus on augmenting a single modality or multiple ones separately. \nFor example, there is potential to further explore simultaneous image and text augmentation for image captioning, such as a combination of \\textsc{CutMix}  and caption editing.", "cites": [107], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly touches on the challenge of multimodal data augmentation, citing one paper (CutMix) to illustrate a possible approach for combining image and text augmentation. While it suggests an analytical direction by identifying the current limitation and a potential solution, the synthesis is minimal and does not connect multiple ideas or trends in the literature. The critical analysis is weak, as it does not evaluate the cited method or broader limitations of existing approaches. The abstraction is also limited, offering only a surface-level suggestion without generalizing to broader principles."}}
{"id": "30114ec6-d641-41ac-9e17-540b1a338144", "title": "Working in specialized domains", "level": "paragraph", "subsections": [], "parent_id": "0b817777-283f-4c05-a011-2f70634ff3c9", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Challenges \\& Future Directions"], ["paragraph", "Dissonance between empirical novelties and theoretical narrative:"], ["paragraph", "Minimal benefit for pretrained models on in-domain data:"], ["paragraph", "Working in specialized domains"]], "content": "such as those with domain-specific vocabulary and jargon (e.g. \\emph{medicine}) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data .", "cites": [7943], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section makes a basic connection between data augmentation challenges and the limitations of pretrained models in specialized domains, citing one relevant paper. It provides a modest critique by pointing out the reduced benefit of DA with out-of-domain data but does not deeply evaluate or compare the cited work. The abstraction level is limited to general observations rather than identifying overarching principles or frameworks."}}
{"id": "13341726-92bb-440d-bb06-a01dd4fbb98b", "title": "Self-supervised learning:", "level": "paragraph", "subsections": [], "parent_id": "0b817777-283f-4c05-a011-2f70634ff3c9", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Challenges \\& Future Directions"], ["paragraph", "Dissonance between empirical novelties and theoretical narrative:"], ["paragraph", "Minimal benefit for pretrained models on in-domain data:"], ["paragraph", "Self-supervised learning:"]], "content": "More recently, \\textsc{DA} has been increasingly used as a key component of self-supervised learning, particularly in vision . In NLP, BART  showed that predicting deleted tokens as a pretraining task can achieve similar performance as the masked LM, and \\textsc{Electra}  found that pretraining by predicting corrupted tokens outperforms BERT given the same model size, data, and compute. We expect future work will continue exploring how to effectively manipulate text for both pretraining and downstream tasks.", "cites": [1557, 1096], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly mentions the use of data augmentation in self-supervised learning and cites two papers (ELECTRA and BART) to illustrate different pretraining methods. It makes a minimal connection between these methods, but lacks deeper synthesis or a broader framework. There is limited critical analysis of the cited works, and the abstraction is modest, touching on trends without articulating overarching principles or theoretical implications."}}
{"id": "f106c1cf-ad71-4cc4-ad08-a71c4cd6dbaa", "title": "Lack of unification", "level": "paragraph", "subsections": [], "parent_id": "0b817777-283f-4c05-a011-2f70634ff3c9", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Challenges \\& Future Directions"], ["paragraph", "Dissonance between empirical novelties and theoretical narrative:"], ["paragraph", "Minimal benefit for pretrained models on in-domain data:"], ["paragraph", "Lack of unification"]], "content": "is a challenge for the current literature on data augmentation for NLP, and popular methods are often presented in an auxiliary fashion. Whereas there are well-accepted frameworks for DA for CV (e.g. default augmentation libraries in PyTorch, \\emph{RandAugment} ), there are no such \"generalized\" DA techniques for NLP. Further, we believe that DA research would benefit from the establishment of standard and unified benchmark tasks and datasets to compare different augmentation methods.", "cites": [8393], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section briefly references a single paper (RandAugment) to contrast with the lack of unification in NLP data augmentation, suggesting a need for standardized methods. While it points out a gap in the literature, the synthesis is limited and primarily serves to highlight the difference between CV and NLP. The abstraction is modest, as it identifies a general trend but does not propose a higher-level framework."}}
{"id": "aef0cbaf-e2a0-4c77-9532-dd397a9b3b28", "title": "\\textsc{DA", "level": "section", "subsections": [], "parent_id": "91cd4abf-5403-4058-9b30-84529d4887a4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "\\textsc{DA"]], "content": "Methods Table - Description of Columns and Attributes}\\label{sec:appendix_table_typology}\nTable \\ref{table:comparison} in the main body compares a non-exhaustive selection of \\textsc{DA} methods along various aspects relating to their applicability, dependencies, and requirements. Below, we provide a more extensive description of each of this table's columns and their attributes.\n\\begin{enumerate}\n    \\item \\textbf{Ext.Know}: Short for external knowledge, this column is \\ding{51} when the data augmentation process requires knowledge resources which go beyond the immediate input examples and the task definition, such as WordNet  or PPDB . Note that we exclude the case where these resources are pretrained models under a separate point (next) for clarity, since these are widespread enough to merit a separate category.\n    \\item \\textbf{Pretrained}: Denotes that the data augmentation process requires a pretrained model, such as BERT  or GPT-2 . \n    \\item \\textbf{Preprocess}: Denotes the preprocessing steps, e.g. tokenization (\\textit{tok}), dependency parsing (\\textit{dep}), etc. required for the DA process. A hyphen (-) means either no preprocessing is required or that it was not explicitly stated.\n    \\item \\textbf{Level}: Denotes the depth and extent to which elements of the instance/data are modified by the \\textsc{DA}. Some primitives modify just the \\textsc{Input} (e.g. word swapping), some modify both \\textsc{Input} and \\textsc{Label} (e.g. negation), while others make changes in the embedding or hidden space (\\textsc{Embed/Hidden}) or higher representation layers enroute to the task model.\n    \\item \\textbf{Task-Agnostic}: This is an approximate, partially subjective column denoting the extent to which a \\textsc{DA} method can be applied to different tasks. When we say \\ding{51} here, we don't denote a very rigid sense of the term task-agnostic, but mean that it would possibly easily extend to most NLP tasks as understood by the authors. Similarly, an $\\times$ denotes being restricted to a specific task (or small group of related tasks) only. There can be other labels, denoting applicability to broad task families. For example, \\textsc{substructural} denotes the family of tasks where sub-parts of the input are also valid input examples in their own right, e.g. constituency parsing. \\textsc{Sentence Pairs} denotes tasks which involve pairwise sentence scoring such as paraphrase identification, duplicate question detection, and semantic textual similarity.\n\\end{enumerate}\n\\begin{comment}\n\\setlength{\\tabcolsep}{1pt}\n\\begin{table*}[!ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n    \\textbf{\\textsc{DA} Method} & Ext.Know & Pretrained & Preprocess & Level  & Task-Agnostic    \\\\ \n    \\midrule\n    \\textsc{Synonym Replacement}  & \\ding{51} & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{Random Deletion}  & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{Random Swap}  & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n        \\textsc{Backtranslation}  & $\\times$ & \\ding{51} & Depends & Input  & \\ding{51}   \\\\\n        \\textsc{SCPN}  & $\\times$ & \\ding{51} & const & Input  & \\ding{51}   \\\\\n    \\textsc{Semantic Text Exchange}  & $\\times$ & \\ding{51} & const & Input  & \\ding{51}   \\\\\n    \\textsc{ContextualAug}  & $\\times$ & \\ding{51} & - & Input  & \\ding{51}   \\\\\n    \\textsc{LAMBADA}  & $\\times$ & \\ding{51} & - & Input & $\\times$   \\\\\n    \\textsc{GECA}  & $\\times$  & $\\times$ & tok & Input  & $\\times$ \\\\\n       \\textsc{SeqMixUp}   & $\\times$ & $\\times$ & tok & Input & $\\times$ \\\\\n    \\textsc{SwitchOut}   & $\\times$  & $\\times$ & tok & Input  & $\\times$ \\\\   \n    \\textsc{EMix}   & $\\times$  & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n        \\textsc{MixText}   & $\\times$  & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n   \\textsc{SignedGraph}   & $\\times$  & $\\times$ & - & Input  & $\\times$ \\\\\n      \\textsc{DTreeMorph}   & $\\times$  & $\\times$ & dep & Input  & \\ding{51} \\\\\n            \\textsc{$Sub^2$}   & $\\times$  & $\\times$ & dep & Input  & Substructural \\\\\n\\textsc{DAGA}   & $\\times$  & $\\times$ & tok & Input+Label  & $\\times$ \\\\\n\\textsc{WN-Hypers}   & \\ding{51}  & $\\times$ & const+KWE & Input  & \\ding{51} \\\\\n\\textsc{Synthetic Noise}   & $\\times$  & $\\times$ & tok & Input  & \\ding{51} \\\\\n\\textsc{UEdin-MS} (DA portion)  & \\ding{51}  & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{Nonce}  & \\ding{51} & $\\times$ & const & Input & \\ding{51} \\\\\n\\textsc{XLDA}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n\\textsc{SeqMix}  & $\\times$ & \\ding{51} & tok & Input+Label & $\\times$ \\\\\n\\textsc{Slot-Sub}  & $\\times$ & $\\times$ & const & Input & $\\times$ \\\\\n\\textsc{Slot-Sub-LM}  & $\\times$ & \\ding{51} & tok & Input & \\ding{51} \\\\\n\\textsc{SNI}  & \\ding{51} & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{UBT \\& TBT}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n\\textsc{Soft Contextual DA}  & $\\times$ & \\ding{51} & tok & Emb/Hidden & \\ding{51} \\\\\n\\textsc{Data Diversification}  & $\\times$ & \\ding{51} & Depends & Input & \\ding{51} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparing a non-exhaustive selection of \\textsc{DA} methods along various aspects relating to their applicability, dependencies, and requirements. \\emph{Ext.Know}, \\emph{KWE}, \\emph{tok}, \\emph{const}, and \\emph{dep} stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. \n    See Appendix \\ref{sec:appendix_table_typology} for further explanation about this table's columns and attributes.}\n    \\textbf{\\label{table:comparison}}\n    \\vspace{-5ex}\n\\end{table*}\n\\end{comment}", "cites": [5483, 1096, 7, 5492, 7936, 5477, 5490], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section is primarily descriptive, offering column definitions for a comparison table of DA methods. It integrates minimal insight from the cited papers, focusing on categorization rather than deep synthesis or analysis. There is no substantial critique or evaluation of the methods, and while it attempts to generalize via categories like 'Task-Agnostic' and 'Level,' the abstraction remains limited and surface-level."}}
{"id": "0b995381-896c-4b37-a0a2-37d59f45fa9d", "title": "Additional Figure", "level": "section", "subsections": [], "parent_id": "91cd4abf-5403-4058-9b30-84529d4887a4", "prefix_titles": [["title", "A Survey of Data Augmentation Approaches for NLP"], ["section", "Additional Figure"]], "content": "\\label{sec:pedro}\n\\begin{figure}[ht]\n\\includegraphics[width=0.48\\textwidth]{sections/pedro.png}\\\\\n\\vspace{-0.8\\abovedisplayskip}\n  \\caption{\\label{tab:pedro} Pedro Domingos' quip about offline data augmentation.}\n\\end{figure}\n\\setlength{\\tabcolsep}{1pt}\n\\begin{table*}[!ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{0.25\\linewidth}|p{0.74\\linewidth}}\n    \\toprule\n    \\textbf{Paper/Work} & \\textbf{Brief Description} \\\\ \n    \\midrule\n     & Generate synthetic noised examples of Wikipedia sentences using backtranslation through various languages. \\\\\n    \\hline\n     & Detailed comparative study of the DA for GEC systems UEdin-MS  and Kakao\\&Brain . \\\\\n    \\hline\n     & Introduces error generation tool called GenERRate which learns to generate ungrammatical text with various errors by using an error analysis file. \\\\\n    \\hline\n     & Use a set of syntactic rules for common Japanese grammatical errors to generate augmented error-correct sentence pairs for Japanese GEC.\\\\\n    \\hline\n     & Thesis that surveys previous work on error generation and investigates some new approaches using random and probabilistic methods.\\\\\n    \\hline\n     & Noises using five error types: concatenation, misspelling, substitution, deletion, and transposition. Decent performance on the BEA 2019 Shared Task.\\\\\n    \\hline\n     & Explore backtranslation and feature discrimination for DA.\\\\\n    \\hline\n     & DA by extracting Japanese GEC training data from the revision log of a language learning SNS.\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Additional DA works for grammatical error correction (GEC), along with a brief description of each.}\n    \\textbf{\\label{table:additional_DA_works_GEC}}\n    \\vspace{-5ex}\n\\vspace{2cm}\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{0.25\\linewidth}|p{0.74\\linewidth}}\n    \\toprule\n    \\textbf{Paper/Work} & \\textbf{Brief Description} \\\\ \n    \\midrule\n     & Present a \\textit{synthetic noise induction} model which heuristically adds social media noise to text, and \\textit{labeled backtranslation}.\\\\\n    \\hline\n     & Present a DA method to project words from closely-related high-resource languages to low-resource languages using word embedding representations.\\\\\n    \\hline\n     & Propose \\textit{AdvAug}, an adversarial augmentation method for NMT, by sampling adversarial examples from a new vicinity distribution and using their embeddings to augment training.\\\\\n    \\hline\n     & Investigate improvements to sampling-based approaches and the synthetic data generated by backtranslation.\\\\\n    \\hline\n     & Propose DA approaches for NMT that leverage information retrieved from a Translation Memory (TM) and using fuzzy TM matches.\\\\\n    \\hline\n     & Propose an NMT model \\textsc{KG-NMT} which is augmented by knowledge graphs to enhance semantic feature extraction and hence the translation of entities and terminological expressions.\\\\\n    \\hline\n     & Propose dictionary-based DA (\\textsc{DDA}) for cross-domain NMT by synthesizing a domain-specific dictionary and automatically generating a pseudo in-domain parallel corpus.\\\\\n    \\hline\n     & Present a DA method using sentence boundary segmentation to improve the robustness of NMT on ASR transcripts.\\\\\n    \\hline\n     & Introduce DA methods for multi-source NMT that fills in incomplete portions of multi-source training data.\\\\\n    \\hline\n     & Investigate effectiveness of DA by backtranslation for context-aware NMT.\\\\\n    \\hline\n     & Present DA methods to improve NMT robustness to noise while keeping models small, and explore the use of noise from external data (speech transcripts).\\\\\n    \\hline\n     & Propose DA method to create synthetic data by leveraging the embedding representation of sentences.\\\\\n    \\hline\n     & Propose two methods for pipeline-based speech translation through the introduction of errors through 1. utilizing a speech processing workflow and 2. a rule-based method.\\\\\n    \\hline\n     & Investigate extremely low-resource settings for NMT and a DA approach using a noisy dictionary and language models.\\\\\n    \\hline\n     & Investigate a DA method for lexically constraint-aware NMT to construct constraint-aware synthetic training data.\\\\\n    \\hline\n     & Propose a diversity DA method for low-resource NMT by generating diverse synthetic parallel data on both source and target sides using a restricted sampling strategy during decoding.\\\\\n    \\hline\n     & Propose syntax-aware DA methods with sentence-specific word selection probabilities using dependency parsing.\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Additional DA works for neural machine translation (NMT), along with a brief description of each.}\n    \\textbf{\\label{table:additional_DA_works_MT}}\n    \\vspace{-5ex}\n\\end{table*}\n\\clearpage\n\\setlength{\\tabcolsep}{1pt}\n\\begin{table*}[ht!]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{0.15\\linewidth}|p{0.84\\linewidth}}\n    \\toprule\n    \\textbf{Paper/Work} & \\textbf{Brief Description} \\\\ \n    \\midrule\n     & Propose a paraphrase augmented response generation (PARG) framework to improve dialogue generation by automatically constructing augmented paraphrased training examples based on dialogue state and act labels. \\\\\n    \\hline\n     & Introduce a graph-based representation of dialogues called Conversation Graph (ConvGraph) that can be used for DA by creating new dialogue paths. \\\\\n    \\hline\n     & Propose an RL-based DA approach for dialogue state tracking (DST). \\\\\n    \\hline\n     & Propose a simple DA algorithm to improve the training of copy-mechanism models for dialogue state tracking (DST). \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Additional DA works for dialogue, along with a brief description of each.}\n    \\textbf{\\label{table:additional_DA_works_dialogue}}\n    \\vspace{-5ex}\n    \\vspace{2cm}\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{0.15\\linewidth}|p{0.84\\linewidth}}\n    \\toprule\n    \\textbf{Paper/Work} & \\textbf{Brief Description} \\\\ \n    \\midrule\n     & Propose a DA method for emotion recognition from a combination of audio, visual, and textual modalities.\\\\\n    \\hline\n     & Introduce a DA method for Audio-Video Scene-Aware Dialogue, which involves dialogue containing a sequence of QA pairs about a video.\\\\\n    \\hline\n     & Investigate DA techniques for video QA including mirroring and horizontal flipping.\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Additional DA works for multimodal tasks, along with a brief description of each.}\n    \\textbf{\\label{table:additional_DA_works_multimodal}}\n    \\vspace*{5.1in}\n    \\vspace{-5ex}\n\\end{table*}\n\\begin{comment}\n\\setlength{\\tabcolsep}{1pt}\n\\begin{table*}[t!]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{0.10\\linewidth}|p{0.20\\linewidth}|p{0.70\\linewidth}}\n    \\toprule\n    \\textbf{Task} & \\textbf{Paper} & \\textbf{Brief Description} \\\\ \n    \\midrule\n    \\textsc{GEC} &  & Generate synthetic noised examples of Wikipedia sentences using backtranslation through various languages. \\\\\n    \\textsc{GEC} &  & Detailed comparative study of the DA for GEC systems UEdin-MS  and Kakao\\&Brain . \\\\\n    \\textsc{GEC} &  & Introduces error generation tool called GenERRate which learns to generate ungrammatical text with various errors by using an error analysis file. \\\\\n    \\textsc{GEC} &  & Use a set of syntactic rules for common Japanese grammatical errors to generate augmented error-correct sentence pairs for Japanese GEC.\\\\\n    \\textsc{GEC} &  & Thesis that surveys previous work on error generation and investigates some new approaches using random and probabilistic methods.\\\\\n    \\textsc{GEC} &  & Noises using five error types: concatenation, misspelling, substitution, deletion, and transposition. Decent performance on the BEA 2019 Shared Task.\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Additional DA works broken down by task, along with a brief description of each.}\n    \\textbf{\\label{table:additional_DA_works_by_task}}\n    \\vspace{-5ex}\n\\end{table*}\n\\end{comment}\n\\begin{comment}\n\\setlength{\\tabcolsep}{1pt}\n\\begin{table*}[!ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n    \\textbf{\\textsc{DA} Method} & Ext.Know & Pretrained & Learnt\\textsc{DA} & Preprocess & Level  & Task-Agnostic    \\\\ \n    \\midrule\n    \\textsc{SR}  & \\ding{51} & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{RD}  & $\\times$ & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n    \\textsc{RS}  & $\\times$ & $\\times$ & $\\times$ & tok & Input  & \\ding{51}   \\\\\n        \\textsc{Backtranslation}  & $\\times$ & \\ding{51} & $\\times$ & Depends & Input  & \\ding{51}   \\\\\n        \\textsc{SCPN}  & $\\times$ & \\ding{51} & $\\times$ & const & Input  & \\ding{51}   \\\\\n    \\textsc{Semantic Text Exchange}  & $\\times$ & \\ding{51} & $\\times$ & const & Input  & \\ding{51}   \\\\\n    \\textsc{ContextualAug}  & $\\times$ & \\ding{51} & $\\times$ & - & Input  & \\ding{51}   \\\\\n    \\textsc{LAMBADA}  & $\\times$ & \\ding{51} & $\\times$ & - & Input  & $\\times$   \\\\\n    \\textsc{GECA}  & $\\times$  & $\\times$ & $\\times$ & tok & Input  & $\\times$ \\\\\n       \\textsc{SeqMixUp}   & $\\times$  & $\\times$ & $\\times$ & tok & Input  & $\\times$ \\\\\n    \\textsc{SwitchOut}   & $\\times$  & $\\times$ & $\\times$ & tok & Input  & $\\times$ \\\\   \n    \\textsc{EMix}   & $\\times$  & $\\times$ & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n        \\textsc{MixText}   & $\\times$  & $\\times$ & $\\times$ & - & Emb/Hidden  & \\ding{51} \\\\\n   \\textsc{SignedGraph}   & $\\times$  & $\\times$ & $\\times$ & - & Input  & $\\times$ \\\\\n      \\textsc{DTreeMorph}   & $\\times$  & $\\times$ & $\\times$ & dep & Input  & \\ding{51} \\\\\n            \\textsc{$Sub^2$}   & $\\times$  & $\\times$ & $\\times$ & dep & Input  & Substructural \\\\\n\\textsc{DAGA}   & $\\times$  & $\\times$ & $\\times$ & tok & Input+Label  & $\\times$ \\\\\n\\textsc{WN-Hypers}   & \\ding{51}  & $\\times$ & $\\times$ & const+KWE & Input  & \\ding{51} \\\\\n\\textsc{Synthetic Noise}   & $\\times$  & $\\times$ & $\\times$ & tok & Input  & \\ding{51} \\\\\n\\textsc{UEdin-MS (DA portion)}  & \\ding{51}  & $\\times$ & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{Nonce}  & \\ding{51}  & $\\times$ & $\\times$ & const & Input & \\ding{51} \\\\\n\\textsc{XLDA}  & $\\times$ & \\ding{51} & $\\times$ & Depends & Input & \\ding{51} \\\\\n\\textsc{SeqMix}  & $\\times$ & \\ding{51} & $\\times$ & tok & Input+Label & $\\times$ \\\\\n\\textsc{Slot-Sub}  & $\\times$ & $\\times$ & $\\times$ & const & Input & $\\times$ \\\\\n\\textsc{Slot-Sub-LM}  & $\\times$ & \\ding{51} & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{SNI}  & \\ding{51} & $\\times$ & $\\times$ & tok & Input & \\ding{51} \\\\\n\\textsc{UBT \\& TBT}  & $\\times$ & \\ding{51} & $\\times$ & Depends & Input & \\ding{51} \\\\\n\\textsc{Soft Contextual DA}  & $\\times$ & \\ding{51} & $\\times$ & tok & Emb/Hidden & \\ding{51} \\\\\n\\textsc{Data Diversification}  & $\\times$ & \\ding{51} & $\\times$ & Depends & Input & \\ding{51} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparing a non-exhaustive selection of \\textsc{DA} methods along various aspects relating to their applicability, dependencies, and requirements. \\emph{Ext.Know}, \\emph{KWE}, \\emph{tok}, \\emph{const}, and \\emph{dep} stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. \n    See Appendix \\ref{sec:appendix_table_typology} for further explanation about this table's columns and attributes.}\n    \\textbf{\\label{table:comparison}}\n    \\vspace{-5ex}\n\\end{table*}\n\\end{comment}\n\\end{document}", "cites": [7943, 5487, 5503, 5483, 1096, 5499, 5024, 5492, 5490, 5504, 4977, 4976, 5505, 7936, 5477, 7947], "cite_extract_rate": 0.4222222222222222, "origin_cites_number": 45, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a descriptive listing of data augmentation methods from multiple papers, grouped by NLP tasks such as GEC, NMT, dialogue, and multimodal tasks. However, it lacks synthesis across the works, critical evaluation of their strengths or weaknesses, and abstraction to broader principles or trends in data augmentation research."}}
