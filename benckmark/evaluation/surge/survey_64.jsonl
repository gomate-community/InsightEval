{"id": "69856a4e-b74f-448b-acb3-7c296be1ae42", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nCurrently, there are nearly $7$ billion connected Internet of Things (IoT) devices and $3$ billion smartphones around the world. These devices are equipped with increasingly advanced sensors, computing, and communication capabilities. As such, they can potentially be deployed for various crowdsensing tasks, e.g., for medical purposes  and air quality monitoring . Coupled with the rise of Deep Learning (DL) , the wealth of data collected by end devices opens up countless possibilities for meaningful research and applications. \nIn the traditional cloud-centric approach, data collected by mobile devices is uploaded and processed centrally in a cloud-based server or data center. In particular, data collected by IoT devices and smartphones such as measurements , photos , videos , and location information  are aggregated at the data center . Thereafter, the data is used to provide insights or produce effective inference models. However, this approach is no longer sustainable for the following reasons. Firstly, data owners are increasingly privacy sensitive. Following privacy concerns among consumers in the age of big data, policy makers have responded with the implementation of data privacy legislations such as the European Commission's General Data Protection Regulation (GDPR)  and Consumer Privacy Bill of Rights in the US . In particular, the consent (GDPR Article 6) and data minimalization principle (GDPR Article 5) limits data collection and storage only to what is consumer-consented and absolutely necessary for processing. Secondly, a cloud-centric approach involves long propagation delays and incurs unacceptable latency  for applications in which real-time decisions have to be made, e.g., in self-driving car systems . Thirdly, the transfer of data to the cloud for processing burdens the backbone networks especially in tasks involving unstructured data, e.g., in video analytics . This is exacerbated by the fact that cloud-centric training is relatively reliant on wireless communications . As a result, this can potentially impede the development of new technologies. \nWith data sources mainly located outside the cloud today , Mobile Edge Computing (MEC) has naturally been proposed as a solution in which the computing and storage capabilities  of end devices and edge servers are leveraged on to bring model training closer to where data is produced . As defined in , an end-edge-cloud computing network comprises: (i) end devices, (ii) edge nodes, and (iii) cloud server. For model training in conventional MEC approaches, a collaborative paradigm has been proposed in which training data are first sent to the edge servers for model training up to lower level DNN layers, before more computation intensive tasks are offloaded to the cloud ,  (Fig. \\ref{Edge_AI}). However, this arrangement incurs significant communication costs and is unsuitable especially for applications that require persistent training . In addition, computation offloading and data processing at edge servers still involve the transmission of potentially sensitive personal data. This can discourage privacy-sensitive consumers from taking part in model training, or even violate increasingly stringent privacy laws . Although various privacy preservation methods, e.g.,\ndifferential privacy (DP)~, have been proposed, a number of users are still not willing to expose their private\ndata for fear that their data may be inspected by external servers. In the long run, this discourages the\ndevelopment of technologies as well as new applications. \n\\begin{figure}[t!]\n \\centering\n\\includegraphics[width=\\columnwidth]{Figs/cloudedgefl}\n \\caption{\\small Edge AI approach brings AI processing closer to where data is produced. In particular, FL allows training on devices where the data is produced.}\n \\label{Edge_AI}\n\\end{figure}\nTo guarantee that training data remains on personal devices and to facilitate collaborative machine learning of complex models among distributed devices, a decentralized ML approach called Federated Learning (FL) is introduced in . In FL, mobile devices use their local data to cooperatively train an ML model required by an FL server. They then send the model updates, i.e., the model's weights, to the FL server for aggregation. The steps are repeated in multiple rounds until a desirable accuracy is achieved. This implies that FL can be an enabling technology for ML model training at mobile edge networks. As compared to conventional cloud-centric ML model training approaches, the implementation of FL for model training at mobile edge networks features the following advantages. \n\\begin{itemize}\n\\item \\textit{Highly efficient use of network bandwidth:} Less information is required to be transmitted to the cloud. For example, instead of sending the raw data over for processing, participating devices only send the updated model parameters for aggregation. As a result, this significantly reduces costs of data communication and relieves the burden on backbone networks. \n\\item \\textit{Privacy:} Following the above point, the raw data of users need not be sent to the cloud. Under the assumption that FL participants and servers are non-malicious, this enhances user privacy and reduces the probability of eavesdropping to a certain extent. In fact, with enhanced privacy, more users will be willing to take part in collaborative model training and so, better inference models can be built.\n\\item \\textit{Low latency:} With FL, ML models can be consistently trained and updated. Meanwhile, in the MEC paradigm, real-time decisions, e.g., event detection, can be made locally at the edge nodes or end devices. Therefore, the latency is much lower than that when decisions are made in the cloud before transmitting them to the end devices. This is vital for time critical applications such as self-driving car systems in which the slightest delays can potentially be life threatening . \n\\end{itemize}\nGiven the aforementioned advantages, FL has seen recent successes in several applications. For example, the Federated Averaging algorithm (\\textit{FedAvg}) proposed in  has been applied to Google's Gboard  to improve next-word prediction models. In addition, several studies have also explored the use of FL in a number of scenarios in which data is sensitive in nature, e.g., to develop predictive models for diagnosis in health AI  and to foster collaboration across multiple hospitals  and Government agencies .\nBesides being an enabling technology for ML model training \\textit{at} mobile edge networks, FL has also been increasingly applied as an enabling technology \\textit{for} mobile edge network optimization. Given the computation and storage constraints of increasingly complex mobile edge networks, conventional network optimization approaches that are built on static models fare relatively poorly in modelling dynamic networks . As such, a data-driven Deep Learning (DL) based approach  for optimizing resource allocation is increasingly popular. For example, DL can be used for representation learning of network conditions  whereas Deep Reinforcement Learning (DRL) can optimize decision making through interactions with the dynamic environment . However, the aforementioned approaches require user data as an input and these data may be sensitive or inaccessible in nature due to regulatory constraints. As such, in this survey, we also consider FL's potential to serve as an enabling technology for optimizing mobile edge networks, e.g., in cell association , computation offloading , and vehicular networks .\nHowever, there are several challenges to be solved before FL can be implemented at scale. Firstly, even though raw data no longer needs to be sent to the cloud servers, communication costs remain an issue due to the high dimensonality of model updates and limited communication bandwith of participating mobile devices. In particular, state-of-the art DNN model training can involve the communication of millions of parameters for aggregation. Secondly, in a large and complex mobile edge network, the heterogeneity of participating devices in terms of data quality, computation power, and willingness to participate have to be well managed from the resource allocation perspective. Thirdly, FL does not guarantee privacy in the presence of malicious participants or aggregating servers. In particular, recent research works have clearly shown that a malicious participant may exist in FL and can infer the information of other participants just from the shared parameters alone. As such, privacy and security issues in FL still need to be considered.\nAlthough there are surveys on MEC and FL, the existing studies usually treat the two topics separately. For existing surveys on FL, the authors in  place more emphasis on discussing the architecture and categorization of different FL settings to be used for the varying distributions of training data. The authors in  highlight the applications of FL in wireless communications but do not discuss the issues pertaining to FL implementation. In addition, the focus of  is on cellular network architecture rather than mobile edge networks. In contrast, the authors in  provide a brief tutorial on FL and the challenges related to its implementation, but do not consider the issue of resource allocation in FL, or the potential applications of FL for mobile edge network optimization. On the other hand, for surveys in MEC that focus on implementing ML model training at edge networks, a macroscopic approach is usually adopted in which FL is briefly mentioned as one of the enabling technologies in the MEC paradigm, but without detailed elaboration with regards to its implementation or the related challenges. In particular, the authors in , , and  study the architectures and process of training and inference at edge networks without considering the challenges to FL implementation. In addition, surveys studying the implementation of DL for mobile edge network optimization mostly do not focus on FL as a potential solution to preserve data privacy. For example, the authors in  discuss strategies for optimizing caching and computation offloading for mobile edge networks, but do not consider the use of privacy preserving federated approaches in their studies. Similarly,  considers the use of DRL in communications and networking but do not include federated DRL approaches. \n\\begin{table*}\n\\centering\n\\arrayrulecolor{black}\n\\caption{\\small An overview of selected surveys in FL and MEC}\n\\label{surveytable}\n\\begin{tabular}{!{\\color{black}\\vrule}l|l!{\\color{black}\\vrule}l!{\\color{black}\\vrule}} \n\\hline\n\\rowcolor[rgb]{0.682,0.667,0.667} \\multicolumn{1}{|l|}{\\textbf{Ref.}} & \\textbf{Subject}      & \\textbf{Contribution}                                                                                                                \\\\ \n\\hline\n                                                                    & \\multirow{3}{*}{FL}   & Introductory tutorial on categorization of different FL settings, e.g., vertical FL, horizontal FL, and Federated Transfer Learning  \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                   &                       & FL in optimizing resource allocation for wireless networks while preserving data privacy                                             \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                    &                       & Tutorial on FL and discussions of implementation challenges in FL                                                                    \\\\ \n\\cline{1-1}\\arrayrulecolor{black}\\cline{2-2}\\arrayrulecolor{black}\\cline{3-3}\n                                                                   & \\multirow{10}{*}{MEC} & Computation offloading strategy to optimize DL performance in edge computing                                                         \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                   &                       & Survey on architectures and frameworks for edge intelligence                                                                         \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                  &                       & ML for IoT management, e.g., network management and security                                                                         \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                   &                       & Survey on computation offloading in MEC                                                                                              \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                     &                       & Survey on DRL approaches to address issues in communications and networking                                                          \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                    &                       & Survey on techniques for computation offloading                                                                                      \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                   &                       & Survey on architectures and applications of MEC                                                                                      \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                    &                       & Survey on computing, caching, and communication issues at mobile edge networks                                                       \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                   &                       & Survey on the phases of caching and comparison among the different caching schemes                                                   \\\\ \n\\cline{1-1}\\cline{3-3}\n                                                                    &                       & Survey on joint mobile computing and wireless communication resource management in MEC                                               \\\\\n\\hline\n\\end{tabular}\n\\end{table*}\n\\begin{figure*}[!]\n \\centering\n\\includegraphics[width=\\linewidth]{Figs/flowchart}\n \\caption{\\small Classification of related studies to be discussed in this survey. }\n \\label{flowchart}\n\\end{figure*}\nIn summary, most existing surveys on FL do not consider the applications of FL in the context of mobile edge networks, whereas existing surveys on MEC do not consider the challenges to FL implementation, or the potential of FL to be applied in mobile edge network optimization. This motivates us to have a comprehensive survey that has the following contributions: \n\\begin{itemize}\n\\item We motivate the importance of FL as an important paradigm shift towards enabling collaborative ML model training. Then, we provide a concise tutorial on FL implementation and present to the reader a list of useful open-source frameworks that paves the way for future research on FL and its applications. \n\\item We discuss the unique features of FL relative to a centralized ML approach and the resulting implementation challenges. For each of this challenge, we present to the reader a comprehensive discussion of existing solutions and approaches explored in the FL literature.\n\\item We discuss FL as an enabling technology for mobile edge network optimization. In particular, we discuss the current and potential applications of FL as a privacy-preserving approach for applications in edge computing.\n\\item We discuss the challenges and future research directions of FL.\n\\end{itemize}\nFor the reader's convenience, we classify the related studies to be discussed in this survey in Fig. \\ref{flowchart}. The classification is based on (i) FL at mobile edge network, i.e., studies that focus on solving the challenges and issues related to implementing the collaborative training of ML models on end devices, and (ii) FL for mobile edge network, i.e., studies that specifically explore the application of FL for mobile edge network optimization. While the former group of studies works on addressing the fundamental issues of FL, the latter group uses FL as an application tool to solve issues in edge computing. We also present a list of common abbreviations for reference in Table \\ref{tab:abbrev}. \nThe rest of this paper is organized as follows. Section \\ref{sec:intro_FD} introduces the background and fundamentals of FL. Section \\ref{sec: communication} reviews solutions provided to reduce communication costs. Section \\ref{sec:resource} discusses resource allocation approaches in FL. Section \\ref{sec:security} discusses privacy and security issues. Section \\ref{sec:application} discusses applications of FL for mobile edge network optimization. Section \\ref{sec:challenges_open_issues} discusses the challenges and future research directions in FL. Section \\ref{sec:conclusions} concludes the paper. \n\\begin{table}[]\n\\centering{}\\caption{\\small List of common abbreviations. \\label{tab:abbrev}}\n\\begin{tabular}{|l|l|}\n\\hline\n\\rowcolor[HTML]{9B9B9B} \n\\textbf{Abbreviation} & \\textbf{Description}                                                                    \\\\ \\hline\nBAA                   & Broadband Analog Aggregation                                                            \\\\ \\hline\nCNN                   & Convolutional Neural Network                                                            \\\\ \\hline\nCV                    & Computer Vision                                                                         \\\\ \\hline\nDDQN                  & Double Deep Q-Network                                                                   \\\\ \\hline\nDL                    & Deep Learning                                                                           \\\\ \\hline\nDNN                   & Deep Neural Network                                                                     \\\\ \\hline\nDP                    & Differential Privacy                                                                    \\\\ \\hline\nDQL                   & Deep Q-Learning                                                                         \\\\ \\hline\nDRL                   & Deep Reinforcement Learning                                                             \\\\ \\hline\nFedAvg                & Federated Averaging                                                                     \\\\ \\hline\nFL                    & Federated Learning                                                                      \\\\ \\hline\nGAN                   & Generative Adversarial Network                                                          \\\\ \\hline\nIID                   & \\begin{tabular}[c]{@{}l@{}}Independent and Identically \\\\ Distributed\\end{tabular}      \\\\ \\hline\nIoT                   & Internet of Things                                                                      \\\\ \\hline\nIoV                   & Internet of Vehicles                                                                    \\\\ \\hline\nLSTM                  & Long Short Term Memory                                                                  \\\\ \\hline\nMEC                   & Mobile Edge Computing                                                                   \\\\ \\hline\nML                    & Machine Learning                                                                        \\\\ \\hline\nMLP                   & Multilayer Perceptron                                                                   \\\\ \\hline\nNLP                   & Natural Language Processing                                                             \\\\ \\hline\nOFDMA                 & \\begin{tabular}[c]{@{}l@{}}Orthogonal Frequency-division\\\\ Multiple Access\\end{tabular} \\\\ \\hline\nQoE                   & Quality of Experience                                                                   \\\\ \\hline\nRNN                   & Recurrent Neural Network                                                                \\\\ \\hline\nSGD                   & Stochastic Gradient Descent                                                             \\\\ \\hline\nSMPC                  & Secure Multiparty Computation                                                           \\\\ \\hline\nSNR                   & Signal-to-noise ratio                                                                   \\\\ \\hline\nSVM                   & Support Vector Machine                                                                  \\\\ \\hline\nTFF                   & TensorFlow Federated                                                                    \\\\ \\hline\nUE                    & User Equipment                                                                          \\\\ \\hline\nURLLC                 & Ultra reliable low latency communication                                                \\\\ \\hline\n\\end{tabular}\n\\end{table}", "cites": [166, 5973, 3357, 3582, 5977, 5974, 5976, 8646, 5972, 602, 584, 2918, 547, 582, 5971, 7608, 671, 3420, 5975, 3422], "cite_extract_rate": 0.47619047619047616, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes concepts from multiple papers on FL, MEC, and DL, presenting a cohesive narrative on the evolution and application of FL in mobile edge networks. It critically highlights limitations of cloud-centric and MEC-based approaches, such as privacy and latency issues. The abstraction is strong, as it identifies broader patterns like the convergence of edge computing and decentralized learning for intelligent systems."}}
{"id": "a45b8f2e-5693-4022-b460-4f6e38041188", "title": "Background and Fundamentals of Federated Learning", "level": "section", "subsections": ["451fa65e-d0ea-454a-85ca-028b1856d5df", "7b753a48-9322-4168-8412-014839ef1401", "827e50d4-03f3-44e3-b393-22f92ba46fce", "15b0125f-a145-4c6d-9f58-0624dc573cf0", "f43bdcdd-1aef-4e5b-b1bc-37b0dca38e84"], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"]], "content": "\\label{sec:intro_FD}\n\\label{sec:intro_edge_AI}\nArtificial Intelligence (AI) has become an essential part of our lives today, following the recent successes and progression of DL in several domains, e.g., Computer Vision (CV)  and Natural Language Processing (NLP) . In traditional training of Deep Neural Networks (DNNs), a cloud based approach is adopted whereby data is centralized and model training occurs in powerful cloud servers. However, given the ubiquity of mobile devices that are equipped with increasingly advanced sensing and computing capabilities, the trend of migrating intelligence from the cloud to the edge, i.e., in the MEC paradigm, has naturally arisen. In addition, amid growing privacy concerns, the concept of FL has been proposed. \nFL involves the collaborative training of DNN models on end devices. There are, in general, two steps in the FL training process namely (i) local model training on end devices and (ii) global aggregation of updated parameters in the FL server. In this section, we first provide a brief introduction to DNN model training, which generalizes local model training in FL. Note that while FL can be applied to the training of ML models in general, we focus specifically on DNN model training in this section as a majority of the papers that we subsequently review study the federated training of DNN models. In addition, the DNN models are easily aggregated and outperform conventional ML techniques especially when the data is large. The implementation of FL at mobile edge networks can thus naturally leverage on the increasing computing power and wealth of data collected by distributed end devices, both of which are driving forces contributing to the rise of DL . As such, a brief introduction to general DNN model training will be useful for subsequent sections. Thereafter, we proceed to provide a tutorial of the FL training process that incorporates both global aggregation and local training. In addition, we also highlight the statistical challenges of FL model training and present the protocols and open-source frameworks of FL.", "cites": [8394], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of FL and DNN training, referencing a general overview paper on deep learning without engaging in deep synthesis or critical analysis. It integrates minimal cross-source connections and does not offer a nuanced critique or meta-level abstraction of the concepts. The narrative is introductory and lacks insight into trends or comparative evaluation of approaches."}}
{"id": "451fa65e-d0ea-454a-85ca-028b1856d5df", "title": "Deep Learning", "level": "subsection", "subsections": [], "parent_id": "a45b8f2e-5693-4022-b460-4f6e38041188", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"], ["subsection", "Deep Learning"]], "content": "Conventional ML algorithms rely on \\textit{hand-engineered} feature extractors to process raw data . As such, domain expertise is often a prerequisite for building an effective ML model. In addition, feature selection has to be customized and reinitiated for each new problem. On the other hand, DNNs are representation learning based, i.e., DNNs can automatically discover and learn these features from raw data  and thus often outperform conventional ML algorithms especially when there is an abundance of data. \nDL lies within the domain of the brain-inspired computing paradigm, of which the neural network is an important part of . In general, a neural network design emulates that of a neuron . It comprises three layers: (i) input layer, (ii) hidden layer, and (iii) output layer. In a conventional feedforward neural network, a weighted and bias-corrected input value is passed through a non-linear activation function to derive an output  (Fig. \\ref{fig:backprop}). Some activation functions include the ReLu and softmax functions . A typical DNN comprises multiple hidden layers that map an input to an output. For example, the goal of a DNN trained for image classification  is to produce a vector of scores as the output, in which the positional index of the highest score corresponds to the class to which the input image is classified to belong. As such, the objective of training a DNN is to optimize the weights of the network such that the loss function, i.e., difference between the ground truth and model output, is minimized. \n\\begin{figure}[tbh]\n\\begin{centering}\n\\includegraphics[width=\\columnwidth]{Figs/backprop}\n\\par\\end{centering}\n\\caption{\\small In forward pass, an output is derived from the weights and inputs. In backpropagation, the input gradient $e$ is used to calibrate the weights of the DNN model.\\label{fig:backprop}}\n\\end{figure}\nBefore training, the dataset is first split into the training and test dataset. Then, the training dataset is used as input data for the optimization of weights in the DNN. The weights are calibrated through stochastic gradient descent (SGD), in which the weights are updated by the product of (i) the learning rate $lr$, i.e., the step size of gradient descent in each iteration, and (ii) partial derivative of the loss function $L$ with respect to the weight $w$. The SGD formula is as follows:\n\\begin{align}\nW = W - lr \\frac{\\partial L}{\\partial W} \\label{eq:sgd_dl_1}\\\\\n\\frac{\\partial L}{\\partial W} \\approx \\frac{1}{m} \\sum_{i\\epsilon B} \\frac{\\partial l^{(i)}}{\\partial W}\\label{eq:sgd_dl_2}\n\\end{align}\nNote that the SGD formula presented in (\\ref{eq:sgd_dl_1}) is that of a mini-batch GD. In particular, equation (\\ref{eq:sgd_dl_2}) is derived as the average gradient matrix over the gradient matrices of $B$ batches, in which each batch is a random subset consisting of $m$ training samples. This is preferred over the full batch GD, i.e., where the entirety of the training set is included in computing the partial derivative, since the full batch GD can lead to slow training and batch memorization . The gradient matrices are derived through backpropagation from the input gradient $e$ (Fig. \\ref{fig:backprop}) .\nThe training iterations are then repeated over many epochs, i.e., full passes over the training set, for loss minimalization. A well-trained DNN generalizes well, i.e., achieve high \\textit{inference} accuracy when applied to data that it has not seen before, e.g., the test set. There are other alternatives to supervised learning, e.g., semi-supervised learning , unsupervised learning  and reinforcement learning . In addition, there also exists several DNN networks and architectures tailored to process the varying natures of input data, e.g., Multilayer Perceptron (MLP) , Convolutional Neural Network (CNN)  typically for CV tasks, and Recurrent Neural Network (RNN)  usually for sequential tasks. However, an in-depth discussion is out of the scope of this paper. We refer interested readers to  for comprehensive discussions of DNN architectures and training strategies. We next focus on FL, an important paradigm shift towards enabling privacy preserving and collaborative DL model training.", "cites": [166, 1003, 8394, 7633, 289, 1344, 5978, 652, 620], "cite_extract_rate": 0.45, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of Deep Learning (DL) and its components, such as neural networks and training algorithms like SGD. It integrates some general concepts from the cited papers but does so in a superficial manner without connecting diverse ideas into a novel or deeper narrative. There is minimal critical analysis or abstraction of broader principles, focusing instead on foundational knowledge."}}
{"id": "7b753a48-9322-4168-8412-014839ef1401", "title": "Federated Learning", "level": "subsection", "subsections": [], "parent_id": "a45b8f2e-5693-4022-b460-4f6e38041188", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"], ["subsection", "Federated Learning"]], "content": "\\label{fltrain}\nMotivated by privacy concerns among data owners, the concept of FL is introduced in . FL allows users to collaboratively train a\nshared model while keeping personal data on their devices, thus alleviating\ntheir privacy concerns. As such, FL can serve as an enabling technology for ML model training at mobile edge networks. For an introduction to the categorizations of different FL settings, e.g., vertical and horizontal FL, we refer the interested readers to .\nIn general, there are two main entities in the FL\nsystem, i.e., the data owners (viz. \\textit{participants}) and the model owner (viz. \\textit{FL server}). Let $\\mathcal{N}=\\left\\{ 1,\\ldots,N\\right\\} $\ndenote the set of $N$ data owners, each of which has\na private dataset $D_{i\\in\\mathcal{N}}$. Each data owner $i$ uses\nits dataset $D_{i}$ to train a\\emph{ local model} $\\mathbf{w}_{i}$\nand send only the local model parameters to the FL server. Then,\nall collected local models are aggregated $\\mathbf{\\mathbf{w}}=\\cup_{i\\in\\mathcal{N}}\\mathbf{w}_{i}$\nto generate a \\emph{global model} $\\mathbf{w}_{G}$. This\nis different from the traditional centralized training which uses\n$\\mathbf{D}=\\cup_{i\\in\\mathcal{N}}D_{i}$ to train a model $\\mathbf{w}_{T}$, i.e., data from each individual source is aggregated first before model training takes place centrally.\n\\begin{figure}[tbh]\n\\begin{centering}\n\\includegraphics[width=\\columnwidth]{Figs/flsteps}\n\\par\\end{centering}\n\\caption{\\small General FL training process involving \\textit{N} participants.\\label{fig:Federated-learning-model}}\n\\end{figure}\nA typical architecture and training process of an FL system is shown in\nFig.~\\ref{fig:Federated-learning-model}. In this system, the data\nowners serve as the FL participants which collaboratively\ntrain an ML model required by an aggregate server. An underlying assumption is that the data owners are honest, which\nmeans they use their real private data to do the training and submit the\ntrue local models to the FL server. Of course, this assumption may not always be realistic  and we discuss the proposed solutions subsequently in Sections \\ref{sec:resource} and \\ref{sec:security}. \nIn general,\nthe FL training process includes the following three steps.\nNote: the \\textit{local} model refers to\nthe model trained at each participating device, whereas the \\textit{global} model refers to\nthe model aggregated by the FL server. \n\\begin{itemize}\n\\item \\textit{Step 1 (Task initialization)}: The server decides the training task, i.e., the target application, and the corresponding data requirements.\nThe server also specifies the hyperparameters of the global model\nand the training process, e.g., learning rate. Then, the server broadcasts the initialized\nglobal model $\\mathbf{w}_{G}^{0}$ and task to selected participants. \n\\item \\textit{Step 2 (Local model training and update)}: Based on the global model\n$\\mathbf{w}_{G}^{t}$, where $t$ denotes the current iteration index,\neach participant respectively uses its local data and device to update\nthe local model parameters $\\mathbf{w}_{i}^{t}$. The goal of participant $i$ in iteration \\emph{$t$}\nis to find optimal parameters $\\mathbf{w}_{i}^{t}$ that minimize\nthe loss function $L(\\mathbf{w}_{i}^{t})$, i.e., \n\\begin{equation}\n\\mathbf{w}_{i}^{t^{*}}=\\arg\\min_{\\mathbf{w}_{i}^{t}}L(\\mathbf{w}_{i}^{t}).\\label{eq:local_training_goal}\n\\end{equation}\nThe updated local model parameters are subsequently sent\nto the server. \n\\item \\textit{Step 3 (Global model aggregation and update)}: The server aggregates\nthe local models from participants and then sends the updated\nglobal model parameters $\\mathbf{w}_{G}^{t+1}$ back to the data owners.\n\\end{itemize}\nThe server wants to minimize the global loss function $L(\\mathbf{w}_{G}^{t})$, i.e., \n\\begin{equation}\nL(\\mathbf{w}_{G}^{t})=\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathbf{w}_{i}^{t}).\\label{eq:global_goal}\n\\end{equation}\nSteps $2$-$3$ are repeated until the global loss function converges or a desirable training accuracy is achieved.\n Note\nthat the FL training process can be used for different ML\nmodels that essentially use the SGD method such as Support\nVector Machines (SVMs) , neural networks, and linear regression .\nA training dataset usually contains a set of $n$ data feature vectors\n$\\mathbf{x}=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\}$ and a set of\ncorresponding data labels\\footnote{In the case of unsupervised learning, there is no data label.}\n$\\mathbf{y}=\\{y_{1},\\ldots,y_{n}\\}$. In addition, let $\\hat{y_{j}}=f(\\mathbf{x}_{j};\\mathbf{w})$\ndenote the predicted result from the model $\\mathbf{w}$ updated/trained by data\nvector $x_{j}$. Table~\\ref{tab:loss-functions} summarizes several\nloss functions of common ML models~. \n\\begin{table}[tbh]\n\\centering{}\\caption{\\small Loss functions of common ML models\\label{tab:loss-functions}}\n\\begin{tabular}{|>{\\centering}m{1.9cm}|>{\\centering}m{5cm}|}\n\\hline \nModel & Loss function $L(\\mathbf{w}_{i}^{t})$\\tabularnewline\n\\hline \nNeural network & $\\frac{1}{n}\\sum_{j=1}^{n}(y_{i}-f(\\mathbf{x}_{j};\\mathbf{w}))^{2}$\\linebreak (Mean\nSquared Error)\\tabularnewline\n\\hline \nLinear regression & $\\frac{1}{2}\\left\\Vert y_{j}-\\mathbf{w}^{T}\\mathbf{x}_{j}\\right\\Vert ^{2}$\\tabularnewline\n\\hline \nK-means & $\\sum_{j}\\left\\Vert \\mathbf{x}_{j}-f(\\mathbf{x}_{j};\\mathbf{w})\\right\\Vert $\\linebreak ($f(\\mathbf{x}_{j};\\mathbf{w})$\nis the centroid of all objects assigned to $x_{j}$'s class)\\tabularnewline\n\\hline \nsquared-SVM & $[\\frac{1}{n}\\sum_{j=1}^{n}\\max(0,1-y_{j}(\\mathbf{w}^{T}\\mathbf{x}_{j}-bias))]$\\linebreak $+\\lambda\\left\\Vert \\mathbf{w}^{T}\\right\\Vert ^{2}$($bias$\nis the bias parameter and $\\lambda$ is const.)\\tabularnewline\n\\hline \n\\end{tabular}\n\\end{table}\nGlobal model aggregation is an integral part of FL. A straightforward\nand classical algorithm for aggregating the local models is the \\textit{FedAvg} algorithm proposed in , which is similar to that of local SGD . The pseudocode for \\textit{FedAvg} is given in Algorithm~\\ref{alg:AveragingAlg}.\n\\begin{algorithm}[tbh]\n\\scriptsize\n\\begin{algorithmic}[1]\n\\Require{Local minibatch size $B$, number of participants $m$ per iteration, number of local epochs $E$, and learning rate $\\eta$.}\n\\Ensure{Global model $\\mathbf{w}_{G}$.}\n\\State{[Participant $i$]}\n\\State{\\textbf{LocalTraining}($i$, $\\mathbf{w}$):}\n\t\\State{Split local dataset $D_i$ to minibatches of size $B$ which are included into the set $\\mathcal{B}_i$.}\n\t\\For{each local epoch $j$ from $1$ to $E$}\n\t\t\\For{each $b \\in \\mathcal{B}_i$}\n\t\t\t\\State{$\\mathbf{w} \\gets \\mathbf{w} - \\eta\\Delta L(\\mathbf{w};b)$ \\qquad ($\\eta$ is the learning rate and $\\Delta L$ is the gradient of $L$ on $b$.)}\n\t\t\\EndFor\n\t\\EndFor\n\t\\State{}\n\t\\State{[Server]}\n\t\\State{Initialize $\\mathbf{w}_{G}^{0}$}\n\t\\For{each iteration $t$ from $1$ to $T$}\n\t\t\\State{Randomly choose a subset $\\mathcal{S}_t$ of $m$ participants from $\\mathcal{N}$}\n\t\t\\For{each partipant $i \\in \\mathcal{S}_t$ $\\textbf{parallely}$}\n\t\t\t\\State{$\\mathbf{w}_{i}^{t+1} \\gets \\textbf{LocalTraining}$($i$, $\\mathbf{w}_{G}^{t}$)}\n\t\t\\EndFor\n\\State{$\\mathbf{w}_{G}^{t}=\\frac{1}{\\sum_{i\\in\\mathcal{N}}D_{i}}\\sum_{i=1}^{N}D_{i}\\mathbf{w}_{i}^{t}$ \\qquad (Averaging aggregation)} \n\t\\EndFor\n\\end{algorithmic} \n\\caption{Federated averaging algorithm~\\label{alg:AveragingAlg}}\n\\end{algorithm}\n As described in Step 1 above, the server first initializes the task (lines 11-16). Thereafter, in Step 2, the participant $i$ implements the local\ntraining and optimizes the target in (\\ref{eq:local_training_goal})\non minibatches from the original local dataset (lines 2-8). Note that a minibatch refers to a randomized subset of each participant's dataset. At the $t^{th}$\niteration (line 17), the server minimizes the global loss in (\\ref{eq:global_goal})\nby the averaging aggregation which is formally defined as\n\\begin{equation}\n\\mathbf{w}_{G}^{t}=\\frac{1}{\\sum_{i\\in\\mathcal{N}}D_{i}}\\sum_{i=1}^{N}D_{i}\\mathbf{w}_{i}^{t}.\\label{eq:averaging_aggr}\n\\end{equation}\nThe FL training process is iterated till the global loss function converges, or a desirable accuracy is achieved.", "cites": [3431, 5979, 671, 582], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of the Federated Learning (FL) framework, including its components, training steps, and the FedAvg algorithm. However, it lacks synthesis of ideas across the cited papers and offers minimal critical evaluation or abstraction beyond basic definitions."}}
{"id": "827e50d4-03f3-44e3-b393-22f92ba46fce", "title": "Statistical Challenges of FL", "level": "subsection", "subsections": [], "parent_id": "a45b8f2e-5693-4022-b460-4f6e38041188", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"], ["subsection", "Statistical Challenges of FL"]], "content": "\\label{stats}\nFollowing an elaboration of the FL training process in the previous section, we now proceed to discuss the statistical challenges faced in FL. \nIn traditional distributed ML, the central server has access to\nthe whole training dataset. As such, the server can split the dataset\ninto subsets that follow similar distributions. The subsets are subsequently sent to\nparticipating nodes for distributed training. However, this approach is\nimpractical for FL since the local dataset is only accessible by the data\nowner. \nIn the FL setting, the participants may have local datasets that follow different distributions, i.e., the datasets of participants are non-IID. While the authors in  show that the aforementioned \\textit{FedAvg} algorithm is able to achieve desirable accuracy even when data is non-IID across participants, the authors in  found otherwise. For example, the accuracy of a \\textit{FedAvg}-trained CNN model has 51\\% lower accuracy than centrally-trained CNN model for CIFAR-10 . This deterioration in accuracy is further shown to be quantified by the earth mover's distance (EMD) , i.e., difference in FL participant's data distribution as compared to the population distribution. As such, when data is non-IID and highly skewed, data-sharing is proposed in which a shared dataset with uniform distribution across all classes is sent by the FL server to each FL participant. Then, the participant trains its local model on its private data together with the received data. The simulation result shows that accuracy can be increased by 30\\% with 5\\% shared data due to reduced EMD.   However, a common dataset may not always be available for sharing by the FL server. An alternative solution to gather contributions towards building the common dataset is subsequently discussed in section \\ref{sec:resource}.\nThe authors in  also find that global imbalance, i.e., the situation in which the collection of data held across all FL participants is class imbalanced, leads to a deterioration in model accuracy. As such, the Astraea framework is proposed. On initialization, the FL participants first send their data distribution to the FL server. A rebalancing step is introduced before training begins in which each participant performs data augmentation  on the minority classes, e.g., through random rotations and shifts. After training on the augmented data, a mediator is created to coordinate intermediate aggregation, i.e., before sending the updated parameters to the FL server for global aggregation. The mediator selects participants with data distributions that best contributes to an uniform distribution when aggregated. This is done through a greedy algorithm approach to minimize the Kullback-Leibler Divergence  between local data and uniform distribution. The simulation results show accuracy improvement when tested on imbalanced datasets.\nGiven the heterogeneity of data distribution across devices, there has been an increasing number of studies that borrow concepts from multi-task learning  to learn separate, but structurally related models for each participant. Instead of minimizing the conventional loss function presented previously in Table \\ref{tab:loss-functions}, the loss function is modified to also model the relationship amongst tasks . Then, the MOCHA algorithm is proposed in which an alternating optimization approach  is used to approximately solve the minimization problem. Interestingly, MOCHA can also be calibrated based on the resource constraints of a participating device. For example, the quality of approximation can be adaptively adjusted based on network conditions and CPU states of the participating devices. However, MOCHA cannot be applied to non-convex DL models. \nSimilarly, the authors in  also borrow concepts from multi-task learning to deal with the statistical heterogeneity in FL. The FEDPER approach is proposed in which all FL participants share a set of base layers trained using the \\textit{FedAvg} algorithm. Then, each participant separately trains another set of personalization layers using its own local data. In particular, this approach is suitable for building recommender's systems given the diverse preferences of participants. The authors show empirically using the Flickr-AES dataset  that the FEDPER approach outperforms a pure \\textit{FedAvg} approach since the personalization layer is able to represent the personal preference of an FL participant. However, it is worth to note that the collaborative training of the base layers are still important to achieve a high test accuracy, since each participant has insufficient local data samples for purely personalized model training.\nApart from data heterogeneity, the convergence of a distributed learning algorithm is always\na concern. Higher convergence rate helps to save a large amount\nof time and resources for the FL participants, and also significantly increases\nthe success rate of the federated training since fewer communication\nrounds imply reduced participant dropouts. To ensure convergence, the study in  propose \\textit{FedProx}, which modifies the loss function to also include a tunable parameter that restricts how much local updates can affect the prevailing model parameters. The \\textit{FedProx} algorithm can be adaptively tuned, e.g., when training loss is increasing, model updates can be tuned to affect the current parameters less. Similarly, the authors in  also propose the \\textit{LoAdaBoost FedAvg} algorithm to complement the aforementioned data-sharing approach  in ML on medical data. In \\textit{LoAdaBoost FedAvg}, participants train the model on their local data and compare the cross-entropy loss with the median loss from the \\textit{previous} training round. If the current cross-entropy loss is higher, the model is retrained before global aggregation so as to increase learning efficiency. The simulation results show that faster convergence is achieved as a result. \nIn fact, the statistical challenges of FL coexist with other issues that we explore in subsequent sections. For example, the communication costs incurred in FL can be reduced by faster convergence. Similarly, resource allocation policies can also be designed to solve statistical heterogeneity. As such, we revisit these concepts in greater detail subsequently.", "cites": [596, 7721, 5980, 5442, 5981, 582, 7720], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key statistical challenges in FL and integrates findings from multiple papers, such as the impact of non-IID data and class imbalance, with a coherent narrative. It critically evaluates limitations of FedAvg and proposes alternative methods like FedProx and FEDPER, while highlighting trade-offs and constraints. However, while it identifies broader patterns (e.g., data heterogeneity), the abstraction is not fully meta-level, as the discussion remains grounded in specific techniques and datasets."}}
{"id": "15b0125f-a145-4c6d-9f58-0624dc573cf0", "title": "FL protocols and frameworks", "level": "subsection", "subsections": [], "parent_id": "a45b8f2e-5693-4022-b460-4f6e38041188", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"], ["subsection", "FL protocols and frameworks"]], "content": "\\label{proto}\nTo improve scalability, an FL protocol \nhas been proposed in  from the system level. This protocol deals with\nissues regarding unstable device connectivity and communication security etc.\nThe FL protocol (Fig.~\\ref{fig:Federated-learning-protocol}) consists of three phases in each training round: \n\\begin{enumerate}\n\\item \\emph{Selection:} In the participant selection phase, the FL server chooses a subset of connected devices to participate in a training round. The selection criteria may subsequently be calibrated to the server's needs, e.g., training efficiency . In Section \\ref{sec:resource}, we further elaborate on proposed participant selection methods.\n\\item \\emph{Configuration:} The server is configured accordingly to the aggregation mechanism preferred, e.g. simple or secure aggregation . Then, the server sends the training schedule and global model to each participant.\n\\item \\emph{Reporting:} The server receives updates from participants. Thereafter, the updates can be aggregated, e.g., using the \\textit{FedAvg} algorithm.\n\\end{enumerate}\nIn addition, to manage device connections accordingly to varying FL population size, pace steering is also recommended. Pace steering adaptively manages the optimal time window for participants to reconnect to the FL server . For example, when the FL population is small, pace steering is used to ensure that there is a sufficient number of participating devices that connect to the server simultaneously. In contrast, when there is a large population, pace steering randomly chooses devices to participate to prevent the situation in which too many participating devices are connected at one point of time.\nApart from communication efficiency, communication security during local updates transmission is\nanother problem to be resolved. Specifically,\nthere are mainly two aspects in communication security:\n\\begin{enumerate}\n\\item \\emph{Secure aggregation}: To prevent local updates from being traced and utilized\nto infer the identity of the FL participant, \na virtual and trusted third party server is deployed for local model\naggregation . The Secret Sharing mechanism~ is also used for\ntransmission of local updates with authenticated encryption. \n\\item \\emph{Differential privacy}: Similar to secure aggregation, differential\nprivacy (DP) prevents the FL server from identifying the owner of a local\nupdate. The difference is that to achieve the goal of privacy preservation,\nthe DP in FL~ adds a certain degree of noise in\nthe original local update while providing theoretical guarantees on\nthe model quality.\n\\end{enumerate}\nThese concepts on privacy and security are presented in detail in Section \\ref{sec:security}. Recently, some open-source frameworks for FL have been developed as follows:\n\\begin{enumerate}\n\\item \\emph{TensorFlow Federated (TFF)}: TFF  is a framework based on Tensorflow developed by Google for decentralized ML and other distributed\ncomputations. TFF consists of two layers (i) FL and (ii) Federated Core (FC). The FL layer is a high-level interface that allows the implementation of FL to existing TF models without the user having to apply the FL algorithms personally. The FC layer combines TF with communication operators to allow users to experiment with customized and newly designed FL algorithms.\n\\item \\emph{PySyft}: PySyft~ is a framework based on PyTorch\nfor performing encrypted, privacy-preserving DL and implementations of\nrelated techniques, such as Secure Multiparty Computation (SMPC) and\nDP, in untrusted environments while protecting data. Pysyft is developed such that it retains the native Torch interface, i.e., the ways to execute all tensor operations remain unchanged from that of Pytorch. When a SyftTensor is created, a LocalTensor is automatically created to also apply the input command to the native Pytorch tensor. To simulate FL, participants are created as \\textit{Virtual Workers}. Data, i.e., in the structure of tensors, can be split and distributed to the Virtual Workers as a simulation of a practical FL setting. Then, a PointerTensor is created to specify the data owner and storage location. In addition, model updates can be fetched from the Virtual Workers for global aggregation.\n\\item \\emph{LEAF}: An open source framework  of datasets that can be used as benchmarks in FL, e.g., Federated Extended MNIST (FEMNIST), an MNIST  dataset partitioned based on writer of each character, and Sentiment140 , a dataset partitioned based on different users. In these datasets, the writer or user is assumed to be a participant in FL, and their corresponding data is taken to be the local data held in their personal devices. The implementation of newly designed algorithms on these benchmark datasets allow for reliable comparison across studies.\n\\item \\emph{FATE}: Federated AI Technology Enabler (FATE) is an open-source framework by WeBank  that supports the federated and secure implementation of several ML models. \n\\end{enumerate}\n\\begin{figure*}[t]\n\\begin{centering}\n\\includegraphics[width=0.85\\textwidth]{fl-system}\n\\par\\end{centering}\n\\caption{\\small Federated learning protocol .\\label{fig:Federated-learning-protocol}}\n\\end{figure*}", "cites": [590, 619, 656, 653, 5419], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of FL protocols and frameworks, drawing from multiple cited works. It synthesizes the core components of FL (selection, configuration, reporting) and introduces related concepts like pace steering, secure aggregation, and differential privacy. However, it remains largely descriptive with minimal critical evaluation or abstraction to broader principles."}}
{"id": "f43bdcdd-1aef-4e5b-b1bc-37b0dca38e84", "title": "Unique characteristics and issues of FL", "level": "subsection", "subsections": [], "parent_id": "a45b8f2e-5693-4022-b460-4f6e38041188", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Background and Fundamentals of Federated Learning"], ["subsection", "Unique characteristics and issues of FL"]], "content": "Besides the statistical challenges we present in Section \\ref{stats}, FL has some unique characteristics and features  as compared to other distributed ML approaches:\n\\begin{enumerate}\n\\item \\textit{Slow and unstable communication}: In the traditional distributed training\nin a data center, the communication environment can be assumed to\nbe perfect where the information transmission rate is very high and\nthere is no packet loss. However, these assumptions are not applicable to \nthe FL environment where heterogeneous devices are involved in training. For example, the Internet upload speed is typically\nmuch slower than download speed . Also, some participants with unstable wireless communication channels may consequently\ndrop out due to disconnection from the Internet.\n\\item \\textit{Heterogeneous devices}: Apart from bandwidth constraints, FL involves heterogeneous devices with varying resource constraints. For example, the devices can have different computing capabilities, i.e., CPU states and battery level. The devices can also have different levels of \\textit{willingness} to participate, i.e., FL training is resource consuming and given the distributed nature of training across numerous devices, there is a possibility of free ridership.\n\\item \\textit{Privacy and security concerns}:\nAs we have previously discussed, data owners are increasingly privacy sensitive. However, as will be subsequently presented in Section \\ref{sec:security}, malicious participants are able to infer sensitive information from shared parameters, which potentially negates privacy preservation. In addition, we have previously assumed that all participants and FL servers are trustful. In reality, they may be malicious.\n\\end{enumerate}\nThese unique characteristics of FL lead to several practical issues in FL implementation\nmainly in three aspects that we now proceed to discuss, i.e., (i) statistical challenges (ii) communication costs (iii) resource allocation and (iv) privacy and security. In the following sections, we review related work that address each of these issues.", "cites": [1309, 623], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts from the cited papers by highlighting communication inefficiency, device heterogeneity, and privacy concerns in FL. It abstracts these into broader issues relevant to FL in mobile edge networks. However, the critical analysis is limited to pointing out potential problems without deeper evaluation or comparison of the cited works."}}
{"id": "b279d0a9-a81b-4e03-b78c-6bfca3339a27", "title": "Communication Cost", "level": "section", "subsections": ["505350da-04c7-4276-95b4-c41837226063", "bc0516f6-5205-43e4-879f-70e89c2c489b", "af37e371-e11a-40d3-86e3-4cb9ab5fa789", "1ee6a9c5-8838-4e8d-9441-2cab8985fa6c"], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Communication Cost"]], "content": "\\label{sec: communication}\nIn FL, a number of rounds of communications between the participants and the FL server may be required to achieve a target accuracy (Fig. \\ref{fig:Federated-learning-protocol}). For complex DL model training involving, e.g. CNN, each update may comprise millions of parameters . The high dimensionality of the updates can result in the incurrence of high communication costs and can lead to a training bottleneck. In addition, the bottleneck can be worsened due to (i) unreliable network conditions of participating devices  and (ii) the asymmetry in Internet connection speeds in which upload speed is faster than download speed, resulting in delays in model uploads by participants . As such, there is a need to improve the communication efficiency of FL. The following approaches to reduce communication costs are considered:\n\\begin{itemize}\n\\item \\textit{Edge and End Computation}: In the FL setup, the communication cost often dominates computation cost . The reason is that on-device dataset is relatively small whereas mobile devices of participants have increasingly fast processors. On the other hand, participants may only be willing to participate in the model training only if they are connected to Wi-Fi . As such, more computation can be performed on edge nodes or on end devices before each global aggregation so as to reduce the number of communication rounds needed for the model training. In addition, algorithms to ensure faster convergence can also reduce number of communication rounds involved, at the expense of more computation on edge servers and end devices.\n\\item \\textit{Model Compression}: This is a technique commonly used in distributed learning . Model or gradient compression involves the communication of an update that is transformed to be more compact, e.g., through sparsification, quantization or subsampling , rather than the communication of full update. However, since the compression may introduce noise, the objective is to reduce the size of update transferred during each round of communication while maintaining the quality of trained models .\n\\item \\textit{Importance-based Updating}: This strategy involves selective communication such that only the important or relevant updates  are transmitted in each communication round. In fact, besides saving on communication costs, omitting some updates from participants can even improve the global model performance.\n\\end{itemize}", "cites": [627, 617, 623, 97, 582, 5982], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key ideas from multiple papers into a structured discussion of communication cost reduction in FL, categorizing them under three approaches: Edge and End Computation, Model Compression, and Importance-based Updating. It provides some critical observations, such as the trade-off between communication and computation, but lacks deeper comparative analysis or limitations of each method. The abstraction is moderate, identifying general strategies rather than focusing only on specific implementations."}}
{"id": "505350da-04c7-4276-95b4-c41837226063", "title": "Edge and End Computation", "level": "subsection", "subsections": [], "parent_id": "b279d0a9-a81b-4e03-b78c-6bfca3339a27", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Communication Cost"], ["subsection", "Edge and End Computation"]], "content": "To decrease the number of communication rounds, additional computation can be performed on participating end devices before each iteration of communication for global aggregation (Fig. \\ref{fig:computation}(a)). The authors in  consider two ways to increase computation on participating devices: (i) increasing parallelism in which more participants are selected to participate in each round of training and (ii) increasing computation per participant whereby each participant performs more local updates before communication for global aggregation. A comparison is conducted for the FederatedSGD (\\textit{FedSGD}) algorithm and the proposed \\textit{FedAvg} algorithm. For the \\textit{FedSGD} algorithm, all participants are involved and only one pass is made per training round in which the minibatch size comprises of the entirety of the participant's dataset. This is similar to the full-batch training in centralized DL frameworks. For the proposed \\textit{FedAvg} algorithm, the hyperparameters are tuned such that more local computations are performed by the participants. For example, the participant can make more passes over its dataset or use a smaller local minibatch size to increase computation before each communication round. The simulation results show that increased parallelism does not lead to significant improvements in reducing communication cost, once a certain threshold is reached. As such, more emphasis is placed on increasing computation per participant while keeping the fraction of selected participants constant. For MNIST CNN simulations, increased computation using the proposed \\textit{FedAvg} algorithm can reduce communication rounds by more than $30$ times when the dataset is IID. For non-IID dataset, the improvement is less significant ($2.8$ times) using the same hyperparameters. However, for Long Short Term Memory (LSTM) simulations , improvements are more significant even for non-IID data ($95.3$ times). In addition, \\textit{FedAvg} increases the accuracy eventually since model averaging produces regularization effects similar to dropout , which prevents overfitting. \nAs an extension, the authors in  also validate that a similar concept as that of  works for vertical FL. In vertical FL, collaborative model training is conducted across the same set of participants with different data features. The Federated Stochastic Block Coordinate Descent (FedBCD) algorithm is proposed in which each participating device performs multiple local updates first before communication for global aggregation. In addition, convergence guarantee is also provided with an approximate calibration of the number of local updates per interval of communication.\n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\columnwidth]{Figs/edgeendcompute}\n\t\\caption{Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication  (b) Two-stream training with global model as a reference  and (c) Intermediate edge server aggregation .}\n\t\\label{fig:computation}\n\\end{figure}\nAnother way to decrease communication cost can also be through modifying the training algorithm to increase convergence speed, e.g., through the aforementioned \\textit{LoAdaBoost FedAvg} in . Similarly, the authors in  also propose increased computation on each participating device by adopting a two-stream model (Fig. \\ref{fig:computation}(b)) commonly used in transfer learning and domain adaption . During each training round, the global model is received by the participant and fixed as a reference in the training process. During training, the participant learns not just from local data, but also from other participants with reference to the fixed global model. This is done through the incorporation of Maximum Mean Discrepancy (MMD) into the loss function. MMD measures the distance between the means of two data distributions , . Through minimizing MMD loss between the local and global models, the participant can extract more generalized features from the global model, thus accelerating the convergence of training process and reducing communication rounds. The simulation results on the CIFAR-10 and MNIST dataset using DL models such as AlexNet and 2-CNN respectively show that the proposed two-stream FL can reach the desirable test accuracy in 20\\% fewer communication rounds even when data is non-IID. However, while convergence speed is increased, more computation resources have to be consumed by end devices for the aforementioned approaches. Given the energy constraints of participating mobile devices in particular, this necessitates resource allocation optimization that we subsequently discuss in Section \\ref{sec:resource}.\nWhile the aforementioned studies consider increasing computation on participating \\textit{devices}, the authors in  propose an edge computing inspired paradigm in which proximate edge \\textit{servers} can serve as intermediary parameter aggregators given that the propagation latency from participant to the edge server is smaller than that of the participant-cloud communication (Fig. \\ref{fig:computation}(c)). A hierarchical FL (\\textit{HierFAVG}) algorithm is proposed whereby for every few local participant updates, the edge server aggregates the collected local models. After a predefined number of edge server aggregations, the edge server communicates with the cloud for global model aggregation. As such, the communication between the participants and the cloud occurs only once after an interval of multiple local updates. Comparatively, for the \\textit{FedAvg} algorithm proposed in , the global aggregation occurs more frequently since no intermediate edge server aggregation is involved. The authors further prove the covergence of \\textit{HierFAVG} for both convex and non-convex objective functions given non-IID user data. The simulation results show that for the same number of local updates between two global aggregations, more intermediate edge aggregations before each global aggregation can lead to reduced communication overhead as compared to the \\textit{FedAvg} algorithm. This result holds for both IID and non-IID data, implying that intermediate aggregation on edge servers may be implemented on top of \\textit{FedAvg} so as to reduce communication costs. However, when applied to non-IID data, the simulation results show that \\textit{HierFAVG} fails to converge to the desired accuracy level (90\\%) in some instances, e.g., when edge-cloud divergence is large or when there are many edge servers involved. As such, a further study is required to better understand the tradeoffs between adjusting local and edge aggregation intervals, so as to ensure that the parameters of the \\textit{HierFAVG} algorithm can be optimally calibrated to suit other settings. Nevertheless, \\textit{HierFAVG} is a promising approach for the implementation of FL at mobile edge networks, since it leverages on the proximity of intermediate edge server to reduce communication costs, and potentially relieve the burden on the remote cloud.", "cites": [5066, 582], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and integrates insights from multiple papers to explain how computation at edge and end devices can reduce communication costs in FL. It provides critical analysis by evaluating the effectiveness of different approaches (e.g., FedSGD vs. FedAvg, HierFAVG limitations with non-IID data) and identifies trade-offs in resource usage and accuracy. The discussion abstracts beyond individual papers to highlight general principles such as the benefit of local computation and the role of intermediate aggregation in communication efficiency."}}
{"id": "bc0516f6-5205-43e4-879f-70e89c2c489b", "title": "Model Compression", "level": "subsection", "subsections": [], "parent_id": "b279d0a9-a81b-4e03-b78c-6bfca3339a27", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Communication Cost"], ["subsection", "Model Compression"]], "content": "To reduce communication costs, the authors in  propose structured and sketched updates to reduce the size of model updates sent from participants to the FL server during each communication round. \n\\textit{Structured updates} restrict participant updates to have a pre-specified structure, i.e., low rank and random mask. For the low rank structure, each update is enforced to be a low rank matrix expressed as a product of two matrices. Here, one matrix is generated randomly and held constant during each communication round whereas the other is optimized. As such, only the optimized matrix needs to be sent to the server. For the random mask structure, each participant update is restricted to be a sparse matrix following a pre-defined random sparsity pattern generated independently during each round. As such, only the non-zero entries are required to be sent to the server. \nOn the other hand, \\textit{sketched updates} refer to the approach of encoding the update in a compressed form before communication with the server, which subsequently decodes the updates before aggregation. One example of sketched update is the subsampling approach, in which each participant communicates only a random subset of the update matrix. The server then averages the subsampled updates to derive an unbiased estimate of the true average. Another example of sketched update is the probabilistic quantization approach , in which the update matrices are vectorized and quantized for each scalar. To reduce the error from quantization, a structured random rotation that is the product of a Walsh-Hadamard matrix and\nbinary diagonal matrix  can be applied before quantization. \nThe simulation results on the CIFAR-10 image classification task show that for structured updates, the random mask performs better than that of the low rank approach. The random mask approach also achieves higher accuracy than sketching approaches since the latter involves a removal of some information obtained during training. However, the combination of all three sketching tools, i.e., subsampling, quantization, and rotation, can achieve higher compression rate and faster convergence, albeit with some sacrifices in accuracy. For example, by using 2 bits for quantization and sketching out all but 6.25\\% of update data, the number of bits needed to represent updates can be reduced by 256 times and the accuracy level achieved is 85\\%. In addition, sketching updates can achieve higher accuracy in training when there are more participants trained per round. This suggests that for practical implementation of FL where there are many participants available, more participants can be selected for training per round so that subsampling can be more aggressive to reduce communication costs. \n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{Figs/compress}\n\t\\caption{\\small The compression techniques considered are summarized above by the diagram from authors in . (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation}\n\t\\label{fig:compress}\n\\end{figure}\nThe authors in  extend the studies in  by proposing lossy compression and federated dropout to reduce \\textit{server-to-participant} communication costs. A summary of the proposed techniques are adapted from the authors' work in Fig. \\ref{fig:compress}. For participant-to-server upload of model parameters that we discuss previously, the decompressions can be averaged over many updates to receive an unbiased estimate. However, there is no averaging for server-to-participant communications since the same global model is sent to all participants during each round of communication. Similar to , subsampling and probabilistic quantization are considered. For the application of structured random rotation before subsampling and quantization, Kashin's representation  is applied instead of the Hadamard transformation since the former is found to perform better in terms of accuracy-size tradeoff. \nIn addition to the subsampling and quantization approaches, the federated dropout approach is also considered in which a fixed number of activation functions at each fully-connected layer is removed to derive a smaller sub-model. The sub-model is then sent to the participants for training. The updated submodel can then be mapped back to the global model to derive a complete DNN model with all weights updated during subsequent aggregation. This approach reduces the server-to-participant communication cost, and also the size of participant-to-server updates. In addition, local computation is reduced since fewer parameters have to be updated. The simulations are performed on MNIST, CIFAR-10, and EMNIST  datasets. For the lossy compression, it is shown that the subsampling approach taken by  does not reach an acceptable level of performance. The reason is that the update errors can be averaged out for participant-to-server uploads but not for server-to-participant downloads. On the other hand, quantization with Kashin's representation can achieve the same performance as the baseline without compression while having communication cost reduced by nearly 8 times when the model is quantized to 4 bits. For federated dropout approaches, the results show that a dropout rate of 25\\% of weight matrices of fully-connected layers (or filters in the case of CNN) can achieve acceptable accuracy in most cases while ensuring around 43\\% reduction in size of models communicated. However, if dropout rates are more aggressive, convergence of the model can be slower.  \nThe aforementioned two studies suggest useful model compression approaches in reducing communication costs for both server-to-participant and participant-to-server communications. As one may expect, the reduction in communication costs come with sacrifices in model accuracy. It will thus be useful to formalize the compression-accuracy tradeoffs, especially since this varies for different tasks, or when different number of FL participants are involved.", "cites": [627, 623, 5984, 5983], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple model compression techniques from cited papers, connecting structured updates, sketched updates, and federated dropout into a coherent discussion on communication efficiency in FL. It includes critical analysis by evaluating the trade-offs in accuracy and performance between different methods. While it identifies patterns (e.g., the impact of subsampling and quantization on performance), the abstraction remains at a moderate level without rising to a meta-level conceptualization."}}
{"id": "af37e371-e11a-40d3-86e3-4cb9ab5fa789", "title": "Importance-based Updating", "level": "subsection", "subsections": [], "parent_id": "b279d0a9-a81b-4e03-b78c-6bfca3339a27", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Communication Cost"], ["subsection", "Importance-based Updating"]], "content": "Based on the observation that most parameter values of a DNN model are sparsely distributed and close to zero , the authors in  propose the edge Stochastic Gradient Descent (eSGD) algorithm that selects only a small fraction of important gradients to be communicated to the FL server for parameter update during each communication round. The eSGD algorithm keeps track of loss values at two consecutive training iterations. If the loss value of the current iteration is smaller than the preceding iteration, this implies that current training gradients and model parameters are important for training loss minimalization and thus, their respective hidden weights are assigned a positive value. In addition, the gradient is also communicated to the server for parameter update. Once this does not hold, i.e., the loss increases as compared to the previous iteration, other parameters are selected to be updated based on their hidden weight values. A parameter with larger hidden weight value is more likely to be selected since it has been labeled as important several times during training. To account for small gradient values that can delay convergence if they are ignored and not updated completely , these gradient values are accumulated as residual values. Since the residuals may arise from different training iterations, each update to the residual is weighted with a discount factor using the momentum correction technique . Once the accumulated residual gradient reaches a threshold, they are chosen to replace the least important gradient coordinates according to the hidden weight values. The simulation results show that eSGD with a 50\\% drop ratio can achieve higher accuracy than that of the thresholdSGD algorithm proposed in , which uses a fixed threshold value to determine which gradient coordinates to drop. eSGD can also save a large proportion of gradient size communicated. However, eSGD still suffers from accuracy loss as compared to standard SGD approaches. For example, when tested on simple classification tasks using the MNIST dataset, the model accuracy converges to just 91.22\\% whereas standard SGD can achieve 99.77\\% accuracy. If extended to more sophisticated tasks, the accuracy can potentially deteriorate to a larger extent. In addition, the accuracy and convergence speed of the eSGD approach fluctuates arbitrarily based on hyperparameters used, e.g., minibatch size. As such, further studies have to be conducted to formally balance the tradeoffs between communication costs and training performance.\nWhile  studies the selective communication of gradients, the authors in  propose the Communication-Mitigated Federated Learning (CMFL) algorithm that uploads only relevant local model updates to reduce communication costs while guaranteeing global convergence. In each iteration, a participant's local update is first compared with the global update to identify if the update is relevant. A relevance score is computed where the score equates to percentage of same-sign parameters in the local and global update. In fact, the global update is not known in advance before aggregation. As such, the global update made in the previous iteration is used as an estimate for comparison since it was found empirically that more than 99\\% of the normalized difference of two sequential global updates are smaller than 0.05 in both MNIST CNN and Next-Word-Prediction LSTM. An update is considered to be irrelevant if its relevance score is smaller than a predefined threshold. The simulation results show that CMFL requires 3.47 times and 13.97 times fewer communication rounds to reach 80\\% accuracy for MNIST CNN and Next-Word-Prediction LSTM, respectively, as compared to the benchmark \\textit{FedAvg} algorithm. In addition, CMFL can save significantly more communication rounds as compared to Gaia. Note that Gaia is a geo-distributed ML approach suggested in  which measures relevance based on \\textit{magnitude} of updates rather than sign of parameters. When applied with the aforementioned MOCHA algorithm \\ref{stats} , CMFL can reduce communication rounds by 5.7 times for the Human Activity Recognition dataset  and 3.3 times for the Semeion Handwritten Digit dataset . In addition, CMFL can achieve slightly higher accuracy since it involves the elimination of irrelevant updates that are outliers which harm training. \n\\begin{table*}[]\n\\centering \\caption{\\small Approaches to communication cost reduction in FL. \\label{tab:communication}}\n\\begin{adjustbox}{width=\\textwidth}\n\\begin{tabular}{|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{C0C0C0} \n\\multicolumn{1}{|c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Approaches}}                      & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Ref.}} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Key Ideas}}                                                                                                                          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Tradeoffs and Shortcomings}}                                                     \\\\ \\hline\n                                                                                       &            & \\begin{tabular}[c]{@{}l@{}}More local updates in between communication for global aggregation, to reduce \\\\ instances of communication\\end{tabular}                                      & \\begin{tabular}[c]{@{}l@{}}Increased computation cost and poor  performance\\\\ in non-IID setting\\end{tabular}                        \\\\ \\cline{2-4} \n                                                                                       &                & \\begin{tabular}[c]{@{}l@{}}Similar to the ideas of , but with convergence guarantees for vertical FL\\end{tabular}                     & \\begin{tabular}[c]{@{}l@{}}Increased computation cost and delayed\\\\ convergence if global aggregation is too infrequent\\end{tabular} \\\\ \\cline{2-4} \n                                                                                       &                          & \\begin{tabular}[c]{@{}l@{}}Transfer learning-inspired two-stream model for FL participants to learn from the fixed \\\\global model so as to accelerate training convergence\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Increased computation cost and delayed \\\\ convergence\\end{tabular}                                        \\\\ \\cline{2-4} \n\\multirow{-4}{*}{\\begin{tabular}[c]{@{}l@{}}Edge and End \\\\ Computation\\end{tabular}}  &                         & \\begin{tabular}[c]{@{}l@{}}MEC-inspired edge server assisted FL that aids in intermediate parameter aggregation to \\\\ reduce instances of communication\\end{tabular}                     & \\begin{tabular}[c]{@{}l@{}}System model is not scalable when there are \\\\ more edge servers\\end{tabular}                             \\\\ \\hline\n                                                                                       &               & \\begin{tabular}[c]{@{}l@{}}Structured and sketched updates to compress local models communicated from participant\\\\  to FL server\\end{tabular}                                           & Model accuracy and convergence issues                                                                                                \\\\ \\cline{2-4} \n\\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}Model \\\\ Compression\\end{tabular}}         &                 & \\begin{tabular}[c]{@{}l@{}}Similar to the ideas of , but for communication from  FL server \\\\  to participants\\end{tabular}                   & Model accuracy and convergence issues                                                                                                \\\\ \\hline\n                                                                                       &                         & \\begin{tabular}[c]{@{}l@{}}Selective communication of gradients that are assigned importance scores, \\\\ i.e., to reduce training loss\\end{tabular}                                & \\begin{tabular}[c]{@{}l@{}}Only empirically tested on simple datasets and \\\\ tasks, with fluctuating results\\end{tabular}            \\\\ \\cline{2-4} \n\\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}Importance-based \\\\ Updating\\end{tabular}} &                            & \\begin{tabular}[c]{@{}l@{}}Selective communication of local model updates that have higher relevance scores when \\\\ compared to previous global model\\end{tabular}                             & \\begin{tabular}[c]{@{}l@{}}Difficult to implement when global aggregations\\\\ are less frequent\\end{tabular}                          \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\end{table*}", "cites": [627, 596, 623, 4338, 582, 5985], "cite_extract_rate": 0.4, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes and integrates key concepts from multiple papers, particularly linking eSGD and CMFL by showing how each addresses communication cost through importance-based selection. It provides critical analysis by discussing the tradeoffs in accuracy, convergence speed, and empirical limitations of these methods. While it identifies patterns in approaches (e.g., using relevance scores or gradients), the abstraction remains moderate and does not fully generalize to a meta-level framework."}}
{"id": "1ee6a9c5-8838-4e8d-9441-2cab8985fa6c", "title": "Summary and Lessons Learned", "level": "subsection", "subsections": [], "parent_id": "b279d0a9-a81b-4e03-b78c-6bfca3339a27", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Communication Cost"], ["subsection", "Summary and Lessons Learned"]], "content": "In this section, we have reviewed three main approaches for communication cost reduction in FL, and for each approach, we discuss the solutions proposed in different studies. We summarize the approaches along with references in Table \\ref{tab:communication}. From this review, we gather the following lessons learned:\n\\begin{itemize}\n\\item Communication cost is a key issue to be resolved before we can implement FL at scale. In particular, the state-of-the-art DL models have high inference accuracy but are increasingly complex with millions of parameters. The slow upload speed of mobile devices can thus impede the implementation of efficient FL. \n\\item This section explores several key approaches to communication cost reduction. However, many of the approaches, e.g., model compression, result in a deterioration in model accuracy or incur high computation cost. For example, when too many local updates are implemented between communication rounds, the communication cost is indeed reduced but the convergence can be significantly delayed . The tradeoff between these sacrifices and communication cost reduction thus has to be well-managed. \n\\item The current studies of this tradeoff are often mainly empirical in nature, e.g., several experiments have to be done to find the optimal number of local training iterations before communication. With more effective optimization approaches formalized theoretically and tested empirically, FL can eventually become more scalable in nature. For example, the authors in  study the tradeoffs between the completion time of FL training and energy cost expended. Then, a weighted sum of completion time and energy consumption is minimized using an iterative algorithm. For delay-sensitive scenarios, the weights can be adjusted such that the FL participants consume more energy for completion time minimization. \n\\item Apart from working to directly reduce the size of model communicated, studies on FL can draw inspiration from applications and approaches in the MEC paradigm. For example, a simple case study introduced in  considers the base station as an intermediate model aggregator to reduce instances of device-cloud communication. Unfortunately, there are convergence issues when more edge servers or mobile devices are considered. This is exacerbated by the non-IID distribution of data across different edge nodes. For future works, this statistical challenge can be met, e.g., through inspirations from multi-task learning as we have discussed in Section \\ref{stats}. In addition, more effective and innovative system models can be explored such that FL networks can utilize the wealth of computing and storage resources that are closer to the data sources to facilitate efficient FL.\n\\item For the studies that we have discussed in this section, the heterogeneity among mobile devices, e.g., in computing capabilities, is often not considered. For example, one of the ways to reduce communication cost is to increase computation on edge devices, e.g., by performing more local updates  before each communication round. In fact, this does not merely lead to the expenditure of greater computation cost. The approach may also not be feasible for devices with weak processing power, and can lead to the \\textit{straggler effect}. As such, we further explore issues on resource allocation in the next section.\n\\end{itemize}", "cites": [582, 5986], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key communication cost reduction approaches in FL, integrating insights from cited works to present a coherent narrative. It offers critical perspectives by highlighting trade-offs between communication efficiency, model accuracy, and computational cost, while also pointing out the empirical nature of current studies. The discussion abstracts beyond individual papers, identifying broader challenges such as data heterogeneity and the straggler effect, and suggests future research directions informed by MEC and multi-task learning."}}
{"id": "59289545-bba6-4ef3-9e35-c47a8165f934", "title": "Resource Allocation", "level": "section", "subsections": ["f946e8d6-f9e9-41a5-8e79-0bb6c5623dd3", "ab71673f-1afa-4ab4-b2ac-6ddd4c0ad067", "5eca4010-24be-4e22-9b08-93587a264909", "e51a9ff7-da03-4d21-b611-1d80c08bdbdb", "2fd067fb-1843-47b6-9c86-2aebbd838d92"], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"]], "content": "\\label{sec:resource}\nFL involves the participation of heterogeneous devices that have different dataset qualities, computation capabilities, energy states, and willingness to participate. Given the device heterogeneity and resource constraints, i.e., in device energy states and communication bandwidth, resource allocation has to be optimized to maximize the efficiency of the training process. In particular, the following resource allocation issues need to be considered:\n\\begin{itemize}\n\\item \\textit{Participant Selection:} As part of the FL protocol presented in Section \\ref{proto}, participant selection refers to the selection of devices to participate in each training round. Typically, a set of participants is randomly selected by the server to participate. Then, the server has to aggregate parameter updates from all participating devices in the round before taking a weighted average of the models . As such, the training progress of FL is limited by the training time of the slowest participating devices, i.e., stragglers . New participant selection protocols are thus investigated in order to address the training bottleneck in FL. \n\\item \\textit{Joint Radio and Computation Resource Management:} Even though computation capabilities of mobile devices have grown rapidly, many devices still face a scarcity of radio resources . Given that local model transmission is an integral part of FL, there has been a growing number of studies that focus on developing novel wireless communication techniques for efficient FL.\n\\item \\textit{Adaptive Aggregation:} As discussed in Section \\ref{fltrain}, FL involves global aggregation in which model parameters are communicated to the FL server for aggregation. The conventional approach to global aggregation is a synchronous one, i.e., global aggregations occur in fixed intervals after all participants complete a certain number of rounds of local computation. However, adaptive calibrations of global aggregation frequency can be investigated to increase training efficiency subject to resource constraints .\n\\item \\textit{Incentive Mechanism:}  In the practical implementation of FL, participants may be reluctant to participate in a federation without receiving compensation since training models is resource-consuming. In addition, there exists information asymmetry between the FL server and participants since participants have greater knowledge of their available computation resources and data quality. Therefore, incentive mechanisms have to be carefully designed to both incentivize participation and reduce the potential adverse impacts of information asymmetry.\n \\end{itemize}", "cites": [5987, 582], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces key resource allocation issues in FL for mobile edge networks but does not deeply synthesize or integrate the cited papers into a broader narrative. It briefly mentions the relevance of communication efficiency and decentralized data, referencing the cited works, but lacks critical analysis or comparison of approaches. Some general patterns (e.g., stragglers, adaptive aggregation) are acknowledged, but the section remains largely at a descriptive level without offering high-level abstraction or evaluation of the cited research."}}
{"id": "f946e8d6-f9e9-41a5-8e79-0bb6c5623dd3", "title": "Participant Selection", "level": "subsection", "subsections": [], "parent_id": "59289545-bba6-4ef3-9e35-c47a8165f934", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"], ["subsection", "Participant Selection"]], "content": "To mitigate the training bottleneck, the authors in  propose a new FL protocol called \\textit{FedCS}. This protocol is illustrated  in Fig. \\ref{fig:fedcs}. The system model is a MEC framework in which the operator of the MEC is the FL server that coordinates training in a cellular network that comprises participating mobile devices that have heterogeneous resources. Accordingly, the FL server first conducts a \\textit{Resource Request} step to gather information such as wireless channel states and computing capabilities from a subset of randomly selected participants. Based on this information, the MEC operator selects the maximum possible number of participants that can complete the training within a prespecified deadline for the subsequent global aggregation phase. By selecting the maximum possible number of participants in each round, accuracy and efficiency of training are preserved. To solve the maximization problem, a greedy algorithm  is proposed, i.e., participants that take the least time for model upload and update are iteratively selected for training. The simulation results show that compared with the FL protocol which only accounts for training deadline without performing participant selection, \\textit{FedCS} can achieve higher accuracy since \\textit{FedCS} is able to involve more participants in each training round . However, \\textit{FedCS} has been tested only on simple DNN models. When extended to the training of more complex models, it may be difficult to estimate how many participants should be selected. For example, more training rounds may be needed for the training of complex models, and the selection of too few participants may lead to poor performance considering that some participants may drop out during training. In addition, there is bias towards selecting participants with devices that have better computing capabilities. These participants may not hold data that is representative of the population distribution. In particular, we revisit the fairness issue  subsequently in this section. \n\\begin{figure}[!]\n\t\\centering \n\t\\includegraphics[width=\\columnwidth]{Figs/partselection}\n\t\\caption{\\small Participant selection under the FedCS and Hybrid-FL protocol.}\n\t\\label{fig:fedcs}\n\\end{figure}\nWhile \\textit{FedCS} addresses heterogeneity of resources among participants in FL, the authors in  extend their work on the \\textit{FedCS} protocol with the Hybrid-FL protocol that deals with differences in data distributions among participants. The dataset of participants participating in FL may be non-IID since it is reflective of each individual user's specific characteristics. As we have discussed in Section \\ref{stats}, the non-IID dataset may significantly degrade the performance of the \\textit{FedAvg} algorithm . One proposed measure to address the non-IID nature of the dataset is to distribute publicly available data to participants, such that the EMD between their on-device dataset and the population distance is reduced. However, such a dataset may not always exist, and participants may not download them for security reasons. Thus, an alternative solution is to construct an approximately IID dataset using inputs from a limited number of privacy insensitive participants . In the Hybrid-FL protocol, during the \\textit{Resource Request} step  (Fig.~\\ref{fig:fedcs}), the MEC operator asks random participants if they permit their data to be uploaded. During the participant selection phase, apart from selecting participants based on computing capabilities, participants are selected such that their uploaded data can form an approximately IID dataset in the server, i.e., the amount of collected data in each class has close values (Fig.~\\ref{fig:fedcs}). Thereafter, the server trains a model on the collected IID dataset and merge this model with the global model trained by participants. The simulation results show that even with just 1\\% of participants sharing their data, classification accuracy for non-IID data can be significantly improved as compared to the aforementioned \\textit{FedCS} benchmark where data is not uploaded at all. However, the recommended protocol can violate the privacy and security of users, especially if the FL server is malicious. In the case when participants are malicious, data can be falsified before uploading, as we will further discuss in Section \\ref{sec:security}. In addition, the proposed measure can be costly especially in the case of videos and images. As such, it is unlikely that participants will volunteer for data uploading when they can free ride on the efforts of other volunteers. For feasibility, a well-designed incentive and reputation mechanism is needed to ensure that only trustworthy participants are allowed to upload their data. \nIn general, the mobile edge network environment in which FL is implemented on is dynamic and uncertain with variable constraints, e.g., wireless network and energy conditions. Thus, this can lead to training bottlenecks. To this end, Deep Q-Learning (DQL) can be used to optimize resource allocation for model training as proposed in . The system model is a Mobile Crowd Machine Learning (MCML) setting which enables participants in a mobile crowd network to collaboratively train DNN models required by a FL server. The participating mobile devices are constrained by energy, CPU, and wireless bandwidth. Thus, the server needs to determine proper amounts of data, energy, and CPU resources that the mobile devices use for training to minimize energy consumption and training time. Under the uncertainty of the mobile environment, a stochastic optimization problem is formulated. In the problem, the server is the agent, the state space includes the CPU and energy states of the mobile devices, and the action space includes the number of data units and energy units taken from the mobile devices. To achieve the objective, the reward function is defined as a function of the accumulated data, energy consumption, and training latency. To overcome the large state and action space issues of the server, the DQL technique based on Double Deep Q-Network (DDQN)  is adopted to solve the server's problem. The simulation results show that the DQL scheme can reduce energy consumption by around 31\\% compared with the greedy algorithm, and training latency is reduced up to 55\\% compared with the random scheme. However, the proposed scheme is applicable only in federations with few participating mobile devices. \nAs an extension to , the authors in  also proposes a resource allocation approach using DRL, with the added uncertainty that FL participants are mobile and so they may venture out of the network coverage range with a certain probability. Without prior knowledge of the mobile network, the FL server is able to optimize resource allocation across participants, e.g., channel selection and device energy consumption.\nThe aforementioned resource allocation approaches focus on improving the training efficiency of FL. However, this may cause some FL participants to be left out of the aggregation phase because they are stragglers with limited computing or communication resources. \nOne consequence of this  \\textit{unfair} resource allocation, a topic that is commonly explored in resource allocation for wireless networks  and ML . For example, if the participant selection protocol selects mobile devices with higher computing capabilities to participate in each training round , the FL model will be overrepresented by the distribution of data owned by participants with devices that have higher computing capabilities. Therefore, the authors in  and  consider fairness as an additional objective in FL. Fairness is defined in  to be the \\textit{variance} of performance of an FL model across participants. If the variance of the testing accuracy is large, this implies the presence of more bias or less fairness, since the learned model may be highly accurate for certain participants and less so for other underrepresented participants. The authors in  propose the \\textit{q}-Fair FL (\\textit{q}-FFL) algorithm that reweighs the objective function in \\textit{FedAvg} to assign higher weights in the loss function to devices with higher loss. The modified objective function is as follows:\n\\begin{equation}\n\\min _{w} F_{q}(w)=\\sum_{k=1}^{m} \\frac{p_{k}}{q+1} F_{k}^{q+1}(w)\n\\end{equation}\nwhere $F_k$ refers to the standard loss functions presented in Table \\ref{tab:loss-functions}, $q$ refers to the calibration of fairness in the system model, i.e., setting $q=0$ returns the formulation to the typical FL objective, and $p_{k}$ refers to ratio of local samples to the total number of training samples. In fact, this is a generalization of the Agnostic FL (AFL) algorithm proposed in , in which the device with the \\text{highest} loss dominates the entire loss function. The simulation results show that the proposed \\textit{q}-FFL can achieve lower variance of testing accuracy and converges more quickly than the AFL algorithm. However, as expected, for some calibrations of the \\textit{q}-FFL algorithm, there can be convergence slowdown since stragglers can delay the training process. As such, an asynchronous aggregation approach (to be subsequently discussed in this section) can potentially be considered for use with the \\textit{q}-FFL algorithm.\nIn contrast, the authors in  propose a neural network based approach to estimate the local models of FL participants that are left out during training. In the system model, resource blocks are first allocated by the base station to users whose models have larger effects on the global FL model. In particular, one user is selected to always be connected to the base station. This user's model parameters are then used as input to the feedforward neural network to estimate the model parameters of users who are left out during the training iteration. This allows the base stations to be able to integrate more locally trained FL model parameters to each iteration of global aggregation, thus improving the FL convergence speed.", "cites": [619, 7721, 7928, 5988, 5445, 582, 1408], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple approaches (FedCS, Hybrid-FL, DQL) to discuss participant selection in FL, showing a coherent synthesis of how different methods address heterogeneity, non-IID data, and fairness. It critically evaluates limitations such as privacy risks, fairness bias, and scalability issues. While it identifies patterns in the trade-offs between efficiency and fairness, it does not abstract to a higher-level conceptual or theoretical framework."}}
{"id": "ab71673f-1afa-4ab4-b2ac-6ddd4c0ad067", "title": "Joint Radio and Computation Resource Management", "level": "subsection", "subsections": [], "parent_id": "59289545-bba6-4ef3-9e35-c47a8165f934", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"], ["subsection", "Joint Radio and Computation Resource Management"]], "content": "While most FL studies have previously assumed orthogonal-access schemes such as Orthogonal Frequency-division Multiple Access (OFDMA) , the authors in  propose a multi-access Broadband Analog Aggregation (BAA) design for communication-latency reduction in FL. Instead of performing communication and computation separately during global aggregation at the server, the BAA scheme builds on the concept of over-the-air computation  to \\textit{integrate} computation and communication through exploiting the signal superposition property of a multiple-access channel. The proposed BAA scheme allows the reuse of the whole bandwidth (Fig. \\ref{fig:aircomp}(a)) whereas OFDMA orthogonalizes bandwidth allocation (Fig. \\ref{fig:aircomp}(b)). As such, for orthogonal-access schemes, communication latency increases in direct proportion with the number of participants whereas for multi-access schemes, latency is independent of the number of participants. The bottleneck of signal-to-noise ratio (SNR) during BAA transmission is the participating device with the longest propagation distance given that devices that are nearer have to lower their transmission power for amplitude alignment with devices located further. To increase SNR, participants with longer propagation distance have to be dropped. However, this leads to the truncation of model parameters. As such, to manage the SNR-truncation tradeoff, three scheduling schemes are considered namely i) \\textit{Cell-interior scheduling}: participants beyond a distance threshold are not scheduled, ii) \\textit{All-inclusive scheduling}: all participants are considered, and iii) \\textit{Alternating scheduling}: edge server alternates between the two aforementioned schemes. The simulation results show that the proposed BAA scheme can achieve similar test accuracy as the OFDMA scheme while achieving latency reduction from 10 times to 1000 times. As a comparison between the three scheduling schemes, the cell-interior scheme outperforms the all-inclusive scheme in terms of test accuracy for high mobility networks where participants have rapidly changing locations. For low mobility networks, the alternating scheduling scheme outperforms cell-interior scheduling. \nAs an extension, the authors in  also introduce error accumulation and gradient sparsification in addition to over-the-air computation. In , gradient vectors that are not transmitted as a result of power constraints are completely dropped. To improve the model accuracy, the untransmitted gradient vectors can first be stored in an error accumulation vector. In the next round, local gradient estimates are then corrected using the error vector. In addition, when there are bandwidth limitations, the participating device can apply gradient sparsification to keep only elements with the highest magnitudes for transmission. The elements that are not transmitted are subsequently added on to the error accumulation vector for gradient estimate correction in the next round. The simulation results show that the proposed scheme can achieve higher test accuracy than over-the-air computation without error accumulation or gradient sparsification since it corrects gradient estimates with the error accumulation vector and allows for a more efficient utilization of the bandwidth. \n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth, height= 9cm]{Figs/aircomp}\n\t\\caption{\\small A comparison  between (a) BAA by over-the-air computation which reuses bandwidth (above) and (b) OFDMA (below) which uses only the allocated bandwidth.}\n\t\\label{fig:aircomp}\n\\end{figure}\nSimilar to  and , the authors in  propose an integration of computation and communication via over-the-air computation. However, it is observed that aggregation error incurred during over-the-air computation can lead to a drop in model accuracy  as a result of signal distortion. As such, a participant selection algorithm is proposed in which the number of devices selected for training is maximized so as to improve statistical learning performance  while keeping the signal distortion below a threshold. Due to the nonconvexity  of the mean-square-error (MSE) constraint and intractability of the optimization problem, a difference-of-convex functions (DC) algorithm  is proposed to solve the maximization problem. The simulation results show that the proposed DC algorithm is scalable and can also achieve near-optimal performance that is comparable to global optimization, which is non-scalable due to its exponential time complexity. In comparison with other state-of-the-art approaches such as the semidefinite relaxation technique (SDR) proposed in , the proposed DC algorithm can also select more participants, thus also achieving higher model accuracy.", "cites": [5989, 8967, 8968, 8966, 582], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by connecting concepts like over-the-air computation, error accumulation, and participant selection into a cohesive narrative on joint radio and computation resource management. It critically evaluates tradeoffs (e.g., SNR vs. truncation, signal distortion vs. participant selection) and the scalability of proposed algorithms. The abstraction level is strong as it identifies general principles such as the interplay between communication efficiency, model accuracy, and system constraints in FL."}}
{"id": "5eca4010-24be-4e22-9b08-93587a264909", "title": "Adaptive Aggregation", "level": "subsection", "subsections": [], "parent_id": "59289545-bba6-4ef3-9e35-c47a8165f934", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"], ["subsection", "Adaptive Aggregation"]], "content": "The proposed \\textit{FedAvg} algorithm synchronously aggregates parameters as shown in Fig. \\ref{fig:async}(a) and is thus susceptible to the straggler effect, i.e., each training round only progresses as fast as the slowest device since the FL server waits for \\textit{all} devices to complete local training before global aggregation can take place . In addition, the model does not account for participants that can join halfway when the training round is already in progress. As such, the asynchronous model is proposed to improve the scalability and efficiency of FL. For asynchronous FL, the server updates the global model whenever it receives a local update (Fig. \\ref{fig:async}(b)). The authors in  find empirically that an asynchronous approach is robust to participants joining halfway during a training round, as well as when the federation involves participating devices with heterogeneous processing capabilities. However, the model convergence is found to be significantly delayed when data is non-IID and unbalanced. As an improvement,  propose the \\textit{FedAsync} algorithm in which each newly received local updates are adaptively weighted according to staleness, that is defined as the difference between the current epoch and iteration in which the received update belongs to. For example, a stale update from a straggler is outdated since it should have been received in previous training rounds. As such, it is weighted less. In addition, the authors also prove the convergence guarantee for a restricted family of non-convex problems. However, the current hyperparameters of the \\textit{FedAsync} algorithm still have to be tuned to ensure convergence in different settings. As such, the algorithm is still unable to generalize to suit the dynamic computation constraints of heterogeneous devices. In fact, given the uncertainty surrounding the reliability of asynchronous FL, synchronous FL remains to be the approach most commonly used today .\n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\columnwidth]{Figs/asyncnew}\n\t\\caption{\\small A comparison between (a) synchronous and (b) asynchronous FL.}\n\t\\label{fig:async}\n\\end{figure}\nFor most existing implementations of the \\textit{FedAvg} algorithm, the global aggregation phase occurs after a fixed number of training rounds. To better manage the dynamic resource constraints, the authors in  propose an adaptive global aggregation scheme which varies the global aggregation frequency so as to ensure desirable model performance while ensuring an efficient use of available resources, e.g., energy, during the FL training process. In ,  the MEC system model  used consists of (i) the local update phase where the model is trained using local data, (ii) edge aggregation phase where the intermediate aggregation occurs and (iii) global aggregation phase where updated model parameters are received and aggregated by the FL server. In particular, the authors study how the training loss is affected when the total number of edge server aggregation and local updates between global aggregation intervals vary. For this, a convergence bound of gradient descent with non-IID data is first derived. Then, a control algorithm is subsequently proposed to adaptively choose the optimal global aggregation frequency based on the most recent system state. For example, if global aggregation is too time consuming, more edge aggregations will take place before communication with the FL server is initiated. The simulation results show that the adaptive aggregation scheme outperforms the fixed aggregation scheme in terms of loss function minimization and accuracy within the same time budget. However, the convergence guarantee of the adaptive aggregation scheme is only considered for convex loss functions currently.", "cites": [590, 1325, 3431], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the concepts of synchronous and asynchronous FL from the cited papers, connecting them to resource allocation challenges in MEC. It provides critical analysis by discussing limitations of both approaches, such as the straggler effect in FedAvg and convergence delays in non-IID data for FedAsync. While it abstracts some general principles, such as the trade-off between aggregation frequency and resource constraints, the abstraction is slightly limited by focusing on specific algorithmic behaviors."}}
{"id": "e51a9ff7-da03-4d21-b611-1d80c08bdbdb", "title": "Incentive Mechanism", "level": "subsection", "subsections": [], "parent_id": "59289545-bba6-4ef3-9e35-c47a8165f934", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"], ["subsection", "Incentive Mechanism"]], "content": "The authors in  propose a service pricing scheme in which participants serve as training service providers for a model owner. In addition, to overcome energy inefficiency in the transfer of model updates, a cooperative relay network is proposed to support model update transfer and trading. The interaction between participants and model owner is modelled as a Stackelberg game  in which the model owner is the buyer and participants are the sellers. The Stackelberg game is proposed in which each rational participant can noncooperatively decide on its own profit maximization price. In the lower-level subgame, the model owner determines size of training data to maximize profits with consideration of the increasing concave relationship between learning accuracy of the model and size of training data. In the upper-level subgame, the participants decide the price per unit of data to maximize their individual profits. The simulation results show that the proposed mechanism can ensure uniqueness of the Stackelberg equilibrium. For example, model updates that contain valuable information are priced higher at the Stackelberg equilibrium. In addition, model updates can be transferred cooperatively, thus reducing congestion in communication and improving energy efficiency. However, the simulation environment only involves relatively few mobile devices.\nSimilar to , the authors in  also model the interaction between participants and model owner as a Stackelberg game, which is well-suited to represent the FL server-participant interaction involved in FL. \nHowever, unlike the aforementioned conventional approaches to solving Stackelberg formulations, a DRL-based approach is adopted together with the Stackelberg game by the authors in . In the DRL formulation, the FL server acts as an agent that decides a payment in response to the participation level and payment history of edge nodes, with the objective of minimizing incentive expenses. Then, the edge nodes determine an optimal participation level in response to the payment policy. This learning based incentive mechanism design enables the FL server to derive an optimal policy in response to its observed state, without requiring any prior information. \n \\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{Figs/contractnew}\n\t\\caption{\\small Participants with unknown resource constraints maximize their utility only if they choose the bundle that best reflects their constraints.}\n\t\\label{fig:contract}\n\\end{figure}\nIn contrast to , the authors in  propose an incentive design using a contract theoretic  approach to attract participants with high-quality data for FL. In particular, well-designed contracts can reduce information asymmetry through self-revealing mechanisms in which participants select only the contracts specifically designed for their types. For feasibility, each contract must satisfy the Individual Rationality (IR) and Incentive Compatibility (IC) constraints. For IR, each participant is assured of a positive utility when the participant participates in the federation. For IC, every utility maximizing participant only chooses the contract designed for its type. The model owner aims to maximize its own profits subject to IR and IC constraints. As illustrated in Fig. \\ref{fig:contract}, the optimal contracts derived are self-revealing such that each high-type participant with higher data quality only chooses contracts designed for its type, whereas each low-type participant with lower data quality does not have the incentive to imitate high-type participants.  The simulation results show that all types of participants only achieve maximum utility when they choose the contract that matches their types. In addition, the proposed contract theory approach also has better performance in terms of profit for the model owner compared with the Stackelberg game-based incentive mechanism. This is because under the contract theoretic approach, the model owner can extract more profits from the participants whereas under the Stackelberg game approach, the participants can optimize their individual utilities. In fact, the information asymmetry between FL servers and participants make contract theory a powerful and efficient tool for mechanism design in FL. As an extension, the authors in  introduced a \\textit{multi-dimensional} contract in which each FL participant determines the optimal computation power and image quality it is willing to contribute for model training, in exchange for contract rewards in each iteration.\nThe authors in  further introduce reputation as a metric to measure the reliability of FL participants and design a reputation-based participant selection scheme for reliable FL . In this setting, each participant has a reputation value  derived from two sources, (i) direct reputation opinions from past interactions with the FL server and (ii) indirect reputation opinions from other task publishers, i.e., other FL servers. The indirect reputation opinions are stored in an open-access reputation blockchain  to ensure secure reputation management in a decentralized manner. Before model training, the participants choose a contract that best fits its dataset accuracy and resource conditions. Then, the FL server chooses the participants that have reputation scores which are larger than a prespecified threshold. After the FL task is completed, i.e., a desirable accuracy is achieved, the FL server updates the reputation opinions, which are subsequently stored in the reputation blockchain. The simulation results show that the proposed scheme can significantly improve the accuracy of the FL model since unreliable workers are detected and not selected for FL training.\n\\begin{table*}[]\n\\centering \\caption{\\small Approaches to resource allocation in FL. \\label{tab:resource}}\n\\begin{adjustbox}{width=\\textwidth}\n\\begin{tabular}{|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{C0C0C0} \n\\multicolumn{1}{|c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Approaches}}                                              & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Ref.}}                                      & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Key Ideas}}                                                                                                                  & \\multicolumn{1}{c|}{\\cellcolor[HTML]{C0C0C0}\\textbf{Tradeoffs and Shortcomings}}                                                                                                               \\\\ \\hline\n                                                                                                               &                                                         & \\begin{tabular}[c]{@{}l@{}}FedCS to select participants based on computation capabilities so as to\\\\ complete FL training before specified deadline\\end{tabular}                 & \\begin{tabular}[c]{@{}l@{}}Difficult to estimate training duration accurately for\\\\ complex models\\end{tabular}                                                                                           \\\\ \\cline{2-4} \n                                                                                                               &                                                        & \\begin{tabular}[c]{@{}l@{}}Following , Hybrid-FL to select participants so as to accumulate \\\\ IID, distributable data for FL model training\\end{tabular}           & \\begin{tabular}[c]{@{}l@{}}Request of data sharing may defeat the \\\\ original intent of FL\\end{tabular}                                                                                        \\\\ \\cline{2-4} \n                                                                                                               &                                                         & DRL to determine resource consumption by FL participants                                                                                                                         &                                                                                                                                                                                                \\\\ \\cline{2-3}\n                                                                                                               &                                                       & \\begin{tabular}[c]{@{}l@{}}Following , DRL for resource allocation with\\\\ mobility-aware FL participants\\end{tabular}                    & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}DRL models are difficult to train especially\\\\ when the number of FL participants are large\\end{tabular}}                                          \\\\ \\cline{2-4} \n\\multirow{-5}{*}{\\begin{tabular}[c]{@{}l@{}}Participant\\\\ Selection\\end{tabular}}                              &                                                               & Fair resource allocation to reduce variance of model performance                                                                                                                 & Convergence delays with more fairness                                                                                                                                                                            \\\\ \\hline\n                                                                                                               &                                                               & \\begin{tabular}[c]{@{}l@{}}Propose BAA to integrate computation and communication through\\\\ exploiting the signal superposition property of multiple-access channel\\end{tabular} &                                                                                                                                                                                                \\\\ \\cline{2-3}\n                                                                                                               &                                                       & \\begin{tabular}[c]{@{}l@{}}Improves on  by accounting for gradient vectors that are not\\\\ transmitted due to power constraints\\end{tabular}    &                                                                                                                                                                                                \\\\ \\cline{2-3}\n\\multirow{-3}{*}{\\begin{tabular}[c]{@{}l@{}}Joint Radio and \\\\ Computation\\\\ Resource Management\\end{tabular}} &                                                        & \\begin{tabular}[c]{@{}l@{}}Improves on  using the DC algorithm to minimize\\\\ aggregation error\\end{tabular}                                    & \\multirow{-3}{*}{\\begin{tabular}[c]{@{}l@{}}Signal distortion can lead to drop in accuracy,\\\\ the scalability is also an issue when large \\\\ heterogeneous networks are involved\\end{tabular}} \\\\ \\hline\n                                                                                                               &                                                  & \\begin{tabular}[c]{@{}l@{}}Asynchronous FL where model aggregation occurs whenever local\\\\ updates are received by FL server\\end{tabular}                                        & \\begin{tabular}[c]{@{}l@{}}Significant delay in convergence in non-IID\\\\ and unbalanced dataset\\end{tabular}                                                                                   \\\\ \\cline{2-4} \n\\multirow{-2}{*}{\\begin{tabular}[c]{@{}l@{}}Adaptive \\\\ Aggregation\\end{tabular}}                              &                                                         & Adaptive global aggregation frequency based on resource constraints                                                                                                                                            & \\begin{tabular}[c]{@{}l@{}}Convergence guarantees are limited to restrictive\\\\ assumptions\\end{tabular}                                                                                        \\\\ \\hline\n                                                                                                               &  & \\begin{tabular}[c]{@{}l@{}}Stackelberg game for incentivizing higher quantities of training data\\\\ or compute resource contributed \\end{tabular}                                              & \\begin{tabular}[c]{@{}l@{}}FL server derives lower profits. Also, assumption\\\\ that there is only one FL server\\end{tabular}                                                                   \\\\ \\cline{2-4} \n                                                                                                               &                                        & Contract theoretic approach to incentivize FL participants                                                                                                                                                                  &                                                                                                                                                                                                \\\\ \\cline{2-3}\n\\multirow{-3}{*}{\\begin{tabular}[c]{@{}l@{}}Incentive \\\\ Mechanism\\end{tabular}}                               &                                                           & Reputation mechanism to select effective workers                                                                                                                                                             & \\multirow{-2}{*}{Assumption that there is only one FL server}                                                                                                                                  \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\end{table*}", "cites": [619, 8967, 5993, 5992, 5990, 5989, 7928, 3431, 5991], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by connecting Stackelberg game-based, DRL-based, and contract-theoretic incentive mechanisms in FL, showing how they model interactions between model owners and participants. It includes critical evaluation, such as noting the simulation limitations of some approaches and comparing the performance and trade-offs of different mechanisms. The abstraction is strong, as it highlights the broader implications of information asymmetry and mechanism design in FL systems."}}
{"id": "2fd067fb-1843-47b6-9c86-2aebbd838d92", "title": "Summary and Lessons Learned", "level": "subsection", "subsections": [], "parent_id": "59289545-bba6-4ef3-9e35-c47a8165f934", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Resource Allocation"], ["subsection", "Summary and Lessons Learned"]], "content": "In this section, we have discussed four main issues in resource allocation. The issues and approaches are summarized in Table \\ref{tab:resource}. From this section, the lessons learned are as follows:\n\\begin{itemize}\n\\item In heterogeneous mobile networks, the consideration of resource allocation is important to ensure efficient FL. For example, each training iteration is only conducted as quickly as the slowest FL participant, i.e., the straggler effect. In addition, the model accuracy is highly dependent on the quality of data used for training by FL participants. In this section, we have explored different dimensions of resource heterogeneity for consideration, e.g., varying computation and communication capabilities, willingness to participate, and quality of data for local model training. In addition, we have explored various tools that can be considered for resource allocation. For example, DRL is useful given the dynamic and uncertain wireless network conditions experienced by FL participants, whereas contract theory can serve as a powerful tool in mechanism design under the context of information asymmetry. Naturally, traditional optimization approaches have also been well explored in radio resource management for FL, given the high dependency on communications efficiency in FL.\n\\item In Section \\ref{sec: communication}, communication cost reduction comes with a sacrifice in terms of either higher computation costs or lower inference accuracy. Similarly, there exist different tradeoffs to be considered in resource allocation. A scalable model is thus one that enables customization to suit varying needs. For example, the study of  allows the FL server to calibrate levels of fairness when allocating training importance, whereas the study in  enables the tradeoffs between training completion time and energy expense to be calibrated by the FL system adminstrator.\n\\item In synchronous FL, the FL system is susceptible to the straggler effect. As such, asynchronous FL has been proposed as a solution in  and . In addition, asynchronous FL also allows participants to join the FL training halfway even while a training round is in progress. This is more reflective of practical FL settings and can be an important contributing factor towards ensuring the scalability of FL. However, synchronous FL remains to be the most common approach used due to convergence guarantees . Given the many advantages of asynchronous FL, new asynchronous algorithms should be investigated. In particular, for future proposed algorithms, the convergence guarantee in a non-IID setting for non-convex loss functions needs to be considered.\n\\item The study of incentive mechanism design is a particularly important aspect of FL. In particular, due to data privacy concerns, the FL servers are unable to check for training data quality. With the use of self-revealing mechanisms in contract theory, or through modeling the interactions between FL server and participants with game theoretic concepts, high quality data can be motivated as contributions from FL participants. However, existing studies in , , and  generally assume that a\nfederation enjoys a monopoly. In particular, each system model is assumed to only consist of multiple individual participants collaborating with a sole FL server. There can be exceptions to this setting as follows: (i) the participants may be competing data owners who are reluctant to share their model parameters since the competitors\nalso benefit from a trained global model and (ii) the FL servers\nmay compete with other FL servers, i.e., model owners.\nIn this case, the formulation of the incentive mechanism design\nwill be vastly different from that proposed. A relatively novel approach has been to model the regret  of each FL participants in joining the various competing federations for model training. For future works, a system model with multiple competing federations can be considered together with Stackelberg games and contract theoretic approaches.\n\\item In this section, we have assumed that FL assures the privacy and security of participants. However, as discussed in the following section, this assumption may not hold in the presence of malicious participants or FL server. \n\\end{itemize}", "cites": [590, 7928, 5991, 5990, 5986, 1325], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information across the cited papers to form a coherent narrative on resource allocation in FL, connecting topics like fairness, computation/communication tradeoffs, and incentive design. It provides critical analysis by highlighting limitations such as the monopoly assumption in incentive mechanisms and the need for convergence guarantees in asynchronous FL. The section abstracts from individual studies to identify broader patterns and design principles, such as the tradeoffs inherent in resource allocation and the importance of scalability and fairness in FL systems."}}
{"id": "ce302473-c70b-411d-815f-c3000491c151", "title": " Privacy and Security Issues", "level": "section", "subsections": ["9436f299-d77d-488c-81f2-7749bbacfea4", "4de273f1-2950-457b-a4ee-9560525e1191", "0b63118b-0dab-4522-83aa-c544dfcbfe18"], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"]], "content": "\\label{sec:security}\nOne of the main objectives of FL is to protect the privacy of participants, i.e., the participants only need to share parameters of the trained model instead of sharing their actual data. However, some recent research works have shown that privacy and security concerns may arise when the FL participants or FL servers are malicious in nature. In particular, this defeats the purpose of FL since the resulting global model can be corrupted, or the participants may even have their privacy compromised during model training. In this section, we discuss the following issues:\n\\begin{itemize}\n\t\\item \\textit{Privacy}: Even though FL does not require the exchange of data for collaborative model training, a malicious participant can still infer sensitive information, e.g., gender, occupation, and location, from other participants based on their shared models. For example, in~, when training a binary gender classifier on the FaceScrub~ dataset, the authors show that they can infer if a certain participant's inputs are included in the dataset just from inspecting the shared model, with a very high accuracy of up to 90\\%. Thus, in this section, we discuss privacy issues related to the shared models in FL and review solutions proposed to preserve the privacy of participants. \n\t\\item \\textit{Security}: In FL, the participants locally train the model and share trained parameters with other participants in order to improve the accuracy of prediction. However, this process is susceptible to a variety of attacks, e.g., data and model poisoning, in which a malicious participant can send incorrect parameters or corrupted models to falsify the learning process during global aggregation. Consequently, the global model will be updated incorrectly, and the whole learning system becomes corrupted. This section discusses more details on emerging attacks in FL as well as some recent countermeasures to deal with such attacks.\n\\end{itemize}", "cites": [614], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying key privacy and security challenges in FL and referencing a specific paper to illustrate how model updates can leak sensitive data. It integrates the cited work into the broader context of FL vulnerabilities, showing some synthesis. However, it lacks deeper critical analysis or evaluation of multiple approaches, and the abstraction is limited to general categories like 'data leakage' and 'poisoning attacks' without forming a higher-level conceptual framework."}}
{"id": "e6875e90-30f2-47e7-bb9b-64bd78cd4e74", "title": "Information exploiting attacks in machine learning - A brief overview", "level": "subsubsection", "subsections": [], "parent_id": "9436f299-d77d-488c-81f2-7749bbacfea4", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Privacy Issues"], ["subsubsection", "Information exploiting attacks in machine learning - A brief overview"]], "content": "One of the first research works that shows the possibility of extracting information from a trained model is~. In this paper, the authors show that during the training phase, the correlations implied in the training samples are gathered inside the trained model. Thus, if the trained model is released, it can lead to an unexpected information leakage to attackers. For example, an adversary can infer the ethnicity or gender of a user from its trained voice recognition system. In~, the authors develop a model-inversion algorithm which is very effective in exploiting information from decision tree-based or face recognition trained models. The idea of this approach is to compare the target feature vector with each of the possible value and then derive a weighted probability estimation which is the correct value. The experiment results reveal that by using this technique, the adversary can reconstruct an image of the victim's face from its label with a very high accuracy.\nRecently, the authors in~ show that it is even possible for an adversary to infer information of a victim through queries to the prediction model. In particular, this occurs when a malicious participant has the access to make prediction queries on a trained model. Then, the malicious participant can use the prediction queries to extract the trained model from the data owner. More importantly, the authors point out that this kind of attack can successfully extract model information from a wide range of training models such as decision trees, logistic regressions, SVMs, and even complex training models including DNNs. Some recent research works have also demonstrated the vulnerabilities of DNN-based training models against model extraction attacks~. Therefore, this raises a serious privacy concern for participants in sharing training models in FL.", "cites": [2676, 603, 7318], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple works on information exploitation in ML, integrating them into a narrative about privacy risks in FL. It provides a high-level explanation of attack mechanisms but lacks deeper comparative or critical analysis of the cited papers. Some abstraction is present, particularly in highlighting the broader privacy implications for FL participants, but the analysis remains focused on summarizing attack types."}}
{"id": "6d8162e0-32ab-437f-8104-83ee9b129d2f", "title": "Differential privacy-based protection solutions for FL participants", "level": "subsubsection", "subsections": [], "parent_id": "9436f299-d77d-488c-81f2-7749bbacfea4", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Privacy Issues"], ["subsubsection", "Differential privacy-based protection solutions for FL participants"]], "content": "In order to protect the privacy of parameters trained by DNNs, the authors in~ introduce a technique, called \\emph{differentially private stochastic gradient descent}, which can be effectively implemented on DL algorithms. The key idea of this technique is to add some ``noise'' to the trained parameters by using a differential privacy-preserving randomized mechanism~, e.g., a Gaussian mechanism, before sending such parameters to the server. In particular, at the gradient averaging step of a normal FL participant, a Gaussian distribution is used to approximate the differentially private stochastic gradient descent. Then, during the training phase, the participant keeps calculating the probability that malicious participants can exploit information from its shared parameters. Once a predefined threshold is reached, the participant will stop its training process. In this way, the participant can mitigate the risk of revealing private information from its shared parameters. \nInspired by this idea, the authors in~ develop an approach which can achieve a better privacy-protection solution for participants. In this approach, the authors propose two main steps to process data before sending trained parameters to the server. In particular, for each learning round, the aggregate server first selects a random number of participants to train the global model. Then, if a participant is selected to train the global model in a learning round, the participant will adopt the method proposed in~, i.e., using a Gaussian distribution to add noise to the trained model before sending the trained parameters to the server. In this way, a malicious participant cannot infer information of other participants by using the parameters of shared global model as it has no information regarding who has participated in the training process in each learning round.", "cites": [7727, 7608], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of differential privacy-based protection solutions for FL participants, referencing two papers. While it connects the general idea of adding noise via Gaussian mechanisms from both sources, it lacks deeper synthesis, critical evaluation, or abstraction into broader principles. The narrative remains focused on describing specific methods without offering comparative insights or identifying limitations."}}
{"id": "22f0ecb5-77e0-4892-a883-3246442b6787", "title": "Collaborative training solutions", "level": "subsubsection", "subsections": [], "parent_id": "9436f299-d77d-488c-81f2-7749bbacfea4", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Privacy Issues"], ["subsubsection", "Collaborative training solutions"]], "content": "While DP solutions can protect private information of a honest participant from other malicious participants in FL, they only work well if the server is trustful. If the server is malicious, it can result in a more serious privacy threat to all participants in the network. Thus, the authors in~ introduce a collaborative DL framework to render multiple participants to learn the global model without uploading their explicit training models to the server. The key idea of this technique is that instead of uploading the whole set of trained parameters to the server and updating the whole global parameters to its local model, each participant wisely selects the number of gradients to upload and the number of parameters from the global model to update as illustrated in Fig.~\\ref{fig:CollTrain}. In this way, malicious participants cannot infer explicit information from the shared model. One interesting result of this paper is that even when the participants do not share all trained parameters and do not update all parameters from the shared model, the accuracy of proposed solution is still close to that of the case when the server has all dataset to train the global model. For example, for the MNIST dataset~, the accuracy of prediction model when the participants agree to share 10\\% and 1\\% of their parameters are respectively 99.14\\% and 98.71\\%, compared with 99.17\\% for the centralized solution when the server has full data to train. However, the approach is yet to be tested on more complex classification tasks.\n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{Figs/CollTrain}\n\t\\caption{\\small Selective parameters sharing model.}\n\t\\label{fig:CollTrain}\n\\end{figure}\nAlthough selective parameter sharing and DP solutions can make information exploiting attacks more challenging, the authors in~ show that these solutions are susceptible to a new type of attack, called powerful attack, developed based on Generative Adversarial Networks (GANs) . GANs is a class of ML technique which uses two neural networks, namely generator network and discriminator network, that compete with each other to train data. The generator network tries to generate the fake data by adding some ``noise'' to the real data. Then, the generated fake data is passed to the discriminator network for classification. After the training process, the GANs can generate new data with the same statistics as the training dataset. Inspired by this idea, the authors in~ develop a powerful attack which allows a malicious participant to infer sensitive information from a victim participant even with just a part of shared parameters from the victim as illustrated in Fig.~\\ref{fig:GAN_attacks}. To deal with the GAN attack, the authors in~ introduce a solution using secret sharing scheme with extreme boosting algorithm. This approach executes a lightweight secret sharing protocol before transmitting the newly trained model in plaintext to the server at each round. Thereby, other participants in the network cannot infer information from the shared model. However, the limitation of this approach is the reliance on a trusted third party to generate signature key pairs. \n\\begin{figure*}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{Figs/GAN_attacks}\n\t\\caption{\\small GAN Attack on collaborative deep learning.}\n\t\\label{fig:GAN_attacks}\n\\end{figure*}\nDifferent from all aforementioned works, the authors in~ introduce a collaborative training model in which all participants cooperate to train a federated GANs model. The key idea of this method is that the federated GANs model can generate artificial data that can replace participants' real data, and thus protecting the privacy of real data for the honest participants. In particular, in order to guarantee participants' data privacy while still maintaining flexibility in training tasks, this approach produces a federated generative model. This model can output artificial data that does not belong to any real user in particular, but comes from the common cross-user data distribution. As a result, this approach can significantly reduce the possibility of malicious exploitation of information from real data. However, this approach inherits existing limitations of GANs, e.g., training instability due to the generated fake data, which can dramatically reduce the performance of collaborative learning models.", "cites": [5994, 3477], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates two papers to present a narrative on collaborative training solutions for privacy in FL, showing how different techniques (selective parameter sharing, secret sharing with boosting, and federated GANs) address privacy threats. It includes some critical evaluation by pointing out limitations of each approach, such as reliance on a trusted third party or training instability. However, the synthesis is moderate and the abstraction remains at a surface level without clearly articulating overarching principles or unifying frameworks."}}
{"id": "1705aaee-44c7-4c66-aaa4-b8557c9e40df", "title": "Data Poisoning Attacks", "level": "subsubsection", "subsections": [], "parent_id": "4de273f1-2950-457b-a4ee-9560525e1191", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Security Issues"], ["subsubsection", "Data Poisoning Attacks"]], "content": "In FL, a participant trains its data and sends the trained model to the server for further processing. In this case, it is intractable for the server to check the real training data of a participant. Thus, a malicious participant can poison the global model by creating \\emph{dirty-label} data to train the global model with the aim of generating falsified parameters. For example, a malicious participant can generate a number of samples, e.g., photos, under a designed label, e.g., a clothing branch, and use them to train the global model to achieve its business goal, e.g., the prediction model shows results of the targeted clothing branch. Dirty-label data poisoning attacks are demonstrated to achieve high misclassifications in DL processes, up to 90\\%, when a malicious participant injects relatively few dirty-label samples (around 50) to the training dataset~. This calls for urgent solutions to deal with data poisoning attacks in FL. \nIn~, the authors investigate impacts of a sybil-based data poisoning attack to a FL system. In particular, for the sybil attack, a malicious participant tries to improve the effectiveness of data poisoning in training the global model by creating multiple malicious participants. In Table~\\ref{tab:Syblil}, the authors show that with only two malicious participants, the attack success rate can achieve up to 96.2\\%, and now the FL model is unable to correctly classify the image of ``1'' (instead it always incorrectly predicts them to be the image of ``7''). To mitigate sybil attacks, the authors then propose a defense strategy, namely FoolsGold. The key idea of this approach is that honest participants can be distinguished from sybil participants based on their updated gradients. Specifically, in the non-IID FL setting, each participant's training data has its own particularities, and sybil participants will contribute gradients that appear more similar to each other than those of other honest participants. With FoolsGold, the system can defend the sybil data poisoning attack with minimal changes to the conventional FL process and without requiring any auxiliary information outside of the learning process. Through simulations results on 3 diverse datasets (MNIST~, KDDCup~, Amazon Reviews~), the authors show that FoolsGold can mitigate the attack under a variety of conditions, including different distributions of participant data, varying poisoning targets, and various attack strategies. \n\\begin{table}[!]\n\t\\centering\n\t\\caption{\\small The accuracy and attack success rates for no-attack scenario and attacks with 1 and 2 sybils in a FL system with MNIST dataset~.}\n\t\\label{tab:Syblil}\n\t\\begin{tabular}{||c||c|c|c||} \\hline\n\t\t& \\textbf{Baseline} & \\textbf{Attack 1} & \\textbf{Attack 2} \\\\ \n\t\t\\hline\n\t\t\\hline \t\t\n\t\tNumber of honest participants & 10 & 10 & 10   \t\t\t\\\\ \\cline{1-4}\n\t\tNumber of sybil participants & 0 & 1 & 2     \t\t\t\\\\ \\cline{1-4}\n\t\tThe accuracy (digits: 0, 2-9) & 90.2\\% & 89.4\\% & 88.8\\%    \\\\ \\cline{1-4}\n\t\tThe accuracy (digit: 1) & 96.5\\% & 60.7\\% & 0.0\\%    \\\\ \\cline{1-4}\n\t\t\\textbf{Attack success rate} & 0.0\\% & 35.9\\% & 96.2\\%    \\\\ \\cline{1-4}\n\t\t\\hline\\end{tabular}\n\\end{table}", "cites": [8969, 5468], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the two cited papers to explain data poisoning attacks, particularly sybil-based attacks, in FL systems. It connects the problem of intractable data verification with specific examples of attack effectiveness and defense mechanisms. While it provides some critical analysis of the vulnerability of FL to such attacks, it does not extensively compare or evaluate multiple defense strategies or identify broader theoretical limitations. The abstraction is moderate, as it identifies patterns in how sybil participants affect gradient similarity and model performance."}}
{"id": "0ebb5bb6-388b-43b9-a52a-920b017ef82b", "title": "Model Poisoning Attacks", "level": "subsubsection", "subsections": [], "parent_id": "4de273f1-2950-457b-a4ee-9560525e1191", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Security Issues"], ["subsubsection", "Model Poisoning Attacks"]], "content": "Unlike data poisoning attacks which aim to generate fake data to cause adverse impacts to the global model, a model poisoning attack attempts to directly poison the global model that it sends to the server for aggregation. As shown in~ and , model poisoning attacks are much more effective than those of data poisoning attacks, especially for large-scale FL with many participants. The reason is that for data poisoning attacks, a malicious participant's updates are scaled based on its dataset and the number of participants in the federation. However, for model poisoning attacks, a malicious participant can modify the updated model, which is sent to the server for aggregation, directly. As a result, even with one single attacker, the whole global model can be poisoned. The simulation results in~ also confirm that even a highly constrained adversary with limited training data can achieve high success rate in performing model poisoning attacks. Thus, solutions to protect the global model from model poisoning attacks have to be developed. \nIn~, some solutions are suggested to prevent model poisoning attacks. Firstly, based on an updated model shared from a participant, the server can check whether the shared model can help to improve the global model's performance or not. If not, the participant will be marked to be a potential attacker, and after few rounds of observing the updated model from this participant, the server can determine whether this is a malicious participant or not. The second solution is based on the comparison among the updated models shared by the participants. In particular, if an updated model from a participant is too different from the others, the participant can potentially be a malicious one. Then, the server will continue observing updates from this participant before it can determine whether this is a malicious user or not. However, model poisoning attacks are extremely difficult to prevent because when training with millions of participants, it is intractable to evaluate the improvement from every single participant. As such, more effective solutions need to be further investigated. \nIn~, the authors introduce a more effective model poisoning attack which is demonstrated to achieve 100\\% accuracy on the attacker's task within just a single learning round. In particular, a malicious participant can share its poisoned model which not only is trained for its intentional purpose, but which also contains a backdoor function. In this paper, the authors consider to use a semantic backdoor function to inject into the global model. The reason is that this function can make the global model misclassify even without a need to modify the input data of the malicious participant. For example, an image classification backdoor function can inject an attacker-chosen label to all images with some certain features, e.g., all dogs with black stripes can be misclassifed to be cats. In the simulations, the authors show that this attack can greatly outperform other conventional FL data poisoning attacks. For example, in a word-prediction task with 80,000 total participants, compromising just eight of them is enough to achieve 50\\% backdoor accuracy, as compared to 400 malicious participants needed to perform the data-poisoning attack.", "cites": [5467, 2673], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the two cited papers, highlighting the differences and relative effectiveness between model and data poisoning attacks in FL. It presents a coherent explanation of how model poisoning works and integrates the findings from both papers. The analysis includes some critical evaluation, such as the difficulty in preventing model poisoning at scale, but could offer a deeper critique of the proposed solutions. It also provides a level of abstraction by identifying the broader threat of model poisoning in large-scale FL systems, including the use of backdoors and semantic manipulation."}}
{"id": "0d9b23c3-967b-4457-85bf-955848611c53", "title": "Free-Riding Attacks", "level": "subsubsection", "subsections": [], "parent_id": "4de273f1-2950-457b-a4ee-9560525e1191", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", " Privacy and Security Issues"], ["subsection", "Security Issues"], ["subsubsection", "Free-Riding Attacks"]], "content": "\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{Figs/BlockFL}\n\t\\caption{\\small An illustration of (a) conventional FL and (b) the proposed BlockFL architectures.}\n\t\\label{fig:BlockFL}\n\\end{figure}\nFree-riding is another attack in FL that occurs when a participant wants to benefit from the global model without contributing to the learning process. The malicious participant, i.e., free-rider, can pretend that it has very small number of samples to train or it can select a small set of its real dataset to train, e.g., to save its resources. As a result, the honest participants need to contribute more resources in the FL training process. To address this problem, the authors in~ introduce a blockchain-based FL architecture, called BlockFL, in which the participants' local learning model updates are exchanged and verified by leveraging the blockchain technology. In particular, each participant trains and sends the trained global model to its associated miner in the blockchain network and then receives a reward that is proportional to the number of trained data samples as illustrated in Fig.~\\ref{fig:BlockFL}. In this way, this framework can not only prevent the participants from free-riding, but also incentivize all participants to contribute to the learning process. A similar blockchain-based model is also introduced in~ to provide data confidentiality, computation auditability, and incentives for the participants of FL. However, the utilization of the blockchain technology implies the incurrence of a significant cost for implementing and maintaining miners to operate the blockchain network. Furthermore, consensus protocols used in blockchain networks, e.g., proof-of-work (PoW), can cause a long delay in information exchange, and thus they may not be appropriate to implement on FL models. \n\\begin{table*}[!]\n\t\\caption{\\small A summary of attacks and countermeasures in FL.}\n\t\\label{tab:Sec_Pri}\n\t\\scriptsize\t\n\t\\begin{centering}\n\t\t\\begin{tabular}{|>{\\centering\\arraybackslash}m{2cm}|>{\\centering\\arraybackslash}m{4.5cm}|>{\\centering\\arraybackslash}m{10.5cm}|}\n\t\t\t\\hline\n\t\t\t\\cellcolor{mygray} & \\cellcolor{mygray} &\\cellcolor{mygray} \\tabularnewline\n\t\t\t\\cellcolor{mygray} \\multirow{-2 }{*}{\\textbf{Attack Types}} &\\cellcolor{mygray} \\multirow{-2}{*} {\\textbf{Attack Method}} &\\cellcolor{mygray} \\multirow{-2}{*} {\\textbf{Countermeasures}} \\tabularnewline\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tInformation exploiting attacks (privacy issues) & Attackers try to illegally exploit information from the shared model.  &  \\begin{itemize}\n\t\t\t\t\\item \\emph{Differentially private stochastic gradient descent}:  Add ``noise'' to the trained parameters by using a differential privacy-preserving randomized mechanism~.\n\t\t\t\t\\item \\emph{Differentially private and selective participants}:  Add ``noise'' to the trained parameters and select randomly participants to train global model in each round~.\n\t\t\t\t\\item \\emph{Selective parameter sharing}: Each participant wisely selects the number of gradients to upload and the number of parameters from the global model to update~.\n\t\t\t\t\\item \\emph{Secrete sharing scheme with extreme boosting algorithm}: This approach executes a lightweight secret sharing protocol before transmitting the newly trained model in plaintext to the server at each round~.\n\t\t\t\t\\item \\emph{GAN model training}: All participants are cooperative to train a federated GANs model~.\n\t\t\t\\end{itemize} \\\\\n\t\t\t\\hline\n\t\t\tData poisoning attacks & Attackers poison the global model by creating \\emph{dirty-label} data and use such data to train the global model.  &  \\begin{itemize} \\item \\emph{FoolsGoal}: Distinguish honest participants based on their updated gradients. It is based on the fact that in the non-IID FL setting, each participant's training data has its own particularities, and malicious participants will contribute gradients that appear more similar to each other than those of the honest participants~. \\end{itemize} \\\\\n\t\t\t\\hline\n\t\t\tModel poisoning attacks & Attackers attempt to directly poison the global model that they send to the server for aggregation.  &  \\begin{itemize}\n\t\t\t\t\\item Based on an updated model shared from a participant, the server can check whether the shared model can help to improve the global model's performance or not. If not, the participant will be marked to be a potential attacker~.\n\t\t\t\t\\item Compare among the updated global models shared by the participants, and if an updated global model from a participant is too different from others, it could be a potential malicious participant~.\n\t\t\t\\end{itemize}  \\\\\n\t\t\t\\hline\n\t\t\tFree-riding attacks & Attackers benefit from the global model without contributing to the learning process, e.g., by pretending that they have very small number of samples to train.  &  \\begin{itemize}\n\t\t\t\t\\item \\emph{BlockFL}: Participants' local learning model updates are exchanged and verified by leveraging blockchain technology. In particular, each participant trains and sends the trained global model to its associated miner in the blockchain network and then receives a reward that is proportional to the number of trained data samples~.\n\t\t\t\\end{itemize}  \\\\\n\t\t\t\\hline\t\t\t\n\t\t\\end{tabular}\n\t\t\\par\\end{centering}\n\\end{table*}", "cites": [5468, 7608, 5994, 5467, 7727], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of free-riding attacks and introduces the BlockFL solution, but it lacks integration with other cited papers. There is minimal synthesis, as the discussion is largely confined to one framework. The critical evaluation is limited to pointing out the cost and delay of blockchain, without deeper comparative or analytical insights. The abstraction is also weak, as it does not generalize the concept of incentives or blockchain use in FL beyond the specific examples given."}}
{"id": "a10f5c93-1fee-4fa8-a6ac-ad80e6dd1548", "title": "Applications of Federated Learning for Mobile Edge Computing", "level": "section", "subsections": ["a6ca9148-8aad-4b67-a42c-c3513f305da8", "9bb6bf60-a8e6-4f3a-928f-802af0dd72af", "bdbb9fa8-26c1-4b46-8920-41f52291e380", "090fafc1-63ae-4a0f-8017-679d11acea66"], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Applications of Federated Learning for Mobile Edge Computing"]], "content": "\\label{sec:application}\nIn the aforementioned studies, we have discussed the issues pertaining to the implementation of FL as an enabling technology that allows collaborative learning at mobile edge networks. In this section, we focus instead on the applications of FL for mobile edge network optimization. \nAs highlighted by the authors in , the increasing complexity and heterogeneity of wireless networks enhance the appeal of adopting a data-driven ML based approach  for optimizing system designs and resource allocation decision making for mobile edge networks. However, as discussed in previous sections, the private data of users may be sensitive in nature. As such, existing learning based approach can be combined with FL for privacy-preserving applications.\nIn this section, we consider four applications of FL in edge computing:\n\\begin{itemize}\n\\item \\textit{Cyberattack Detection:} The ubiquity of IoT devices and increasing sophistication of cyberattacks  imply that there is a need to improve existing cyberattack detection tools. Recently, DL has been widely successful in cyberattack detection. Coupled with FL, cyberattack detection models can be learned collaboratively while maintaining user privacy.\n\\item \\textit{Edge Caching and Computation Offloading:} Given the computation and storage capacity constraints of edge servers, some computationally intensive tasks of end devices have to be offloaded to the remote cloud server for computation. In addition, commonly requested files or services should be placed on edge servers for faster retrieval, i.e., users do not have to communicate with the remote cloud when they want to access these files or services. As such, an optimal caching and computation offloading scheme can be collaboratively learned and optimized with FL.\n\\item \\textit{Base Station Association:} In a dense network, it is important to optimize base station association so as to limit interference faced by users. However, traditional learning based approaches that utilize user data often assume that such data is centrally available. Given user privacy constraints, an FL based approach can be adopted instead.\n\\item \\textit{Vehicular Networks:} The Internet of Vehicles (IoV)  features smart vehicles with data collection, computation and communication capabilities for relevant functions, e.g., navigation and traffic management. However, this wealth of knowledge is again, private and sensitive in nature since it can reveal the driver's location and personal information. In this section, we discuss the use of an FL based approach in traffic queue length prediction and energy demand in electric vehicle charging stations done at the edge of IoV networks.\n\\end{itemize}\n\\begin{table*}[]\n\\centering \\caption{\\small FL based approaches for mobile edge network optimization. \\label{tab:application}}\n\\begin{tabular}{|l|l|l|}\n\\hline\n\\textbf{Applications}                                             & \\textbf{Ref.} & \\textbf{Description}                                             \\\\ \\hline\n\\multirow{3}{*}{Cyberattack Detection}                   &       & Cyberattack detection with edge nodes as participants   \\\\ \\cline{2-3} \n                                                         &       & Cyberattack detection with IoT gateways as participants \\\\ \\cline{2-3} \n                                                         &      & Blockchain to store model updates                       \\\\ \\hline\n\\multirow{4}{*}{Edge caching and computation offloading} &      & DRL for caching and offloading in UEs                   \\\\ \\cline{2-3} \n                                                         &      & DRL for computation offloading in IoT devices           \\\\ \\cline{2-3} \n                                                         &      & Stacked autoencoder learning for proactive caching      \\\\ \\cline{2-3} \n                                                         &      & Greedy algorithm to optimize service placement schemes  \\\\ \\hline\n\\multirow{2}{*}{Base station assoication}                &       & Deep echo state networks for VR application             \\\\ \\cline{2-3} \n                                                         &       & Mean field game with imitation for cell association     \\\\ \\hline\n\\multirow{2}{*}{Vehicular networks}                      &      & Extreme value theory for large queue length prediction  \\\\ \\cline{2-3} \n                                                         &        & Energy demand learning in electric vehicular networks   \\\\ \\hline\n\\end{tabular}\n\\end{table*}", "cites": [5997, 5975, 5977, 5971, 5974, 5996, 5995], "cite_extract_rate": 0.4666666666666667, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section organizes FL applications into four categories and briefly describes each with references, showing some synthesis by grouping related works. However, it does not deeply connect or compare the cited papers. Critical analysis is minimal, and abstraction is limited to surface-level generalization without identifying broader principles or trends."}}
{"id": "a6ca9148-8aad-4b67-a42c-c3513f305da8", "title": "Cyberattack Detection", "level": "subsection", "subsections": [], "parent_id": "a10f5c93-1fee-4fa8-a6ac-ad80e6dd1548", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Applications of Federated Learning for Mobile Edge Computing"], ["subsection", "Cyberattack Detection"]], "content": "Cyberattack detection is one of the most important steps to promptly prevent and mitigate serious consequences of attacks in mobile edge networks. Among different approaches to detect cyberattacks, DL is considered to be the most effective tool to detect a wide range of attacks with high accuracy. In~, the authors show that DL can outperform all conventional ML techniques with very high accuracy in detecting intrusions on three datasets, i.e., KDDcup 1999, NSL-KDD , and UNSW-NB15 . However, the detection accuracy of solutions based on DL depends very much on the available datasets. Specifically, DL algorithm only can outperform other ML techniques when given sufficient data to train. However, this data may be sensitive in nature. Therefore, some FL-based attack detection models for mobile edge networks have been introduced recently to address this problem. \nIn~, the authors propose a cyberattack detection model for an edge network empowered by FL. In this model, each edge node operates as a participant who owns a set of data for intrusion detection. To improve the accuracy in detecting attacks, after training the global model, each participant will send its trained model to the FL server. The server will aggregate all parameters from the participants and send the updated global model back to all the participants as illustrated in Fig.~\\ref{fig:IDS_FL}. In this way, each edge node can learn from other edge nodes without a need of sharing its real data. As a result, this method can not only improve accuracy in detecting attacks, but also enhance the privacy of intrusion data at the edge nodes and reduce traffic load for the whole network. A similar idea is also presented in~ in which IoT gateways operate as FL participants and an IoT security service provider works as a server node to aggregate trained models shared by the participants. The authors in~ show empirically that by using FL, the system can successfully detect 95.6\\% of attacks in approximately 257 ms without raising any false alarm when evaluated in a real-world smart home deployment setting.\n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\columnwidth]{Figs/IDS_FL}\n\t\\caption{\\small FL-based attack detection architecture for IoT edge networks.}\n\t\\label{fig:IDS_FL}\n\\end{figure}\nIn both~ and~, it is assumed that the participants, i.e., edge nodes and IoT gateways, are honest, and they are willing to contribute in training their updated model parameters. However, if some of the participants are malicious, they can make the whole intrusion detection corrupted. Thus, the authors in~ propose to use blockchain technology in managing data shared by the participants. By using the blockchain, all incremental updates to the anomaly detection ML model are stored in the ledger, and thus a malicious participant can be easily identified. Furthermore, based on shared models from honest participants stored in the ledger, the intrusion detection system can easily recover the proper global model if the current global model is poisoned.", "cites": [5998], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of FL-based cyberattack detection in mobile edge networks, integrating different works and highlighting how FL improves privacy and accuracy compared to centralized DL. It makes basic connections between papers and identifies a key limitationparticipant honestythough the critique is not deeply developed. The abstraction is moderate, as it begins to generalize the role of FL and blockchain in securing distributed learning systems."}}
{"id": "9bb6bf60-a8e6-4f3a-928f-802af0dd72af", "title": "Edge Caching and Computation Offloading", "level": "subsection", "subsections": [], "parent_id": "a10f5c93-1fee-4fa8-a6ac-ad80e6dd1548", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Applications of Federated Learning for Mobile Edge Computing"], ["subsection", "Edge Caching and Computation Offloading"]], "content": "To account for the dynamic and time-varying conditions in a MEC system, the authors in  propose the use of DRL with FL to optimize caching and computation offloading decisions in an MEC system. The MEC system consists of a set of user equipments (UEs) covered by base stations. For caching, the DRL agent makes the decision to cache or not to cache the downloaded file, and which local file to replace should caching occur. For computation offloading, the UEs can choose to either offload\ncomputation tasks to the edge node via wireless channels,\nor perform the tasks locally. This caching and offloading decision process is illustrated in Fig. \\ref{fig:caching}. The states of the MEC system include wireless network conditions,\nUE energy consumption, and task queuing states, whereas the reward function is\ndefined as quality of experience (QoE) of the UEs. Given the large state and action space in the MEC environment, a DDQN approach is adopted. To protect the privacy of users, an FL approach is proposed in which training can occur with data remaining on the UEs. In addition, existing FL algorithms, e.g., \\textit{FedAvg} , can also ensure that training is robust to the unbalanced and non-IID data of the UEs. The simulation\nresults show that the DDQN with FL approach achieves similar average\nutilities among UEs as compared to the centralized DDQN\napproach, while consuming less communication resources and\npreserving user privacy. However, the simulations are only performed with $10$ UEs. If the implementation is expanded to target a larger number of heterogeneous UEs, there can be significant delays in the training process especially since the training of a DRL model is computationally intensive. As an extension, transfer learning  can be used to increase the efficiency of training, i.e., training is not initialized from scratch.\nSimilar to , the authors in  propose the use of DRL in optimizing computation offloading decisions in IoT systems. The system model consists of IoT devices and edge nodes. The IoT devices can harvest energy units  from the edge nodes to be stored in the energy queue. In addition, an IoT device also maintains a local task queue with unprocessed and unsuccessfully processed tasks. These tasks can be processed locally or offloaded to the edge nodes for processing, in a First In First Out (FIFO) order . In the DRL problem formulation, the network states are defined to be a function of energy queue length, task execution delay, task handover delay from edge node association, and channel gain between the IoT device and edge nodes. A task can fail to be executed, e.g., when there is insufficient energy units or communication bandwidth for computation offloading. The utility considered is a function of task execution delay, task queuing delay, number of failed tasks and penalty of execution failure. The DRL agent makes the decision to either offload computation to the edge nodes or perform computation locally. To ensure privacy of users, the agent is trained without users having to upload their own data to a centralized server. In each training round, a random set of IoT devices are selected to download the model parameters of the DRL agent from the edge networks. The model parameters are then updated using their own data, e.g., energy resource level, channel gain, and local sensing data. Then, the updated parameters of the DRL agent are sent to the edge nodes for model aggregation. The simulation results show that the FL based approach can achieve same levels of total utility as the centralized DRL approach. This is robust to varying task generation probabilities. In addition, when task generation probabilities are higher, i.e., there are more tasks for computation in the IoT device, the FL based scheme can achieve a lower number of dropped tasks and shorter queuing delay than the centralized DRL scheme. However, the simulation only involves $15$ IoT devices serviced by relatively many edge nodes. To better reflect practical scenarios where fewer edge nodes have to cover several IoT devices, further studies can be conducted on optimizing the edge-IoT collaboration. For example, the limited communication bandwidth can cause significant task handover delay during computation offloading. In addition, with more IoT devices, the DRL training will take a longer time to converge especially since the devices have heterogeneous computation capabilities.\n\\begin{figure}[!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{Figs/caching}\n\t\\caption{\\small FL-based (a) caching and (b) computation offloading.}\n\t\\label{fig:caching}\n\\end{figure}\nInstead of using a DRL approach, the authors in  propose the use of an FL based stacked autoencoder learning model, i.e., FL based proactive content caching scheme (FPCC), to predict content popularity for optimized caching while protecting user privacy. In the system model, each user is equipped with a mobile device that connects to the base station that covers its geographical location. Using a stacked autoencoder learning model, the latent representation of a user's information, e.g., location, and file rating, i.e., content request history, is learned. Then, a similarity matrix between the user and its historically requested files is obtained in which each element of the matrix represents the distance between the user and the file. Based on this similarity matrix, the $K$ nearest neighbours of each user are determined, and the similarity between the user's historical watch list and the neighbours' are computed. An aggregation approach is then used to predict the most popular files for caching, i.e., files with highest similarity scores across all users. Being the most popular files across users that are most frequently retrieved, the cached files need not be re-downloaded from its source server everytime it is demanded. To protect the privacy of users, FL is adopted to learn the parameters of the stacked autoencoder without the user having to reveal its personal information or its content request history to the FL server. In each training round, the user first downloads a global model from the FL server. Then, the model is trained and updated using their local data. The updated models are subsequently uploaded to the FL server and aggregated using the \\textit{FedAvg} algorithm. The simulation results show that the proposed FPCC scheme could achieve the highest cache efficiency, i.e, the ratio of cached files matching user requests, as compared to other caching methods such as the Thompson sampling methods . In addition, privacy of the user is preserved.\nThe authors in  introduce a privacy-aware service placement scheme to deploy user-preferred services on edge servers with consideration for resource constraints in the edge cloud. The system model consists of a mobile edge cloud serving various mobile devices. The user's preference model is first built based on information such as number of times of requests for a service, and other user context information, e.g., ages and locations. However, since this can involve sensitive personal information, an FL based approach is proposed to train the preference model while keeping users' data on their personal devices.  Then, an optimization problem is formulated in which the objective is to maximize quantity of services demanded from the edge based on user preferences, subject to constraints of storage capacity, computation capability, uplink and downloading bandwidth. The optimization problem is then solved using a greedy algorithm, i.e., the service which most improves the objective function is added till resource constraints are met. The simulation results show that the proposed scheme can outperform the popular service placement scheme, i.e., where only the most popular services are placed on the edge cloud, in terms of number of requests processed on edge clouds since it also considers the aforementioned resource constraints in maximizing quantity of services.", "cites": [5977, 582], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple cited papers to present a cohesive discussion on how FL can be used in edge caching and computation offloading. It critically evaluates limitations, such as scalability and training delays, and suggests potential solutions like transfer learning. While it identifies some patterns in the use of FL for privacy preservation, it does not generalize to a meta-level framework, keeping abstraction moderate."}}
{"id": "bdbb9fa8-26c1-4b46-8920-41f52291e380", "title": "Base Station Association", "level": "subsection", "subsections": [], "parent_id": "a10f5c93-1fee-4fa8-a6ac-ad80e6dd1548", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Applications of Federated Learning for Mobile Edge Computing"], ["subsection", "Base Station Association"]], "content": "The authors in  propose an FL based deep echo state networks (ESNs) approach to minimize breaks in presence (BIPs)  for users of virtual reality (VR) applications. A BIP event can be a result of delay in information transmission which can be caused when the user's body movements obstruct the wireless link. BIPs cause the user to be aware that they are in a virtual environment, thus reducing their quality of experience. As such, a user association policy has to be designed such that BIPs are minimized. The system model consists of base stations that cover a set of VR users. The base stations receive uploaded tracking information from each associated user, e.g., physical location and orientation, while the users download VR videos for their use in the VR application. For data transmission, the VR users have to associate with one of the base stations. As such, a minimization problem is formulated where BIPs are minimized with respect to expected locations and orientations of the VR user. To derive a prediction of user locations and orientations, the base station has to rely on the historical information of users. However, the historical information stored at each base station only collects partial data from each user, i.e., a user connects to multiple base stations and its data is distributed across them. As such, an FL based approach is implemented whereby each base station first trains a local model using its partial data. Then, the local models are aggregated to form a global model capable of generalization, i.e., comprehensively predicting a user's mobility and orientations. The simulation results show that the federated ESN algorithm can achieve lower BIPs experienced by users as compared to the centralized ESN algorithm proposed in , since a centralized approach only makes partial prediction with the incomplete data from sole base stations, whereas the federated ESN approach can make predictions based on a model learned collaboratively from more complete data.\nFollowing the ubiquity of IoT devices, the traditional cloud-based approach may no longer be sufficient to cater to dense cellular networks. As computation and storage moves to the edge of networks, the association of users to base stations are increasingly important to facilitate efficient ML model training among the end users. To this end, the authors in  consider solving the problem of cell association in dense wireless networks with a collaborative learning approach. \nIn the system model, the base stations cover a set of users in an LTE cellular system. In a cellular system, users are likely to face similar channel conditions as their neighbors and thus can benefit from learning from their neighbours that are already associated with base stations. As such, the cell association problem is formulated as a mean-field game (MFG) with imitation  in which each user maximizes its own throughput while minimizing the cost of imitation.\nThe MFG is further\nreduced into a single-user Markov decision process that is then solved\nby a neural Q-learning algorithm. In\nmost other proposed solution for cell association, it is assumed that\nall information is known to the base stations and users. However, given\nprivacy concerns, the assumption of information sharing may\nnot be practical. As such, a collaborative learning approach can be considered where only the outcome of the learning algorithm is exchanged\nduring the learning process whereas usage data\nis kept locally in each user's own device. The simulation results show that imitating users can attain higher utility within a shorter\ntraining duration as compared to non-imitating users.", "cites": [5996, 5971], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the two cited papers by connecting FL-based ESN for VR and collaborative AI for cell association, highlighting how both aim to improve user experience through distributed learning. It provides a critical comparison between federated and centralized approaches, noting the privacy and performance limitations of the latter. While it offers some abstraction by framing FL as a solution for distributed data and privacy, it does not fully generalize to broader principles or frameworks across the field."}}
{"id": "090fafc1-63ae-4a0f-8017-679d11acea66", "title": "Vehicular Networks", "level": "subsection", "subsections": [], "parent_id": "a10f5c93-1fee-4fa8-a6ac-ad80e6dd1548", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Applications of Federated Learning for Mobile Edge Computing"], ["subsection", "Vehicular Networks"]], "content": "Ultra reliable low latency communication (URLLC) in vehicular networks\nis an essential prerequisite towards developing an intelligent transport system.\nHowever, existing radio resource management techniques do\nnot account for rare events such as large queue lengths at\nthe tail-end distribution. To model the occurrence of such low\nprobability events, the authors in  propose the use of extreme value theory (EVT) . The approach requires\nsufficient samples of queue state information (QSI) and data\nexchange among vehicles. As such, an FL approach is proposed in which\nvehicular users (VUEs) train the learning model with data kept locally and\nupload only their updated model parameters to the roadside units (RSU). The\nRSU then averages out the model parameters and return an updated\nglobal model to the VUEs. In a synchronous approach, all\nVUEs upload their models at the end of a prespecified interval. However, the simultaneous uploading by multiple vehicles can lead to delays in communication.\nIn contrast for an asynchronous approach, each VUE only\nevaluates and uploads their model parameters after a predefined number\nof QSI samples are collected. The global model is also updated whenever a local update is received, thus reducing communication delays. To further reduce overhead,\nLyapunov optimization  for power allocation is also utilized. The\nsimulation results show that under this framework, there is\na reduction of the number of vehicles experiencing large queue lengths\nwhereas FL can ensure minimal data exchange relative to a\ncentralized approach.\nApart from QSI, the vehicles in vehicular networks are also exposed to a wealth of useful captured images that can be adopted to build better inference models, e.g., for traffic optimization. However, these images are sensitive in nature since they can give away the location information of vehicular clients. As such, an FL approach can be used to facilitate collaborative ML while ensuring privacy preservation. However, the images captured by vehicles are often varying in quality due to motion blurs. In addition, another source of heterogeneity is the difference in computing capabilities of vehicles. Given the information asymmetry involved, the authors in  propose a multi-dimensional contract design in which the FL server designs contract bundles comprising varying levels of data quality, compute resources, and contractual payoffs. Then, the vehicular client chooses the contract bundle that maximizes its utility, in accordance to its hidden type. Similar to the results in , the simulation results show that the FL server derives greatest utility under the proposed contract theoretic approach, in contrast to the linear pricing or Stackelberg game approach.\nThe authors in  propose a federated energy demand learning (FEDL) approach to manage energy resources in charging stations (CSs) for electric vehicles (EVs). When a large number of EVs congregate at a CS, this can lead to energy transfer congestion. To resolve this, energy is supplied from the power grids and reserved in advance to meet the real-time demands from the EVs , rather than having the CSs request for energy from the power grid only upon receiving charging requests. As such, there is a need to forecast energy demand for EV networks using historical charging data. However, this data is usually stored separately at each of the CS that the EVs utilize and is private in nature. As such, an FEDL approach is adopted in which each CS trains the demand prediction model on its own dataset before sending only the gradient information to the charging station provider (CSP). Then, the gradient information from the CS is aggregated for global model training. To further improve model accuracy, the CSs are clustered using the constrained K-means algorithm  based on their physical locations. The clustering-based FEDL reduces the cost of biased prediction . The simulation results show that the root mean squared error of a clustered FEDL model is lower than conventional ML algorithms, e.g., multi-layer perceptron regressor . However, the privacy of user data is still not protected by this approach, since user data is stored in each of the CS. As an extension, the user data can possibly be stored in each EVs separately, and model training can be conducted in the EVs rather than the CSs. This can allow more user features to be considered to enhance the accuracy of EDL, e.g., user consumption habits.\n\\textit{Summary:} In this section, we discuss that FL can also be used for mobile edge network optimization. In particular, DL and DRL approaches are suitable for modelling the dynamic environment of increasingly complex edge networks but require sufficient data for training. With FL, model training can be carried out while preserving the privacy of users. A summary of the approaches are presented in Table \\ref{tab:application}.", "cites": [5997, 5990, 5995], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from three different papers, connecting FL applications in vehicular networks for reliability, privacy, and energy demand forecasting. It provides some critical analysis, such as noting that privacy is still not fully protected in one approach and suggesting improvements. The section also begins to abstract by identifying broader themes like data privacy, heterogeneity, and communication efficiency in FL for vehicular contexts, but does not reach a deep meta-level insight."}}
{"id": "297974f4-53c5-47b3-9406-058a62082d77", "title": "Challenges and Future Research Directions", "level": "section", "subsections": [], "parent_id": "2a7871ed-4981-4f35-ad2d-8f4aca73a40f", "prefix_titles": [["title", "Federated Learning in Mobile Edge Networks: A Comprehensive Survey"], ["section", "Challenges and Future Research Directions"]], "content": "\\label{sec:challenges_open_issues}\nApart from the aforementioned issues, there are still challenges new research directions in deploying FL at scale to be discussed as follows.\n\\begin{itemize}\n\\item \\textit{Dropped participants:} The approaches discussed in Section \\ref{sec:resource}, e.g., , , and , propose new algorithms for participant selection and resource allocation to address the training bottleneck and resource heterogeneity. In these approaches, the wireless connections of participants are assumed to be always available. However, in practice, participating mobile devices may go offline and can drop out from the FL system due to connectivity or energy constraints. A large number of dropped devices from the training participation can significantly degrade the performance , e.g., accuracy and convergence speed, of the FL system. New FL algorithms need to be robust to device drop out in the networks and anticipate the scenarios in which only a small number of participants are left connected to participate in a training round. One potential solution is that the FL model owner provides free dedicated/special connection, e.g., cellular connections, as an incentive to the participants to avoid drop out. \n\\item \\textit{Privacy concerns:} FL is able to protect the privacy of each participants since the model training may be conducted locally, with just the model parameters exchanged with the FL server. However, as specified in ,~, and , communicating the model updates during the training process can still reveal sensitive information to an adversary or a third-party. The current approaches propose security solutions such as DP, e.g., , , and , and collaborative training, e.g.,  and~. However, the adoption of these approaches sacrifices the performance, i.e., model accuracy. They also require significant computation on participating mobile devices. Thus, the tradeoff between privacy guarantee and system performance has to be well balanced when implementing the FL system. \n\\item \\textit{Unlabeled data:} It is important to note that the approaches reviewed in the survey are proposed for supervised learning tasks. This means that the approaches assume that labels exist for all of the data in the federated network. However, in practice, the data generated in the network may be unlabeled or mislabeled . This poses a big challenge to the server to find participants with appropriate data for model training. Tackling this challenge may require the challenges of\nscalability, heterogeneity, and privacy in the FL systems to be addressed. One possible solution is to enable mobile devices to construct their labeled data by learning the ``labeled data'' from each other. Emerging studies have also considered the use of semi-supervised learning inspired techniques . \n\\item \\textit{Interference among mobile devices:} The existing resource allocation approaches, e.g.,   and , address the participant selection based on the resource states of their mobile devices. In fact, these mobile devices may be geographically close to each other, i.e., in the same cell. This introduces an interference issue when they update local models to the server. As such, channel allocation policies may need to be combined with the resource allocation approaches to address the interference issue. While studies in , , and  consider multi-access schemes and over-the-air computation, it remains to be seen if such approaches are scalable, i.e., able to support a large federation of many participants. To this end, data driven learning based solutions, e.g., federated DRL, can be considered to model the dynamic environment of mobile edge networks and make optimized decisions.\n\\item \\textit{Communication security:} The privacy and security threats studied in Section \\ref{sec:securityissues} revolve mainly around data-related compromises, e.g., data and model poisoning. Due to the exposed nature of the wireless medium, FL is also vulnerable to communication security issues such as Distributed Denial-of-Service (DoS)  and jamming attacks . In particular, for jamming attacks, an attacker can transmit radio frequency jamming signals with high power to disrupt or cause interference to the communications between the mobile devices and the server. Such an attack can cause errors to the model uploads/downloads and consequently degrade the performance, i.e., accuracy, of the FL systems. Anti-jamming schemes  such as frequency hopping, e.g., sending one more copy of the model update over different frequencies, can be adopted to address the issue.\n\\item \\textit{Asynchronous FL:} In synchronous FL, each training round only progresses as quickly as the slowest device, i.e., the FL system is susceptible to the straggler effect. As such, asynchronous FL has been proposed as a solution in  and . In addition, asynchronous FL also allows participants to join the FL training halfway even while a training round is in progress. This is more reflective of practical FL settings and can be an important contributing factor towards ensuring the scalability of FL. However, synchronous FL remains to be the most common approach used due to convergence guarantees . Given the many advantages of asynchronous FL, new asynchronous algorithms should be explored. In particular, for future proposed algorithms, the convergence guarantee in a non-IID setting for non-convex loss functions need to be considered. An approach to be considered is the possibile inclusion of \\textit{backup} workers following the studies of .\n\\item \\textit{Comparisons with other distributed learning methods:} Following the increased scrutiny on data privacy, there has been a growing effort on developing new privacy preserving distributed learning algorithms. One study proposes \\textit{split learning} , which also enables collaborative ML without requiring the exchange of raw data with an external server. In split learning, each participant first trains the neural network up to a cut layer. Then, the outputs from training are transmitted to an external server that completes the other layers of training. The resultant gradients are then back propagated up to the cut layer, and eventually returned to the participants to complete the local training. In contrast, FL typically involves the communication of full model parameters. The authors in  conduct an empirical comparison between the communication efficiencies of split learning and FL. The simulation results show that split learning performs well when the model size involved is larger, or when there are more participants involved, since the participants do not have to transmit the weights to an aggregating server. However, FL is much easier to implement since the participants and FL server are running the same global model, i.e., the FL server is just in charge of aggregation and thus FL can work with one of the participants serving as the master node. As such, more research efforts can be directed towards guiding system administrators to make an informed decision as to which scenario warrants the use of either learning methods.\n\\item \\textit{Further studies on learning convergence:} One of the essential considerations of FL is the\nconvergence of the algorithm. FL finds weights to minimize the global model aggregation. This is actually a distributed optimization problem, and the convergence is not always guaranteed. Theoretical analysis and evaluations\non the convergence bounds of the gradient descent based\nFL for convex and non-convex loss functions are important research directions. While existing studies have covered this topic, many of the guarantees are limited to restrictions, e.g., convexity of the loss function.\n\\item \\textit{Usage of tools to quantify statistical heterogeneity:} Mobile devices typically generate and collect data in a non-IID manner across the network. Moreover, the number of data samples among the mobile devices may vary significantly. To improve the convergence of FL algorithm, the statistical heterogeneity of the data needs to be quantified. Recent works, e.g., , have developed tools for quantifying statistical heterogeneity through metrics such as local dissimilarity. However, these metrics cannot be easily calculated over the federated network before training begins. The importance of these metrics motivates future directions such as the development of efficient algorithms to quickly determine the level of heterogeneity in federated networks.\n\\item \\textit{Combined algorithms for communication reduction:} Currently, there are three common techniques of communication reduction in FL as discussed in Section \\ref{sec: communication}. It is important to study how these techniques can be combined with each other to improve the performance further. For example, the model compression technique can be combined with the edge server-assisted FL. The combination is able to significantly reduce the size of model updates, as well as the instances of communication with the FL server. However, the feasibility of this combination has not been explored. In addition, the tradeoff between accuracy and communication overhead for the combination technique needs to be further evaluated. In particular, for simulation results we discuss in Section \\ref{sec: communication}, the accuracy-communication cost reduction tradeoff is difficult to manage since it varies for different settings, e.g., data distribution, quantity, number of edge servers, and number of participants.\n\\item \\textit{Cooperative mobile crowd ML:} In the existing approaches, mobile devices need to communicate with the server directly and this may increase the energy consumption. In fact, mobile devices nearby can be grouped in a cluster, and the model downloading/uploading between the server and the mobile devices can be facilitated by a ``cluster head'' that serves as a relay node . The model exchange between the mobile devices and the cluster head can then be done in Device-to-Device (D2D) connections. Such a model can improve the energy efficiency significantly. Efficient coordination schemes for the cluster head can thus be designed to further improve the energy efficiency of a FL system. \n\\item \\textit{Applications of FL:} Given the advantages of guaranteeing data privacy, FL has an increasingly important role to play in many\napplications, e.g., healthcare, finance and transport systems. For most current studies on FL applications, the focus mainly lies in the federated training of the learning model, with the implementation challenges neglected.\nFor future studies on the applications of FL, besides\na need to consider the aforementioned issues in the survey,\ni.e., communication costs, resource allocation, and privacy\nand security, there is also a need to consider the specific issues related to the system model in which FL will be adopted in. For example, for delay critical applications, e.g, in vehicular networks, there will be more emphasis on training efficiency and less on energy expense.\n\\end{itemize}", "cites": [590, 619, 8967, 3477, 5985, 5999, 582, 6002, 2676, 5989, 7608, 1325, 6000, 7727, 6001], "cite_extract_rate": 0.5357142857142857, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers to identify key challenges in FL for mobile edge networks, such as dropped participants, privacy, and communication security. It provides some critical evaluation of trade-offs in privacy-preserving methods and limitations of current asynchronous FL approaches. The section abstracts these challenges into broader themes like scalability and practical implementation considerations, but its insights remain grounded in the surveyed literature without offering highly novel or meta-level frameworks."}}
