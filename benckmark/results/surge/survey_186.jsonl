{"id": "3a4c67cf-71b4-453b-9217-81d36d637f56", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Introduction"]], "content": "Graph data is ubiquitous in our daily life. We can use graphs to model the complex relationships and dependencies between entities ranging from small molecules in proteins and particles in physical simulations to large national-wide power grids and global airlines. Therefore, machine learning on graphs has long been an important research direction for both academics and industry~. In particular, network embedding~ and graph neural networks (GNNs)~ have drawn increasing attention in the last decade. They are successfully applied to recommendation systems~, fraud detection~, bioinformatics~, physical simulation~, traffic forecasting~, knowledge representation~, drug re-purposing~ and pandemic prediction~ for Covid-19.\nDespite the popularity of graph machine learning algorithms, the existing literature heavily relies on manual hyper-parameter or architecture design\nto achieve the best performance,\nresulting in costly human efforts when a vast number of models emerge for various graph tasks. \nTake GNNs as an example. At least one hundred new general-purpose architectures have been published in top-tier machine learning and data mining conferences in the year 2020 alone, not to mention cross-disciplinary researches of task-specific designs. More and more human efforts are inevitably needed if we stick to the manual try-and-error paradigm in designing the optimal algorithms for targeted tasks.\nOn the other hand, automated machine learning (AutoML) has been extensively studied to reduce human efforts in developing and deploying machine learning models~. Complete AutoML pipelines have the potential to automate every step of machine learning, including auto data collection and cleaning, auto feature engineering, and auto model selection and optimization, etc. Due to the popularity of deep learning models, hyper-parameter optimization (HPO)~ and neural architecture search (NAS)~ are most widely studied. AutoML has achieved or surpassed human-level performance~ with little human guidance in areas such as computer vision~.\n\\textbf{Automated machine learning on graphs}, combining the advantages of AutoML and graph machine learning, naturally serves as a promising research direction to further boost the model performance, which has attracted an increasing number of interests from the community.\n\\iffalse\nA critical next step is to combine the strength of graph machine learning and AutoML, leading towards \\textbf{automated machine learning on graphs}. However, this problem is highly non-trivial due to the following challenges.\n\\begin{itemize}\n\\item \\textbf{The uniqueness of graph machine learning}: Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space~. Thus, graph machine learning usually has unique architectures and designs. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs~.\n\\item \\textbf{Complexity and diversity of graph tasks}: As aforementioned, graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints~. How to impose proper \\emph{inductive bias} and integrate \\emph{domain knowledge} into a graph AutoML method is indispensable.\n\\item \\textbf{Scalability}: Many real graphs such as social networks or the Web are incredibly large-scale with billions of nodes and edges~. Besides, the nodes in the graph are interconnected and cannot be treated as independent samples. Designing scalable AutoML algorithms for graphs poses significant challenges since both graph machine learning and AutoML are already notorious for being compute-intensive.\n\\end{itemize}\nWith both opportunities and challenges, it is high time to review and carry forward the studies of automated machine learning on graphs. \n\\fi\nIn this paper, we provide a comprehensive and systematic review of automated machine learning on graphs, to the best of our knowledge, for the first time. Specifically, we focus on two major topics: HPO and NAS of graph machine learning. For HPO, we focus on how to develop scalable methods. For NAS, we follow the literature and compare different methods from search spaces, search strategies, and performance estimation strategies. How different methods tackle the challenges of AutoML on graphs are discussed along the way. \nThen, we review libraries related to automated graph machine learning and discuss AutoGL, the first dedicated framework and open-source library for automated machine learning on graphs. We highlight the design principles of AutoGL and briefly introduce its usages, which are all specially designed for AutoML on graphs. We believe our review in this paper will significantly facilitate and further promote the studies and applications of automated machine learning on graphs.\nThe rest of the paper is organized as follows. In Section~\\ref{sec:review}, we point out the challenges for automated graph machine learning and briefly introduce basic formulations of machine learning on graphs and AutoML. We comprehensively review HPO on graph machine learning in Section~\\ref{sec:hpo} and NAS for graph machine learning in Section~\\ref{sec:nas}, followed by our overview and discussions of related libraries in Section~\\ref{sec:tool}. Lastly, we outline future research opportunities in Section~\\ref{sec:future}.", "cites": [261, 6504, 273, 870, 219, 871, 169, 553, 544, 3344, 6502, 872, 2787, 217, 6503, 7007, 9030, 23, 25, 6500, 684, 550, 8453, 215, 6501, 210], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes key concepts from multiple papers on graph machine learning and AutoML, connecting them to highlight the need for automation in graph tasks. It provides some abstraction by discussing common challenges such as scalability and inductive bias. However, it lacks deep critical analysis of individual papers' strengths and limitations, focusing more on general motivations and applications."}}
{"id": "8e2e3ff4-ae76-4091-99c0-5d65c34567af", "title": "Automated Machine Learning on Graphs", "level": "section", "subsections": ["c5e577c8-c1aa-493b-b3e6-51b83bbe92d6", "059749b2-8704-41a6-a6d3-7269d2230c82"], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Automated Machine Learning on Graphs"]], "content": "\\label{sec:review}\nAutomated machine learning on graphs, which non-trivially combines the strength of AutoML and graph machine learning, faces the following challenges.\n\\begin{itemize}\n\\item \\textbf{The uniqueness of graph machine learning:} Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space~. Thus, graph machine learning usually has unique architectures and designs. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs~.\n\\item \\textbf{Complexity and diversity of graph tasks:} As aforementioned, graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints~. How to impose proper \\emph{inductive bias} and integrate \\emph{domain knowledge} into a graph AutoML method is indispensable.\n\\item \\textbf{Scalability:} Many real graphs such as social networks or the Web are incredibly large-scale with billions of nodes and edges~. Besides, the nodes in the graph are interconnected and cannot be treated as independent samples. Designing scalable AutoML algorithms for graphs poses significant challenges since both graph machine learning and AutoML are already notorious for being compute-intensive.\n\\end{itemize}\nApproaches with HPO or NAS for graph machine learning reviewed in later sections target handling at least one of these three challenges. We briefly introduce basic problem formulations before moving to the next section.", "cites": [2787, 210], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the main challenges in applying AutoML to graph data, drawing on concepts from the cited papers such as non-Euclidean structures and the diversity of graph tasks. While it integrates ideas from both papers to form a coherent narrative, it does not deeply compare or critique specific approaches. It generalizes well by identifying overarching principles like inductive bias and scalability issues."}}
{"id": "c5e577c8-c1aa-493b-b3e6-51b83bbe92d6", "title": "Machine Learning on Graphs", "level": "subsection", "subsections": [], "parent_id": "8e2e3ff4-ae76-4091-99c0-5d65c34567af", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Automated Machine Learning on Graphs"], ["subsection", "Machine Learning on Graphs"]], "content": "\\label{sec:mlgform}\nConsider a graph $\\mathcal{G} = \\left( \\mathcal{V},\\mathcal{E}\\right)$ where $\\mathcal{V} = \\left\\{v_1,v_2,...,v_{\\left|\\mathcal{V}\\right|}\\right\\}$ is a set of nodes and $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ is a set of edges. The neighborhood of node $v_i$ is denoted as $\\mathcal{N}(i)=  \\left\\{v_j: (v_i,v_j) \\in \\mathcal{E}\\right\\}$. The nodes can also have features denoted as $\\mathbf{F}\\in \\mathbb{R}^{\\left|\\mathcal{V} \\right| \\times f}$, where $f$ is the number of features. We use bold uppercases (e.g., $\\mathbf{X}$) and bold lowercases (e.g., $\\mathbf{x}$) to represent matrices and vectors, respectively.\nMost tasks of graph machine learning can be divided into the following two categories: \n\\begin{itemize}[leftmargin=0.6cm]\n\t\\item Node-level tasks: the tasks are associated with individual nodes or pairs of nodes. Typical examples include node classification and link prediction. \n\t\\item Graph-level tasks: the tasks are associated with the whole graph, such as graph classification and graph generation.\t\n\\end{itemize}\nFor node-level tasks, graph machine learning models usually learn a node representation $\\mathbf{H} \\in \\mathbb{R}^{\\left|\\mathcal{V}\\right| \\times d}$ and then adopt a classifier or predictor on the node representation to solve the task. For graph-level tasks, a representation for the whole graph is learned and fed into a classifier/predictor.\nGNNs are the current state-of-the-art in learning node and graph representations.\nThe message-passing framework of GNNs~ is formulated as follows.\n\\begin{gather}\\label{eq:mpnn}\n    \\mathbf{m}^{(l)}_i = \\text{AGG}^{(l)}\\left(\\left\\{a_{ij}^{(l)} \\mathbf{W}^{(l)} \\mathbf{h}^{(l)}_i, \\forall j \\in \\mathcal{N}(i) \\right\\} \\right) \\\\\n    \\mathbf{h}^{(l+1)}_i = \\sigma\\left(\\text{COMBINE}^{(l)}\\left[ \\mathbf{m}^{(l)}_i, \\mathbf{h}^{(l)}_i\\right] \\right),\n\\end{gather}\nwhere $\\mathbf{h}^{(l)}_i$ denotes the node representation of node $v_i$ in the $l^{th}$ layer, $\\mathbf{m}^{(l)}$ is the message for node $v_i$, $\\text{AGG}^{(l)}(\\cdot)$ is the aggregation function, $a_{ij}^{(l)}$ denotes the weights from node $v_j$ to node $v_i$, $\\text{COMBINE}^{(l)}(\\cdot)$ is the combining function, $\\mathbf{W}^{(l)}$ are learnable weights, and $\\sigma(\\cdot)$ is an activation function. The node representation is usually initialized as node features $\\mathbf{H}^{(0)} = \\mathbf{F}$, and the final representation is obtained after $L$ message-passing layers $\\mathbf{H} = \\mathbf{H}^{(L)}$.\nFor the graph-level representation, pooling methods (also called readout) are applied to the node representations\n\\begin{equation}\\label{eq:pool}\n\t\\mathbf{h}_{\\mathcal{G}} = \\text{POOL}\\left(\\mathbf{H}\\right),\n\\end{equation}\ni.e., $\\mathbf{h}_{\\mathcal{G}}$ is the representation of $\\mathcal{G}$.", "cites": [216], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear and factual description of graph machine learning concepts, including the message-passing framework in GNNs and the distinction between node-level and graph-level tasks. It cites one paper to support the formulation of message passing, but it does not synthesize ideas across multiple sources, nor does it offer critical evaluation or comparative analysis. The abstraction is limited to a general overview of GNN operations without deeper insights into broader trends or principles."}}
{"id": "fc1b9f9a-bd06-49bd-9aa7-289c1b6c5f0b", "title": "HPO for Graph Machine Learning", "level": "section", "subsections": [], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "HPO for Graph Machine Learning"]], "content": "\\label{sec:hpo}\nIn this section, we review HPO for machine learning on graphs. The main challenge here is scalability, i.e., a real graph can have billions of nodes and edges, and each trial on the graph is computationally expensive. Next, we elaborate on how different methods tackle the efficiency challenge. Notice that we omit some straightforward HPO methods such as random search and grid search~. \nAutoNE~ first tackles the efficiency problem of HPO on graphs by proposing a transfer paradigm that samples subgraphs as proxies for the large graph, which is similar in principle to sampling instances in previous HPO methods. Specifically, AutoNE has three modules: the sampling module, the signature extraction module, and the meta-learning module. In the sampling module, multiple representative subgraphs are sampled from the large graph using a multi-start random walk strategy. Then, AutoNE conducts HPO on the sampled subgraphs using Bayesian optimization~ and learns representations of subgraphs using the signature extraction module. Finally, the meta-learning module extracts meta-knowledge from HPO results and representations of subgraphs. AutoNE fine-tunes hyper-parameters on the large graph using the meta-knowledge. In this way, AutoNE achieves satisfactory results while maintaining scalability since multiple HPO trials on the sampled subgraphs and a few HPO trails on the large graph are properly integrated.\nJITuNE~ proposes to replace the sampling process of AutoNE with graph coarsening to generate a hierarchical graph synopsis. A similarity measurement module is proposed to ensure that the coarsened graph shares sufficient similarity with the large graph. Compared with sampling, such graph synopsis can better preserve graph structural information. Therefore, JITuNE argues that the best hyper-parameters in the graph synopsis can be directly transferred to the large graph. Besides, since the graph synopsis is generated in a hierarchy, the granularity can be more easily adjusted to meet the time constraints of downstream tasks. \nHESGA~ proposes another strategy to improve efficiency using a hierarchical evaluation strategy together with evolutionary algorithms. Specifically, HESGA proposes to evaluate the potential of hyper-parameters by interrupting training after a few epochs and calculating the performance gap with respect to the initial performance with random model weights. This gap is used as a fast score to filter out unpromising hyper-parameters. Then, the standard full evaluation, i.e., training until convergence, is adopted as the final assessor to select the best hyper-parameters to be stored in the population of the evolutionary algorithm.\nBesides efficiency, AutoGM~ focuses on proposing a unified framework for various graph machine learning algorithms. Specifically, AutoGM finds that many popular GNNs can be characterized in a framework similar to Eq.~\\eqref{eq:mpnn} with five hyper-parameters: the number of message-passing neighbors, the number of message-passing steps, the aggregation function, the dimensionality, and the non-linearity. AutoGM adopts Bayesian optimization to optimize these hyper-parameters.", "cites": [6506, 6505, 6501, 9031], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key HPO approaches for graph machine learning, integrating the methods from AutoNE, JITuNE, HESGA, and AutoGM into a coherent discussion about scalability and efficiency. It provides an analytical view by contrasting different strategies (e.g., sampling vs. coarsening) and explaining their design choices. While it includes some critical observations, such as the limitations of sampling in preserving structural information, it does not extensively evaluate or critique the effectiveness of these methods beyond their design, leaving room for deeper analysis."}}
{"id": "386b63ce-ff51-405a-91a5-7835886c11ca", "title": "NAS for Graph Machine Learning", "level": "section", "subsections": ["802e04d8-1c86-40f3-9bc7-cdd38ef690fd", "bb56a929-8f09-47d4-b687-514bf714237d", "7d794060-fefb-4bc6-99c3-f6e1ab43fee6", "099a53a1-e27f-4422-9846-b206b28721ae"], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"]], "content": "\\label{sec:nas}\n\\begin{table*}[ht]\n\t\\centering\n\t\\begin{scriptsize}\n\t\t\\begin{tabular}{c|P{0.3cm}P{0.3cm}P{0.4cm}P{0.15cm}P{0.55cm}|P{0.23cm}P{0.43cm}|c|c|c} \\toprule\n\t\t\t\\multirow{2}{*}{Method}  & \\multicolumn{5}{c|}{Search space} & \\multicolumn{2}{c|}{Tasks} & \\multirow{2}{*}{Search Strategy} & Performance  &\\multirow{2}{*}{Other Characteristics}  \\\\\n\t\t\t                                             & Micro & Macro &Pooling& HP    & Layers & Node   & Graph &                               &Estimation &- \\\\ \\midrule \n\t\t\tGraphNAS~\\shortcite{ijcai2020-195}           &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& RNN controller + RL           & -         &- \\\\ \n\t\t\tAGNN~\\shortcite{zhou2019auto}                &\\cmark &\\xmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& Self-designed controller + RL & Inherit weights  &- \\\\ \n\t\t\tSNAG~\\shortcite{zhao2020simplifying}         &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& RNN controller + RL           & Inherit weights  &Simplify the micro search space \\\\ \n\t\t\tPDNAS~\\shortcite{zhao2020probabilistic}      &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& Differentiable                & Single-path one-shot  &- \\\\ \n\t\t\tPOSE~\\shortcite{ding2020propagation}         &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& Differentiable                & Single-path one-shot  & Support heterogenous graphs \\\\ \n\t\t\tNAS-GNN~\\shortcite{nunes2020neural}          &\\cmark &\\xmark &\\xmark &\\cmark & Fixed  & \\cmark & \\xmark& Evolutionary algorithm        & -         &- \\\\ \n\t\t\tAutoGraph~\\shortcite{li2020autograph}        &\\cmark &\\cmark &\\xmark &\\xmark & Various& \\cmark & \\xmark& Evolutionary algorithm        & -         &- \\\\ \n\t\t\tGeneticGNN~\\shortcite{shi2020evolutionary}  &\\cmark &\\xmark &\\xmark &\\cmark & Fixed  & \\cmark & \\xmark& Evolutionary algorithm        & -         &- \\\\ \n\t\t\tEGAN~\\shortcite{zhao2021efficient}           &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\cmark& Differentiable                & One-shot  & Sample small graphs for efficiency \\\\ \n\t\t\tNAS-GCN~\\shortcite{jiang2020graph}           &\\cmark &\\cmark &\\cmark &\\xmark & Fixed  & \\xmark & \\cmark& Evolutionary algorithm        & -         & Handle edge features               \\\\ \n\t\t\tLPGNAS~\\shortcite{zhao2020learned}           &\\cmark &\\cmark &\\xmark &\\xmark & Fixed  & \\cmark & \\xmark& Differentiable                & Single-path one-shot  & Search for quantisation options    \\\\ \n\t\t\tYou~\\textit{et al.}~\\shortcite{you2020design}&\\cmark &\\cmark &\\xmark &\\cmark & Various& \\cmark & \\cmark & Random search                & -         & Transfer across datasets and tasks \\\\ \n\t\t\tSAGS~\\shortcite{li2020sgas}                  &\\cmark &\\xmark &\\xmark &\\xmark & Fixed  & \\cmark & \\cmark & Self-designed algorithm      & -         &-    \\\\ \n\t\t\tPeng~\\textit{et al.}~\\shortcite{peng2020learning}&\\cmark &\\xmark &\\xmark &\\xmark & Fixed & \\xmark & \\cmark &CEM-RL~\\shortcite{pourchot2018cemrl}&- & Search spatial-temporal modules \\\\ \n\t\t\tGNAS\\shortcite{cai2021rethinking}            &\\cmark &\\cmark &\\xmark &\\xmark & Various& \\cmark & \\cmark & Differentiable & One-shot & - \\\\\n\t\t\tAutoSTG\\shortcite{pan2021autostg}            &\\xmark &\\cmark &\\xmark &\\xmark & Fixed & \\cmark  & \\xmark & Differentiable    & One-shot+meta learning & Search spatial-temporal modules \\\\\n\t\t\tDSS\\shortcite{li2021one} & \\cmark & \\cmark & \\xmark & \\xmark & Fixed & \\cmark & \\xmark & Differentiable & One-shot & Dynamically update search space  \\\\\n\t\t\tSANE\\shortcite{zhao2021search}   & \\cmark & \\cmark & \\xmark &  \\xmark & Fixed & \\cmark & \\xmark & Differentiable & One-shot & - \\\\\t\n\t\t\tAutoAttend\\shortcite{guan2021autoattend} & \\cmark & \\cmark & \\xmark & \\xmark & Fixed & \\cmark & \\cmark & Evolutionary algorithm & One-shot & Cross-layer attention \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{scriptsize}\n\t\\caption{A summary of different NAS methods for graph machine learnings.}\n\t\\label{tab:graphnas}\n\\end{table*}\nNAS methods can be compared in three aspects~: search space, search strategy, and performance estimation strategy. Next, we review NAS methods for graph machine learning from these three aspects. We mainly review NAS for GNNs fitting Eq.~\\eqref{eq:mpnn} and summarize the characteristics of different methods in Table~\\ref{tab:graphnas}. \n\\begin{table}\n\t\\centering\n\t\\begin{small}\n\t\\begin{tabular}{c|l} \\toprule\n\t\tType        &  Formulation  \\\\ \\midrule\n\t\tCONST       & $a_{ij}^{\\text{const}} = 1$  \\\\\n\t\tGCN         & $a_{ij}^{\\text{gcn}} =  \\frac{1}{\\sqrt{\\left| \\mathcal{N}(i)\\right| \\left| \\mathcal{N}(j) \\right| }} $ \\\\\n\t\tGAT         & $a_{ij}^{\\text{gat}} = \\text{LeakyReLU} \\left( \\text{ATT} \\left(\\mathbf{W}_a\\left[\\mathbf{h}_i, \\mathbf{h}_j\\right]\\right) \\right)$ \\\\\n\t\tSYM-GAT     & $a_{ij}^{\\text{sym}} = a_{ij}^{\\text{gat}}+ a_{ji}^{\\text{gat}}$ \\\\\n\t\tCOS         & $a_{ij}^{\\text{cos}} = \\text{cos}\\left(\\mathbf{W}_a \\mathbf{h}_i, \\mathbf{W}_a \\mathbf{h}_j \\right)$ \\\\\n\t\tLINEAR      & $a_{ij}^{\\text{lin}} = \\text{tanh}\\left(\\text{sum}\\left( \\mathbf{W}_a \\mathbf{h}_i + \\mathbf{W}_a \\mathbf{h}_j \\right) \\right)$ \\\\\n\t\tGENE-LINEAR & $a_{ij}^{\\text{gene}} = \\text{tanh}\\left(\\text{sum}\\left( \\mathbf{W}_a \\mathbf{h}_i + \\mathbf{W}_a \\mathbf{h}_j \\right) \\right)\\mathbf{W}_{a}^\\prime $ \\\\ \\bottomrule\n\t\\end{tabular}\n\t\\end{small}\n\t\\caption{A typical search space of different aggregation weights.}\n\t\\label{tab:weights}\n\\end{table}", "cites": [872], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a comparative overview of NAS methods for graph machine learning by organizing them in a table, but it lacks deeper synthesis or analysis of their relationships and implications. It summarizes key features and strategies without evaluating strengths, weaknesses, or broader trends. The abstraction is limited to basic categorization (e.g., search space types), without higher-level insights into the field's evolution or future directions."}}
{"id": "41db2cf2-3dbe-4386-b117-6f788637ccb5", "title": "Micro Search Space", "level": "subsubsection", "subsections": [], "parent_id": "802e04d8-1c86-40f3-9bc7-cdd38ef690fd", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Space"], ["subsubsection", "Micro Search Space"]], "content": "Following the message-passing framework in Eq.~\\eqref{eq:mpnn}, the micro search space defines how nodes exchange messages with others in each layer. Commonly adopted micro search spaces~ compose the following components:\n\\begin{small}\n\\begin{itemize}[leftmargin = 0.4cm]\n\t\\item Aggregation function $\\text{AGG}(\\cdot)$: SUM, MEAN, MAX, and MLP.\n\t\\item Aggregation weights $a_{ij}$: common choices are listed in Table~\\ref{tab:weights}.\n\t\\item Number of heads when using attentions: 1, 2, 4, 6, 8, 16, etc.\n\t\\item Combining function $\\text{COMBINE}(\\cdot)$: CONCAT, ADD, and MLP.\n\t\\item Dimensionality of $\\mathbf{h}^{l}$: 8, 16, 32, 64, 128, 256, 512, etc.\n\t\\item Non-linear activation function $\\sigma(\\cdot)$: Sigmoid, Tanh, ReLU, Identity,\n\tSoftplus, Leaky ReLU, ReLU6, and ELU.\n\\end{itemize}\n\\end{small}\nHowever, directly searching all these components results in thousands of possible choices in a single message-passing layer. Thus, it may be beneficial to prune the space to focus on a few crucial components depending on applications and domain knowledge~.", "cites": [6507, 6508], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive list of components typically included in the micro search space for NAS in graph machine learning, drawing from the cited papers. However, it lacks synthesis of ideas across the sources and does not offer a critical evaluation of their approaches or limitations. There is minimal abstraction beyond the specific components listed, limiting the depth of insight."}}
{"id": "11cc6d7b-3b97-40b8-9a89-52848e656fb4", "title": "Macro Search Space", "level": "subsubsection", "subsections": [], "parent_id": "802e04d8-1c86-40f3-9bc7-cdd38ef690fd", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Space"], ["subsubsection", "Macro Search Space"]], "content": "Similar to residual connections and dense connections in CNNs, node representations in one layer of GNNs do not necessarily solely depend on the immediate previous layer~. These connectivity patterns between layers form the macro search space. Formally, such designs are formulated as\n\\begin{equation}\n\t\\mathbf{H}^{(l)} = \\sum \\nolimits_{j <l} \\mathcal{F}_{jl} \\left(\\mathbf{H}^{(j)}\\right),\n\\end{equation}\nwhere $\\mathcal{F}_{jl}(\\cdot)$ can be the message-passing layer in Eq.~\\eqref{eq:mpnn}, ZERO (i.e., not connecting), IDENTITY (e.g., residual connections), or an MLP. Since the dimensionality of $\\mathbf{H}^{(j)}$ can vary, IDENTITY can only be adopted if the dimensionality of each layer matches.", "cites": [4005, 6509], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the macro search space in NAS for GNNs, drawing on concepts from the cited papers such as residual/dense connections and layer-wise aggregation. It synthesizes ideas from different domains (e.g., CNNs and GNNs) and formulates a general equation to represent connectivity patterns. However, it lacks deeper critical evaluation of the cited works' limitations or trade-offs and could offer more abstract insights into the implications of macro search space design in automated graph learning."}}
{"id": "02307698-489d-44e4-aec9-1830ed0df106", "title": "Pooling Methods", "level": "subsubsection", "subsections": [], "parent_id": "802e04d8-1c86-40f3-9bc7-cdd38ef690fd", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Space"], ["subsubsection", "Pooling Methods"]], "content": "In order to handle graph-level tasks, information from all the nodes is aggregated to form graph-level representations using the pooling operation in Eq~\\eqref{eq:pool}.  propose a pooling search space including row- or column-wise sum, mean, or maximum, attention pooling, attention sum, and flatten. More advanced methods such as hierarchical pooling~ could also be added to the search space with careful designs.", "cites": [224, 6510], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of pooling methods in the context of NAS for graph machine learning but lacks meaningful synthesis of the cited papers. It mentions the existence of advanced methods like hierarchical pooling without elaborating on their implications or how they relate to the broader NAS framework. The analysis is minimal, and no deeper evaluation or generalization is offered."}}
{"id": "8963c543-a9d0-4acf-9c0f-590fb909538a", "title": "Hyper-parameters", "level": "subsubsection", "subsections": [], "parent_id": "802e04d8-1c86-40f3-9bc7-cdd38ef690fd", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Space"], ["subsubsection", "Hyper-parameters"]], "content": "Besides architectures, other training hyper-parameters can be incorporated into the search space, i.e., similar to jointly conducting NAS and HPO. Typical hyper-parameters include the learning rate, the number of epochs, the batch size, the optimizer, the dropout rate, and the regularization strengths such as the weight decay. These hyper-parameters can be jointly optimized with architectures or separately optimized after the best architectures are found. HPO methods in Section~\\ref{sec:hpo} can also be combined here.\nAnother critical choice is the number of message-passing layers. Unlike CNNs, most currently successful GNNs are shallow, e.g., with no more than three layers, possibly due to the over-smoothing problem~. Limited by this problem, the existing NAS methods for GNNs usually preset the number of layers as a small fixed number. How to automatically design deep GNNs while integrating techniques to alleviate over-smoothing remains mostly unexplored. On the other hand, NAS may also bring insights to help to tackle the over-smoothing problem~.", "cites": [6509, 221], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the role of hyper-parameters in NAS for GNNs, connecting them to broader AutoML practices. It critically addresses the limitations of current methods (e.g., shallow GNNs due to over-smoothing) and identifies underexplored areas like designing deep GNNs with NAS. The discussion abstracts beyond individual papers to highlight general patterns and challenges in the field."}}
{"id": "b5e50631-c85d-4297-96ab-da8f2c43a63f", "title": "Controller + RL", "level": "subsubsection", "subsections": [], "parent_id": "bb56a929-8f09-47d4-b687-514bf714237d", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Strategy"], ["subsubsection", "Controller + RL"]], "content": "A widely adopted NAS search strategy uses a controller to generate the neural architecture descriptions and train the controller with reinforcement learning to maximize the model performance as rewards. For example, if we consider neural architecture descriptions as a sequence, we can use RNNs as the controller~. Such methods can be directly applied to GNNs with a suitable search space and performance evaluation strategy.", "cites": [684], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes the Controller + RL approach for NAS in graph machine learning, referencing one key paper. It lacks synthesis of multiple sources, critical evaluation of the method's strengths or limitations, and abstraction to broader principles or frameworks. The explanation is minimal and primarily descriptive."}}
{"id": "31f6efc7-7ce3-447f-8e8f-bfc7012c1e7e", "title": "Differentiable", "level": "subsubsection", "subsections": [], "parent_id": "bb56a929-8f09-47d4-b687-514bf714237d", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Strategy"], ["subsubsection", "Differentiable"]], "content": "Differentiable NAS methods such as DARTS~ and SNAS~ have gained popularity in recent years. Instead of optimizing different operations separately, differentiable methods construct a single super-network (known as the \\emph{one-shot model}) containing all possible operations. Formally, we denote\n\\begin{equation}\n\t\\mathbf{y} = o^{(x,y)}(\\mathbf{x}) = \\sum \\nolimits_{o \\in \\mathcal{O}} \\frac{\\exp( \\mathbf{z}_{o}^{(x,y)})}{\\sum \\nolimits_{o^\\prime \\in \\mathcal{O}}\\exp( \\mathbf{z}_{o^\\prime}^{(x,y)})} o(\\mathbf{x}), \n\\end{equation}\nwhere $o^{(x,y)}(\\mathbf{x})$ is an operation in the GNN with input $\\mathbf{x}$ and output $\\mathbf{y}$, $\\mathcal{O}$ are all candidate operations, and $\\mathbf{z}^{(x,y)}$ are learnable vectors to control which operation is selected. Briefly speaking, each operation is regarded as a probability distribution of all possible operations.\nIn this way, the architecture and model weights can be jointly optimized via gradient-based algorithms. The main challenges lie in making the NAS algorithm differentiable, where several techniques such as Gumbel-softmax~ and concrete distribution~ are resorted to. When applied to GNNs, slight modification may be needed to incorporate the specific operations defined in the search space.", "cites": [544, 4422, 6511, 782], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers on differentiable NAS, connecting them through the concept of one-shot models and gradient-based optimization. It abstracts the core mechanism using a mathematical formulation and identifies general challenges such as differentiability. However, it lacks deeper critical evaluation of the methods, such as their relative strengths, weaknesses, or empirical performance, which would elevate the insight level further."}}
{"id": "8a228158-ff47-4cf9-8ce8-f0065c68f0cc", "title": "Evolutionary Algorithms", "level": "subsubsection", "subsections": [], "parent_id": "bb56a929-8f09-47d4-b687-514bf714237d", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Strategy"], ["subsubsection", "Evolutionary Algorithms"]], "content": "Evolutionary algorithms are a class of optimization algorithms inspired by biological evolution. For NAS, randomly generated architectures are considered initial individuals in a population. Then, new architectures are generated using mutations and crossover operations on the population. The architectures are evaluated and selected to form the new population, and the same process is repeated. The best architectures are recorded while updating the population, and the final solutions are obtained after sufficient updating steps.   \nFor GNNs, regularized evolution (RE) NAS~ has been widely adopted. RE's core idea is an aging mechanism, i.e., in the selection process, the oldest individuals in the population are removed. \nGenetic-GNN~ also proposes an evolution process to alternatively update the GNN architecture and the learning hyper-parameters to find the best fit of each other.", "cites": [6512, 870], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of evolutionary algorithms in the context of NAS for graph machine learning but does not deeply synthesize or connect the cited papers to a broader narrative. It mentions two approaches (Regularized Evolution and Genetic-GNN) in isolation without critical comparison or evaluation of their strengths and limitations. The level of abstraction is minimal, focusing on surface-level descriptions rather than overarching principles."}}
{"id": "35ab6913-9e89-45f6-957e-bb2e49500cb5", "title": "Combinations", "level": "subsubsection", "subsections": [], "parent_id": "bb56a929-8f09-47d4-b687-514bf714237d", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Search Strategy"], ["subsubsection", "Combinations"]], "content": "It is also feasible to combine these three types of search strategies mentioned above. For example, AGNN~ proposes a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller and train the controller with RL. By only generating slightly different architectures, the controller can find well-performing GNNs more efficiently.   adopt CEM-RL~, which combines evolutionary and differentiable methods.", "cites": [6513, 6508], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions two papers that combine different search strategies for NAS on graphs but does not deeply synthesize or connect their approaches. It lacks critical evaluation of the methods and limitations, and offers minimal abstraction or generalization beyond the specific examples cited."}}
{"id": "7d794060-fefb-4bc6-99c3-f6e1ab43fee6", "title": "Performance Estimation Strategy", "level": "subsection", "subsections": [], "parent_id": "386b63ce-ff51-405a-91a5-7835886c11ca", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Performance Estimation Strategy"]], "content": "Due to the large number of possible architectures, it is infeasible to fully train each architecture independently. Next, we review some performance estimation strategies. \nA commonly adopted ``trick'' to speed up performance estimation is to reduce fidelity~, e.g., by reducing the number of epochs or the number of data points. This strategy can be directly generalized to GNNs.\nAnother strategy successfully applied to CNNs is sharing weights among different models, known as parameter sharing or weight sharing~. For differentiable NAS with a large one-shot model, parameter sharing is naturally achieved since the architectures and weights are jointly trained. However, training the one-shot model may be difficult since it contains all possible operations. To further speed up the training process, single-path one-shot model~ has been proposed where only one operation between an input and output pair is activated during each pass.\nFor NAS without a one-shot model, sharing weights among different architecture is more difficult but not entirely impossible. For example, since it is known that some convolutional filters are common feature extractors, inheriting weights from previous architectures is feasible and reasonable in CNNs~. However, since there is still a lack of understandings of what weights in GNNs represent, we need to be more cautious about inheriting weights~. AGNN~ proposes three constraints for parameter inheritance: same weight shapes, same attention and activation functions, and no parameter sharing in batch normalization and skip connections.", "cites": [6507, 6508, 872, 8247, 8955], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of performance estimation strategies in NAS for GNNs, drawing from general NAS literature and specific graph-based NAS works. It connects ideas across different papers, such as the use of parameter sharing in CNNs and its adaptation to GNNs. While it offers some critical insights into the challenges of weight sharing in GNNs, the critique remains relatively surface-level without deeper evaluation of trade-offs or methodological flaws. The section abstracts general principles, like the importance of fidelity reduction and constraints on parameter inheritance, but stops short of forming a novel, unifying framework."}}
{"id": "e6b45486-b889-4d51-b108-df28ccb5c208", "title": "The Search Space", "level": "subsubsection", "subsections": [], "parent_id": "099a53a1-e27f-4422-9846-b206b28721ae", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Discussions"], ["subsubsection", "The Search Space"]], "content": "Besides the basic search space presented in Section~\\ref{sec:searchspace}, different graph tasks may require other search spaces. For example,  meta-paths are critical for heterogeneous graphs~, edge features are essential in modeling molecular graphs~, and spatial-temporal modules are needed in skeleton-based recognition~. Sampling mechanisms to accelerate GNNs are also critical, especially for large-scale graphs~. A suitable search space usually requires careful designs and domain knowledge.", "cites": [6513, 6510], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly mentions different graph tasks and their associated search space requirements but lacks a detailed synthesis of the cited papers. It identifies that domain-specific elements like meta-paths or spatial-temporal modules are important, yet it does not connect these ideas across the cited works in a structured or insightful manner. There is minimal critical analysis or abstraction beyond the specific examples provided."}}
{"id": "2f85ebe5-320d-4cd0-88ac-ea0c2cdba37d", "title": "Transferability", "level": "subsubsection", "subsections": [], "parent_id": "099a53a1-e27f-4422-9846-b206b28721ae", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "NAS for Graph Machine Learning"], ["subsection", "Discussions"], ["subsubsection", "Transferability"]], "content": "It is non-trivial to transfer GNN architectures across different datasets and tasks due to the complexity and diversity of graph tasks.  adopt a fixed set of GNNs as anchors on different tasks and datasets. \nThen, the rank correlation serves as a metric to measure the similarities between different datasets and tasks. The best-performing GNNs of the most similar tasks are transferred to solve the target task.", "cites": [6514], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the challenge of transferring GNN architectures and outlines a method that uses rank correlation to measure similarity, referencing the cited paper. However, it lacks substantial synthesis of multiple sources, deeper critical evaluation of the approach, and broader abstraction to overarching principles. The narrative remains focused on a specific technique without broader context or insight."}}
{"id": "26336b83-0b83-4757-8c27-ff739e1401a2", "title": "Libraries for AutoML on Graphs", "level": "section", "subsections": ["7f049455-7dca-4026-b87a-4f2b4a83f7dd"], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Libraries for AutoML on Graphs"]], "content": "\\label{sec:tool}\nPublicly available libraries are important to facilitate and advance the research and applications of AutoML on graphs. Popular libraries for graph machine learning include PyTorch Geometric , Deep Graph Library , GraphNets , AliGraph , Euler , PBG , Stellar Graph , Spektral , CodDL , OpenNE , GEM , Karateclub , DIG , and classical NetworkX . However, these libraries do not support AutoML.\nOn the other hand, AutoML libraries such as NNI , AutoKeras , AutoSklearn , Hyperopt , TPOT , AutoGluon , MLBox , and MLJAR  are widely adopted. Unfortunately, because of the uniqueness and complexity of graph tasks, they cannot be directly applied to automate graph machine learning.\n\\begin{figure}[t] \n\t\\centering\n\t\\includegraphics[width=0.99\\linewidth]{work.pdf} \n\t\\caption{The overall framework of AutoGL.}\n\t\\label{fig:framework} \n\\end{figure}\nRecently, some HPO and NAS methods for graphs such as AutoNE , AutoGM , GraphNAS  GraphGym  have open-sourced their codes, facilitating reproducibility and promoting AutoML on graphs. Besides, AutoGL~\\footnote{AutoGL homepage: \\url{https://mn.cs.tsinghua.edu.cn/AutoGL}}, the first dedicated library for automated graph learning, is developed. Next, we review AutoGL in detail. \nFigure~\\ref{fig:framework} shows the overall framework of AutoGL. The main characteristics of AutoGL are three-folded:\n\\begin{itemize}[leftmargin=0.5cm]\n\t\\item Open-source: all the source codes of AutoGL are publicly available under the MIT license. \n\t\\item Easy to use: AutoGL is designed to be easy to use. For example, less than ten lines of codes are needed to conduct some quick experiments of AutoGL.  \n\t\\item Flexible to be extended: AutoGL adopts a modular design with high-level base classes API and extensive documentations, allowing flexible and customized extensions.\n\\end{itemize}\nWe briefly review the dataset management and four core components of AutoGL: Auto Feature Engineering, Model Training, Hyper-Parameters Optimization, and Auto Ensemble. These components are designed in a modular and object-oriented fashion to enable clear logic flows, easy usages, and flexibility in extending.", "cites": [6506, 6518, 6519, 8249, 208, 6515, 8248, 8250, 6517, 6520, 1180, 476, 1181, 6516], "cite_extract_rate": 0.5185185185185185, "origin_cites_number": 27, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of existing libraries for graph machine learning and AutoML but lacks deeper synthesis of the cited works. It makes minimal connections between the capabilities and limitations of the libraries. While it identifies the absence of AutoML support in general-purpose graph libraries and highlights AutoGL as a solution, it does not offer a critical analysis or broader abstraction of trends in the field."}}
{"id": "7f049455-7dca-4026-b87a-4f2b4a83f7dd", "title": "AutoGL Dataset.", "level": "paragraph", "subsections": ["0c85a9b2-404d-4841-b89c-357f5af1e04b"], "parent_id": "26336b83-0b83-4757-8c27-ff739e1401a2", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Libraries for AutoML on Graphs"], ["paragraph", "AutoGL Dataset."]], "content": "inherits from PyTorch Geometric~, covering common benchmarks for node and graph classification, including the recent Open Graph Benchmark~. Users can easily add customized datasets following documentations.", "cites": [2787, 1180], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions that AutoGL's dataset module inherits from PyTorch Geometric and includes the Open Graph Benchmark. It provides minimal synthesis by only listing the datasets included and does not compare or evaluate their strengths and weaknesses. There is no critical analysis or abstraction to broader patterns or principles in automated graph learning."}}
{"id": "b7a80cbb-c2e4-43d4-94f6-03724c6658a8", "title": "Future Directions", "level": "section", "subsections": [], "parent_id": "dc8ddd0f-1e90-459f-8d88-4c6ccbd90373", "prefix_titles": [["title", "Automated Machine Learning on Graphs: A Survey"], ["section", "Future Directions"]], "content": "\\label{sec:future}\n\\begin{itemize}\n\\item \\textbf{Graph models for AutoML}: In this paper, we mainly focus on \nhow AutoML methods are extended to graphs. The other direction, i.e., using graphs to help AutoML, is also feasible and promising. For example, we can model neural networks as a directed acyclic graph (DAG) to analyze their structures~ or adopt GNNs to facilitate NAS~. Ultimately, we expect graphs and AutoML to form tighter connections and further facilitate each other.\n\\item \\textbf{Robustness and explainability}: Since many graph applications are risk-sensitive, e.g., finance and healthcare, model robustness and explainability are indispensable for actual usages. Though there exist some initial studies on the robustness~ and explainability~ of graph machine learning, how to generalize these techniques into AutoML on graphs remains to be further explored .\n\\item \\textbf{Hardware-aware models}: To further improve the scalability of automated machine learning on graphs, hardware-aware models may be a critical step, especially in real industrial environments. Both hardware-aware graph models~ and hardware-aware AutoML models~ have been studied, but integrating these techniques still poses significant challenges.\n\\item \\textbf{Comprehensive evaluation protocols}: Currently, most AutoML on graphs are tested on small traditional benchmarks such as three citation graphs, i.e., Cora, CiteSeer, and PubMed~. However, these benchmarks have been identified as insufficient to compare different graph machine learning models~, not to mention AutoML on graphs. More comprehensive evaluation protocols are needed, e.g., on recently proposed graph machine learning benchmarks~ or new dedicated graph AutoML benchmarks similar to the NAS-bench series~.\n\\end{itemize}\n\\section*{Acknowledgments}\nThis work was supported in part by the National Key Research and Development Program of China (No.2020AAA0106300, 2020AAA0107800,  2018AAA0102000) and National Natural Science Foundation of China No.62050110.\n\\bibliographystyle{unsrt}\n\\bibliography{ijcai21}\n\\end{document}", "cites": [8389, 3989, 6524, 6514, 2787, 6522, 6523, 8251, 6512, 9032, 7010, 3413, 6521], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple ideas from cited papers to form a coherent narrative on future research directions in AutoML on graphs. It critically identifies gaps, such as the lack of robustness and comprehensive benchmarks, and links concepts like graph-based NAS and hardware-aware models to broader trends. The abstraction is strong, as it generalizes from specific papers to suggest overarching principles for advancing the field."}}
