{"id": "adfae688-a94e-4b79-9ffa-32308b8cb709", "title": "Introduction", "level": "section", "subsections": ["184cb113-77bd-4cc7-8492-66a0ed82a4fa", "f0ded957-2bcf-40e1-958b-807b63d5a972", "0f442737-0148-49f3-aafe-a19417e9a54e"], "parent_id": "64dae0de-0654-4c28-90d7-95856f2349e2", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\n\\IEEEPARstart{G}{raphs}, also referred to as networks, can be extracted from various real-world relations among abundant entities. Some common graphs have been widely used to formulate different relationships, such as social networks, biological networks, patent networks, traffic networks, citation networks, and communication networks~. A graph is often defined by two sets, i.e., vertex set and edge set. Vertices represent entities in  graph, whereas edges represent relationships between those entities. Graph learning has attracted considerable attention because of its wide applications in the real world, such as data mining and knowledge discovery. Graph learning methods have gained  increasing popularity for capturing complex relationships, as graphs exploit essential and relevant relations among vertices~. For example, in microblog networks, the spread trajectory of rumors can be tracked by detecting information cascades. In biological networks, new treatments for difficult diseases can be discovered by inferring protein interactions. In traffic networks, human mobility patterns can be predicted by analyzing the co-occurrence phenomenon with different timestamps~. Efficient analysis of these networks massively depends on the way how networks are represented.", "cites": [6092, 6093, 6094, 212], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction provides a general overview of graph learning and mentions a few application areas, but it integrates the cited papers only minimally and lacks a clear synthesis. It does not critically evaluate the cited works or their limitations, and while it touches on common themes (e.g., representation learning, network analysis), it does not abstract beyond individual papers to present deeper, meta-level insights."}}
{"id": "184cb113-77bd-4cc7-8492-66a0ed82a4fa", "title": "What is Graph Learning?", "level": "subsection", "subsections": [], "parent_id": "adfae688-a94e-4b79-9ffa-32308b8cb709", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Introduction"], ["subsection", "What is Graph Learning?"]], "content": "Generally speaking, graph learning refers to machine learning on graphs. Graph learning methods map the features of a graph to feature vectors with the same dimensions in the embedding space. A graph learning model or algorithm directly converts the graph data into the output of the graph learning architecture without projecting the graph into a low dimensional space. Most graph learning methods are based on or generalized from deep learning techniques, because deep learning techniques can encode and represent graph data into vectors. The output vectors of graph learning are in continuous space. The target of graph learning is to extract the desired features of a graph. Thus the representation of a graph can be easily used by downstream tasks such as node classification and link prediction without an explicit embedding process. Consequently, graph learning is a more powerful and meaningful technique for graph analysis.\nIn this survey paper, we try to examine machine learning methods on graphs in a comprehensive manner. As shown in Fig.~\\ref{fig1}, we focus on existing methods that fall into the following four categories: graph signal processing (GSP) based methods, matrix factorization based methods, random walk based methods, and deep learning based methods. Roughly speaking, GSP deals with sampling and recovery of graph, and learning topology structure from data. Matrix factorization can be divided into graph Laplacian matrix factorization and vertex proximity matrix factorization. Random walk based methods include structure-based random walk, structure and node information based random walk, random walk in heterogeneous networks, and random walk in time-varying networks. Deep learning based methods include graph convolutional networks, graph attention networks, graph auto-encoder, graph generative networks, and graph spatial-temporal networks. Basically, the model architectures of these methods/techniques differ from each other. This paper presents an extensive review of the state-of-the-art graph learning techniques.\n\\begin{figure*}[htb]\n\t\\centering\n\t\\includegraphics[width=6in]{fig1.png}\n\t\\caption{The categorization of graph learning.}\\label{fig1}\n\\end{figure*}\nTraditionally, researchers adopt an adjacency matrix to represent a graph, which can only capture the relationship between two adjacent vertices. However, many complex and irregular structures cannot be captured by this simple representation. When we analyze large-scale networks, traditional methods are computationally expensive and hard to be implemented in real-world applications. Therefore, effective representation of these networks is a paramount problem to solve~. Network Representation Learning (NRL) proposed in recent years can learn latent features of network vertices with low dimensional representation~. When the new representation has been learned, previous machine learning methods can be employed for analyzing the graph data as well as discovering relationships hidden in the data.\nWhen complex networks are embedded into a latent, low dimensional space, the structural information and vertex attributes can be preserved~. Thus the vertices of networks can be represented by low dimensional vectors. These vectors can be regarded as the features of input in previous machine learning methods. Graph learning methods pave the way for graph analysis in the new representation space, and many graph analytical tasks, such as link prediction, recommendation and classification, can be solved efficiently~. Graphical network representation sheds light on various aspects of social life, such as communication patterns, community structure, and information diffusion~. According to the attributes of vertices, edges and subgraph, graph learning tasks can be divided into three categories, which are vertices based, edges based, and subgraph based, respectively. The relationships among vertices in a graph can be exploited for, e.g., classification, risk identification, clustering, and community detection~. By judging the presence of edges between two vertices in graphs, we can perform recommendation and knowledge reasoning, for instance. Based on the classification of subgraphs~, the graph can be used for, e.g., polymer classification, 3D visual classification, etc. For GSP, it is significant to design suitable graph sampling methods to preserve the features of the original graph, which aims at recovering the original graph efficiently~. Graph recovery methods can be used for constructing the original graph in the presence of incomplete data~. Afterwards, graph learning can be exploited to learn the topology structure from graph data. In summary, graph learning can be used to tackle the following challenges, which are difficult to solve by using traditional graph analysis methods~.\n\\begin{enumerate}\n  \\item \\textbf{Irregular domains:} Data collected by traditional sensors have a clear grid structure. However, graphs lie in an irregular domain (i.e., non-Euclidean space). In contrast to regular domain (i.e., Euclidean space), data in non-Euclidean space are not ordered regularly. Distance is hence difficult to be defined. As a result, basic methods based on traditional machine learning and signal processing cannot be directly generalized to graphs.\n  \\item \\textbf{Heterogeneous networks:} In many cases, networks involved in the traditional graph analysis algorithms are homogeneous. The appropriate modeling methods only consider the direct connection of the network and strip other irrelevant information, which significantly simplifies the processing. However, it is prone to cause information loss. In the real world, the edges among vertices and the types of vertices are usually diverse, such as in the academic network shown in Fig.~\\ref{fig:Heterogenous}. Thus it isn't easy to discover potential value from heterogeneous information networks with abundant vertices and edges.\n  \\item \\textbf{Distributed algorithms:} In big social networks, there are often millions of vertices and edges~. Centralized algorithms cannot handle this since the computational complexity of these algorithms would significantly increase with the growth of vertex number. The design of distributed algorithms for dealing with big networks is a critical problem yet to be solved~. One major benefit of distributed algorithms is that the algorithms can be executed in multiple CPUs or GPUs simultaneously, and hence the running time can be reduced significantly.\n\\end{enumerate}", "cites": [6098, 6095, 318, 6096, 6100, 212, 8975, 1010, 6099, 6097, 550], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonably coherent overview of graph learning by integrating ideas from multiple cited papers, particularly in discussing representation learning, heterogeneous networks, and distributed algorithms. It synthesizes concepts like NRL and GSP and links them to broader challenges in graph analysis. However, the critical evaluation is limited, with few explicit comparisons or limitations identified. The abstraction level is moderate, as it identifies categories of tasks and challenges but stops short of offering a deeper meta-framework."}}
{"id": "f0ded957-2bcf-40e1-958b-807b63d5a972", "title": "Related Surveys", "level": "subsection", "subsections": [], "parent_id": "adfae688-a94e-4b79-9ffa-32308b8cb709", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Introduction"], ["subsection", "Related Surveys"]], "content": "There are several surveys that are partially related to the scope of this paper. Unlike these surveys, we aim to provide a comprehensive overview of graph learning methods, with a focus on four specific categories. In particular, graph signal processing is introduced as one approach for graph learning, which is not covered by other surveys.\nGoyal and Ferrara~ summarized graph embedding methods, such as matrix factorization, random walk and their applications in graph analysis. Cai et al.~ reviewed graph embedding methods based on problem settings and embedding techniques. Zhang et al.~ summarized NRL methods based on two categories, i.e., unsupervised NRL and semi-supervised NRL, and discussed their applications. Nickel et al.~ introduced knowledge extraction methods from two aspects: latent feature models and graph based models. Akoglu et al.~ reviewed state-of-the-art techniques for event detection in data represented as graphs, and their applications in the real world. Zhang et al.~ summarized deep learning based methods for graphs, such as graph neural networks (GNNs), graph convolutional networks (GCNs) and graph auto-encoders (GAEs). Wu et al.~ reviewed state-of-the-art GNN methods and discussed their applications in different fields. Ortega et al.~ introduced GSP techniques for representation, sampling and learning, and discussed their applications. Huang et al.~ examined the applications of GSP in functional brain imaging and addressed the problem of how to perform brain network analysis from signal processing perspective.\nIn summary, none of the existing surveys provides a comprehensive overview of graph learning. They only cover some parts of graph learning, such as network embedding and deep learning based network representation. The NRL and/or GNN based surveys do not cover the GSP techniques. In contrast, we review GSP techniques in the context of graph learning, as it is an important approach for GNNs. Specifically, this survey paper integrates state-of-the-art machine learning techniques for graph data, gives a general description of graph learning, and discusses its applications in various domains.", "cites": [553, 3344, 7233, 215, 212, 9134, 219, 550], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited surveys by highlighting their thematic focus and noting their omission of certain areas, particularly GSP. It begins to abstract these works into broader categories like graph embedding, NRL, GNN, and GSP, but the critical analysis remains limited to stating what these surveys lack rather than deeply evaluating their strengths or weaknesses. The narrative is coherent and shows some analytical intent by framing the current survey as a more comprehensive effort."}}
{"id": "b80c97cb-21ee-4d1e-8989-5d7676554f7d", "title": "Graph Learning Models and Algorithms", "level": "section", "subsections": ["b98b1608-63ed-40e4-8a31-3e8023ef0207", "d840a266-c9d1-40a4-b01c-161e41f531dc", "3d125124-0f76-495a-849b-3574679d280e", "9610d7f6-5599-46e1-869b-238d95753770"], "parent_id": "64dae0de-0654-4c28-90d7-95856f2349e2", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"]], "content": "\\label{sec:algorithm}\nThe feature vectors that represent various categorical attributes are viewed as the input in previous machine learning methods. However, the mapping from the input feature vectors to the output prediction results need to be handled by graph learning~. Deep learning has been regarded as one of the most successful techniques in artificial intelligence~. Extracting complex patterns by exploiting deep learning from a massive amount of irregular data has been found very useful in various fields, such as pattern recognition and image processing. Consequently, how to utilize deep learning techniques to extract patterns from complex graphs has attracted lots of attention. Deep learning on graphs, such as GNNs, GCNs, and GAEs, has been recognized as a powerful technique for graph analysis~. Besides, GSP has also been proposed to deal with graph analysis~. One of the most typical scenarios is that a set of values reside on a set of vertices, and these vertices are connected by edges~. Graph signals can be adopted to model various phenomena in real world. For example, in social networks, users in Facebook can be viewed as vertices, and their friendships can be modeled as edges. The number of followers of each vertex is marked in this social network. Based on this assumption, many techniques (e.g., convolution, filter, wavelet, etc.) in classical signal processing can be employed for GSP with suitable modifications~.\nIn this section, we review graph learning models and algorithms under four categories as mentioned before, namely GSP based methods, matrix factorization based methods, random walk based methods, and deep learning based methods. In Table~\\ref{tab:abbreviations}, we list the abbreviations used in this paper.\n\\begin{table}[htbp]\n  \\centering\n  \\caption{Definitions of abbreviations}\n    \\begin{tabular}{cc}\n    \\toprule\n    \\textbf{Abbreviation} & \\textbf{Definition} \\\\\n    \\midrule\n    PCA   & Principal component analysis  \\\\\n    NRL   &  Network representation learning \\\\\n    LSTM  &  Long short-term memory (networks)  \\\\\n    GSP   & Graph signal processing \\\\\n    GNN  &  Graph neural network \\\\\n    GMRF  & Gauss markov random field \\\\\n    GCN  & Graph convolutional network \\\\\n    GAT  &  Graph attention network  \\\\\n    GAN   &  Generative adversarial network  \\\\\n    GAE  & Graph auto-encoder \\\\\n    ASP   & Algebraic signal processing \\\\\n     RNN &  Recurrent neural network \\\\\n     CNN & Convolutional neural network \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:abbreviations}\n\\end{table}", "cites": [166, 9134, 219, 244, 550], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces graph learning models and algorithms by briefly mentioning key methods like GNNs, GCNs, and GAEs, along with GSP and its applications. While it draws from multiple cited papers, the synthesis is limited to stating that these techniques are used for graph analysis without deeper connections between them. There is minimal critical evaluation of the cited works, and abstraction is modest, focusing mainly on surface-level generalizations of graph signal processing and deep learning."}}
{"id": "ae89c93b-e71a-4b64-9f6b-e73bb685f27f", "title": "Representation on Graphs", "level": "subsubsection", "subsections": [], "parent_id": "b98b1608-63ed-40e4-8a31-3e8023ef0207", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Graph Signal Processing"], ["subsubsection", "Representation on Graphs"]], "content": "A meaningful representation of graphs has contributed a lot to the rapid growth of graph learning. There are two main models of GSP, i.e., adjacency matrix based GSP~ and Laplacian based GSP~. An adjacency matrix based GSP comes from algebraic signal processing (ASP)~, which interprets linear signal processing from algebraic theory. Linear signal processing contains signals, filters, signal transformation, etc. It can be applied in both continuous and discrete time domains. The basic assumption of linear algebra is extended to the algebra space in ASP. By selecting signal model appropriately, ASP can obtain different instances in linear signal processing. In adjacency matrix based GSP, the signal model is generated from a shift. Similar to traditional signal processing, a shift in GSP is a filter in graph domain~. GSP usually defines graph signal models using adjacency matrices as shifts. Signals of a graph are normally defined at vertices.\nLaplacian based GSP originates from spectral graph theory. High dimensional data are transferred into a low dimensional space generated by a part of the Laplacian basis~. Some researchers exploited sensor networks~ to achieve distributed processing of graph signals. Other researchers solved the problem globally under the assumption that the graph is smooth. Unlike adjacency matrix based GSP, Laplacian matrix is symmetric with real and non-negative edge weights, which is used to index undirected graphs.\nAlthough the models use different matrices as basic shifts, most of the notions in GSP are derived from signal processing. Notions with different definitions in these models may have similar meanings. All of them correspond to concepts in signal processing. Signals in GSP are values defined on graphs, and they are usually written as a vector, \\(\\bm{s}=[ s_{0} ,s_{1} ,\\dots ,s_{N-1}] \\in \\mathbb{C}^{N}.\\) \\(N\\) is the number of vertices, and each element in the vector represents the value on a vertex. Some studies~ allow complex-value signals, even though most applications are based on real-value signals.\nIn the context of adjacency matrix based GSP, a graph can be represented as a triple \\(G(V,E,\\bm{W})\\), where \\(V\\) is the vertex set, \\(E\\) is the edge set and \\(\\bf{W}\\) is the adjacency matrix. With the definition of graphs, we can also define degree matrix \\(\\bm{D}_{ii}=\\bm{d}_{i}\\), where \\(\\bm{D}\\) is a diagonal matrix, and \\(\\bm{d}_{i}\\) is the degree of vertex \\(i\\). Graph Laplacian is defined as \\(\\bm{L}=\\bm{D}-\\bm{W}\\), and normalized Laplacian is defined as \\(\\bm{L}_{norm}=\\bm{D}^{-1/2}\\bm{L}\\bm{D}^{-1/2}\\). Filters in signal processing can be seen as a function that amplifies or reduces relevant frequencies, eliminating irrelevant ones. Matrix multiplication in linear space equals to scale changing, which is identical with filter operation in frequency domain. It is obvious that we can use matrix multiplication as a filter in GSP, which is written as \\(\\bm{s}_{out} =\\bm{Hs}_{in}\\), where $\\bm{H}$ stands for a filter.\nShift is an important concept to describe variation in signal, and time-invariant signals are used frequently~. In fact, there are different choices of shifts in GSP. Adjacency matrix based GSP uses \\(\\bm{A}\\) as shift. Laplacian based GSP uses \\(\\bf{L}\\)~, and some researchers also use other matrices . By following time invariance in traditional signal processing, shift invariance is defined in GSP. If filters are commutative with shift, they are shift-invariant, which can be written as \\(\\bm{AH}=\\bm{HA}\\). It is proved that shift-invariant filter can be represented by the shift. The properties of shift are vital, and they determine the fashion of other definitions such as Fourier transform and frequency.\nIn adjacency matrix based GSP, eigenvalue decomposition of shift \\(\\bm{A}\\) is \\(\\bm{A}=\\bm{V\\Lambda V}^{-1}\\). \\(\\bm{V}\\) is the matrix of eigenvectors \\([\\bm{v}_{0}, \\bm{v}_{1},\\dots,\\bm{v}_{N-1}]\\) and\n\\[\n  \\bm{\\Lambda} =\n  \\begin{bmatrix}\n    \\bm{\\lambda}_{0} & & \\\\\n    & \\ddots & \\\\\n    & & \\bm{\\lambda}_{N-1}\n  \\end{bmatrix}\n\\]\nis a diagonal matrix of eigenvalues. The Fourier transform matrix is the inverse of \\(\\bm{V}\\), i.e., \\(\\bm{F}=\\bm{V}^{-1}\\). Frequency of shift is defined as total variation, which states the difference after shift \\[TV_{G} =||\\bm{v}_{k} -\\frac{1}{\\lambda _{max}} \\bm{A}\\bm{v}_{k} ||_{1},\\] where \\(\\frac{1}{\\lambda _{max}}\\) is a normalized factor of matrix. It means that the frequencies of eigenvalue far away from the largest eigenvalues on complex plane are large. A large frequency means that signals are changed with a large scale after shift filtering. The differences between minimum and maximum \\(\\bm{\\lambda}\\) can be seen in Fig.~\\ref{fig:lambda}. Generally, the total variation tends to be relatively low with larger frequency, and vice versa. Eigenvectors of larger eigenvalues can be used to construct low-frequency filters, which capture fundamental characteristics, and smaller ones can be employed to capture the variation among neighbor nodes.\n\\begin{figure}[htb]\n\\centering\n\\subfigure[The maximum  frequency]{\n\\label{maxfre}\n\\includegraphics[width=0.5\\textwidth]{fig4-1.png}}\n\\subfigure[The minimum frequency]{\n\\label{minfre}\n\\includegraphics[width=0.5\\textwidth]{fig4-2.png}}\n\\caption{Illustration of difference between minimum and maximum frequencies.}\n\\label{fig:lambda}\n\\end{figure}\nFor topology learning problems, we can distinguish the corresponding solutions depending on known information. When topology information is partly known, we can use the known information to infer the whole graph. In case the topology information is unknown while we still can observe the signals on the graph, the topology structure has to be inferred from the signals. The former one is often solved as a sampling and recovery problem, and blind topology inference is also known as graph topology (or structure) learning.", "cites": [8499, 245, 9134, 6101, 244], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple GSP concepts from cited papers, connecting ideas from spectral theory, shift operators, and signal definitions to form a coherent overview. It critically analyzes the differences between adjacency matrix and Laplacian-based GSP, as well as the implications of shift invariance. The abstraction is strong, generalizing the role of shift operators and frequencies in GSP to broader signal processing principles."}}
{"id": "0f2c92c2-0d30-4395-818c-bed78b2932d8", "title": "Sampling and Recovery", "level": "subsubsection", "subsections": [], "parent_id": "b98b1608-63ed-40e4-8a31-3e8023ef0207", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Graph Signal Processing"], ["subsubsection", "Sampling and Recovery"]], "content": "Sampling is not a new concept defined in GSP. In conventional signal processing, we normally need to reconstruct original signals with the least samples and retain all information of original signals for a sampling problem. Few samples lead to the lack of information and more samples need more space to store. The well-known Nyquist-Shannon sampling theorem gives the sufficient condition of perfect recovery of signals in time domain.\nResearchers have migrated the sampling theories into GSP to study the sampling problem on graphs. As the volume of data is large in some real-world applications such as sensor networks and social networks, sampling less and recovering better are vital for GSP. In fact, most algorithms and frameworks solving sampling problems require that the graph models correlations within signals observed on it~. The sampling problem can be defined as reconstructing signals from samples on a subset of vertices, and signals in it are usually band-limited. Nyquist-Shannon sampling theorem was extended to graph signals in~. Based on the normalized Laplacian matrix, sampling theorem and cut-off frequency are defined for GSP. Moreover, the authors provided a method for computing cut-off frequency from a given sampling set and a method for choosing sampling set for a given bandwidth. It should be noted that the sampling theorem proposed therein is merely applied to undirected graph. As Laplacian matrix represents undirected graphs only, sampling theory for directed graph adopts adjacent matrix. An optimal operator with a guarantee for perfect recovery was proposed in~, and it is robust to noise for general graphs.\nOne of the explicit distinctions between classical signal processing and GSP is that signals of the former fall in regular domain while the latter falls in irregular domain. For sampling and recovery problems, classical signal processing samples successive signals and can recover successive signals from samplings. GSP samples a discrete sequence, and recovers the original sequences from samplings. By following this order, the solution is generally separated into two parts, i.e., finding sampling vertex sets and reconstructing original signals based on various models.\nWhen the size of the dataset is small, we can handle the signal and shift directly. However, for a large-scale dataset, some algorithms require matrix decomposition to obtain frequencies and save eigenvalues in the procedure, which are almost impossible to realize. As a simple technique applicable to large-scale datasets, a random method can also be used in sampling. Puy et al.~ proposed two sample strategies: a non-adaptive one depending on a parameter and an adaptive random sampling strategy. By relaxing the optimized constraint, they extended random sampling to large scale graphs. Another common strategy is greedy sampling. For example, Shomorony and Avestimehr~ proposed an efficient method based on linear algebraic conditions that can exactly compute cut-off frequency. Chamon and Ribeiro~ provided near-optimal guarantee for greedy sampling, which guarantees the performance of greedy sampling in the worst cases.\nAll of the sampling strategies mentioned above can be categorized as selecting sampling, where signals are observed on a subset of vertices . Besides selecting sampling, there exists a type of sampling called aggregation sampling~, which uses observations taken at a single vertex as input, containing a sequential applications of graph shift operator.\nSimilar to classical signal processing, the reconstruction task on graphs can also be interpreted as data interpolation problem . By projecting the samples on a proper signal space, researchers obtain interpolated signals. Least squares reconstruction is an available method in practice. Gadde and Ortega~ defined a generative model for signal recovery derived from a pairwise Gaussian random field (GRF) and a covariance matrix on graphs. Under sampling theorem, the reconstruction of graph signals can be viewed as the maximum posterior inference of GRF with low-rank approximation. Wang et al.~ aimed at achieving the distributed reconstruction of time-varying band limited signal, where the distributed least squares reconstruction (DLSR) was proposed to recover the signals iteratively. DLSR can track time-varying signals and achieve perfect reconstruction. Di Lorenzo et al.~ proposed a linear mean squares (LMS) strategy for adaptive estimation. LMS enables online reconstruction and tracking from the observation on a subset of vertices. It also allows the subset to vary over time. Moreover, a sparse online estimation was proposed to solve the problems with unknown bandwidth.\nAnother common technique for recovering original signals is smoothness. Smoothness is used for inferring missing values in graph signals with low frequencies. Wang et al.~ defined the concept of local set. Based on the definition of graph signals, two iterative methods were proposed to recover band limited signals on graphs. Besides, Romero et al.~ advocated kernel regression as a framework for GSP modeling and reconstruction. For parameter selection in estimators, two multi-kernel methods were proposed to solve a single optimization problem as well. In addition, some researchers investigated different recovery problems with compressed sensing~.\nIn addition, there exists some researches on sampling of different kinds of signals such as smooth graph signals, piece-wise constant signals and piece-wise smooth signals~. Chen et al. ~ gave a uniform framework to analyze graph signals. The reconstruction of a known graph signal was studied in~, where the signal is sparse, which means only a few vertices are non-zeros. Three kinds of reconstruction schemes corresponding to various seeding patterns were examined. By analyzing single simultaneous injection, single successive value injection, and multiple successive simultaneous injections, the conditions for perfect reconstruction on any vertices were derived.", "cites": [6108, 6112, 6109, 6107, 6105, 6102, 6110, 6106, 6104, 245, 8975, 6103, 6111], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple papers to present a structured narrative on sampling and recovery in graph signal processing. It abstracts concepts like band-limitedness, sampling strategies (e.g., random, greedy, aggregation), and reconstruction methods (e.g., least squares, kernel regression, LMS) into broader patterns. While some critical evaluation is present (e.g., noting limitations of matrix decomposition for large-scale graphs), deeper comparative or evaluative insights could be enhanced."}}
{"id": "e6b8b008-dd20-4650-9ffa-d2d8c5d120f5", "title": "Learning Topology Structure from Data", "level": "subsubsection", "subsections": [], "parent_id": "b98b1608-63ed-40e4-8a31-3e8023ef0207", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Graph Signal Processing"], ["subsubsection", "Learning Topology Structure from Data"]], "content": "In most application scenes, graphs are constructed according to connections of entity correlations. For example, in sensor networks, the correlations between sensors are often consistent with geographic distance. Edges in social networks are defined as relations such as friends or colleagues~. In biochemical networks, edges are generated by interactions. Although GSP is an efficient framework for solving problems on graphs such as sampling, reconstruction, and detection, there lacks a step to extract relations from datasets. Connections exist in many datasets without explicit records. Fortunately, they can be inferred in many ways.\nAs a result, researchers want to learn complete graphs from datasets. The problem of learning graph from a dataset is stated as estimating graph Laplacian, or graph topology~. Generally, they require the graph to satisfy some properties, such as sparsity and smoothness. Smoothness is a widespread assumption in networks generated from datasets. Therefore, it is usually used to constrain observed signals and provide a rational guarantee for graph signals. Researchers have applied it to graph topology learning. The intuition behind smoothness based algorithms is that most signals on graph are stationary, and the result filtered by shift tends to be the lowest frequency. Dong et al.~ adopted a factor analysis model for graph signals, and also imposed a Gaussian prior on latent variables to obtain a Principal Component Analysis (PCA) like representation. Kalofolias~ formulated the objective as a weighted \\(l_1\\) problem and designed a general framework to solve it.\nGauss Markov Random Field (GMRF) is also a widely used theory for graph topology learning in GSP . The models of GRMF based graph topology learning select graphs that are more likely to generate signals which are similar to the ones generated by GMRF. Egilmez et al.~ formulated the problem as a maximum posterior parameter estimation of GMRF, and the graph Laplacian is a precision matrix. Pavez and Ortega~ also formulated the problem as a precision matrix estimation, and the rows and columns are updated iteratively by optimizing a quadratic problem. Both of them restrict the result matrix, which should be Laplacian. In~, Pavez et al. chose a two steps framework to find the structure of the underlying graph. First, a graph topology inference step is employed to select a proper topology. Then, a generalized graph Laplacian is estimated. An error bound of Laplacian estimation is computed. In the next step, the error bound can be utilized to obtain a matrix in a specific form as the precision matrix estimation. It is one of the first work that suggests adjusting the model to obtain a graph satisfying the requirement of various problems.\nDiffusion is also a relevant model that can be exploited to solve the topology interfering problem~. Diffusion refers to that the node continuously influences its neighborhoods. In graphs, nodes with larger values will have higher influence on their neighborhood nodes. Using a few components to represent signals will help to find the main factors of signal formation. The models of diffusion are often under the assumption of independent identically-distributed signals. Pasdeloup et al.~ gave the concept of valid graphs to explain signals and assumed that the signals are observed after diffusion. Segarra et al.~ agreed that there exists a diffusion process in the shift, and the signals can be observed. The signals in~ were explained as a linear combination of a few components.\nFor time series recorded in data, researchers tried to construct time-sequential networks. For instance, Mei and Moura~ proposed a methodology to estimate graphs, which considers both time and space dependencies and models them by auto-regressive process. Segarra et al.~ proposed a method that can be seen as an extension of graph learning. The aim of the paper was to solve the problem of joint identification of a graph filter and its input signal.\nFor recovery methods, a well-known partial inference problem is recommendation~. The typical algorithm used in recommendation is collaborative filtering (CF)~. Given the observed ratings in a matrix, the objective of CF is to estimate the full rating matrix. Huang et al.~ demonstrated that collaborative filtering could be viewed as a specific band-stop graph filter on networks representing correlations between users and items. Furthermore, linear latent factor methods can also be modeled as band limited interpretation problem.", "cites": [6113, 6118, 6114, 6105, 8976, 6117, 6116, 5167, 6119, 8977, 6115], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to provide a coherent narrative on graph topology learning, especially focusing on smoothness and GMRF-based methods. It highlights the common assumptions and techniques used across these works. While it offers some critical analysis (e.g., pointing out the ill-posed nature of the problem and the importance of assumptions), it does not deeply evaluate the limitations of each approach. The section identifies broader patterns such as the use of sparsity and smoothness in graph learning but does not fully abstract to a meta-level understanding."}}
{"id": "3d125124-0f76-495a-849b-3574679d280e", "title": "Random Walk Based Methods", "level": "subsection", "subsections": ["aeae4916-091c-4fb3-9196-b086759a00f2", "b0c51e02-fd00-4569-aa41-615bfe766b54", "9bc956d7-ac64-4377-afad-8de347ff8311", "ba7ea660-f325-425d-9818-58b7145cd6a2", "79cc0d88-cd98-44e0-a1cb-3c845388c13f"], "parent_id": "b80c97cb-21ee-4d1e-8989-5d7676554f7d", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Random Walk Based Methods"]], "content": "Random walk is a convenient and effective way to sample networks~. This method can generate sequences of nodes meanwhile preserving original relations between nodes. Based on network structure, NRL can generate feature vectors of vertices so that downstream tasks can mine network information in a low dimensional space. An example of NRL is shown in Fig.~\\ref{fig:NRL}. The image in Euclidean space is shown in Fig.~\\ref{enc}, and the corresponding graph in non-Euclidean space is shown in Fig.~\\ref{noenc}. As one of the most successful NRL algorithms, random walks play an important role in dimensionality reduction.\n\\begin{figure}[htb]\n\\centering\n\\subfigure[Image in Euclidean space]{\n\\label{enc}\n\\includegraphics[width=0.4\\textwidth]{fig5a}}\n\\subfigure[Graph in non-Euclidean space]{\n\\label{noenc}\n\\includegraphics[width=0.4\\textwidth]{fig5b.png}}\n\\caption{An example of NRL mapping an image from Euclidean space into non-Euclidean space.}\n\\label{fig:NRL}\n\\end{figure}", "cites": [6120], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited material, primarily paraphrasing the general concept of random walks and their role in network representation learning (NRL). There is no meaningful integration of the cited paper's content or deeper discussion of its implications. The section lacks critical analysis and does not attempt to evaluate, compare, or critique the works. Additionally, it does not abstract beyond specific examples, focusing only on basic descriptions and figures without identifying broader trends or principles in the field."}}
{"id": "aeae4916-091c-4fb3-9196-b086759a00f2", "title": "Structure Based Random Walks", "level": "subsubsection", "subsections": [], "parent_id": "3d125124-0f76-495a-849b-3574679d280e", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Random Walk Based Methods"], ["subsubsection", "Structure Based Random Walks"]], "content": "Graph-structured data have various data types and structures. The information encoded in a graph is related to graph structure and vertex attributes, which are the two key factors affecting the reasoning of networks. In real-world applications, many networks only have structural information, but lack vertex attribute information. How to identify network structure information effectively, such as important vertices and invisible links, attracts the interest of network scientists~. Graph data have high dimensional characteristics. Traditional network analysis methods cannot be used for analyzing graph data in a continuous space.\nIn recent years, various NRL methods have been proposed, which preserve rich structural information of networks. DeepWalk~ and Node2vec~ are two representative methods for generating network representation of basic network topology information. These methods use random walk models to generate random sequences on networks. By treating the vertices as words and the generated random sequences of vertices as word sequences (sentences), the models can learn the embedding representation of the vertices by inputting these sequences into the Word2vec model~. The principle of the learning model is to maximize the co-occurrence probability of vertices such as Word2vec. In addition, Node2vec shows that network has complex structural characteristics, and different network structure samplings can obtain different results. The sampling mode of DeepWalk is not enough to capture the diversity of connection patterns in networks. Node2vec designs a random walk sampling strategy, which can sample the networks with the preference of breadth-first sampling and depth-first sampling by adjusting the parameters.\nThe NRL algorithms mentioned above focused on the first-order proximity information of vertices. Tang et al.~ proposed a method called LINE for large-scale network embedding. LINE can maintain the first and second order approximations. The first-order neighbor refers to the one-hop neighbor between two vertices, and the second-order neighbor is the neighbor with two hops. LINE is not a deep learning based model, but it is often compared with these edge modeling based methods.\nIt has been proved that the network structure information plays an important role in various network analysis tasks. In addition to this structural information, network attributes in the original network space are also critical in modeling the formation and evolution of the network~.", "cites": [8978, 218, 1010, 6121, 282], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several key random walk-based methods (DeepWalk, Node2vec, LINE) and explains how they use structural information to learn vertex embeddings. It makes basic comparisons, such as noting that Node2vec improves on DeepWalkâ€™s sampling limitations and that LINE preserves both first- and second-order proximity. However, the synthesis is limited to methodological descriptions rather than a novel framework, and the critical analysis is shallow, with few in-depth evaluations or limitations identified."}}
{"id": "b0c51e02-fd00-4569-aa41-615bfe766b54", "title": "Structure and Vertex Information Based Random Walks", "level": "subsubsection", "subsections": [], "parent_id": "3d125124-0f76-495a-849b-3574679d280e", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Random Walk Based Methods"], ["subsubsection", "Structure and Vertex Information Based Random Walks"]], "content": "In addition to network topology, many types of networks also have rich vertex information, such as vertex content or label in networks. Yang et al.~ proposed an algorithm called TADW. The model is based on DeepWalk and considers the text information of vertices. The MMDW~ is another model based on DeepWalk, which is a kind of semi-supervised network embedding algorithm, by leveraging labelling\ninformation of vertices to enhance the performance.\nFocusing on the structural identity of nodes, Ribeiro et al.~ formulated a framework named Struc2vec. The framework considers nodes with similar local structure rather than neighborhood and labels of nodes. With hierarchy to evaluate structural similarity, the framework constrains structural similarity more stringently. The experiments indicate that DeepWalk and Node2vec are worse than Struc2vec which considers structural identity. There are some other NRL models, such as Planetoid~, which learn network representation using the feature of network structure and vertex attribute information. It is well known that vertex attributes provide effective information for improving network representation and help to learn embedded vector space. In the case of relatively sparse network topology, vertex attribute information can be used as supplementary information to improve the accuracy of representation. In practice, how to use vertex information effectively and how to apply this information to network vertex embedding are the main challenges in NRL.\nResearchers not only investigate random walk based NRL on vertices but also on graphs. Adhikari et al.~ proposed an unsupervised scalable algorithm, Sub2Vec, to learn arbitrary subgraph. To be more specific, they proposed a method to measure the similarities between subgraphs without disturbing local proximity. Narayanan et al.~ proposed graph2vec, which is a neural embedding framework. Modeling on neural document embedding models, graph2vec takes a graph as a document and the subgraph around words as vertices. By migrating the model to graphs, the performance of graph2vec significantly exceeds other substructure representation learning algorithms.\nGenerally, random walk can be regarded as a Markov process. The next state of the process is only related to last state, which is known as Markov chain. Inspired by vertex-reinforced random walks, Benson et al.~ presented spacey random walk, a non-Markovian stochastic process. As a specific type of a more general class of vertex-reinforced random walks, it takes the view that the probability of time remained on each vertex relates to the long term behavior of dynamical systems. They proved that dynamical systems can converge to a stationary distribution under sufficient conditions.\nRecently, with the development of Generative Adversarial Network (GAN), researchers combined random walks with the GAN method~. Existing research on NRL can be divided into generative models and discriminative models. GraphGAN~ integrated these two kinds of models and played a game-theoretical minimax game. With the process of the game, the performance of the two models can be strengthened. Random walk is used as a generator in the game. NetGAN~ is a generative model that can model network in real applications. The method takes the distribution of biased random walk as input, and can produce graphs with known patterns. It preserves important topology properties and does not need to define them in model definition.", "cites": [7009, 6123, 7495, 6122, 3953], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by connecting ideas from TADW, MMDW, Struc2vec, Planetoid, Sub2Vec, and graph2vec, forming a narrative on incorporating vertex and structural information in random walk-based methods. It includes some critical analysis, such as noting that DeepWalk and Node2vec perform worse than Struc2vec in capturing structural identity. The section also abstracts to a degree by highlighting broader challenges like the effective use of vertex attributes and the role of GANs in graph learning, but it stops short of offering a meta-level framework or deeper theoretical synthesis."}}
{"id": "9bc956d7-ac64-4377-afad-8de347ff8311", "title": "Random Walks in Heterogeneous Networks", "level": "subsubsection", "subsections": [], "parent_id": "3d125124-0f76-495a-849b-3574679d280e", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Random Walk Based Methods"], ["subsubsection", "Random Walks in Heterogeneous Networks"]], "content": "In reality, most networks contain more than one type of vertex, and hence networks are heterogeneous. Different from homogeneous NRL, heterogenous NRL should well reserve various relationships among different vertices~. Considering the ubiquitous existence of heterogeneous networks, many efforts have been made to learn network representations of heterogeneous networks. Compared to homogeneous NRL, the proximity among entities in heterogeneous NRL is more than a simple measure of distance or closeness. The semantics among vertices and links should be considered. Some typical scenarios include knowledge graphs and social networks.\nKnowledge graph is a popular research domain in recent years. A vital part in knowledge base population is relational inference. The central problem of relational inference is inferring unknown knowledge from the existing facts in knowledge bases~. There are three types of common relational inference method in general: statistical relational learning (SRL), latent factor models (LFM) and random walk models (RWM). Relational learning methods based on statistics lack generality and scalability. As a result, latent factor model based graph embedding and relational paths based random walk have been adopted more widely.\nIn a knowledge graph, there exist various vertices and various types of relationships among different vertices. For example, in a scholar related knowledge graph~, the types of vertices include scholar, paper, publication venue, institution, etc. The types of relationships include coauthor, citation, publication, etc. The key idea of knowledge graph embedding is to embed vertices and their relationships into a low dimensional vector space, while the inherent structure of the knowledge graph can be reserved~.\nFor relational paths based random walk, the path ranking algorithm (PRA) is a path finding method using random walks to generate relational features on graph data~. Random walks in PRA are with restart, and combine features with a logistic regression. However, PRA cannot predict connection between two vertices if there does not exist a path between them. Gardner et al.~ introduced two ways to improve the performance of PRA. One method enables more efficient processing to incorporate new corpus into knowledge base, while the other method uses vector space to reduce the sparsity of surface forms. To resolve cascade errors in knowledge construction, Wang and Cohen~ proposed a joint information extraction and knowledge base based model with a recursive random walk. Using latent context of the text, the model obtains additional improvement. Liu et al.~ developed a new random walk based learning algorithm named Hierarchical Random-walk inference (HiRi). It is a two-tier scheme: the upper tier recognizes relational sequence pattern, and the lower tier captures information from subgraphs of knowledge bases.\nAnother widely-investigated type of heterogeneous networks is social networks, such as online social networks and location based social networks. Social networks are heterogeneous in nature because of the different types of vertices and relations. There are two main ways to embed heterogeneous social networks, including meta path-based approaches and random walk-based approaches.\nA meta path in heterogeneous networks is defined as a sequence of vertex types encoding significant composite relations among various types of vertices. Aiming to employ the rich information in social networks by exploiting various types of relationships among vertices, Fu et al.~ proposed HIN2Vec, which is a representation learning framework based on meta-paths. HIN2Vec is a neural network model and the meta-paths are well embedded based on two independent phases, i.e., training data preparation and representation learning. Experimental results on various social network datasets show that HIN2Vec model is able to automatically learn vertex vector in heterogeneous networks to support a variety of applications. Metapath2vec~ was designed by formalizing meta-path based random walks to construct the neighborhoods of a vertex in heterogeneous networks. It takes the advantage of a heterogeneous skip-gram model to perform vertex embedding.\nMeta path based methods require either prior knowledge for optimal meta-path selection or extended computations for path length selection. To overcome these challenges, random walk based approaches have been proposed. Hussein et al.~ proposed the JUST model, which is a heterogeneous graph embedding approach using random walks with jump and stay strategies so that the aforementioned bias can be overcomed effectively. Another method  which does not require prior knowledge for meta-path definition is MPDRL~, meta-path discovery with reinforcement earning. This method employs the reinforcement learning algorithm to perform multi-hop reasoning to generate path instances and then further summarizes the important meta-paths using the Lowest Common Ancestor principle. Shi et al.~ proposed the HERec model, which utilizes the heterogeneous information network embedding for providing accurate recommendations in social networks. HERec is designed based on a random walk based approach for generating meaningful vertex sequences for heterogeneous network embedding. HERec can effectively adopt the auxiliary information in heterogeneous information networks. Other typical heterogeneous social network embedding approaches include, e.g., PTE~ and SHNE~.", "cites": [1155, 1419, 6094, 1421], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a well-structured synthesis of random walk methods in heterogeneous networks, integrating insights from various papers to highlight the evolution from path-based approaches to more sophisticated embedding strategies. It critically discusses limitations of earlier methods like PRA and introduces how newer techniques address these issues. The section also abstracts broader patterns, such as the role of meta-paths and the use of reinforcement learning for meta-path discovery, contributing to a conceptual understanding of the field."}}
{"id": "ba7ea660-f325-425d-9818-58b7145cd6a2", "title": "Random Walks in Time-varying Networks", "level": "subsubsection", "subsections": [], "parent_id": "3d125124-0f76-495a-849b-3574679d280e", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Random Walk Based Methods"], ["subsubsection", "Random Walks in Time-varying Networks"]], "content": "Network is evolving over time, which means that new vertices may emerge and new relations may appear. Therefore, it is significant to capture the temporal behaviour of networks in network analysis. Many efforts have been made to learn time-varying network embedding (e.g., dynamic networks or temporal networks)~. In contrast to static network embedding, time-varying NRL should consider the network dynamics, which means that old relationships may become invalid and new links may appear.\nThe key of time-varying NRL is to find a suitable way to incorporate the time characteristic into embedding via reasonable updating approaches. Nguyen et al.~ proposed the CTDNE model for continuous dynamic network embedding based on random walk with \"chronological\" paths which can only move forward as time goes on. Their model is more suitable for time-dependent network representation that can capture the important temporal characteristics of continuous-time dynamic networks. Results on various datasets show that CTDNE outperforms static NRL approaches. Zuo et al.~ proposed the HTNE model which is a temporal NRL approach based on the Hawkes process. HTNE can well integrate the Hawkes process into network embedding so that the influence of historical neighbors on the current neighbors can be accurately captured.\nFor unseen vertices in a dynamical network, GraphSAGE~ was presented to efficiently generate embeddings for new vertices in network. In contrast to methods that training embedding for every vertex in the network, GraphSAGE designs a function to generate embedding for a vertex with features of the neighborhoods locally. After sampling neighbors of a vertex, GraphSAGE uses different aggregators to update the embedding of the vertex. However, current graph neural methods are proficient of only learning local neighborhood information and cannot directly explore the higher-order proximity and the community structure of graphs.", "cites": [242], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of random walk-based methods for time-varying networks and mentions a few models such as CTDNE, HTNE, and GraphSAGE. It briefly connects them by highlighting their approaches to temporal behavior and inductive learning. However, it lacks deeper synthesis, critical evaluation, or abstraction, instead focusing on summarizing individual contributions without comparing them or drawing broader insights."}}
{"id": "208cf390-35c2-4759-957e-630de3176dd3", "title": "Graph Convolutional Networks", "level": "subsubsection", "subsections": [], "parent_id": "9610d7f6-5599-46e1-869b-238d95753770", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Deep Learning on Graphs"], ["subsubsection", "Graph Convolutional Networks"]], "content": "GCN works on the basis of grid structure domain and graph structure domain~.\n\\textbf{Time Domain and Spectral Methods}. Convolution is one of a common operation in deep learning. However, since graph lacks a grid structure, standard convolution over images or text cannot be directly applied to graphs. Bruna et al.~ extended the CNN algorithm from image processing to the graph using the graph Laplacian matrix, dubbed as spectral graph CNN. The main idea is similar to Fourier basis for signal processing. Based on~, Henaff et al.~ defined kernels to reduced the learning parameters by analogizing the local connection of CNNs on the image. Defferrard et al.~ provided two ways for generalizing CNNs to graph structure data based on graph theory. One method is to reduce the parameters by using polynomial kernel, and this method can be accelerated by using Chebyshev polynomial approximation. The other method is the special pooling method, which is pooling on the binary tree constructed from vertices. An improved version of  was introduced by Kipf and Welling~. The proposed method is a semi-supervised learning method for graphs. The algorithm employs an excellent and straightforward neural network followed by a layer-by-layer propagation rule, which is based on the first-order approximation of spectral convolution on the graph and can be directly acted on the graph.\nThere are some other time domain based methods. Based on the mixture model of CNNs, for instance, Monti et al.~ generalized the CNN to non-Euclidean space. Zhou and Li~ proposed a new CNN graph modeling framework, which designs two modules for graph structure data: K-order convolution operator and adaptive filtering module. In addition, the high-order adaptive graph convolution network (HA-GCN) framework proposed in~ is a general architecture that is suitable for many applications of vertices and graph centers. Manessi et al.~ proposed a dynamic graph convolution network algorithm for dynamic graphs. The core idea of the algorithm is to combine the expansion of graph convolution with the improved Long Short Term-Memory networks (LSTM) algorithm, and then train and learn the downstream recursive unit by using graph structure data and vertex features. The spectral based NRL methods have many applications, such as vertex classification~, traffic forecasting~, and action recognition~.\n\\textbf{Space Domain and Spatial Methods}. Spectral graph theory provides a convolution method on graphs, but many NRL methods directly use convolution operation on graphs in space domain. Niepert et al.~ applied graph labeling procedures such as Weisfeiler-Lehman kernel on graphs to generate unique order of vertices. The generated sub-graphs can be fed to the traditional CNN operation in space domain. Duvenaud et al.~ designed Neural fingerprints (FP), which is a spatial method using the first-order neighbors similar to the GCN algorithm. Atwood and Towsley~ proposed another convolution method, called diffusion-convolutional neural network, which incorporates transfer probability matrix and replaces the characteristic basis of convolution with diffusion basis. Gilmer et al.~ reformulated existing models into a single common framework, and exploited this framework to discover new variations. Allamanis et al.~ represented the structure of code from syntactic and semantic, and utilized the GNN method to recognize program structures.\nZhuang and Ma~ designed dual graph convolution networks (DGCN), which use diffusion basis and adjacency basis. DGCN uses two convolutions: one is the characteristic form of polynomial filter, and the other is to replace the adjacency matrix with the PPMI (Positive Pointwise Mutual Information) of the transition probability~. Dai et al.~ proposed the SSE algorithm, which uses asynchronous random to learn vertex representation so as to improve learning efficiency. In this model, a recursive method is adopted to learn vertex latent representation and the sampled batch data are utilized to update parameters. The recursive function of SSE is calculated from the weighted average of historical state and new state. Zhu et al.~ proposed a graph smoothing splines neural network which exploits non-smoothing node features and global topological knowledge such as centrality for graph classification. Gao et al.~ proposed a large scale graph convolution network (LGCN) based on vertex feature information. In order to adapt to the scene of large scale graphs, they proposed a sub-graph training strategy, which first trained the sampled sub-graph in a small batch. Based on a deep generative graph model, a novel method called DeepNC for inferring the missing parts of a network was proposed in~.\n\\begin{figure*}[htb]\n\t\\centering\n\t\\includegraphics[width=4.5in,height=1.5in]{fig6.png}\n\t\\caption{A brief history of algorithms of deep learning on graphs.}\n\t\\label{fig3}\n\\end{figure*}\nA brief history of deep learning on graphs is shown in Fig.~\\ref{fig3}. GNN has attracted lots of attention since 2015, and it is widely studied and used in various fields.", "cites": [25, 7012, 225, 8313, 3966, 4001, 228, 7213, 23, 7212, 213, 216, 235, 6124, 8332], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various graph convolutional network (GCN) approaches, citing multiple works but primarily listing methods and their core ideas without deep synthesis or comparison. While it attempts to categorize methods into time/spectral and space/spatial domains, it lacks a critical evaluation of their strengths, weaknesses, or trade-offs. Some abstraction is evident in grouping similar methods, but the narrative remains at the level of individual paper descriptions."}}
{"id": "b75e6fab-d3c1-4722-9e3c-4f5032b18f80", "title": "Graph Attention Networks", "level": "subsubsection", "subsections": [], "parent_id": "9610d7f6-5599-46e1-869b-238d95753770", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Deep Learning on Graphs"], ["subsubsection", "Graph Attention Networks"]], "content": "In sequence-based tasks, attention mechanism has been regarded as a standard~. GNNs achieve lots of benefits from the expanded model capacity of attention mechanisms. GATs are a kind of spatial-based GCNs~. It takes the attention mechanism into consideration when determining the weights of vertex's neighbors. Likewise, Gated Attention Networks (GAANs) also introduced the multi-head attention mechanism for updating the hidden state of some vertices~. Unlike GATs, GAANs employ a self-attention mechanism which can compute different weights for different heads. Some other models such as graph attention model (GAM) were proposed for solving different problems~. Take GAM as an example, the purpose of GAM is to handle graph classification. Therefore, GAM is set to process informative parts by visiting a sequence of significant vertices adaptively. The model of GAM contains LSTM network, and some parameters contain historical information, policies, and other information generated from exploration of the graph. Attention Walks (AWs) are another kind of learning model based on GNN and random walks~. In contrast to DeepWalk, AWs use differentiable attention weights when factorizing the co-occurrence matrix~.", "cites": [252, 180, 218, 38, 27], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic description of various graph attention models, including GATs, GAANs, and GAM, and mentions how they differ from or relate to other methods. However, it lacks deeper synthesis of ideas and does not establish a cohesive framework that connects the cited works in a meaningful way. There is minimal critical analysis or abstraction, focusing mainly on summarizing features rather than evaluating their strengths or limitations."}}
{"id": "66e692cf-fb7f-41d4-85ba-1bdd02dff439", "title": "Graph Auto-Encoders", "level": "subsubsection", "subsections": [], "parent_id": "9610d7f6-5599-46e1-869b-238d95753770", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Deep Learning on Graphs"], ["subsubsection", "Graph Auto-Encoders"]], "content": "GAE uses GNN structure to embed network vertices into low dimensional vectors. One of the most general solutions is to employ a multi-layer perception as the encoder for inputs~. Therein the decoder reconstructs neighborhood statistics of the vertex. PPMI or the first and the second nearest neighborhood can be taken into statistics~. Deep neural networks for graph representations (DNGR) employ PPMI. Structural deep network embedding (SDNE) employs stacked auto-encoder to maintain both the first-order and the second-order proximity. Auto-encoder~ is a traditional deep learning model, which can be classified as a self-supervised model~. Deep recursive network embedding (DRNE) reconstructs some vertices' hidden state rather than the entire graph~. It has been found that if we regard GCN as an encoder, and combine GCN with GAN or LSTM with GAN, then we can design the auto-encoder for graphs. Generally speaking, DNGR and SDNE embed vertices by the given structure features, while other methods such as DRNE learn both topology structure and content features~. Variational graph auto-encoder~ is another successful approach that employs GCN as an encoder and a link prediction layer as a decoder. Its successor, adversarially regularized variational graph auto-encoder~, adds a regularization process with an adversarial training approach to learn a more robust embedding.", "cites": [229, 322, 257], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of graph auto-encoders, listing methods like DNGR, SDNE, and DRNE, along with their characteristics. It makes minimal connections between the cited works and does not offer a novel synthesis. There is little critical evaluation of these methods, and the abstraction remains at a surface level, focusing on specific techniques rather than broader principles or theoretical implications."}}
{"id": "e5ad0c9e-68fa-412c-b001-9342caaabe3d", "title": "Graph Generative Networks", "level": "subsubsection", "subsections": [], "parent_id": "9610d7f6-5599-46e1-869b-238d95753770", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Deep Learning on Graphs"], ["subsubsection", "Graph Generative Networks"]], "content": "The purpose of graph generative networks is to generate graphs according to the given observed set of graphs. Many previous methods of graph generative networks have their own application domains. For example, in natural language processing, the semantic graph or the knowledge graph is generated based on the given sentences. Some general methods have been proposed recently. One kind of them considers the generation process as the formation of vertices and edges. Another kind is to employ generative adversarial training. Some GCNs based graph generative networks such as molecular generative adversarial networks (MolGAN) integrate GNN with reinforcement learning~. Deep generative models of graphs (DGMG) achieves a hidden representation of existing graphs by utilizing spatial-based GCNs~. There are some knowledge graph embedding algorithms based on GAN and Zero-Shot Learning~. Vyas et al.~ proposed a Generalized Zero-Shot learning model, which can find unseen semantic in knowledge graphs.", "cites": [222, 6125, 259, 6983], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of graph generative networks and mentions a few key methods, but it lacks deeper synthesis of ideas across the cited papers. It does not compare these approaches or evaluate their strengths and weaknesses, and it offers minimal abstraction or identification of broader trends or principles in the field."}}
{"id": "45ad40ac-209a-468b-a7d6-814a40408855", "title": "Graph Spatial-Temporal Networks", "level": "subsubsection", "subsections": [], "parent_id": "9610d7f6-5599-46e1-869b-238d95753770", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Graph Learning Models and Algorithms"], ["subsection", "Deep Learning on Graphs"], ["subsubsection", "Graph Spatial-Temporal Networks"]], "content": "Graph spatial-temporal networks simultaneously capture the spatial and temporal dependence of graphs. The global structure is included in the spatial-temporal graphs, and the input of each vertex varies with the change of time. For example, in traffic networks, each sensor records the traffic speed of a road continuously as a vertex, in which the edge of the traffic networks is determined by the distance between the sensor pairs~. The goal of a spatial-temporal network can be to predict future vertex values or labels, or to predict spatial-temporal graph labels. Recent studies in this direction have discussed the use of GCNs, the combination of GCNs with RNN or CNN, and recursive structures for graph structures~.\n\\begin{table*}[htbp]\n  \\centering\n  \\caption{Summary of graph learning methods and their applications}\n    \\begin{tabular}{clll}\n    \\toprule\n    \\textbf{Categories} & \\multicolumn{1}{c}{\\textbf{Algorithms}} & \\multicolumn{1}{c}{\\textbf{Neural Component}} & \\multicolumn{1}{c}{\\textbf{Applications}} \\\\\n    \\midrule\n    \\multicolumn{1}{c}{\\multirow{8}[16]{*}{Time Domain and\\newline{} Spectral Methods}}\n                       & SNLCN~ & Graph Neural Network &\n                       Classification\\\\\n\\cmidrule{2-4}          & DCN~ & Spectral Network  & Classification \\\\\n\\cmidrule{2-4}          & ChebNet~ & Convolution Network & Classification \\\\\n\\cmidrule{2-4}          & GCN~ & Spectral Network  & Classification \\\\\n\\cmidrule{2-4}          & HA-GCN~ & GCN   & Classification \\\\\n\\cmidrule{2-4}          & Dynamic GCN~ & GCN, LSTM & Classification \\\\\n\\cmidrule{2-4}          & DCRNN~ & Diffusion Convolution Network & Traffic Forecasting \\\\\n\\cmidrule{2-4}          & ST-GCN ~ & GCN   & Action Recognition \\\\\n    \\midrule\n    \\multicolumn{1}{c}{\\multirow{7}[14]{*}{Space Domain and\\newline{} Spatial Methods}} & PATCHY-SAN~ & Convolutional Network & \\multicolumn{1}{p{11.94em}}{Runtime Analysis,\\newline{}Feature Visualization,\\newline{}Graph Classification} \\\\\n\\cmidrule{2-4}          & \\multicolumn{1}{p{11.5em}}{Neural FP~} &       & Sub-graph Classification \\\\\n\\cmidrule{2-4}          & DCNN~ & DCNN  & Classification \\\\\n\\cmidrule{2-4}          & DGCN~ & \\multicolumn{1}{p{11.5em}}{Graph-Structure-Based Convolution, PPMI-Based Convolution.} & Classification \\\\\n\\cmidrule{2-4}          & SSE~ &       & Vertex Classification \\\\\n\\cmidrule{2-4}          & LGCN~ & Convolutional Neural Network & Vertex Classification \\\\\n\\cmidrule{2-4}          & STGCN~ & Gated Sequential Convolution & Traffic Forecasting \\\\\n    \\midrule\n    \\multicolumn{1}{l}{\\multirow{12}[24]{*}{Deep Learning\\newline{}  Model Based Methods}} & GATs~ & \\multicolumn{1}{l}{\\multirow{3}[6]{*}{Attention Neural Network}} & Classification \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & GAAN~ &       & Vertex Classification \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & GAM~ &       & Graph Classification \\\\\n\\cmidrule{2-4}          & Aws~ & \\multicolumn{1}{l}{\\multirow{4}[8]{*}{Auto-encoder Neural Network}} & \\multicolumn{1}{p{11.94em}}{Link Prediction,  \\newline{}Sensitivity Analysis,\\newline{}Vertex Classification} \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & SDNE~ &       & \\multicolumn{1}{p{11.94em}}{Classification,\\newline{} Link Prediction, \\newline{}Visualization} \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & DNGR~  &       & Clustering, Visualization \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & DRNE~&       & \\multicolumn{1}{p{11.94em}}{Regular Equivalence Prediction,\\newline{}Structural Role Classification,\\newline{}Network Visualization} \\\\\n\\cmidrule{2-4}          & MolGAN~  & \\multicolumn{1}{l}{\\multirow{2}[4]{*}{Generative Neural Network}} & Generative Model \\\\\n\\cmidrule{2-2}\\cmidrule{4-4}          & DGMG~ &       &  Molecule Generation \\\\\n\\cmidrule{2-4}          & DCRNN~ & Diffusion Convolution Network & Traffic Forecasting \\\\\n\\cmidrule{2-4}          & STGCN~ & Gated Sequential Convolution &  \\\\\n\\cmidrule{2-4}          & ST-GCN~ & GCNs  & Action Recognition \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\label{tab:addlabel}\n\\end{table*}", "cites": [25, 225, 8313, 27, 3966, 222, 259, 228, 180, 7213, 23, 8312, 7212, 213, 252, 6124, 8332], "cite_extract_rate": 0.68, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily lists graph spatial-temporal methods and their applications, with minimal synthesis beyond organizing them into categories. It lacks critical evaluation of the approaches and does not highlight limitations, trade-offs, or overarching principles. The content is largely descriptive, relying on a table to summarize methods without deeper analytical connections."}}
{"id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "title": "Applications", "level": "section", "subsections": ["718d1a88-a8ed-4488-8cc4-5730817b768a", "d71dfd60-8d36-44a3-8f75-1c26ea482baf", "74a3bf0e-c434-4b67-a633-48f77c44c49b", "6e167047-2d60-4243-89db-1578cae21cde", "f41d2e3e-418d-40a1-96ae-40e2ba10a214", "98e00854-a5c9-4846-b094-59c75cc496f6"], "parent_id": "64dae0de-0654-4c28-90d7-95856f2349e2", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"]], "content": "\\label{sec:appication}\nMany problems can be solved by graph learning methods, including supervised, semi-supervised, unsupervised, and reinforcement learning. Some researchers classify the applications of graph learning into three categories, i.e., structural scenarios, non-structural scenarios, and other application scenarios~. Structural scenarios refer to the situation where data are performed in explicit relational structures, such as physical systems, molecular structures, and knowledge graphs. Non-structural scenarios refer to the situation where data are with unclear relational structures, such as images and texts. Other application scenarios include, e.g., integrating models and combinatorial optimization problems. Table II lists the neural components and applications of various graph learning methods.", "cites": [550], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic classification of graph learning applications into structural, non-structural, and other scenarios but lacks deeper synthesis of the cited work. It mentions a categorization from a referenced survey without elaborating on the rationale or contrasting it with other classifications. There is minimal critical evaluation or abstraction beyond specific examples."}}
{"id": "d71dfd60-8d36-44a3-8f75-1c26ea482baf", "title": "Text", "level": "subsection", "subsections": [], "parent_id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"], ["subsection", "Text"]], "content": "Many data are in textual form coming from various resources like web pages, emails, documents (technical and corporate), books, digital libraries and customer complains, letters, patents, etc. Textual data are not well structured for obtaining any meaningful information as text often contains rich context information. There exist abundant applications around text, including text classification, sequence labeling, sentiment classification, etc. Text classification is one of the most classical problems in natural language processing. Popular algorithms proposed to handle this problem include GCNs~, GATs~, Text GCNs~, and Sentence LSTM~. Sentence LSTM has also been applied to sequence labeling, text generation, multi-hop reading comprehension, etc~. Syntactic GCN was proposed to solve semantic role labeling and neural machine translation~. Gated Graph Neural Networks (GGNNs) can also be used to address neural machine translation and text generation~. For relational extraction, Tree LSTM, graph LSTM, and GCN are better solutions~.", "cites": [7011, 8410, 180, 242, 1157, 274], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various graph learning methods applied to text tasks but does not deeply connect or synthesize ideas across the cited papers. It lacks critical evaluation or comparison of the methods and does not abstract broader patterns or principles from the literature, remaining primarily descriptive in nature."}}
{"id": "74a3bf0e-c434-4b67-a633-48f77c44c49b", "title": "Images", "level": "subsection", "subsections": [], "parent_id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"], ["subsection", "Images"]], "content": "Graph learning applications pertaining to images include social relationship understanding, image classification, visual question answering, object detection, region classification, and semantic segmentation, etc. For social relationship understanding, for instance, graph reasoning model (GRM) is widely used~. Since social relationships such as friendships are the basis of social networks in real world, automatically interpreting these relationships is important for understanding human behaviors. GRM introduces GGNNs to learn a propagation mechanism. Image classification is a classical problem, in which GNNs have demonstrated promising performance. Visual question answering (VQA) is a learning task that involves both computer vision and natural language processing. A VQA system takes the form of a certain pictures and its open natural language question as input, in order to generate a natural language answer as output. Generally speaking, VQA is question-and-answer for a given picture. GGNNs have been exploited to help with VQA~.", "cites": [6126], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of graph learning applications in images, mentioning specific tasks and a single model (GRM) with limited integration of cited material. It lacks critical evaluation of the methods or identification of broader trends. The narrative is primarily factual and does not synthesize or abstract insights from multiple sources."}}
{"id": "6e167047-2d60-4243-89db-1578cae21cde", "title": "Science", "level": "subsection", "subsections": [], "parent_id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"], ["subsection", "Science"]], "content": "Graph learning has been widely adopted in science. Modeling real-world physical systems is one of the most fundamental perspectives in understanding human intelligence. Representing objects as vertices and relations as edges between them is a simple but effective way to perform physics. Battaglia et al.~ proposed interaction networks (IN) to predict and infer abundant physical systems, in which IN takes objects and relationships as input. Based on IN, the interactions can be reasoned and the effects can be applied. Therefore, physical dynamics can be predicted. Visual interaction networks (VIN) can make predictions from pixels by firstly learning a state code from two continuous input frames per object~.\nOther graph networks based models have been developed to address chemistry and biology problems. Calculating molecular fingerprints, i.e., using feature vectors to represent molecular, is a central step. Researchers~ proposed neural graph fingerprints using GCNs to calculate substructure feature vectors. Some studies focused on protein interface prediction. This is a challenging issue with significant applications in biology. Besides, GNNs can be used in biomedical engineering as well. Based on protein-protein interaction networks, Rhee et al.~ used graph convolution and protein relation networks to classify breast cancer subtypes.", "cites": [4003], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of graph learning applications in science, particularly in physics and biology, referencing a few specific models like interaction networks and graph convolutional networks. However, it lacks deeper synthesis of ideas, critical evaluation of these models, and broader abstraction or identification of trends. The narrative is largely a summary of methods without connecting or contrasting them meaningfully."}}
{"id": "f41d2e3e-418d-40a1-96ae-40e2ba10a214", "title": "Knowledge Graphs", "level": "subsection", "subsections": [], "parent_id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"], ["subsection", "Knowledge Graphs"]], "content": "Various heterogeneous objects and relationships are regarded as the basis for a knowledge graph . GNNs can be applied in knowledge base completion (KBC) for solving the out-of-knowledge-base (OOKB) entity problem~. The OOKB entities are connected to  existing entities. Therefore, the embedding of OOKB entities can be aggregated from existing entities. Such kind of algorithms achieve reasonable performance in both settings of KBC and OOKB. Likewise, GCNs can also be used to solve the problem of cross-lingual knowledge graph alignment. The main idea of the model is to embed entities from different languages into an integrated embedding space. Then the model aligns these entities according to their embedding similarities.\nGenerally speaking, knowledge graph embedding can be categorized into two types: translational distance models and semantic matching models. Translational distance models aim to learn the low dimensional vector of entities in a knowledge graph by employing distance-based scoring functions. These methods calculate the plausibility as the distance between two entities after a translation measured by the relationships between them. Among current translational distance models, TransE~ is the most influential one. TransE can model the relationship of entities by interpreting them as translations operating on the low dimensional embedding. Inspired by TranE, TranH~ was proposed to overcome the disadvantages of TransE in dealing with 1-to-N, N-to-1, and N-to-N relations by introducing relation-specific hyperplanes. Instead of hyperplanes, TransR~ introduces relation-specific spaces to solve the flows in TransE. Meanwhile, various extensions of TransE have been proposed to enhance knowledge graph embeddings, such as TransD~ and TransF~. On the basis of TransE, DeepPath  incorporates reinforcement learning methods for learning relational paths in knowledge graphs. By designing a complex reward function involving accuracy, efficiency and path diversity, the path finding process is better controlled and more flexible.\nSemantic matching models utilize the similarity-based scoring functions. They measure the plausibility among entities by matching latent semantics of entities and relations in low dimensional vector space. Typical models of this type include RESCAL~, DistMult~, ANALOGY~, etc.", "cites": [1719, 1165, 6127, 1166], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of graph learning applications in knowledge graphs, mentioning specific models like TransE, TransH, TransR, and semantic matching approaches. However, it largely describes these models without deep integration or synthesis of the cited papers. There is limited critical evaluation of their strengths and weaknesses, and while it introduces a general categorization (translational vs. semantic models), it does not elevate the discussion to a meta-level or present novel insights."}}
{"id": "98e00854-a5c9-4846-b094-59c75cc496f6", "title": "Combinatorial Optimization", "level": "subsection", "subsections": [], "parent_id": "ad0cb974-233d-49a2-87c6-a6386791ce72", "prefix_titles": [["title", "Graph Learning: A Survey"], ["section", "Applications"], ["subsection", "Combinatorial Optimization"]], "content": "Classical problems such as traveling salesman problem (TSP) and minimum spanning tree (MST) have been solved by using different heuristic solutions. Recently, deep neural networks have been applied to these problems. Some solutions make further use of GNNs thanks to their structures. Bello et al.~ first proposed such kind of methods to solve TSP. Their method mainly contains two steps, i.e., a parameterized reward pointer network and a strategy gradient module for training. Khalil et al.~ improved this work with GNN and achieved better performance by two main procedures. First, they used structure2vec to achieve vertex embedding and then input them into Q-learning module for decision-making. This work also proves the embedding ability of GNN. Nowak et al. ~ focused on the secondary assignment problem, i.e., measuring the similarity of two graphs. The GNN model learns each graph's vertex embedding and uses the attention mechanism to match the two graphs. Other studies use GNNs directly as the classifiers, which can perform the intensive prediction on graphs with two sides. The rest of the model facilitates diverse choices and effective training.", "cites": [178, 187], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of how GNNs and deep learning have been applied to combinatorial optimization, mentioning a few papers and their methods. However, it lacks synthesis by not clearly connecting the cited works into a broader narrative, and offers little critical analysis or abstraction beyond specific examples. The presentation is primarily factual and system-focused."}}
