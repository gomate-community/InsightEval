{"id": "38bdd1a0-a46a-4a30-bb17-e8281f9a0b8e", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "fbe43cf7-0ec8-4b88-bda0-b6458577964d", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nIn the age of big data, data processing and analytics are fundamental, ubiquitous, and crucial to many organizations which undertake a digitalization journey to improve and transform their businesses and operations. Data analytics typically entails other key operations such as data acquisition, data cleansing, data integration, modeling, etc., before insights could be extracted. Big data can unleash significant value creation across many sectors such as healthcare and retail~. However, the complexity of data (e.g., high volume, high velocity, and high variety) presents many challenges in data analytics and hence renders the difficulty in drawing meaningful insights. To tackle the challenge and facilitate the data processing and analytics efficiently and effectively, a large number of algorithms and techniques have been designed and numerous learning systems have also been developed by researchers and practitioners such as Spark MLlib~, and Rafiki~.\nTo support fast data processing and accurate data analytics, a huge number of algorithms rely on rules that are developed based on human knowledge and experience. For example, shortest-job-first is a scheduling algorithm that chooses the job with the smallest execution time for the next execution. However, without fully exploiting characteristics of the workload, it can achieve inferior performance compared to a learning-based scheduling algorithm~. Another example is packet classification in computer networking which matches a packet to a rule from a set of rules. One solution is to construct the decision tree using hand-tuned heuristics for classification. Specifically, the heuristics are designed for a particular set of rules and thus may not work well for other workloads with different characteristics~. We observe three limitations of existing algorithms~. First, the algorithms are suboptimal. Useful information such as data distribution could be overlooked or not fully exploited by the rules. Second, the algorithm lacks adaptivity. Algorithms designed for a specific workload cannot perform well in another different workload. Third, the algorithm design is a time-consuming process. Developers have to spend much time trying a lot of rules to find one that empirically works.\nLearning-based algorithms have also been studied for data processing and analytics. Two types of learning methods are often used: supervised learning and reinforcement learning. They achieve better performance by direct optimization of the performance objective. Supervised learning typically requires a rich set of high-quality labeled training data, which could be hard and challenging to acquire. For example, configuration tuning is important to optimize the overall performance of a database management system (DBMS). There could be hundreds of tuning knobs that are correlated in discrete and continuous space. Furthermore, diverse database instances, query workloads, hardware characteristics render data collection infeasible, especially in the cloud environment. Compared to supervised learning, reinforcement learning shows good performance because it adopts a trial-and-error search and requires fewer training samples to find good configuration for cloud databases~. Another specific example would be query optimization in query processing. Database system optimizers are tasked to find the best execution plan for a query to reduce its query cost. Traditional optimizers typically enumerate many candidate plans and use a cost model to find the plan with minimal cost. The optimization process could be slow and inaccurate~. Without relying on an inaccurate cost model, deep reinforcement learning (DRL) methods improve the execution plan (e.g., changing the table join orders) by interacting with the database. Figure~\\ref{pic:QO} provides a typical workflow for query optimization using DRL. When the query is sent to the agent (i.e., DRL optimizer), it produces a state vector via conducting featurization on essential information, such as the accessed relations and tables. Taking the state as the input, the agent employs neural networks to produce the probability distribution of an action set, where the action set could contain all possible join operations as potential actions. Each action denotes a partial join plan on a pair of tables, and the state will be updated once an action is taken. After taking possible actions, a complete plan is generated, which is then executed by a DBMS to get the reward. In this query optimization problem, the reward can be calculated by the real latency. During the training process with the reward signal, the agent can improve the policy and produce a better join ordering with a higher reward (i.e., less latency).\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{img/example.pdf}\n\\caption{The Workflow of DRL for Query Optimization. A, B, C and D are four tables.}\n\\label{pic:QO}\n\\end{figure}\nReinforcement learning (RL)~ focuses on learning to make intelligent actions in an environment. The RL algorithm works on the basis of exploration and exploitation to improve itself with feedback from the environment. In the past decades, RL has achieved tremendous improvements in both theoretical and technical aspects~. Notably, DRL incorporates deep learning (DL) techniques to handle complex unstructured data and has been designed to learn from historical data and self-exploration to solve notoriously hard and large-scale problems (e.g., AlphaGo). In recent years, researchers from different communities have proposed DRL solutions to address issues in data processing and analytics. We categorize existing works using DRL from two perspectives: system and application. From the system's perspective, we focus on fundamental research topics ranging from general ones, such as scheduling, to system-specific ones, such as query optimization in databases. We shall also emphasize how it is formulated in the Markov Decision Process and discuss how the problem can be solved by DRL more effectively compared to traditional methods. Many techniques such as sampling and simulation are adopted to improve DRL training efficiency because workload execution and data collection in the real system could be time-consuming~. From the application's perspective, we shall cover various key applications in both data processing and data analytics to provide a comprehensive understanding of the DRL's usability and adaptivity. Many domains are transformed by the adoption of DRL, which helps to learn domain-specific knowledge about the applications.\nIn this survey, we aim at providing a broad and systematic review of recent advancements in employing DRL in solving data systems, data processing and analytics issues. In Section 2, we introduce the key concepts, theories, and techniques in RL to lay the foundations. To gain a deeper understanding of DRL, readers could refer to the recently published book~, which covers selected DRL research topics and applications with detailed illustrations. In Section 3, we review the latest important research works on using DRL for system optimization to support data processing and analytics. We cover fundamental topics such as data organization, scheduling, system tuning, index, query optimization, and cache management. In Section 4, we discuss using DRL for applications in data processing and analytics ranging from data preparation, natural language interaction to various real-world applications such as healthcare, fintech, E-commerce, etc. In Section 5, we highlight various open challenges and potential research problems. We conclude in Section 6. This survey focuses on recent advancements in exploring RL for data processing and analytics that spurs great interest, especially in the database and data mining community. There are survey papers discussing DRL for other domains. We refer readers to the survey of DRL for healthcare in , communications and networking in , and RL explainability in . Another work discusses how deep learning can be used to optimize database system design, and vice versa. In this paper, we use \"DRL\" and \"RL\" interchangeably.", "cites": [3582, 3584, 8669, 3576, 3579, 7144, 8670, 3577, 3581, 3578, 3580, 3583], "cite_extract_rate": 0.5217391304347826, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to explain how DRL addresses limitations of traditional data processing algorithms. It provides a critical comparison between heuristic-based and learning-based approaches, and abstracts the core ideas of DRL application in query optimization and scheduling. The narrative is coherent and shows understanding of broader patterns and principles."}}
{"id": "50913305-5a96-4228-a019-cb2dc0b435be", "title": "Basic Techniques", "level": "subsection", "subsections": [], "parent_id": "06d95f1f-0f67-46d9-8f8c-0e849e43dd7e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Basic Techniques"]], "content": "Based on the representation of MDP elements, basic techniques can be categorized into two classes: \\textit{model-based} method and \\textit{model-free} method. The main difference is whether the agent has access to model the environment, i.e. whether the agent knows the transition function and the reward function. These two functions are already known in the model-based method where \\textit{Dynamic Programming} (DP) and \\textit{Alpha-Zero}  are the classical methods which have achieved significant results in numerous applications. In these methods, agents are allowed to think ahead and plan future actions with known effects on the environment. Besides, an agent can learn the optimal policy from the planned experience which results in high sample efficiency.\nIn many RL problems, the reward and the transition function are typically unknown due to the complicated environment and its intricate inherent mechanism. For example, as illustrated in Figure~\\ref{pic:QO}, we are unable to obtain the actual latency as the reward in the joint query optimization example. Besides, in the stochastic job scheduling problem~, it is also impossible to directly model the transition function because of the randomness of the job arrivals in the practical scenarios. Hence, in these problems, agents usually employ model-free methods that can purely learn the policy from the experience gained during the interaction with the environment. Model-free methods can mainly be classified into two categories, namely the \\textit{value-based} method and the \\textit{policy-based} method. In the value-based method, the RL algorithm learns the optimal policy by maximizing the value functions. There are two main approaches in estimating the value functions that are \\textit{Mento-Carlo} (MC) methods and \\textit{Temporal difference} (TD) methods. MC methods calculate the $\\mathcal{V}(s)$ by directly applying its definition, that is Equation \\ref{eq:V_expectation}. MC methods can directly update the value functions once they get a new trajectory $\\tau$ as follows:\n\\begin{eqnarray}\n    \\mathcal{V}^\\pi(s) \\leftarrow \\mathcal{V}^\\pi(s) + \\alpha (\\mathcal{G}_{\\tau \\sim \\pi }(\\tau | s_0=s) - \\mathcal{V}^\\pi(s))\n\\end{eqnarray}\nwhere $\\alpha \\in [0,1)$ denotes the learning rate which controls the rate of updating the policy with new experiences. However, it has an obvious drawback that a complete trajectory requires the agent to reach a terminal state, while it is not practical in some applications, such as online systems. Different from MC methods, the TD method builds on the recursive relationship of value functions, and hence, can learn from the incomplete trajectory. Mathematically, the update of TD methods can be written as:\n\\begin{eqnarray}\n    \\mathcal{V}^\\pi(s) \\leftarrow \\mathcal{V}^\\pi(s) + \\alpha (R(s,a)+\\gamma \\mathcal{V}^\\pi(s') - \\mathcal{V}^\\pi(s))\n\\end{eqnarray}\nHowever, there is bias when estimating the function $\\mathcal{V}$ with TD methods because they learn from the recursive relationship. To reduce the bias, TD methods can extend the length of the incomplete trajectories and update the function $\\mathcal{V}$ by thinking more steps ahead, which is called $n$-steps TD methods. As $n$ grows to the length of whole trajectories, MC methods can be regarded as a special case of TD methods where function $\\mathcal{V}$ is an unbiased estimate. On the other side of the coin, as the length $n$ increases, the variance of the trajectory also increases. In addition to the above consideration, TD-based methods are more efficient and require less storage and computation, thus they are more popular among RL algorithms.\nIn value-based methods, we can obtain the optimal policy by acting greedily via Equation \\ref{eq:pi_maxQ}. The update of the function $\\mathcal{Q}$ with TD methods is similar to the update of the function $\\mathcal{V}$, and is as follows: $\\mathcal{Q}^\\pi(s,a) \\leftarrow \\mathcal{Q}^\\pi(s,a) + \\alpha (R(s,a)+\\gamma \\mathcal{Q}^{\\pi'}(s',a') - \\mathcal{Q}^\\pi(s,a))$ where the agent follows the policy $\\pi$ to take actions and follows the policy $\\pi'$ to maximize the function $\\mathcal{Q}$. If the two policies are the same, that is $\\pi' = \\pi$, we call such RL algorithms the \\textit{on-policy} methods where the \\textit{SARSA} is the representative method. In addition, other policies can also be used in $\\pi'$. For example, in \\textit{Q-learning}, the agent applies the greedy policy and updates the function $\\mathcal{Q}$ with the maximum value in its successor. Its update formula can be written as: $\\mathcal{Q}^\\pi(s,a) \\leftarrow \\mathcal{Q}^\\pi(s,a) + \\alpha (R(s,a)+\\gamma \\max_{a'} \\mathcal{Q}^\\pi(s',a') - \\mathcal{Q}^\\pi(s,a))$. Both value-based methods can work well without the model of the environment, and Q-learning directly learns the optimal policy, whilst SARSA learns a near-optimal policy during exploring. Theoretically, Q-learning should converge quicker than SARSA, but its generated samples have a high variance which may suffer from the problems of converging.\nIn RL, storage and computation costs are very high when there is a huge number of states or actions. To overcome this problem, DRL, as a branch of RL, adopts Deep Neural Network (DNN) to replace tabular representations with neural networks. For function $\\mathcal{V}$, DNN takes the state $s$ as input and outputs its state value $\\mathcal{V}_\\theta (s) \\approx \\mathcal{V}^\\pi (s)$ where the $\\theta$ denotes the parameter in the DNN. When comes to function $\\mathcal{Q}$, It takes the combination of the state $s$ and the action $a$ as input and outputs the value of the state-action pair $\\mathcal{Q}_\\theta (s, a) \\approx \\mathcal{Q}^\\pi (s, a)$, As for the neural networks, we can optimize them by applying the techniques that are widely used in deep learning (e.g. gradient descent). \\textit{Deep Q-learning network} (DQN) , as a representative method in DRL, combines the DNN with Q-learning and its loss function is as follows:\n\\begin{equation}\n    \\mathcal{L}_w = \\mathbf{E}_\\mathcal{D}[(R(s,a) + \\gamma \\max_{a^*\\in \\mathcal{A}}\\mathcal{Q}_w(s',a^*)-\\mathcal{Q}_w(s,a))^2] \\label{eq:DeepQ}\n\\end{equation}\nwhere $\\mathcal{D}$ denotes the \\textit{experience replay} which accumulates the generated samples and can stabilize the training process.\nPolicy-based methods are another branch of the model-free RL algorithm that have a clear representation of the policy $\\pi(a|s)$, and they can tackle several challenges that are encountered in value-based methods. For example, when the action space is continuous, value-based methods need to discretize the action which could increase the dimensionality of the problem, and memory and computation consumption. Value-based methods learn a deterministic policy that generates the action given a state through an optimal function $\\mathcal{Q}$ (i.e. $\\pi(s)=a$). However, for policy-based methods, they can learn a stochastic policy (i.e. $\\pi_\\theta (a_i|s)=p_i, \\sum_i p_i=1$) as the optimal policy, where $p_i$ denotes the probability of taking the action $a_i$ given a state $s$, and $\\theta$ denotes the parameters where neural networks can be used to approximate the policy. \\textit{Policy Gradient}  method is one of the main policy-based methods which can tackle the aforementioned challenges. Its goal is to optimize the parameters $\\theta$ by using the gradient ascent method, and the target can be denoted in a generalized expression:\n\\begin{equation}\n\\nabla_\\theta J(\\theta)=\\mathbf{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau) \\nabla_{\\pi_\\theta} \\log_{\\pi_{\\theta}}(a|s)]\n\\end{equation}\nThe specific proof process can refer to . Sampling via the MC methods, we will get the entire trajectories to improve the policy for the policy-based methods.\nAfter training, the action with higher rewards in expectation will have a higher probability to be chosen and vice versa. As for the continuous action, The optimal policy learned from the Policy Gradient is stochastic which still needs to be sampled to get the action. However, the stochastic policy still requires lots of samples to train the model when the search space is huge. \\textit{Deterministic Policy Gradient} (DPG) , as an extension of the Policy Gradient, overcomes this problem by using a stochastic policy to perform sampling while applying deterministic policy to output the action which demands relatively fewer samples.\nBoth value-based methods and policy-based methods have their strengths and weaknesses, but they are not contradictory to each other. \\textit{Actor-Critic} (AC) method, as the integration of both methods, divides the model into two parts: actor and critic. The actor part selects the action based on the parameterized policy and the critic part concentrates on evaluating the value functions. Different from previous approaches, AC evaluates the advantage function $\\mathcal{A}^\\pi (s, a) = \\mathcal{Q}^\\pi (s, a) - \\mathcal{V}^\\pi (s)$ which reflects the relative advantage of a certain action $a$ to the average value of all actions. The introduction of the value functions also allows AC to update by step through the TD method, and the incorporation of the policy-based methods makes AC be suitable for continuous actions. However, the combination of the two methods also makes the AC method more difficult to converge. Moreover, \\textit{Deep Deterministic Policy Gradient} (DDPG) , as an extension of the AC, absorbs the advanced techniques from the DQN and the DPG which enables DDPG to learn the policy more efficiently.\nIn all the above-mentioned methods, there always exists a trade-off between exploring the unknown situation and exploiting with learned knowledge. On the one hand, exploiting the learned knowledge can help the model converge quicker, but it always leads the model into a local optimal rather than a globally optimal. On the other hand, exploring unknown situations can find some new and better solutions, but always being in the exploring process causes the model hard to converge. To balance these two processes, researchers have been devoting much energy to finding a good heuristics strategy, such as $\\epsilon-greedy$ strategy, Boltzmann exploration (Softmax exploration), upper confidence bound (UCB) algorithm , Thompson sampling , and so on. Here, we consider the $\\epsilon-greedy$, a widely used exploration strategy, as an example. $\\epsilon-greedy$ typically selects the action with the maximal Q value to exploit the learned experience while occasionally selecting an action evenly at random to explore unknown cases. $\\epsilon-greedy$ exploration strategy with $m$ actions can be denoted as follow:\n\\begin{equation}\n\\pi(a|s)=\n\\left\\{\n             \\begin{array}{ll}\n             \\epsilon/m + (1-\\epsilon) & a^* = arg\\max_{a\\in \\mathcal{A}}\\mathcal{Q}(s,a), \\\\\n             \\epsilon/m & a \\not= a^*.\n             \\end{array} \\label{e-greedy}\n\\right.\n\\end{equation}\n$\\epsilon \\in [0,1)$ is an exploration factor. The agent is more likely to select the action at random when the $\\epsilon$ is closer to 1, and the $\\epsilon$ will be continuously reduced during the training process.", "cites": [3585, 620], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of basic RL techniques, integrating concepts from cited papers such as Q-learning and policy gradient methods. It explains the differences between model-based and model-free approaches and the trade-offs between MC and TD methods. While it does not deeply critique the limitations or compare the cited works in a nuanced way, it identifies general challenges (e.g., variance in Q-learning) and abstracts key principles, such as the use of DNNs to address scalability issues in large state/action spaces."}}
{"id": "e3d62651-4a42-48b0-801e-80c4dd4d7471", "title": "Data Sampling", "level": "subsubsection", "subsections": [], "parent_id": "bb2481be-ac7c-4c4d-9326-9eca2d3cc721", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Advanced Techniques"], ["subsubsection", "Data Sampling"]], "content": "Data sampling is one of the most important concerns in training the DRL in data processing and analytics. In most applications, the sample generation process costs a great amount of time and computation resources. For example, a sample may refer to an execution run for workload and repartitioning for the database, which can take about 40 minutes. Hence, to train the model with limited samples, we need to increase data utilization and reduce data correlation.\n\\textbf{Data utilization}:\nMost DRL algorithms train the optimal policy and sample data at the same time. Instead of dropping samples after being trained, \\textit{experience replay} accumulates the samples in a big table where samples are randomly selected during the learning phase. With this mechanism, samples will have a higher utilization rate and a lower variance, and hence, it can stabilize the training process and accelerate the training convergence. Samples after several iterations may differ from the current policy, and hence, \\textit{Growing-batch}  can continuously refresh the table and replace these outdated samples. In addition, samples that are far away from the current policy should be paid more attention and \\textit{Prioritized Experience Replay} uses TD error as the priority to measure the sample importance, and hence, focus more on learning the samples with high errors. In a nutshell, with the experience replay, DRL cannot only stable the learning phase but also efficiently optimize the policy with fewer samples.\n\\textbf{Data correlation}:\nStrong correlation of training data is another concern that may lead the agent to learn a sub-optimal solution instead of the globally optimal one. Apart from the experience replay, the mechanism of the distributed environments is another research direction to alleviate this problem. For example, the \\textit{asynchronous advantage actor-critic} (A3C)  and \\textit{Distributed PPO} (DPPO)  apply multi-threads to build multiple individual environments where multiple agents take actions in parallel, and the update is calculated periodically and separately which can accelerate the sampling process and reduce the data correlation.", "cites": [3587, 1390, 3586], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes well by connecting experience replay, growing-batch, and prioritized experience replay across the cited papers to highlight their roles in improving data utilization. It also links distributed environments to reducing data correlation. However, the critical analysis is limited, as it describes methods without evaluating their trade-offs or limitations. There is moderate abstraction in generalizing these techniques to the broader context of DRL in data processing."}}
{"id": "2ac7e23d-1b38-47aa-b409-d225a83e47a8", "title": "Model Efficiency", "level": "subsubsection", "subsections": [], "parent_id": "bb2481be-ac7c-4c4d-9326-9eca2d3cc721", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Advanced Techniques"], ["subsubsection", "Model Efficiency"]], "content": "\\label{Model Efficiency}\nRL model with better efficiency is the major driving force of the development of RL, and there are many researchers improving it from three major aspects, namely policy, reward function, and value function.\n\\textbf{Policy}:\nThe policy-related techniques focus on stably and effectively learning a comprehensive policy, and the advanced techniques to efficiently learn the policy can be classified into three parts in detail, which are policy exploration, policy representation, and policy optimization.\na) \\textit{Policy exploration}: Its target is to explore as many actions as possible during the training process in case the policy will be trapped into the local optimal. For example, \\textit{entropy regularisation}  adds the entropy of the actions' probabilities into the loss item which can sufficiently explore the actions. Besides, adding noise to the action is another research direction to increase the randomness into policy exploration. The DDPG applies an Ornstein–Uhlenbeck process  to generate temporal noise $\\mathcal{N}$ which are directly injected into policy.\n\\textit{Noisy-Net}  incorporates the noise into the parameters of neural networks which is easy to implement, and it shows a better performance than the $\\epsilon-greedy$ and entropy regularisation methods.\nFurther, Plappert et al.  investigate an effective way to combine the parameter space noise to enrich the exploratory behaviors which can benefit both on-policy methods and off-policy methods.\nb) \\textit{Policy representation}: The states in some RL problems are in a huge dimension which causes challenges during the training. To approximate a better policy, a branch of DRL models improve the policy representation by absorbing convolutional neural networks (CNN) into DQN to analyze the data, such as \\textit{Dueling DQN} , \\textit{DRQN} , and so on. In addition, DRQN also incorporates the LSTM structure to increase the capacity of the policy which is able to capture the temporal information, such as speed, direction.\nc) \\textit{Policy optimization}: The update of the value functions following the Equation \\ref{eq:bellman_v} and \\ref{eq:bellman_q} tends to overestimate the value functions and introduce a bias because they learn estimates from the estimates. Mnih et al. separate the two estimation process by using two same Q-networks which can reduce the correlation of two estimation processes and hence, stabilize the course of training. However, the action with the maximum Q-value may differ between two Q-networks which will be hard for convergence. and \\textit{Double DQN} (DDQN)  alleviate the issue by disaggregating the step of selecting the action and calculating the max Q-value.\nWhen we apply the policy-based RL methods, the learning rate of the policy plays an essential role in achieving superior performance. A higher learning rate can always maximize the improvement on a policy by step, but it also causes the instability of the learning phase. Hence, The \\textit{Trust Region Policy Optimization} (TRPO)  builds constraints on the old policy and new policy via KL divergence to control the change of the policy in an acceptable range. With this constraint, TRPO can iteratively optimize the policy via a surrogate objective function which can monotonically improve policies. However, the design of the KL constraint makes it hard to be trained, and \\textit{Proximal Policy Optimization} (PPO)  simplifies the constraint through two ways: adding it into the objective function, designing a clipping function to control the update rate. Empirically, PPO methods are much simpler to implement and are able to perform at least as well as TRPO.\n\\textbf{Reward}:\nReward function as one of the key components in the MDP plays an essential role in the RL. In some specific problems, the agent has to achieve multiple goals which may have some relationships. For example, the robot can only get out through the door only if it has already found the key. To tackle this challenge, \\textit{Hierarchical DQN}  proposes two levels of hierarchical RL (HRL) models to repeatedly select a new goal and achieve the chosen goal. However, there is a limitation that the goal needs to be manually predefined which may be unknown or unmeasurable in some environments, such as the market and the effect of a drug. To overcome it, \\textit{Inverse RL} (IRL)  learns the rewards function from the given experts' demonstrations (i.e. the handcraft trajectories), but the agent in IRL can only prioritize the entire trajectories over others. It will cause a shift when the agent comes to a state that never appears before, and \\textit{Generative Adversarial Imitation Learning} (GAIL) , as an imitation learning algorithm, applies adversarial training methods to generate fake samples and is able to learn the expert's policy explicitly and directly.\n\\textbf{Value}:\nAs we have mentioned earlier, the tabular representation of the value functions has several limitations which can be alleviated via DRL. Different from directly taking the state-action pair as the input to calculate the Q-function, \\textit{Dueling DQN}  estimates its value by approximating two separate parts that are the state-values and the advantage values, and hence, can distinguish whether the value is brought by the state or the action.\nThe aforementioned advanced algorithms and techniques improve and enhance the DRL from different perspectives, which makes DRL-based algorithms be a promising way to improve data processing and analytics. We observe that problems with the following characteristics may be amenable to DRL-based optimization. First, problems are incredibly complex and difficult. The system and application involve a complicated operational environment (e.g., large-scale, high-dimensional states) and internal implementation mechanisms, which is hard to construct a white-box model accurately. DRL can process complex data and learn from experience generated from interacting, which is naturally suitable for data processing and analytics where many kinds of data exist and are processed frequently. Second, the optimization objectives can be represented and calculated easily as the reward because the RL agent improves itself towards maximizing the rewards and rewards could be computed a lot of times during training. Third, the environment can be well described as MDP. DRL has been shown to solve MDP with theoretical guarantees and empirical results. Thus, problems involving sequential decision making such as planning, scheduling, structure generation (e.g., tree, graph), and searching could be expressed as MDP and a good fit for DRL. Fourth, collecting required labels of data massively is hard. Compared to supervised learning, DRL can utilize data efficiently to gain good performance.\n\\section{\nData System Optimizations}\nDRL learns knowledge about the system by interacting with it and optimizes the system. In this section, we focus on several fundamental aspects with regards to system optimization in data processing and analytics including data organization, scheduling, tuning, indexing, query optimization, and cache management. We discuss how each problem is formulated in MDP by defining three key elements (action, state, and reward) in the system and solved by DRL. Generally, the states are defined by some key characteristics of the system. The actions are possible decisions (e.g., system configuration), that affect the system performance and the reward is calculated based on the performance metrics (e.g. throughput, latency). Table 1 presents a summary of representative works and the estimated dimension ranges on the state and action space of each work are added as signals on the DRL training difficulty. As a comparison, OpenAI Five, a Dota-playing AI, observes the state as 20,000 numbers representing useful game information and about 1,000 valid actions (like ordering a hero to move to a location) for per hero. Dota is a real-time strategy game between two teams of five players where each player controls a character called a “hero”.", "cites": [3591, 3589, 1390, 1408, 3592, 3590, 2219, 1391, 3588, 1409, 8671], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple cited papers on policy, reward, and value function efficiency, presenting a coherent structure across different aspects of DRL. It critically evaluates methods like DDPG, NoisyNet, TRPO, and PPO, noting limitations and improvements. The abstraction is strong, as it generalizes techniques into categories (exploration, representation, optimization) and connects them to the broader suitability of DRL for data processing and analytics."}}
{"id": "04062f49-1862-4737-b78b-7f8c7f1a8e74", "title": "Data Partitioning", "level": "subsubsection", "subsections": [], "parent_id": "4241dff8-50c1-4dd7-9321-93aff749448e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Data Organization"], ["subsubsection", "Data Partitioning"]], "content": "Effective data partitioning strategy is essential to accelerate data processing and analytics by skipping irrelevant data for a given query. It is challenging as many factors need to be considered, including the workload and data characteristics, hardware profiles, and system implementation.\nIn data analytics systems, data is split into blocks in main memory or secondary storage, which are accessed by relevant queries. A query may fetch many blocks redundantly and, therefore, an effective block layout avoids reading unnecessary data and reduces the number of block accesses, thereby improving the system performance. Yang et al. propose a framework called the qd-tree that partitions data into blocks using DRL over the analytical workload. The qd-tree resembles the classic k-d tree and describes the partition of multi-dimensional data space where each internal node splits data using a particular predicate and represents a subspace. The data in the leaf node is assigned to the same block. In the MDP, each state is a node representing the subspace of the whole data and featured as the concatenation of range and category predicates. After the agent takes an action to generate two child nodes, two new states will be produced and explored later. The available action set is the predicates parsed from workload queries. The reward is computed by the normalized number of skipped blocks over all queries. They do not execute queries and a sampling technique is used to estimate the reward efficiently. The formulation of using DRL to learn a tree is similar to NeuroCuts that learns a tree for packet classification. However, the qd-tree may not support a complex workload containing user-defined functions (UDFs) queries.\nHorizontal partitioning in the database chooses attributes of large tables and splits them across multiple machines to improve the performance of analytical workloads. The design relies on either the experience of database administrators (DBAs) or cost models that are often inaccurate to predict the runtime for different partitions. Data collection is too challenging and costly to train the accurate supervised learning model in the cloud environment. Hilprecht et al. learn to partition using DRL on analytical workloads in cloud databases, on the fact that DRL is able to efficiently navigate the partition search and requires less training data. In the MDP, the state consists of two parts. The database part encodes whether a table is replicated, an attribute is used for partitioning, and which tables are co-partitioned. The workload part incorporates normalized frequencies of representative queries. Supported actions are: partitioning a table using an attribute, replicating a table, and changing tables co-partition. The reward is the negative of the runtime for the workload. One challenge is that the cost of database partitioning is high during training. To alleviate the problem, the agent is trained in the simulation environment and is further refined in the real environment by estimating the rewards using sampling. One limitation is that it may not support new queries well because only the frequency features of queries are considered. Durand et al. in  utilize DRL to improve vertical partitioning that optimizes the physical table layout. They show that the DQN algorithm can easily work for a single workload with one table but is hard to generalize to random workloads.\nFor UDFs analytics workloads on unstructured data, partitioning is more challenging where UDFs could express complex computations and functional dependency is unavailable in the unstructured data. Zou et al. propose the Lachesis system to provide automatic partitioning for non-relational data analytics. Lachesis translates UDFs to graph-based intermediate representations (IR) and identifies partition candidates based on the subgraph of IR as a two-terminal graph. Lachesis adopts DRL to learn to choose the optimal candidate. The state incorporates features for each partition extracted from historical workflows: frequency, the execution interval, time of the most recent run, complexity, selectivity, key distribution, number, and size of co-partition. In addition, the state also incorporates other features such as hardware configurations. The action is to select one partition candidate. The reward is the throughput speedup compared to the average throughput of the historical executions of applications. To reduce the training time, the reward is derived from historical latency statistics without partitioning the data when running the applications. One limitation is that Lachesis largely depends on historical statistics to design the state and calculate the reward, which could lead to poor performance when the statistics are inadequate.", "cites": [3581, 3577], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of different DRL approaches to data partitioning, connecting qd-tree with related works like NeuroCuts and Lachesis. It includes critical analysis by highlighting limitations such as inadequate support for UDFs or new queries, and dependence on historical statistics. While it identifies some patterns (e.g., use of MDPs and reward functions based on performance metrics), it does not abstract to a broader theoretical framework or unify the approaches into a meta-level insight."}}
{"id": "cdc47763-ed1a-40ed-b9ec-c9b0b21a9eea", "title": "Data Compression", "level": "subsubsection", "subsections": [], "parent_id": "4241dff8-50c1-4dd7-9321-93aff749448e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Data Organization"], ["subsubsection", "Data Compression"]], "content": "Data compression is widely employed to save storage space. The effectiveness of a compression scheme however relies on the data types and patterns. In time-series data, the pattern can change over time and a fixed compression scheme may not work well for the entire duration. Yu et al. propose a two-level compression framework, where a scheme space is constructed by extracting global features at the top level and a compression schema is selected for each point at the bottom level. The proposed AMMMO framework incorporates compression primitives and the control parameters, which define the compression scheme space. Due to the fact that the enumeration is computationally infeasible, the framework proposes to adopt DRL to find the compression scheme. The agent takes a block that consists of 32 data points with the compressed header and data segment, timestamps, and metrics value as the state. The action is to select a scheme from compression scheme space and then the compression ratio is computed as the reward. The limitation is that the method may not work for other data types like images and videos.\n\\begin{table*}[t]\n\\footnotesize \n\\centering\n\\caption{Representative Works using DRL for Data System Optimizations. D(X) denotes the approximate dimension of X space. }\n\\begin{tabular}{|p{1.4cm}|p{2.7cm}|p{1.6cm}|p{1.3cm}|p{1.5cm}|p{3.2cm}|p{1.1cm}|}\n\\hline\\hline\n\\textbf{Domain} & \\textbf{Work} & \\textbf{Algorithm} & \\textbf{D(State)}& \\textbf{D(Action)} & \\textbf{DRL-based Approach}&\\textbf{Open Source}\\\\\n\\hline\nData organization &Analytical system data partition & PPO & 10 - 100 & 100 - 1000 & Exploit workload patterns and Generate the tree &NO \\\\\n\\cline{2-7}\n&Database horizontal partition~&DQN&100&10 & Navigate the partition search efficiently&NO \\\\\n\\cline{2-7}\n&UDF-centric workload data partition~&A3C&10& 1-10 & Exploit the features of partition and search&YES \\\\\n\\cline{2-7}\n&Time series data compression~&PG&100& 10& Search parameters interactively& NO \\\\\n\\hline\nScheduling &Distributed job processing~&PG&100& 10 & Exploit the job dependencies and learn schedule decision&YES\\\\\n\\cline{2-7}\n&Distributed stream data~&DDPG&100& 10-100 & Learn schedule decision&NO \\\\\n\\hline\nTuning &Database configuration~~&DDPG&100 & 10 & Search configuration parameters interactively&YES \\\\\n\\hline\nIndex &Index Selection~&CEM&100& 10& Search the index interactively&NO\\\\\n\\cline{2-7}\n&R-tree construction~&DQN& 10-100& 10 & Learn to generate the tree&NO \\\\\n\\hline\nQuery Optimization &Join order selection~&PG, DQN, ...&10-100&1-10&Learn to decide the join order&Only~ \\\\\n\\hline\nCache Management &View Materialization~&DQN&100 & 10 & Model the problem as IIP and solve&NO \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\label{tab:1}\n\\end{table*}", "cites": [3595, 3579, 3593, 3594, 8670, 3576, 3581], "cite_extract_rate": 0.4666666666666667, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the AMMMO framework for data compression using DRL but does not deeply integrate this with other cited works. It includes a table that lists various DRL applications in data systems, but the narrative lacks synthesis or comparative insights between these approaches. There is minimal critical analysis or abstraction into broader trends or principles."}}
{"id": "0ab3daa2-5b8e-41b3-959c-0836d42276db", "title": "Scheduling", "level": "subsection", "subsections": [], "parent_id": "06d95f1f-0f67-46d9-8f8c-0e849e43dd7e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Scheduling"]], "content": "Scheduling is a critical component in data processing and analytics systems to ensure that resources are well utilized. Job scheduling in a distributed computing cluster faces many challenging factors such as workload (e.g., job dependencies, sizes, priority), data locality, and hardware characteristics. Existing algorithms using general heuristics such as shortest-job-first do not utilize these factors well and fail to yield top performance. To this end, Mao et al. propose Decima to learn to schedule jobs with dependent stages using DRL for data processing clusters and improve the job completion time. In the data processing systems such as Hive, Pig, Spark-SQL, jobs could have up to hundreds of stages and many stages run in parallel, which are represented as directed acyclic graphs (DAGs) where the nodes are the execution stages and each edge represents the dependency. To handle parallelism and dependencies in job DAGs, Decima first applies graph neural network (GNN) to extract features as the state instead of manually designing them while achieving scalability. Three types of feature embeddings are generated. Node embedding captures information about the node and its children including the number of remaining tasks, busy and available executors, duration, and locality of executors. Job embedding aggregates all node embeddings in the job and cluster embedding combines job embeddings. To balance possible large action space and long action sequences, The action determines the job stage to be scheduled next and the parallelism limit of executors. The reward is based on the average job completion time. To train effectively in a job streaming environment, Decima gradually increases the length of training jobs to conduct curriculum learning. The variance reduction technique is applied to handle stochastic job arrivals for robustness. However, we note that Decima is non-preemptive and does not re-schedule for higher priority jobs.\nIn distributed stream data processing, streams of continuous data are processed at scale in a real-time manner. The scheduling algorithm assigns workers to process data where each worker uses many threads to process data tuples and aims to minimize average data tuple processing time. Li et al. design a scheduling algorithm using DRL for distributed stream data processing, which learns to assign tuples to work threads. The state consists of the scheduling plan (e.g., the current assignment of workers) and the workload information (e.g., tuple arrival rate). The action is to assign threads to machines. The reward is the negative tuple processing time on average. The work shows that DQN does not work well because the action space is large and applies DDPG to train the \\textit{actor-critic} based agent instead. To find a good action, the proposed method looks for k nearest neighbors of the action that the \\textit{actor} network outputs and selects the neighbor with the highest value that the \\textit{critic} network outputs. The algorithm is implemented on Apache Storm and evaluated with representative applications: log stream processing, continuous queries, and word count.\nMany works have been recently proposed to improve scheduling using DRL. Query scheduling determines the execution order of queries, which has a great influence on query performance and resource utilization in the database system. SmartQueue improves query scheduling by leveraging overlapping data access among queries and learns to improve cache hits using DRL. In addition, Tim et al. design a scheduling system in SageDB using RL techniques. Other works using RL for scheduling include Bayesian RL for scheduling in heterogeneous clusters, operation scheduling in devices, application container scheduling in clusters, etc.", "cites": [3579, 3585, 3597, 3594, 3596], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple cited works, connecting DRL applications in job scheduling for batch and stream processing systems. It provides critical evaluations such as Decima's non-preemptive nature and DQN's limitations due to large action spaces. While it identifies some broader themes (e.g., the use of actor-critic methods, feature embeddings), it stops short of forming a unifying theoretical framework."}}
{"id": "d2259e60-6b72-49d5-a98e-f562d3ef0c1d", "title": "Database Index Selection", "level": "subsubsection", "subsections": [], "parent_id": "435d117a-3e40-4f69-901c-4f0ac402f0ba", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Indexing"], ["subsubsection", "Database Index Selection"]], "content": "Database index selection considers which attributes to create an index to maximize query performance. Sharma et al. show how DRL can be used to recommend an index based on a given workload. The state encodes selectivity values for workload queries and columns in the database schema and current column indexes. The action is to create an index on a column. The reward is the improvement compared to the baseline without indexes. The experiments show that the approach can perform as well or better as having indexes on all columns. Sadri et al. utilize DRL to select the index for a cluster database where both query processing and load balancing are considered. Welborn et al. optimize the action space design by introducing task-specific knowledge for index selection tasks in the database. However, these works only consider the situation where single-column indexes are built. Lan et al. propose both single-attribute and multi-attribute indexes selection using DRL. Five rules are proposed to reduce the action and state space, which help the agent learn effective strategy easier. The method uses what-if caller to get the cost of queries under specific index configurations without building indexes physically. These works conduct basic experiments with small and simple datasets. Extensive and large-scale experiments using real datasets are therefore needed to benchmark these methods to ensure that they can scale well.", "cites": [3595, 3598], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of DRL-based approaches for database index selection by citing multiple papers and pointing out that most focus on single-column indexes. It identifies a common limitation—lack of real-scale testing—and suggests the need for more extensive evaluation. While it connects these works and highlights their shared goals, it does not form a novel framework or deeply critique the methodologies."}}
{"id": "aa04e438-6b99-4f53-a189-a8efacabdbeb", "title": "Index Structure Construction", "level": "subsubsection", "subsections": [], "parent_id": "435d117a-3e40-4f69-901c-4f0ac402f0ba", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Indexing"], ["subsubsection", "Index Structure Construction"]], "content": "The learned index is proposed recently as an alternative index to replace the B$^+$-Tree and bloom filter by viewing indexes as models and using deep learning models to act as indexes. DRL can enhance the traditional indexes instead of replacing them.\nHierarchical structures such as the B$^+$-tree and R-tree are important indexing mechanisms to locate data of interest efficiently without scanning a large portion of the database. Compared to the single dimensional counterpart, the R-tree is more complex to optimize due to bounding box efficiency and multi-path traversals. Earlier conventional approaches use heuristics to determine these two operations (i.e. choosing the insertion subtree and splitting an overflowing node) during the construction of the R-tree. Gu et al. propose to use DRL to replace heuristics to construct the R-tree and propose the RLR-tree. The approach models two operations ChooseSubtree and Split as two MDPs respectively and combines them to generate an R-Tree. For ChooseSubtree, the state is represented as the concatenation of the four features (i.e., area, perimeter, overlap, occupancy rate) of each selected child node. More features are evaluated but do not improve the performance in the reported experiments. The action is to select a node to insert from top-k child nodes in terms of the increase of area. The reward is the performance improvement from the RLR-tree. For Split MDP, the state is the areas and perimeters of the two nodes created by all top-k splits in the ascending order of total area. The action is to choose one split rule from k rules and the reward is similar to that of ChooseSubtree. The two agents are trained alternately. As expected, the optimizations render the RLR-tree improved performance in range and KNN queries.\nGraphs can be used as effective indexes to accelerate nearest neighbors search. Existing graph construction methods generally propose different rules to generate graphs, which cannot provide adaptivity for different workloads. Baranchuk et al. employ DRL to optimize the graph for nearest neighbors search. The approach learns the probabilities of edges in the graph and tries to maximize the search efficiency. It considers the initial graph and the search algorithm as the state. The action is to keep an edge or not. The reward is the performance for search. It chooses the TRPO algorithm to train. The reported experimental results show that the agent can refine state-of-the-art graphs and achieve better performance. However, this approach does not learn to explore and add new edges to the initial graph that may affect the performance.\nSearching and constructing a new index structure is another line of interesting research~. Inspired by Neural Architecture Search (NAS), Wu et al. propose an RNN-based neural index search (NIS) framework that employs DRL to search the index structures and parameters given the workload. NIS can generate tree-like index structures layer by layer via formalizing abstract ordered blocks and unordered blocks, which can provide a well-designed search space. The keys in the ordered block are sorted in ascending order, and the skip list or B$^+$-Tree can be used. The keys in the unordered block are partitioned using customized functions and the hash bucket can be used. Overall, the whole learning process is similar to that of NAS.", "cites": [8672, 1391, 684, 3599, 3600], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches to index structure construction using DRL, connecting key ideas from different papers into a cohesive narrative. It offers some critical insights, such as noting that adding more features in Gu et al.'s work does not improve performance, and that Baranchuk et al.'s method does not explore adding new edges. It also abstracts to a general pattern of using DRL for adaptive index design but stops short of forming a unified theoretical framework across all approaches."}}
{"id": "dc550fd1-8926-4814-b0e3-4547f84935df", "title": "Query Optimization", "level": "subsection", "subsections": [], "parent_id": "06d95f1f-0f67-46d9-8f8c-0e849e43dd7e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Query Optimization"]], "content": "Query optimization aims to find the most efficient way to execute queries in database management systems. There are many different plans to access the query data that can have a large processing time variance from seconds to hours. The performance of a query plan is determined mostly by the table join orders. Traditionally, query optimizers use certain heuristics combined with dynamic programming to enumerate possible efficient execution plans and evaluate them using cost models that could produce large errors. Marcus et al. propose Rejoin that applies DRL to learn to select better join orders utilizing past experience. The state encodes join tree structure and join predicates. The action is to combine two subtrees, where each subtree represents an input relation to join. The reward is assigned based on the cost model in the optimizer. The experiments show that ReJOIN can match or outperform the optimizer in PostgreSQL. Compared to ReJoin, DQ presents an extensible featurization scheme for state representation and improves the training efficiency using the DQN algorithm. Heitz et al. compare different RL algorithms including DQN, DDQN, and PPO for join order optimization and use a symmetric matrix to represent the state instead of vector. Yu et al. introduce a graph neural network (GNN) with DRL for join order selection that replaces fixed-length hand-tuned vector in Rejoin and DQ with learned scalable GNN representation and better captures and distinguishes the join tree structure information. These works mainly differ in encoding what information and how to encode them.\nInstead of learning from past query executions, Trummer et al. propose SkinnerDB to learn from the current query execution status to optimize the remaining execution of a query using RL. Specifically, SkinnerDB breaks the query execution into many small time intervals (e.g., tens to thousands of slices per second) and processes the query adaptively. At the beginning of each time interval, the RL agent chooses the join order and measures the execution progress. SkinnerDB adopts a similar adaptive query processing strategy in Eddies and uses the UCT algorithm, which provides formal guarantees that the difference is bounded between the rewards obtained by the agent and those by optimal choices. The reward is calculated by the progress for the current interval. A tailored execution engine is designed to fully exploit the learning strategy with tuple representations and specialized multi-way join algorithms. SkinnerDB offers several advantages. First, it is inherently robust to query distribution changes because its execution only depends on the current query. Second, it relies on less assumption and information (e.g., cardinality models) than traditional optimizers and thus is more suitable for the complicated environment where cardinality is hard to estimate. Third, it predicts the optimal join order based on real performance. However, it may introduce overhead caused by join order switching.\nLearning-based methods that have been proposed to replace traditional query optimizers often incur a great deal of training overhead because they have to learn from scratch. To mitigate the problem, Bao  (the Bandit optimizer)) is designed to take advantage of the existing query optimizers. Specifically, Bao learns to choose the best plan from the query plan candidates provided by available optimizers by passing different flags or hints to them. Bao transforms query plan trees into vectors and adopts a tree convolutional neural network to identify patterns in the tree. Then it formulates the choosing task as a contextual multi-armed bandit problem and uses Thompson sampling to solve it. Bao is a hybrid solution for query optimization. It achieves good training time and is robust to changes in workload~.\n\\begin{table}[t]\n\\footnotesize\n\\centering\n\\caption{Methods of query optimization.}\n\\begin{center}\n\\begin{tabular}{ |p{2.5cm}|p{2.6cm}|p{1.5cm}|p{1.8cm}|}\n\\hline\nMethod&Techniques&Training&Workload Adaptivity\n\\\\\n\\hline\nRejoin, DQ&learn from execution experience & High & Low \\\\\n\\hline\nSkinnerDB~&learn from current execution status & Medium & Medium\n\\\\\n\\hline\nBao&learn to choose existing optimizers & Low & High \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}", "cites": [3601, 2219, 3593, 8670, 3576, 620], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple DRL approaches for query optimization, connecting their methodologies, objectives, and design choices. It provides a critical comparison of techniques (e.g., learning from past executions vs. current execution status) and identifies key trade-offs like training overhead and workload adaptivity. The abstraction is evident in the categorization of methods into broader learning paradigms and the meta-level insights derived from the comparison table."}}
{"id": "730a1f68-9488-4605-94af-49a10378ebd6", "title": "View Materialization", "level": "subsubsection", "subsections": [], "parent_id": "91416b87-991f-42d3-b49e-fb1bb811570a", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Cache Management"], ["subsubsection", "View Materialization"]], "content": "View materialization is the process of deciding which view, i.e., results of query or subquery, to cache. In database systems, a view is represented as a table and other queries could be accelerated by reading this table instead of accessing the original tables. There is an overhead of materializing and maintaining the view when the original table is updated. Existing methods are based on heuristics, which either rely on simple Least-Recently-Used rule or cost-model based approaches. The performance of these approaches is limited because feedback from the historical performance of view materialization is not incorporated. Liang et al. implement Deep Q-Materialization (DQM) system that leverages DRL to improve the view materialization process in the OLAP system. First, DQM analyzes SQL queries to find candidate views for the current query. Second, it trains a DRL agent to select from the set of candidates. Third, it uses an eviction policy to delete the materialized views. In the MDP, the state encodes view state and workload information. The action is to create the view or do nothing. The reward is calculated by the query time improvement minus amortized creation cost. Additionally, the eviction policy is based on credit and it evicts the materialized view with the lowest score.\nYuan et al. present a different way that use DRL to automate view generation and select the most beneficial subqueries to materialize. First, the approach uses a DNN to estimate the benefits of a materialized view where features from tables, queries, and view plans are extracted. Then the approach models selection as an Integer Linear Programming (IIP) problem and introduce an iterative optimization method to figure it out. However, the method cannot guarantee convergence. To address the issue, the problem is formulated as the MDP. The state encodes the subqueries that are selected to materialize and status if queries use these materialized views. The action is to choose the subquery to materialize or not. The reward is the difference between benefit changes of two states. Both cost estimation and view selection models are trained offline using the actual cost of queries and benefits. Then the cost estimation model is used for the online recommendation for view materialization. Performance study shows its good performance; However, it lacks a comparison with DQM.", "cites": [3602], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two approaches to view materialization using DRL, connecting their shared use of MDPs and reward functions. It provides a critical note by pointing out the lack of comparison between the two systems and the non-guaranteed convergence of one method. However, it stops short of abstracting broader principles or offering a novel framework, focusing instead on specific implementations and their limitations."}}
{"id": "836ec3f9-98e8-4716-8ec0-9a7ed880a485", "title": "Storage", "level": "subsubsection", "subsections": [], "parent_id": "91416b87-991f-42d3-b49e-fb1bb811570a", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Theoretical Foundation and Algorithms of Reinforcement Learning"], ["subsection", "Cache Management"], ["subsubsection", "Storage"]], "content": "Cache management impacts the performance of computer systems with hierarchical hardware structures directly. Generally, a caching policy considers which objects to cache, to evict when the cache is full to maximize the object hit rate in the cache. In many systems, the optimal caching policy depends on workload characteristics. Phoebe is the RL-based framework for cache management for storage models. The state encodes the information from a preceding fixed-length sequence of accesses where for each access, nine features are extracted including data block address, data block address delta, frequency, reuse distance, penultimate reuse distance, average reuse distance, frequency in the sliding window, the number of cache misses, and a priority value. The action is to set a priority value ranging within $[-1, 1]$ to the data. The reward is computed from if the cache is hit or missed and values are 1 and -1 respectively. It applies the DDPG algorithm to train the agent. Periodical training is employed to amortize training costs in online training. In network systems, one issue is that the reward delay is very long in systems with a large cache, i.e., CDN cache can host up to millions of objects. Wang et al. propose a subsampling technique by hashing the objects to mitigate the issue when applying RL on caching systems.", "cites": [3603], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of Phoebe and its application in cache management for storage, including details on state, action, reward, and training methods. It mentions a subsampling technique proposed by Wang et al. but does not compare or contrast it with Phoebe in depth. While it integrates two approaches, the analysis remains surface-level with no critical evaluation or broader abstraction of principles."}}
{"id": "59b0c5ea-c0ad-477d-9c23-89c9d971dc89", "title": "Data Analytics Applications", "level": "section", "subsections": ["0fc5622c-7223-44b4-8412-0cf51df1a20e", "241ab76b-a937-47a1-badc-6f5e848cf74a", "4c8b869c-532d-4879-a599-dc33ad531837", "cdf4543b-f552-42cc-833e-1281afa017d0", "29a89fab-585c-4052-841c-cba38df27a14"], "parent_id": "fbe43cf7-0ec8-4b88-bda0-b6458577964d", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"]], "content": "\\begin{table*}[htp]\n\\footnotesize\n\\centering\n\\caption{Representative works for RL applications. D(X) denotes the approximate dimension of X space.}\n\\begin{tabular}{|p{1.6cm}|p{3.0cm}|p{1.5cm}|p{1.3cm}|p{1.5cm}|p{3.5cm}|}\n\\hline\\hline\n\\textbf{Domain} & \\textbf{Work} & \\textbf{Algorithm} & \\textbf{D(State)} & \\textbf{D(Action)} & \\textbf{DRL-based Approach}\\\\\n\\hline\nData processing &Entity matching & PG & 100 - 1000 & 100 - 1000 & Select target entity from the candidate entities \\\\\n\\cline{2-6}\napplication &Database interaction with natural language~&PG& 100 - 1000 &100 - 1000 & Learn to generate the query \\\\\n\\cline{2-6}\n&Feature engineering~&DQN&100& 1-10 & Select features and model feature correlations in states \\\\\n\\cline{2-6}\n&Exploratory data analysis~&A3C&10-100 & 100000 & Learn to query a dataset for key characteristics \\\\\n\\cline{2-6}\n&Abnormal detection~&IRL&1-10 & 1-10& Learn the reward function for normal sequences \\\\\n\\cline{2-6}\n&AutoML pipeline generation~&DQN& 10 & 100 & Learn to select modules of a pipeline \\\\\n\\hline\nHealthcare &Treatment recommendation~&DDPG&10 & 100-1000 & Select treatment from candidate treatments\\\\\n\\cline{2-6}\n&Diagnostic inference~&DQN&100-1000& 1-10 & Learn diagnostic decision \\\\\n\\cline{2-6}\n&Hospital resource allocation~&DDPG&100& 1000-10000 & Learn resource scheduling \\\\\n\\hline\nFintech &Portfolio optimization~&Q-Learning&100 & 100 & Select the portfolio weights for stocks \\\\\n\\cline{2-6}\n&Trading~&IRL& 1-10 & 10 & Learn the reward function of trading behaviors \\\\\n\\cline{2-6}\n&Fraud detection~&IRL&100& 10-100 & Learn the reward function of trading behaviors \\\\\n\\hline\nE- &Online advertising~&DQN&1-10& 1-10& Learn to schedule the advertisements\\\\\n\\cline{2-6}\nCommerce &Online recommendation~&DQN& 100 & 10000 & Learn to schedule recommendations \\\\\n\\cline{2-6}\n&Search results aggregation~&DQN& 10-100& 10-100 & Learn to schedule search results \\\\\n\\hline\nOthers &User profiling~&DQN&100-1000&1000-10000&Select users' next activities by modeling spatial semantics \\\\\n\\cline{2-6}\n&Spammer detection~&PG& 100& 100 & Search for the detector by interacting with spammers \\\\\n\\cline{2-6}\n&Transportation~&PG& 1000-10000& 1000 & Learn to schedule transportation \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\label{tab:3}\n\\end{table*}\nIn this section, we shall discuss DRL applications from the perspective of data processing and data analytics. These two categories of DRL applications form indispensable parts of a pipeline, in which data processing provides a better basis for data analytics. In addition, these two categories share some overlapping topics, making these topics mutually motivating and stimulating. We have summarized the technical comparisons of different applications in Table \\ref{tab:3}. We shall first discuss DRL applications in data preparation and then in data analytics.", "cites": [3606, 3608, 3609, 3612, 3605, 3604, 3611, 3610, 8416, 3607], "cite_extract_rate": 0.5, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, listing DRL applications across various domains with a table summarizing key elements of the cited works. While it organizes the information by domain and provides a basic structure, there is minimal synthesis or analysis of connections between the works. The discussion lacks critical evaluation of the methods, limitations, or comparative insights that would elevate the analysis to a higher level."}}
{"id": "f6f7c3f4-eaae-4e0a-973d-b4b057f23336", "title": "Entity Matching", "level": "subsubsection", "subsections": [], "parent_id": "0fc5622c-7223-44b4-8412-0cf51df1a20e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Data Preparation"], ["subsubsection", "Entity Matching"]], "content": "Entity matching is a data cleaning task that aligns different mentions of the same entity in the context. Clark et al.  identify the issue that the heuristic loss function cannot effectively optimize the evaluation metric $B^3$, and propose using reinforcement learning to directly optimize the metric. The problem is formulated as a sequential decision problem where each action is performed on one mention of a document. The action maps the mention to an entity in the database at each step by a mention ranking model. Then the reward is calculated using the evaluation metric $B^3$. This work originally proposes scaling each action's weight by measuring its impact on the final reward since each action is independent. However, this work does not consider the global relations between entities. Fang et al.  propose a reinforcement learning framework based on the fact that an easier entity will create a better context for the subsequent entity matching. Specifically, both local and global representations of entity mentions are modeled and a learned policy network is devised to choose from the next action (i.e., which entity to recognize). However, the selection of the easier entity to learn the context could be less powerful than context modeling with more recent techniques in NLP such as the transformer.", "cites": [3609, 3607], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of two distinct papers, highlighting their approaches to entity matching using DRL and the challenges they address. It includes critical evaluation by pointing out limitations, such as the lack of global entity relations in one approach and potential weaknesses in the other's entity selection strategy compared to modern NLP techniques. While it captures relevant patterns (e.g., the use of DRL to optimize evaluation metrics), it stops short of forming a broader, meta-level framework or principle."}}
{"id": "f6b69579-d9ed-41d1-b230-4f18700fc139", "title": "Database Interaction With Natural Language", "level": "subsubsection", "subsections": [], "parent_id": "0fc5622c-7223-44b4-8412-0cf51df1a20e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Data Preparation"], ["subsubsection", "Database Interaction With Natural Language"]], "content": "To facilitate query formulation for relational databases, there have been efforts in generating SQL queries from various other means that do not require knowledge of SQL and schema. Zhong et al.  propose to generate SQL from a natural language using Reinforcement Learning. For queries formed by a natural language, the model Seq2SQL will learn a policy transforming the queries into SQL queries. The transformed queries will then be executed in the database system to get results. The results will be compared with the ground truth to generate RL rewards. Earlier work~ using generic autoencoder model for semantic parsing with Softmax as the final layer may generate unnecessarily large output spaces for SQL query generation tasks. Thus the structure of SQL is used to prune the output space of query generating and policy-based reinforcement learning to optimize the part which cannot be optimized by cross-entropy. However, RL is observed to have limited performance enhancement by  due to unnecessary modeling of query serialization.\nEfficiently querying a database of documents is a promising data processing application. Karthik et al.  propose collecting evidence from external sources of documents to boost extraction accuracy to original sources where data might be scarce. The problem is formulated as an MDP problem, where each step the agent needs to decide if current extracted articles are accepted and stop querying, or these articles are rejected and more relevant articles are queried. Both data reconciliation (from original sources) and data retrieval (from external sources) are represented as states. Extraction accuracy and penalties for extra retrieval actions are reflected in the reward function.", "cites": [3605, 8416, 3613, 3614], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by discussing how DRL is applied to natural language database interaction, connecting the use of SQL generation and evidence acquisition. It provides some critical analysis by highlighting limitations such as the unnecessary modeling of query serialization and performance trade-offs. While there is abstraction in identifying the MDP formulation as a common theme, the generalization remains moderate without a deeper meta-level framework."}}
{"id": "e0bb0349-92fc-4023-bff4-9e2b55e9c04d", "title": "Abnormal Detection", "level": "subsubsection", "subsections": [], "parent_id": "0fc5622c-7223-44b4-8412-0cf51df1a20e", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Data Preparation"], ["subsubsection", "Abnormal Detection"]], "content": "Abnormal detection is important for high-stake applications such as healthcare (e.g., predicting patients' status) and fintech (e.g., financial crime). Based on the assumptions, there are two approaches to this problem. One approach models the dynamics in the unlabeled datasets as a sequential decision process where the agent performs an action on each observation. Oh et al.  propose to use IRL to learn a reward function and a Bayesian network to estimate a confidence score for a potential abnormal observation. To achieve this, the prior distribution of the reward function is assumed. Then a reward function is sampled from the distribution to determine the sample generating policy, which generates sample background trajectories. As explained by the reward part of Section \\ref{Model Efficiency}, experts' trajectories are observed. With these experts' trajectories and sample background trajectories, the parameters of the reward function are updated and thus the policy is improved. The sequence of actions is the input into the neural network. This network is trained to learn the normal pattern of a targeted agent and to predict if the next observation is abnormal or not. However, this approach relies too much on mining unlabeled datasets and ignores the labeled dataset. To address this issue, another approach also uses DRL but focus on the Exploit-Explore trade-off on both unlabeled and labeled dataset. Pang et al.  propose a DRL model with a sampling function to select data instances from both the unlabeled and labeled dataset. This sampling function helps the DRL model to exploit the scarce but useful labeled anomaly data instances and to explore the large unlabeled dataset for novel anomaly data instances. Thus, more anomaly data instances are selected to train the DRL model with better model capacity.", "cites": [3611, 8673], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two different DRL-based approaches for abnormal detection, connecting them through the common use of DRL while contrasting their focus on unlabeled versus labeled data. It identifies a limitation of the first approach and explains how the second addresses it, showing some critical analysis. The section abstracts the general idea of combining exploitation and exploration in anomaly detection, but broader meta-level insights or a novel framework are not presented."}}
{"id": "d9d98497-5976-4cbc-9eab-13593754b915", "title": "Treatment Recommendation", "level": "subsubsection", "subsections": [], "parent_id": "241ab76b-a937-47a1-badc-6f5e848cf74a", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Healthcare"], ["subsubsection", "Treatment Recommendation"]], "content": "Treatment recommendation systems are designed to assist doctors to make better decisions based on electronic health records. However, the doctors' prescriptions are not ground truth but valuable suggestions for high stake medical cases. The ground truth is the delayed condition of the patients. Thus model predictions must not deviate from the doctors' judgments too much, and not use those judgments as true labels. To tackle this challenge, Wang et al.  propose an architecture to combine supervised learning and reinforcement learning. This model reduces the inconsistency between indicator signals learned from doctor's prescriptions via supervised learning and evaluation signals learned from the long-term outcome of patients via reinforcement learning. In the formulated MDP, the domain expert makes a decision based on an unknown policy. The goal is to learn a policy that simultaneously reduces the difference between the chosen action of the agent and the expert's decision and to maximize the weighted sum of discounted rewards.", "cites": [3604], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by discussing the unique challenges of treatment recommendation systems in healthcare and how Wang et al. address them by combining supervised and reinforcement learning. It synthesizes the core idea of the cited paper and links it to the broader DRL context in data processing. However, it lacks deeper critical evaluation of the method's limitations and broader abstraction to more general principles or trends in the field."}}
{"id": "4c8b869c-532d-4879-a599-dc33ad531837", "title": "Fintech", "level": "subsection", "subsections": ["1b4f5fcf-70ef-4183-84f2-fc9b415d7203", "af30adb7-b4fa-4f4a-9b2e-fb9f2a077955", "b74ca92e-2085-4d76-8385-077681a7c367"], "parent_id": "59b0c5ea-c0ad-477d-9c23-89c9d971dc89", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Fintech"]], "content": "\\label{fintech}\nReinforcement learning has wide applications in the finance domain. Firstly, reinforcement learning has brought new perspectives to let the finance research community revisit many classic financial research topics. For example, traditional financial research topics such as option pricing that are typically solved by the classic Black–Scholes model can be steered through with a data-driven insight by reinforcement learning . Secondly, portfolio optimization, typically formulated as a stochastic optimal control problem, can be addressed by reinforcement learning. Finally, the agents are financial market participants with different intentions. Reward functions can be learned to model these intentions, and hence, make better decisions as illustrated in Figure \\ref{fig_fintech}. We refer readers with further interest in finance to .\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\textwidth]{img/RLDB_fintech.png}\n\\centering\n\\caption{DRL in fintech applications.}\n\\label{fig_fintech}\n\\end{figure}", "cites": [3615], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions a few DRL applications in fintech, such as option pricing and portfolio optimization, but it does not deeply synthesize the cited paper (e.g., QLBS model) or connect it meaningfully with other sources. There is minimal critical analysis or evaluation of the limitations and trade-offs of the approaches. The content remains largely descriptive with little abstraction or generalization to broader principles or frameworks."}}
{"id": "1b4f5fcf-70ef-4183-84f2-fc9b415d7203", "title": "Dynamic Portfolio Optimization", "level": "subsubsection", "subsections": [], "parent_id": "4c8b869c-532d-4879-a599-dc33ad531837", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Fintech"], ["subsubsection", "Dynamic Portfolio Optimization"]], "content": "The portfolio optimization problem is challenging because of the high scale of the dimensionality and the high noise-to-signal ratio nature of stock price data. The latter problem of noisy observation can cause uncertainty in a learned policy. Therefore,  proposes a novel model structure based on the Q-learning to handle noisy data and to scale to high dimensionality. The quadratic form of reward function is shown to have a semi-analytic solution that is computationally efficient. In the problem formulation, the agent's actions are represented as the changes in the assets at each time step. The states are the concatenation of market signals and the agent's holding assets. This method enhances Q-learning by introducing an entropy term measuring the noise in the data. This term acts as a regularization term forcing the learned policy to be close to a reference policy that is modeled by a Gaussian distribution.", "cites": [3612], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the methodology of one cited paper without integrating it with other works or providing a broader context. There is no critical evaluation of the proposed approach, and it lacks abstraction or generalization to higher-level principles or patterns in DRL for portfolio optimization."}}
{"id": "b74ca92e-2085-4d76-8385-077681a7c367", "title": "Sentiment-based Trading", "level": "subsubsection", "subsections": [], "parent_id": "4c8b869c-532d-4879-a599-dc33ad531837", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Fintech"], ["subsubsection", "Sentiment-based Trading"]], "content": "One of the main predictors in stock trading is sentiment, which drives the demand of bid orders and asks orders. Sentiment scores are often represented by unstructured text data such as news or twitters.\n proposes treating the sentiment as the aggregated action of all the market participants, which has the advantage of simplifying the modeling of the numerous market participants. Specifically, the sentiment scores are categorized into three intervals: high, medium, and low as the action spaces. Compared to previous works, the proposed method can model the dependency between the sentiment and the market state by the policy function.\nThis method is based on Gaussian Inverse Reinforcement Learning  similar to  as discussed at the beginning of Section \\ref{fintech}, which is effective at dealing with uncertainty in the stock environment. This method provides a method for modeling market sentiments. However, as IRL faces the challenge of non-uniqueness of reward  of one agent's actions, the method does not address how aggregated actions of multiple market participants can infer a unique reward function.", "cites": [8669], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of sentiment-based trading using DRL, discussing a specific method and its advantages in modeling aggregated market participant actions. It makes a basic connection to inverse reinforcement learning (IRL) and notes a limitation related to the non-uniqueness of reward functions, showing some critical evaluation. However, the synthesis is limited to a single approach, and abstraction is minimal, focusing on a narrow technical challenge without broader conceptual framing."}}
{"id": "d4cf7597-e235-4478-83c0-4c8fedf2d8e7", "title": "Online Advertising", "level": "subsubsection", "subsections": [], "parent_id": "cdf4543b-f552-42cc-833e-1281afa017d0", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "E-Commerce"], ["subsubsection", "Online Advertising"]], "content": "With the increasing digitalization of businesses, sales and competition for market shares have moved online in tandem. As a result, online advertising has been increasing in its presence and importance and exploiting RL in various aspects. One of the topics in online advertising, bidding optimization, can be formulated as a sequential decision problem: the advertiser is required to have strategic proposals with bidding keywords sequentially to maximize the overall profit. In~, the issue of using static transitional probability to model dynamic environments is identified and a new DRL model is proposed to exploit the pattern discovered from dynamic environments.\nIncluding but not limited to advertising, Feng et al.  propose to consider the whole picture of multiple ranking tasks that occurred in the sequence of user's queries. A new multi-agent reinforcement learning model is proposed to enable multiple agents to partially observe inputs and choose actions through their own actor networks. The agents communicate through a centralized critic model to optimize a shared objective. This allows different ranking algorithms to reconcile with each other when taking their own actions and consider the contextual information.", "cites": [3606, 3616], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the two cited papers by connecting the application of DRL in bidding optimization with multi-agent collaboration in ranking, suggesting a broader analytical perspective on DRL in e-commerce. It briefly identifies limitations, such as the static transitional probability in dynamic environments and lack of collaboration in multi-scenario ranking. However, the analysis remains somewhat surface-level and could benefit from deeper comparative evaluation or more meta-level insights."}}
{"id": "29a89fab-585c-4052-841c-cba38df27a14", "title": "Other Applications", "level": "subsection", "subsections": [], "parent_id": "59b0c5ea-c0ad-477d-9c23-89c9d971dc89", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Data Analytics Applications"], ["subsection", "Other Applications"]], "content": "DRL has been applied to various other applications. These DRL methods are often used with a knowledge graph, confounders, or game theory to model application-specific dynamics. These methods are not only well motivated from their respective applications but also general enough to be applied in other applications. However, these methods often fail to be evaluated by experiments in other applications.\nThe problem of mobile user profiling aims to identify user profiles to provide personalized services. In , the action is the selection of a place of visit. The environment is comprised of all users and a knowledge graph learning the semantic connections between the spatial entities. The knowledge graph is updated once a user's new activity is performed and then affects the agent's prediction. The state is the embedding of a user and the knowledge graph for the current time step. The reward is determined by several metrics measuring the similarity between the predicted spatial entities and the ground truth. This method considers the spatial semantics of entities but does not consider how the change of a user's key attributes (e.g., career) will affect activity prediction and policy learning, which could cause instability in policy updating.\nIn the transportation system, drivers often get recommendations and provide feedback in return to improve the service. However, the recommendation often fails when drivers make decisions in a complex environment. To address this issue, in  a new method is proposed to model hidden causal factors, called confounders, in a complex environment. Specifically, the framework in  is extended to include the confounders. First, all three elements (i.e., policy agent, environment, confounder) are treated as agents. The effect of a confounder is modeled as the policy of the hidden agent, which takes the observation and action of the policy agent as inputs and performs an action. The environment in turn takes the action based on inputs of the hidden agent's action and the policy agent's action and observation.\nThe problem of spammer detection aims to detect spam generating strategies. The challenge is that the detectors only detect easier spams while missing spams with strategies. In , the problem is formulated as two agents counteracting each other. One agent is the spammer, whose policy is to maintain a distribution of spam strategies and the action is to sample from the distribution. Another agent is the detector, whose state is the detection results after a spam attack and the action is to identify the spam. The rewards of two agents are measured by winning or losing revenue manipulation, respectively. The limitation of this method is that there is no guarantee for equilibrium.", "cites": [3592, 3608, 3610], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.7, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows analytical tendencies by integrating methods from different papers (e.g., knowledge graphs, confounders, and game theory) to highlight common themes and application-specific adaptations. It critically evaluates the limitations of each approach, such as the lack of attribute change modeling or equilibrium guarantees. While it identifies broader trends in DRL applications, it does not fully develop a novel or meta-level framework."}}
{"id": "dfe842bb-faaf-405a-b94b-7844bac7ced1", "title": "MDP Formulation and Lack of Justification", "level": "subsubsection", "subsections": [], "parent_id": "cd40ae2a-40b0-48aa-bcdb-93f86c81b9fc", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For System Optimization"], ["subsubsection", "MDP Formulation and Lack of Justification"]], "content": "The design of MDP impacts the performance and efficiency of the RL algorithm greatly. The state should satisfy Markov property that its representation contains enough relevant information for the RL agent to make the optimal decision. It should summarize the environment compactly because a complicated state design will cause more training and inference costs. The action space should be designed carefully to balance learning performance and computational complexity. The reward definition directly affects the optimization direction and the system performance. Additionally, the process of reward calculation can involve costly data collection and computation in the data systems optimization. Currently, many works rely on experimental exploration and experience to formulate MDP while some works exploit domain knowledge to improve the MDP formulation by injecting task-specific knowledge into action space. Generally, MDP can influence computational complexity, data required, and algorithm performance. Unfortunately, many works lack ablation studies of their MDP formulations and do not justify the design in a convincing manner. Therefore, automation of MDP formulation remains an open problem.", "cites": [3598], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical discussion of MDP formulation challenges in DRL for system optimization, highlighting key design considerations and the role of domain knowledge as demonstrated in the cited paper. It connects these ideas to broader implications for computational complexity and algorithm performance, but does not fully synthesize multiple sources or offer deep critique beyond noting a lack of justification and the need for automation."}}
{"id": "eb833842-5e30-4d44-85ed-68d702d05210", "title": "RL Algorithm and Technique Selection", "level": "subsubsection", "subsections": [], "parent_id": "cd40ae2a-40b0-48aa-bcdb-93f86c81b9fc", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For System Optimization"], ["subsubsection", "RL Algorithm and Technique Selection"]], "content": "RL algorithms and techniques have different tradeoffs and assumptions. Value-based DRL algorithms like DQN are not stable and guaranteed convergence. Policy-based DRL algorithms like TRPO and PPO are often not efficient. Model-based DRL algorithms do not guarantee that a better model can result in a better policy. Value-based methods assume full observability while policy-based ones assume episodic learning. Off-policy algorithms are usually more efficient than on-policy algorithms in terms of sample efficiency. One example is that DQ uses off-policy deep Q-learning to increase data efficiency and reduce the number of training queries needed. Training efficiency can be a big concern for DRL-based system optimization, especially when the workload of the system could change dramatically and the model needs to be retrained frequently. Generally, RL algorithms and techniques selection affect the training efficiency and effectiveness greatly.", "cites": [3576], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides some analytical perspective on the tradeoffs between different RL algorithms and techniques, such as value-based vs. policy-based methods, and on-policy vs. off-policy approaches. It briefly references a paper to support the idea of data-driven strategies in optimization but does not deeply synthesize or compare multiple sources. The abstraction level is limited, as it does not move beyond the specific context of system optimization to present broader principles."}}
{"id": "7c9dbc5f-e680-4d22-b985-6dbaff57c302", "title": "Integration with Existing Systems", "level": "subsubsection", "subsections": [], "parent_id": "cd40ae2a-40b0-48aa-bcdb-93f86c81b9fc", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For System Optimization"], ["subsubsection", "Integration with Existing Systems"]], "content": "Integrating RL-based methods into the real system more naturally and seamlessly faces many challenges. The RL agent has to be evolved when the system environment changes (e.g., workload) and the performance is degraded. We need to design new model management mechanisms to monitor, maintain, and upgrade the models. Furthermore, we find that the RL-based solutions can be lightweight or intrusive. The lightweight approach in which the RL agent is not designed as a component of the system, e.g. using RL to generate the qd-tree, is easier to integrate into the system because it does not change the architecture of the system dramatically. In contrast, the intrusive approach such as using RL models for join order optimization is deeply embedded in the system and hence may need a redesign and optimization of the original system architecture to support model inference efficiently. SageDB proposes to learn various database system components by integrating RL and other ML techniques. Nevertheless, the proposed model-driven database system is yet to be fully implemented and benchmarked. It is likely that the data system architecture needs to be overhauled or significantly amended in order to graft data-driven RL solutions into the data system seamlessly to yield an overall performance gain.", "cites": [8670, 3581], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two distinct applications of DRL in database systems (join order optimization and data layout generation) and contrasts their integration styles, providing a coherent narrative on the challenges of integrating DRL solutions. It offers some critical evaluation by highlighting the trade-offs between lightweight and intrusive approaches, as well as the incomplete implementation status of SageDB. However, the abstraction is limited to a general discussion of integration styles without deeper meta-level insights or broader theoretical generalization."}}
{"id": "a5a355aa-8fbc-4f81-9dae-7159d5576349", "title": "Reproducibility and Benchmark", "level": "subsubsection", "subsections": [], "parent_id": "cd40ae2a-40b0-48aa-bcdb-93f86c81b9fc", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For System Optimization"], ["subsubsection", "Reproducibility and Benchmark"]], "content": "In the data system optimization problem, RL algorithms are not easy to be reproduced due to many factors such as lacking open source codes, workload, historic statistics used, and the unstable performance of RL algorithms. The landscape of problems in system optimization is vast and diverse. It could prevent fair comparison and optimization for future research works and deployments in practice. Lacking benchmarks is another challenge to evaluate these RL approaches. The benchmarks are therefore to provide standardized environments and evaluation metrics to conduct experiments with different RL approaches. There are some efforts to mitigate the issue. For example, Park is an open platform for researchers to conduct experiments with RL. However, it only provides a basic interface and lacks system specifications. There is much room to improve with regards to the reproducibility and benchmark in order to promote the development and adoption of RL-based methods.", "cites": [3617], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section discusses the challenges of reproducibility and benchmarking in DRL for system optimization, referencing one paper (Deep Reinforcement Learning that Matters) to support the claim about the difficulty in reproducing RL results. While it identifies a key issue and mentions an existing effort (Park), it lacks deeper synthesis with the cited work and does not connect to broader themes or additional research. The critical analysis is moderate, highlighting limitations and gaps, but the abstraction remains limited to surface-level observations without deriving overarching principles."}}
{"id": "8c80f1ef-235a-4880-b5eb-4eb6cc760a34", "title": "Lack of Adaptability", "level": "subsubsection", "subsections": [], "parent_id": "edba0c81-3720-4bc0-9d97-7e4be96bae69", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For Applications"], ["subsubsection", "Lack of Adaptability"]], "content": "There is a lack of adaptability for methods on a single component of a data pipeline to the whole. For example, many works focus on data cleaning tasks such as entity matching. However, little works have shown their efficiency in deploying their model in an end-to-end data pipeline. These works treat the tasks isolatedly from other tasks in the pipeline, thereby limiting the pipeline's performance. In healthcare, each method is applied in different steps of the whole treatment process, without being integrated and evaluated as one pipeline. One possible direction could be considering DRL as a module in the data pipeline optimization. However, data pipeline optimization has been focusing on models simpler than DRL to enable fast pipeline evaluation . How to efficiently incorporate DRL into the data pipeline optimization remains a challenge.", "cites": [3618], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the adaptability challenge in DRL for data pipelines by highlighting the disconnect between component-level methods and end-to-end integration. It synthesizes the issue using the cited paper on collaborative data analytics pipelines to support the argument. The critique is focused on the current limitations and points to a potential research direction, though it does not deeply compare multiple works or generalize to a broader theoretical framework."}}
{"id": "0b9c2a14-f73d-4155-817b-549791ebb59f", "title": "Difficulty in Comparison with Different Applications", "level": "subsubsection", "subsections": [], "parent_id": "edba0c81-3720-4bc0-9d97-7e4be96bae69", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For Applications"], ["subsubsection", "Difficulty in Comparison with Different Applications"]], "content": "To date, most works with generalized contributions are only evaluated domain-specifically. Research questions are often formulated in their own platform as in E-Commerce. This presents difficulty in evaluating the methods for different environments. For example, the confounders modeling hidden causal factors in  can also contribute to DRL modeling in E-commerce. This is because modeling customers' interests are always subject to changing environments and a new environment may contain hidden causal factors. For example, consumers are more willing to buy relevant products for certain situations such as Covid-19. Thus a general DRL method is yet to show the robustness and effectiveness under the environment of different applications.", "cites": [3610], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section attempts to highlight the challenge of comparing DRL methods across different applications, using one cited paper as a basis for illustrating how environment-specific factors (e.g., hidden confounders) affect model robustness. While it makes a basic connection between the cited work and the broader problem of generalization, it lacks deeper synthesis across multiple papers and more comprehensive abstraction of underlying principles. The critique is present but not extensive or nuanced."}}
{"id": "eb3203b0-2aab-4fbe-90fe-4e657cb79977", "title": "Injecting Domain Knowledge in Experience Replay", "level": "subsubsection", "subsections": [], "parent_id": "edba0c81-3720-4bc0-9d97-7e4be96bae69", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Open Challenges For Applications"], ["subsubsection", "Injecting Domain Knowledge in Experience Replay"]], "content": "In high-stake applications such as healthcare and finance, injecting domain knowledge can make decision making in RL more robust and explainable. One possible way is to inject the knowledge of human beings' experience into an agent's experience pool as a prior distribution for the policy. For example, in dynamic portfolio optimization, a portfolio manager could have a large source of experience for risk management and profit optimization. Such experience could be useful for warming up the agent's exploration in the search space. Some works have shown positive effects of domain knowledge injection on selecting important experiences (i.e., transition samples) . Notwithstanding, it remains a big challenge to inject useful and relevant knowledge from the experience into the agent's experience pool.", "cites": [3586], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of the challenge of injecting domain knowledge into experience replay, referencing Paper 1 to illustrate the importance of prioritizing experiences. While it connects the idea of domain knowledge to the broader concept of experience replay, it does not deeply synthesize multiple sources or offer a novel framework. It identifies a challenge but lacks deeper critique or comparative analysis of different approaches."}}
{"id": "4d702694-54ef-4754-9450-50f739ea169f", "title": "Data Structure Design", "level": "subsubsection", "subsections": [], "parent_id": "cae8439e-2d96-452f-bd63-48df8f398bed", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Future Research Directions"], ["subsubsection", "Data Structure Design"]], "content": "DRL provides an alternative way to find good data structures through feedback instead of designing them based on human knowledge and experience, e.g., decision tree and the qd-tree. These trees are optimized better because they are learned by interacting with the environment. DRL has also been effective in graph designs (e.g., molecular graph). However, large-scale graph generation using DRL is difficult and daunting because it involves a huge search space. Generating other important structures using DRL remains to be explored. Idreos et al. propose a \\textit{Data Alchemist} that learns to synthesize data structures by DRL and other techniques including Genetic Algorithms and Bayesian Optimization. In summary, DRL has a role in the design of more efficient data structures by interacting and learning from the environment. These indexes have to be adaptive to different data distributions and workloads.", "cites": [264, 3577, 3581], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes a few relevant works, linking DRL's application in data structure design to examples like decision trees, qd-trees, and graph generation. It abstracts the role of DRL in creating adaptive and efficient data structures, but the critical analysis is limited—gaps and limitations are mentioned only briefly without in-depth evaluation. The insight level is medium, as it offers some analytical perspective but lacks a deeper, nuanced critique or a novel framework."}}
{"id": "5b8c70bd-cddb-4668-9d4d-d98e137c487b", "title": "Interpretability", "level": "subsubsection", "subsections": [], "parent_id": "cae8439e-2d96-452f-bd63-48df8f398bed", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Future Research Directions"], ["subsubsection", "Interpretability"]], "content": "The underlying logic behind the DRL agent is still unknown. In high-risk application areas such as healthcare, the adoption of DRL will be a big issue in the case that these approaches make wrong decisions and people do not know why it happens due to lack of interpretability. Many techniques have been proposed to mitigate the issue and provide interpretability. However, they neglect domain knowledge from related fields and applications and the explanations are not effective to human users. To instill confidence in the deployment of DRL-based systems in practice, interpretability is an important component and we should avoid treating DRL solutions as black boxes especially in critical applications.", "cites": [3580], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section acknowledges the interpretability issue in DRL and references a relevant survey on explainable reinforcement learning. It provides a general critique of current techniques, noting their neglect of domain knowledge and ineffective explanations for users. However, it lacks deeper synthesis of multiple sources and offers only limited abstraction, focusing more on the problem statement than broader insights or comparative analysis."}}
{"id": "21ea715a-ea15-4091-9406-3174b66d4e61", "title": "Extension to Other Domains", "level": "subsubsection", "subsections": [], "parent_id": "cae8439e-2d96-452f-bd63-48df8f398bed", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Future Research Directions"], ["subsubsection", "Extension to Other Domains"]], "content": "Beyond existing works, many classic problems in the data system and analytics could potentially be solved by DRL. For example, Polyjuice learns the concurrency control algorithm for a given workload by defining fine-grained actions and states in the context of concurrency control. Though they use an evolutionary algorithm to learn and outperform a simple DRL baseline, we believe that there are huge potentials to further improve DRL for niche applications. Hence, we expect that more problems will be explored and solved with DRL in various domains in the near future.", "cites": [3619], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the idea of applying DRL to classic data system problems, citing Polyjuice as an example. It briefly connects the work to the broader DRL context but does not deeply synthesize ideas from multiple sources. While it hints at potential for improvement, it lacks detailed critical evaluation of Polyjuice's approach. The section begins to generalize the potential of DRL in niche applications, suggesting some abstraction, but remains limited in scope."}}
{"id": "bf6627f8-8cc1-4b34-97d2-62559cb8cca8", "title": "Towards Intelligent and Autonomous Databases", "level": "subsubsection", "subsections": [], "parent_id": "cae8439e-2d96-452f-bd63-48df8f398bed", "prefix_titles": [["title", "A Survey on Deep Reinforcement Learning for Data Processing and Analytics"], ["section", "Open Challenges and Future Directions"], ["subsection", "Future Research Directions"], ["subsubsection", "Towards Intelligent and Autonomous Databases"]], "content": "Although DRL algorithms could provide breakthrough performance on many tasks than traditional methods, many issues need to be addressed towards intelligent and autonomous databases. First, database schema could be updated and DRL models trained on the previous snapshots may not work. DRL algorithms need to tackle generalization. Second, it would be so costly and infeasible to train models from scratch for each scenario and setting. Transfer learning from existing models could be a potential way to ease the workload greatly. Third, we have to choose appropriate DRL algorithms automatically, in the same spirit as AutoML. Fourth, current DBMS systems were designed without considering much about the learning mechanism. A radically new DBMS design may be proposed based on the learning-centric architecture. To support intelligent and autonomous database systems, DRL models intelligent behaviors and may provide a solid basis for achieving artificial general intelligence based on reward maximization and trial-and-error experience.", "cites": [3620], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces key challenges in making DRL applicable to intelligent and autonomous databases but only loosely connects the cited paper on generalization to the broader topic without a deep synthesis. It provides some critical points about the limitations of current DRL approaches and DBMS systems but lacks detailed evaluation or comparison of solutions. The section begins to abstract toward a vision of autonomous databases, identifying general trends, but stops short of offering a meta-level framework."}}
