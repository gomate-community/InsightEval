{"id": "44418792-e27d-47ff-9e32-7cfe70b95589", "title": "Providing advice", "level": "subsection", "subsections": [], "parent_id": "8bfb4d47-72df-499c-aa9e-3a08cf22ac53", "prefix_titles": [["title", "Reinforcement learning with human advice: a survey."], ["section", "Reinforcement Learning with human advice"], ["subsection", "Providing advice"]], "content": "\\label{taxonomy}\nThe means by which teaching signals can be communicated to a learning agent vary.\nThey can be provided via natural language , computer vision , hand-written programs , artificial interfaces , or physical interaction .\nDespite the variety of communication channels, we can distinguish two main categories of teaching signals based on how they are produced: advice and demonstration.\nEven though advice and demonstration can share the same communication channels, like computer vision  and artificial interfaces , they are fundamentally different from each other in that demonstration requires the task to be executed by the teacher (demonstrated), while advice does not.\nIn rare cases, demonstration  has been referred to as advice .\nHowever, it is more common to consider demonstration and advice as two distinct and complementary approaches for interactive learning .\nBased on this distinction, we define advice as \\textit{teaching signals that can be communicated by the teacher to the learning system without executing the task}. \nWe mainly distinguish two forms of advice depending on how it is provided to the system: \\textit{general advice} and \\textit{contextual advice} (Fig. \\ref{fig:structure}, Table \\ref{tab:structure}). \n\\textit{General advice} can be communicated to the system, non-interactively, prior to the learning process (offline).\nThis type of advice represents information about the task that do not depend on the context in which they are provided. \nThey are self-sufficient in that they include all the required information for being converted into a usable form (operationalization). \nExamples include specifying general constraints about the task and providing general instructions about the desired behaviour.\n\\textit{Contextual advice}, on the other hand, is context-dependent, in that the communicated information depends on the current state of the task.\nSo, unlike \\textit{general advice}, it must be provided interactively along the task .\n\\textit{Contextual advice} can also be provided in an offline fashion, with the teacher interacting with previously recorded task executions by the learning agent . \nEven in this case, each piece of advice has to be provided at a specific moment of the task execution.\nExamples of \\textit{contextual advice} include evaluative feedback  , corrective feedback , guidance  and contextual instructions . \n\\textbf{General advice}\nAdvice can be used by the human teacher to provide the agent with general information about the task prior to the learning process. \nThese information can be provided to the system in a written form .\nGeneral advice can specify \\textit{general constraints} about the task such as domain concepts, behavioural constraints and performance heuristics.\nFor example, the first ever implemented advice-taking system relied on general constraints that were written as LISP expressions, to specify concepts, rules and heuristics for a card-playing agent .\nA second form of general advice, \\textit{general instructions}, explicitly specifies to the agent what actions to perform in different situations.\nIt can be provided either in the form of \\textit{if-then} rules , or as detailed action plans describing the step-by-step sequence of actions that should be performed in order to solve the task .\nAction plans can be seen as a sequence of low-level or high-level \\textit{contextual instructions} (cf. definition below).\nFor example, a sequence like (e.g. \\textit{\"Click start, point to search, and then click for files or folders.\"}), can be decomposed into a sequence of three low-level \\textit{contextual instructions} .\n\\textbf{Contextual advice:}\nIn contrast to \\textit{general advice}, a \\textit{contextual advice} depends on the state in which it is provided. \nTo use the terms of the advice-taking process, a part of the information that is required for operationalization is implicit, and must be inferred by the learner from the current context.\nConsequently, \\textit{contextual advice} must be progressively provided to the learning agent along the task. \nContextual advice can be divided into two main categories: guidance and feedback. \nGuidance informs about future actions whereas feedback informs about past ones. \n\\textbf{Guidance:} Guidance is a term that is encountered in many papers and has been made popular by the work of Thomaz  about Socially Guided Machine Learning . \nIn the broad sense, guidance represents the general idea of guiding the learning process of an agent.\nIn this sense, all interactive learning methods can be considered as a form of guidance.\nA bit more specific definition of guidance is when human inputs are provided in order to bias the exploration strategy .\nFor instance, in , demonstrations were provided in order to teach the agent how to explore interesting regions of the state space. \nIn , kinesthetic teaching was used for guiding the exploration process for learning object affordances.\nIn the most specific sense, guidance constitutes a form of advice that consists in suggesting a limited set of actions from all the possible ones .\n\\textbf{Contextual instructions:} One particular type of guidance is to suggest only one action to perform. \nWe refer to this type of advice as \\textit{contextual instructions}.\nFor example, in , the authors used both terms of advice and guidance for referring to contextual instructions.\nContextual instructions can be either low-level or high-level .\nLow-level instructions indicate the next action to perform , whereas high-level instructions indicate a more extended goal without explicitly specifying the sequence of actions that should be executed . \nHigh-level instructions were also referred to as commands . \nIn RL terminology, high-level instructions would correspond to performing \\textit{options} .\nContextual instructions can be provided for example through speech , gestures  or myoelectric (EMG) interfaces .\n\\textbf{Feedback:} We distinguish two main forms of feedback: evaluative and corrective.\nEvaluative feedback, also called critique, consists in evaluating the quality of the agent's actions  .\nCorrective feedback, also called instructive feedback, implicitly implies that the performed action is wrong . \nHowever, it goes beyond simply criticizing the performed action, by informing the agent about the correct one. \n\\textbf{Corrective feedback:}\nCorrective feedback can be either a corrective instruction  or a corrective demonstration . \nThe main difference with instructions (resp. demonstrations) is that they are provided after an action (resp. a sequence of actions) is executed by the agent, not before.\nSo, operationalization is made with respect to the previous state instead of the current one.\nSo far, corrective feedback has been mainly used for augmenting LfD systems .\nFor example, in , while the robot is reproducing the provided demonstrations, the teacher could interactively rectify any incorrect action.\nIn , corrective demonstrations were delimited by two predefined verbal commands that were pronounced by the teacher. \nIn , the authors presented a framework based on \\textit{advice-operators}, allowing a teacher to correct entire segments of demonstrations through a visual interface.\nAdvice-operators were defined as numerical operations that can be performed on state-action pairs. \nThe teacher could choose an operator from a predefined set, and apply it to the segment to be corrected.\nIn , the authors took inspiration from advice-operators to propose learning from corrective feedback as a standalone method, contrasting with other methods for learning from evaluative feedback such as TAMER .\n\\textbf{Evaluative feedback:}\nTeaching an agent by evaluating its actions is an alternative solution to the standard RL approach.\nEvaluative feedback can be provided in different forms: a scalar value $f \\in [-1,1]$ , a binary value $f \\in \\{-1,1\\}$ , a positive reinforcer $f \\in \\{\"Good!\",\"Bravo!\"\\}$ , or a categorical information $f \\in \\{Correct, Wrong\\}$ . \nThese values can be provided through buttons , speech , gestures , or electroencephalogram (EEG) signals .\nAnother form of evaluative feedback is to provide preferences between demonstrated trajectories .\nInstead of critiquing one single action or a sequence of actions, the teacher provides a ranking for demonstrated trajectories.\nThe provided human preferences are then aggregated in order to infer the reward function.\nThis form of evaluative feedback has been mainly investigated within the LfD community as an alternative to the standard Inverse Reinforcement Learning approach (IRL), by relaxing the constraint for the teacher to provide demonstrations.\n\\begin{figure}[h!]\n\\begin{center}\n\\includegraphics[trim=3.5cm 4cm 1.5cm 1.5cm, clip=true,scale=0.8]{structure.pdf}\n\\caption{Taxonomy of advice.} \n\\label{fig:structure}\n\\end{center}\n\\end{figure}\n\\begin{table}[!ht]\n\\centering\n\\begin{tabular}[t]{l>{\\raggedright\\arraybackslash}p{0.7\\textwidth}}\n\\hline\nCategory\t            & References\\\\\n\\hline\nGeneral constraints\t    &  \\\\\nGeneral instructions    & \\\\\nGuidance                & \\\\\nContextual instructions & \\\\\nCorrective feedback     & \\\\\nEvaluative feedback     &  \\\\\n                        &  \\\\\n\\hline\n\\end{tabular}\n\\caption{Types of advice.} \n\\label{tab:structure} \n\\end{table}", "cites": [1354, 4113, 1352], "cite_extract_rate": 0.04225352112676056, "origin_cites_number": 71, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers into a structured taxonomy of advice types, distinguishing general and contextual advice, and further subcategorizing contextual advice into guidance and feedback. It abstracts concepts such as operationalization and policy dependency, and offers a coherent framework for understanding how human advice can be integrated into RL. While it provides some critical evaluation (e.g., highlighting the policy dependency of feedback), deeper comparative analysis or identification of limitations is limited."}}
{"id": "fc0b6838-e13f-419f-b20c-8c95fabbd2aa", "title": "Interpreting advice", "level": "subsection", "subsections": [], "parent_id": "8bfb4d47-72df-499c-aa9e-3a08cf22ac53", "prefix_titles": [["title", "Reinforcement learning with human advice: a survey."], ["section", "Reinforcement Learning with human advice"], ["subsection", "Interpreting advice"]], "content": "\\label{interpretation}\nThe second step of the advice-taking process stipulates that advice needs to be converted into an internal representation.\nPredefining the meaning of advice by hand-coding the mapping between raw signals and their internal representation, has been widely used in the literature . \nHowever, this solution has many limitations.\nFirst, programming the meaning of raw advice signals for new tasks requires expert programming skills, which is not accessible to all human users. \nSecond, it limits the possibility for different teachers to use their own preferred signals.\nOne way to address these limitations is to teach the system how to interpret the teacher's raw advice signals. \nThis way, the system would be able to understand advice, that can be expressed for example through natural language or non-verbal cues, without predetermining the meaning of each signal.\nIn this case, we talk about learning with unlabeled teaching signals .\nTo achieve this goal, different approaches have been taken in the literature. \nTable \\ref{tab:interpretation} summarizes the literature addressing the question of interpreting advice.\nWe categorize them according to the type of advice, the communication channel, the interpretation method, and the inputs given to the system for interpretation.\n\\begin{table}[htb!]\n\\centering\n\\begin{tabular}{lllll}\n\\hline\nReference \t\t\t\t\t\t\t& Instructions & Channel & Method  & Inputs \\\\\n\\hline\n  \t        & GI            & Text      & SVM           & Demonstration*            \\\\\n  \t        & EFB           & Speech    & kNN           & Binary EFB classes        \\\\\n  \t        & GLI           & Text      & SVM           & Demonstration             \\\\\n  \t& GHI           & Text      & Graphical model & Demonstration           \\\\\n  \t\t\t& GHI           & Text      & Perceptron    & Rewards/demonstration + LM \\\\\n  \t& GLI           & Text      & MCC           & Demonstration + LM \\\\\n \t\t& GHI           & Text      & Gradient descent& Demonstration           \\\\\n\t\t& CLI           & Gestures  & MLN           & Demonstration*            \\\\\n  \t& EFB and CFB   & SDI       & IRL           & EFB and CFB               \\\\\n \t\t\t& EFB or CLI    & Speech    & EM            & Task models               \\\\\n      & EFB           & EEG       & EM            & Task models               \\\\\n  & GHI           & Text      & EM            & Task and language models   \\\\\n     & GHI           & Text      & EM            & EFB + LM      \\\\\n  \t    & EFB           & Buttons   & EM            & Task models               \\\\\n\t& GLI           & Text      & PGRL          & Rewards                   \\\\\n\t    & GHI           & Text      & MB-PGRL       & Rewards                   \\\\\n\t\t\t& GLI           & Text      & SARSA         & Demonstration             \\\\\n          & CLI           & SDI       & XCS           & Rewards                   \\\\\n       & CLI           & Gestures  & XCS           & EFB                       \\\\\n          & CLI           & Gestures  & Q-learning    & EFB                       \\\\\n\t& CLI           & EMG       & ACRL          & Rewards and/or EFB        \\\\\n       & CLI           & Gestures  & ACRL          & Rewards and/or EFB        \\\\\n\\hline\n\\end{tabular}\n\\caption{Interpreting advice.GI: General instruction. GLI: general low-level instruction. GHI: general high-level instruction. CLI: contextual low-level instruction. EFB: evaluative feedback. CFB: corrective feedback. SVM: Support Vector Machines. kNN: k-nearest neighbors. MCC: multi-class classification. MLN: Markov Logic Networks. IRL: Inverse Reinforcement Learning. PGRL: policy-gradient RL. MB-PGRL: model-based policy-gradient RL. ACRL: Actor-Critic RL. LM: Language model. *The term demonstration here is taken in the general sense as a trajectory, not necessarily the optimal one.} \n\\label{tab:interpretation} \n\\end{table}\n\\textbf{Supervised interpretation:} Some methods relied on interpreters trained with supervised learning methods . \nFor example, in , the system was able to convert general instructions expressed in a constrained natural language into a formal representation using \\textit{if-then} rules, by using a parser that was previously trained with annotated data.\nIn , two different models of contextual instructions were learned in the first place using Markov Logic Networks (MLN) , and then used for guiding a learning agent in a later phase.\nThe most likely interpretation was taken from the instruction model with the highest confidence.\nIn , a binary classification of prosodic features was performed offline, before using it to convert evaluative feedback into a numerical reward signal for task learning.\n\\textbf{Grounded interpretation:} More recent approaches take inspiration from the \\textit{grounded language acquisition} literature , to learn a model that grounds the meaning of advice into concepts from the task. \nFor example, general instructions expressed in natural language can be paired with demonstrations of the corresponding tasks to learn the mapping between low-level contextual instructions and their intended actions .\nIn , the authors proposed a model for grounding general high-level instructions into reward functions from user demonstrations.\nThe agent had access to a set of hypotheses about possible tasks, in addition to command-to-demonstration pairings. \nGenerative models of tasks, language, and behaviours were then inferred using Expectation Maximization (EM) .\nIn addition to having a set of hypotheses about possible reward functions, the agent was also endowed with planning abilities that allowed it to infer a policy according to the most likely task.\nThe authors extended their model in , to ground command meanings in reward functions, using evaluative feedback instead of demonstrations. \nIn a similar work , a robot learned to interpret both low-level contextual instructions and evaluative feedback, while inferring the task using an EM algorithm. \nContextual advice was interactively provided through speech.\nAs in , the robot knew the set of possible tasks, and was endowed with a planning algorithm allowing it to derive a policy for each possible task.\nThis model was also used for interpreting evaluative feedback provided through EEG signals . \nIn , a predefined set of known feedback signals, both evaluative and corrective, were used for interpreting additional signals with IRL .\n\\textbf{RL-based interpretation:} A different approach relies on Reinforcement Learning for interpreting advice .\nIn , the authors used a policy-gradient RL algorithm with a predefined reward function to interpret general low-level instructions for a software application.\nThis model was extended in , to allow for the interpretation of high-level instructions, by learning a model of the environment. \nIn , a similar approach was used for interpreting general low-level instructions, in a path-following task, using the SARSA algorithm.\nThe rewards were computed according to the deviation from a provided demonstration.\nIn , contextual low-level instructions were provided to a prosthetic robotic arm in the form of myoelectric control signals, and interpreted using evaluative feedback with an Actor-Critic architecture. \nIn , a model of contextual low-level instructions was built using the XCS algorithm  in order to predict task rewards, and used simultaneously for speeding-up the learning process.\nThis model was extended in , to predict action values instead of task rewards.\nIn , interpretation was based on evaluative feedback using the Q-learning algorithm.\nIn , several methods for interpreting contextual low-level instructions were compared.\nEach contextual low-level instruction was defined as a \\textit{signal policy} representing a probability distribution over the action-space, in the same way as a RL policy:\n\\begin{equation}\n\\pi(i) = \\{\\pi(i,a); a \\in A\\} = \\{Pr(a|i); a \\in A\\},\n\\end{equation}\nwhere $i$ is an observed instruction signal. Two types of interpretation methods were proposed: batch and incremental.\nThe main idea of batch interpretation methods is to derive a state policy for an instruction signal by combining the policies of every task state in which it has been observed.\nDifferent combination methods were investigated. \nThe Bayes optimal solution derives the signal policy by marginalizing the state policies over all the states where the signal has been observed:\n\\begin{align}\n\\pi(i,a) = Pr(a|i) &= \\sum_{s \\in S} Pr(a|s) \\times Pr(s|i)\\\\\n         &= \\sum_{s \\in S} \\pi(s,a) \\times Pr(i|s) \\times Pr(s)/Pr(i),\n\\end{align}\nwhere $Pr(i|s), Pr(s)$ and $Pr(i)$ represent respectively the probability of observing the signal $i$ in state $s$, the probability of being in state $s$ and the probability of observing the signal $i$.\nOther batch interpretation methods were inspired from ensemble methods , which have been classically used for combining the policies of different learning algorithms.\nThese methods compute preferences $p(i,a)$ for each action, which are then transformed into a policy using the softmax distribution as in Eq. \\ref{eq:actor}.\nThe Boltzmann Multiplication consists in multiplying the policies:\n\\begin{equation}\np(i,a)=\\prod_{s \\in S;i^*(s)=i} \\pi(s,a),\n\\end{equation}\nwhere $i^*(s)$ represents the instruction signal associated to the state $s$. \nThe Boltzamm Addition consists in adding the policies:\n\\begin{equation}\np_t(i,a)=\\sum_{s \\in S;i^*(s)=i} \\pi_t(s,a).\n\\end{equation}\nIn Majority Voting, the most preferred interpretation for a signal $i$ is the action that is optimal the most often over all its contingent states:\n\\begin{equation}\n    p(i,a)=\\sum_{s \\in S;i^*(s)=i} I(\\pi^*(s),a),\n\\end{equation}\nwhere $I(x,y)$ is the indicator function that outputs $1$ when $x=y$ and $0$ otherwise. \nIn Rank Voting, the most preferred action for $i$ is the one that has the highest cumulative ranking over all its contingent states:\n\\begin{equation}\n    p(i,a)=\\sum_{s \\in S;i^*(s)=i} R(s,a),\n\\end{equation}\nwhere $R(s,a)$ is the rank of action $a$ in state $s$, such that if $a_j$ and $a_k$ denote two different actions and $\\pi(s,a_j) \\geq \\pi(s,a_k)$ then $R(s,a_j) \\geq R(s,a_k)$.\nLocal interpretation methods, on the other hand, incrementally update the meaning of each instruction signal using information from the task learning process such as the rewards, the TD error, or the policy gradient.\nWith Reward-based Updating, instruction signals constitute the state space for an alternative MDP, that is solved using a standard RL algorithm. \nThis approach is similar to the one used in .\nIn Value-based Updating, the meaning of instruction is updated with the same amount as the Q-values of its corresponding state:\n\\begin{equation}\n    \\delta p_t(i,a_t)=\\delta Q(s_t,a_t),\n\\end{equation}\nwhereas in Policy-based Updating, it is updated using the policy update:\n\\begin{equation}\n    \\delta \\pi(i,a_t)=\\delta \\pi(s_t,a_t).\n\\end{equation}\nThese methods were compared using both a reward function and evaluative feedback.\nPolicy-based Updating presented the best compromise in terms of performance and computation cost.", "cites": [4113], "cite_extract_rate": 0.023255813953488372, "origin_cites_number": 43, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes various methods for interpreting human advice into a structured framework, categorizing them based on advice type, channel, method, and inputs. It also introduces the concept of grounded interpretation and RL-based interpretation, which provides analytical depth. However, the critical analysis is limited to noting general limitations of predefined mappings without deeply evaluating the strengths or weaknesses of specific approaches. The abstraction level is moderate, as it identifies broader categories and parallels but does not present a novel overarching theory."}}
{"id": "b97b5615-1d9b-4257-8406-50dfb9741cc9", "title": "Shaping with advice", "level": "subsection", "subsections": [], "parent_id": "8bfb4d47-72df-499c-aa9e-3a08cf22ac53", "prefix_titles": [["title", "Reinforcement learning with human advice: a survey."], ["section", "Reinforcement Learning with human advice"], ["subsection", "Shaping with advice"]], "content": "\\label{shaping}\nWe can distinguish several strategies for integrating advice into a RL system, depending to which stage of the learning process is influenced by the the advice.\nThe overall RL process can be summarized as follows.\nFirst, the main source of information to a RL agent is the reward function.\nIn value-based RL, the reward function is used for computing a value function, which is then used for deriving a policy.\nIn policy-based RL, the policy is directly derived from the reward function without computing any value function.\nFinally, the policy is used for decision-making.\nAdvice can be integrated into the learning process at any of these four different stages: the reward function, the value function, the policy or the decision.\nWe qualify the methods used for integrating advice as shaping methods.\nIn the literature, this term has been used exclusively used for evaluative feedback, especially as a technique for providing extra-rewards.\nFor example, we find different terminologies such as reward shaping , interactive shaping  and policy shaping . \nIn some works, the term shaping is not even adopted .\nIn this survey we generalize this term to all types of advice, by considering the term shaping in its general meaning as influencing a RL agent towards a desired behaviour.\nIn this sense, all methods for integrating advice into a RL process are considered as shaping methods, especially that similar shaping patterns can be found across different categories of advice.\nWe distinguish four main strategies for integrating advice into a RL system: reward shaping, value shaping, policy shaping and decision-biasing, depending on the stage in which advice is integrated into the learning process (cf. Table \\ref{tab:shaping}).\nOrthogonal to this categorization, we distinguish model-free from model-based shaping strategies.\nIn model-free shaping, the perceived advice is directly integrated into the learning process, whereas model-based shaping methods build a model of the teacher that is kept in parallel with the agent's own model of the task. \nBoth models can be combined using several combination techniques that we review in this section.\n\\begin{table}[!ht]\n\\centering\n\\begin{tabular}[t]{l>{\\raggedright\\arraybackslash}l>{\\raggedright\\arraybackslash}l>{\\raggedright\\arraybackslash}p{0.4\\linewidth}}\n\\hline\nShaping method\t& Model         & Advice                & References\\\\\n\\hline\nReward shaping  & Model-free    & Contextual instructions & \\\\\n                &               & Evaluative feedback   & \\\\\n                & Model-based   & Contextual instructions& \\\\\n                &               & Evaluative feedback   & \\\\\nValue shaping   & Model-free    & General instructions          & \\\\\n                &               & Evaluative feedback   & \\\\\n                & Model-based   & Contextual instructions& \\\\\n                &               & Evaluative feedback   & \\\\\nPolicy shaping  & Model-free    & Contextual instructions& \\\\\n                &               & Evaluative feedback   & \\\\\n                & Model-based   & Contextual instructions & \\\\\n                &               & Evaluative feedback   & \\\\\n                &               & Corrective feedback   & \\\\\nDecision biasing&               & Guidance              & \\\\\n                &               & Contextual instructions & \\\\\n\\hline\n\\end{tabular}\n\\hspace{1cm}\n\\caption{Shaping methods.} \n\\label{tab:shaping} \n\\end{table}\n\\textbf{Reward shaping:}\nTraditionally, reward shaping has been used as a technique for providing a RL agent with intermediate rewards to speed-up the learning process . \nOne way for providing intermediate rewards is to use evaluative feedback . \nIn these works, evaluative feedback was considered in the same way as the feedback provided by the agent's environment in RL; so intermediate rewards are homogeneous to MDP rewards.\nAfter converting evaluative feedback into a numerical value, it can be considered as a delayed reward, just like MDP rewards, and used for computing a value function using standard RL algorithms (cf. Fig. \\ref{fig:feedback}) . \nThis means that the effect of the provided feedback extends beyond the last performed action.\nWhen the RL agent has also access to a predefined reward function $R$, a new reward function $R'$ is computed by summing both forms of reward: $R'=R + R^h$, where $R^h$ is the human delivered reward.\nThis way of shaping with is model-free in that the numerical values provided by the human teacher are directly used for augmenting the reward function.\nReward shaping can also be performed with instructions (cf. Fig. \\ref{fig:instructions}).\nFor example, in , \\textit{contextual instructions} were integrated into a RL algorithm by positively reinforcing the proposed actions in a model-free fashion.\nOther works considered building an intermediate model of human rewards to perform model-based reward shaping.\nIn the TAMER framework , evaluative feedback was converted into rewards and used for computing a regression model $\\hat{H}$, called the \\textit{\"Human Reinforcement Function\"}.\nThis model predicted the amount of rewards $\\hat{H}(s,a)$ that the human provided for each state-action pair $(s,a)$. \nKnox and Stone  proposed eight different shaping methods for combining the \\textit{human reinforcement function} $\\hat{H}$ with a predefined MDP reward function $R$. \nOne of them, Reward Shaping, generalizes the reward shaping method by introducing a decaying weight factor $\\beta$ that controls the contribution of  $\\hat{H}$ over $R$: \n\\begin{equation} \\label{eq:reward-shaping}\nR'(s,a)=R(s,a)+\\beta*\\hat{H}(s,a).\n\\end{equation}\nModel-based reward shaping can also be performed with \\textit{contextual instructions}.\nIn , a human teacher provided social cues to humanoid robot about the next action to perform.\nA model of these cues was built in order to predict task rewards and used simultaneously for reward shaping.\n\\begin{figure}[h!]\n\\begin{center}\n\\includegraphics[trim=3.5cm 2.5cm 2cm 2.5cm, clip=true,scale=0.8]{feedback.pdf}\n\\caption{Shaping with evaluative feedback. 1: model-free reward shaping. 2: model-based reward shaping. 3: model-free value shaping. 4: model-based values shaping. 5: model-free policy shaping. 6: model-based policy shaping.} \n\\label{fig:feedback}\n\\end{center}\n\\end{figure}\n\\begin{figure}[h!]\n\\begin{center}\n\\includegraphics[trim=3.5cm 2.5cm 2cm 2.5cm, clip=true,scale=0.8]{instruction.pdf}\n\\caption{Shaping with contextual instructions. 1: model-free reward shaping. 2: model-based reward shaping. 3: model-free value shaping. 4: model-based value shaping. 5: model-free policy shaping. 6: model-based policy shaping. 7: decision biasing.} \n\\label{fig:instructions}\n\\end{center}\n\\end{figure}\n\\textbf{Value shaping:}\nWhile investigating reward shaping, some authors pointed out the fundamental difference that exists between immediate and delayed rewards .\nParticularly, they considered evaluative feedback as an immediate information about the value of an action, as opposed to standard MDP rewards . \nFor example, in , the authors used a \\textit{myopic discounting} scheme by setting the discount factor $\\gamma$ to zero. \nThis way, evaluative feedback constituted \\textit{\"immediate reinforcements in response to the actions of the learning agent\"}, which comes to consider rewards as equivalent to action values. \nSo, value shaping constitutes an alternative to reward shaping by considering evaluative feedback as an action-preference function.\nThe work of Dorigo and Colombetti  was one of the earliest examples of model-free value-shaping.\nAnother example can be found in , where evaluative feedback was directly used for updating a robot's action values with \\textit{myopic discounting}. \nModel-free value shaping can also be done with \\textit{general advice}.\nFor example, \\textit{if-then} rules can be incorporated into a kernel-based regression model by using\nthe Knowledge-Based Kernel Regression (KBKR) method .\nThis method was used for integrating \\textit{general constraints} into the value function of a SARSA agent using Support Vector Regression for value function approximation .\nIn this case, advice was provided in the form of constraints on action values (e.g. \\textit{if} condition \\textit{then} $Q(s,a) \\geq1 $), and incorporated into the value function through the KBKR method.\nThis approach was extended in , by proposing a new way of defining constraints on action values.\nIn the new method, pref-KBKR (preference KBKR), the constraints were expressed in terms of action preferences (e.g. \\textit{if} condition \\textit{then} prefer action $a$ to action $b$).\nThis method was also used in .\nAnother possibility is given by the Knowledge-Based Neural Network method (KBANN) which allows incorporating\nknowledge expressed in the form of \\textit{if-then} rules into a neural network .\nThis method was used in RATLE, an advice-taking system based on Q-learning that used a neural network to approximate its Q-function . \n\\textit{General instructions} written in the form of \\textit{if-then} rules and \\textit{while-repeat} loops were incorporated into the Q-function using an extension of KBANN method.\nIn , a SARSA agent was augmented with an \\textit{Advice Unit} that computed additional action values.\n\\textit{General instructions} sere expressed in a specific formal language in the form of \\textit{if-then} rules. \nEach time a rule was activated in a given state, the value of the corresponding action was increased or decreased by a constant in the Advice Unit, depending on whether the rule advised for or against the action.\nThese values were then used for augmenting the values generated by the agent's value function approximator.\nModel-based value shaping with evaluative feedback has been investigated by Knox and Stone , by comparing different discount factors for the \\textit{human reinforcement function} $\\hat{H}$.\nThe authors demonstrated that setting the discount factor to zero was better suited, which came to consider $\\hat{H}$ as an action-value function more than a reward function\\footnote{The authors proposed another mechanism for handling temporal credit assignment, in order to alleviate the effect of highly dynamical tasks . In their system, human-generated rewards were distributed backward to previously performed actions within a fixed time window.}. \nThe numerical representation of evaluative feedback is used for modifying the Q-function rather than the reward function.\nOne of the shaping methods that they proposed, Q-Augmentation  uses the human reinforcement function $\\hat{H}$ for augmenting the MDP Q-function using: \n\\begin{equation} \\label{eq:q-augmentation}\nQ'(s,a)=Q(s,a)+\\beta*\\hat{H}(s,a),\n\\end{equation}\nwhere $\\beta$ is the same decaying weight factor as in Equation \\ref{eq:reward-shaping}.\nModel-based value-shaping can also be done with \\textit{contextual instructions}.\nIn , a robot built a model of contextual instructions in order to predict action values, which were used in turn for updating the value function. \n\\textbf{Policy shaping:}\nThe third shaping strategy is to integrate the advice directly into the agent's policy.\nExamples of model-free policy shaping with evaluative feedback can be found in  and .\nIn both methods, evaluative feedback was used for updating the actor of an Actor-Critic architecture. \nIn  the update term was scaled by the gradient of the policy:\n\\begin{equation} \nw \\gets w + \\alpha \\nabla_w \\ln \\pi_w(a_t|s_t) f_t.\n\\end{equation}\nIn , however, the authors did not consider a multiplying factor for evaluative feedback: \n\\begin{equation} \nw \\gets w + \\alpha f_t.\n\\end{equation}\nModel-free policy shaping with \\textit{contextual instructions} was considered in , in the context of an Actor-Critic architecture, where the error between the instruction and the \\textit{actor}'s decision was used as an additional term to the TD error for updating the \\textit{actor}'s parameters:\n\\begin{equation} \nw \\leftarrow w + \\alpha [k\\delta_t (a^E-a^A) + (1-k) (a^S-a^A)] \\nabla_w \\pi^A(s),\n\\end{equation}\nwhere $a^E$ is the actor's exploratory action, $a^A$ its deterministic action, $a^S$ the teacher's action, $\\pi^A(s)$ the actor's deterministic policy, and $k$ an interpolation parameter.\nKnox and Stone proposed two model-based policy shaping methods for evaluative feedback .\nAction Biasing uses the same equation as Q-Augmentation (Eq. \\ref{eq:q-augmentation}) but only in decision-making, so that the agent's Q-function is not modified: \n\\begin{equation} \\label{eq:action-biasing}\na^*=argmax_a [Q(s,a)+\\beta*\\hat{H}(s,a)].\n\\end{equation}\nThe second method, Control Sharing, arbitrates between the decisions of both value functions based on a probability criterion.\nA parameter $\\beta$ is used as a threshold for determining the probability of selecting the decision according to $\\hat{H}$:\n\\begin{equation} \\label{eq:control-sharing}\nPr(a=argmax_a[\\hat{H}(s,a)])=min(\\beta,1).\n\\end{equation}\nOtherwise, the decision is made according to the MDP policy.\nOther model-based policy shaping methods do not convert evaluative feedback into a scalar but into a categorical information .\nThe distribution of provided feedback is used within a Bayesian framework in order to derive a policy.\nThe method proposed in  outperformed Action Biasing, Control Sharing and Reward Shaping.\nAfter inferring the teacher's policy from the feedback distribution, it computed the Bayes optimal combination with the MDP policy by multiplying both probability distributions: $\\pi \\propto \\pi_R \\times \\pi_F$, where $\\pi_R$ is the policy derived from the reward function and $\\pi_F$ the policy derived from evaluative feedback.\nIn , both evaluative and corrective feedback were considered under a Bayesian IRL perspective.\nModel-based policy shaping can also be performed with \\textit{contextual instructions}.\nFor example, in , the RL agent arbitrates between the action proposed by its Q-learning policy and the one proposed by the instruction model, based on a confidence criterion:\n\\begin{equation}\n\\kappa_{\\pi}(s) = \\max_{a \\in A} \\pi(s,a) - \\max_{b \\in A;b \\neq a} \\pi(s,b).\n\\end{equation}\nThe same arbitration criterion was used in , to decide between the outputs of a an Instruction Model and a Task Model.\n\\textbf{Decision biasing:}\nIn the previous paragraphs, we said that policy shaping methods can be either model-free, by directly modifying the agent's policy or model-based, by building a model that is used at decision-time to bias the output of the policy.\nA different approach consists in using advice to directly bias the output of the policy at decision-time without corrupting the policy nor modelling the advice.\nThis strategy, that we call decision biasing, is the simplest way of using advice as it only biases the exploration strategy of the agent, without modifying any of its internal variables.\nIn this case, learning is done indirectly by experiencing the effects of following the advice.\nThis strategy has been mainly used in the literature with guidance and contextual instructions.\nFor example, in  guidance reduces the set of actions that the agent can perform at a given time-step.\nContextual instructions can also be used for guiding a robot along the learning process . \nFor example, in  and , a LfD system was augmented with verbal instructions, in order to make the robot perform some actions during the demonstrations.\nIn , in addition to model-free policy shaping, the provided instruction was also used for decision biasing.\nThe robot executed a composite real-valued action that was computed as a linear combination of the \\textit{actor}'s decision and the supervisor's instruction:\n\\begin{equation}\na \\leftarrow ka^E+(1-k)a^S,\n\\end{equation}\nwhere $a^E$ is the actor's exploratory action, $a^S$ the supervisor's action and $k$ an interpolation parameter.", "cites": [4113, 1352], "cite_extract_rate": 0.045454545454545456, "origin_cites_number": 44, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple concepts and cited works to present a structured framework of shaping methods. It introduces a generalized taxonomy of advice integration and distinguishes between model-free and model-based approaches, showing analytical depth. While it offers some critical points (e.g., the difference between immediate and delayed rewards), a deeper evaluation of limitations or trade-offs between methods is less evident."}}
{"id": "865ffc73-fda6-4a63-8d18-3e47ce8d5950", "title": "Comparing different interpretation methods", "level": "subsection", "subsections": [], "parent_id": "3b051b58-74ca-40ec-9a32-8c6238d45bb9", "prefix_titles": [["title", "Reinforcement learning with human advice: a survey."], ["section", "Discussion"], ["subsection", "Comparing different interpretation methods"]], "content": "In Section \\ref{interpretation}, we presented three main approaches for interpreting advice.\nThe classical approach, supervised interpretation, relies on annotated data for training linguistic parsers. \nEven though this approach can be effective for building systems that are able to take into account natural language advice, they come at the cost of constituting large corpora of language-to-command alignments.\nThe second approach, grounded interpretation, relaxes this constraint by relying on examples of task executions instead of perfrectly aligned commands. \nThis approach is easier to implement by taking advantage of crowd-sourcing platforms like Amazon Mechanical Turk.\nAlso, the annotation process is facilitated as it can be performed in the reverse order compared to the standard approach.\nFirst, various demonstrations of the task are collected, for example in the form of videos .\nThen, each demonstration is associated to a general instruction.\nEven tough this approach is more affordable than standard language-to-command annotation, it still comes at the cost of providing demonstrations, which can be challenging to provide in some contexts, as discussed in the previous section.\nThe third approach, RL-based interpretation, relaxes these constraints even more by relying only on a predefined performance criterion to guide the interpretation process . \nSome intermediate methods also exists, for example by deriving a reward function from demonstrations and then using a RL algorithm to interpret advice . \nGiven that reward functions can also be challenging to design, some methods rely on predefined advice for interpreting other advice , or a combination of both advice and reward functions .\nOrthogonal to the difference between supervised, grounded, and RL-based interpretation methods, we can distinguish two different strategies for teaching the system how to interpret unlabeled advice.\nThe first strategy is to teach the system how to interpret advice without using it in parallel for task learning.\nFor example, a human can teach an agent how to interpret continuous streams of contextual instructions by using evaluative feedback .\nHere, the main task for the agent is to learn how interpret unlabeled instructions, not to use them for learning another task.\nAnother example is when the agent is first provided with general instructions, either in the form of \\textit{if-then} rules or action plans; and then teaching it how to interpret these instructions using either demonstrations , evaluative feedback  or a predefined reward function .\nIn this case, even though the agent is allowed to interact with its environment, the main task is still to learn how to interpret advice, not to use it for task learning. \nThe second strategy consists in guiding a task-learning process by interactively providing the agent with unlabeled contextual advice. \nIn this case, the agent learns how to interpret advice at the same time as it learns to perform the task .\nFor example, in , the robot is provided with a set of hypotheses about possible tasks and advice meanings.\nThe robot then infers the task and advice meanings that are the most coherent with each other and with the history of observed advice signals. \nIn , task rewards are used for grounding the meaning of contextual instructions, which are used in turn for speeding-up the task-learning process.\nIt is important to understand the difference between these two strategies.\nFirst, when the agent learns how to interpret advice while using it for task learning, we must think about which shaping method to use for integrating the interpreted advice into the task-learning process (cf. Section \\ref{shaping}).\nSecond, when the goal is only to interpret advice, there is no challenge about the optimality nor the sparsity of the unlabeled advice.\nWith the first strategy, advice cannot be erroneous as it constitutes the reference for the interpretation process.\nEven though the methods implementing this strategy do not explicitly assume perfect advice, the robustness of the interpretation methods against inconsistent advice is not systematically investigated.\nWhen advice is also used for task learning, however, we need to take into account whether or not advice is correct with respect to the target task.\nFor example, in , the authors report the performance of their system under erroneous evaluative feedback.\nIn , the system is evaluated in simulation against various levels of error for both evaluative feedback and contextual instructions.\nAlso with the first strategy, advice signals cannot be sparse since they constitute the state-space of the interpretation process. \nFor instance, the standard RL methods that have been used for interpreting general instructions  cannot be used for interpreting sparse contextual instructions. \nIn these methods, instructions constitute the state-space of an MDP over which the RL algorithm is deployed, so they need to be instantiated on every time-step.\nThis problem has been addressed in , where the system was able to interpret sporadic contextual instructions, by using the TD error of the task-learning process.", "cites": [4113], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 4.3}, "insight_level": "high", "analysis": "The section effectively synthesizes the three main interpretation methods (supervised, grounded, RL-based) and connects them through a coherent analytical structure. It critically evaluates the assumptions, limitations, and trade-offs of each approach, particularly in the context of task learning and advice robustness. The abstraction level is strong as it generalizes beyond individual papers to propose two distinct teaching strategies and highlights broader implications for system design and learning paradigms."}}
{"id": "6182591c-bb5a-4ab1-bfa5-8b595783af13", "title": "Toward a unified view", "level": "subsection", "subsections": [], "parent_id": "3b051b58-74ca-40ec-9a32-8c6238d45bb9", "prefix_titles": [["title", "Reinforcement learning with human advice: a survey."], ["section", "Discussion"], ["subsection", "Toward a unified view"]], "content": "Overall, all forms of advice overcome the limitations of autonomous learning, by providing more control over the learning process.\nSince more control comes at the cost of more interaction load, the autonomy of the learning process is important for minimizing the burden on the human teacher.\nConsequently, many advice-taking systems combine different learning modalities in order to balance between autonomy and control.\nFor example, RL can be augmented with evaluative feedback , corrective feedback , instructions , instructions and evaluative feedback , demonstrations , demonstrations and evaluative feedback , or demonstrations, evaluative feedback and instructions .\nDemonstrations can be augmented with corrective feedback , instructions , instructions and feedback, both evaluative and corrective , or with prior RL  .\nIn , the authors proposed a framework for combining different learning modalities in a principled way. \nThe system could balance autonomy and human control by switching from demonstration to guidance to evaluative feedback, using a set of predefined metrics such as performance.\nIntegrating different forms of advice into one single and unified formalism remains an active research question.\nSo far, different forms of advice have been mainly investigated separately by different communities.\nFor example, some shaping methods have been designed exclusively for evaluative feedback and were not tested with other forms of advice such as contextual instructions, and the converse is also true.\nIn this survey, we extracted several aspects that were shared across different forms of advice.\nRegardless of the type of advice, we must ask the same computational questions as we go through the same overall process (Fig. \\ref{fig:advice}): \nFirst, we must think about how advice will be represented and whether its meaning will be predetermined or interpreted by the learning agent.\nSecond, we must decide whether to aggregate advice into a model, or directly use it for influencing the learning process (model-based vs. model-free shaping). \nFinally, we must choose a shaping method for integrating advice (or its model) into the learning process.\nFrom this perspective, all shaping methods that were specifically designed for evaluative feedback could also be used for instructions, and \\textit{vice-versa}.\nFor example, all the methods proposed by Knox and Stone for learning from evaluative feedback , can be recycled for learning from instructions. \nSimilarly, the confidence criterion used in  for learning from contextual instructions constitutes another Control Sharing mechanism, similar to the one proposed in  for learning from evaluative feedback.\n\\begin{figure}[h!]\n\\begin{center}\n\\includegraphics[trim= 2cm 7.5cm 0cm 4cm, clip=true,width=\\textwidth]{advice.pdf}\n\\caption{Shaping with advice, a unified view. When advice is provided to the learning agent, it has first to be encoded into an appropriate representation. If the mapping between teaching signals and their corresponding internal representation is not predetermined, then advice has to be interpreted by the agent. Then advice can be integrated into the learning process (shaping), either in a model-free or a model-based fashion. Optional steps, interpretation and modeling, are sketched in light grey.\n} \n\\label{fig:advice}\n\\end{center}\n\\end{figure}\nIt is also interesting to think about the relationship between interpretation and shaping. \nFor example, we can notice the similarity between interpretation and shaping methods. \nIn Section \\ref{interpretation}, we mentioned that some interpretation methods relying on the task-learning process can be either reward-based, value-based or policy-based.\nThis scheme is reminiscent of the different shaping methods: reward shaping, value shaping and policy shaping. \nFor instance, the policy shaping method proposed in  for combining evaluative feedback with a reward function is mathematically equivalent to the Boltzmann Multiplication method used in  for interpreting contextual instructions.\nSo by extension, the other ensemble methods that have been used for interpreting contextual instructions could also be used for shaping. \nWe also note that the confidence criterion in  was used for both interpreting instructions and policy shaping.\nSo, we can think of the relationship between shaping and interpretation as a reciprocal influence scheme, where advice can be interpreted from the task-learning process in a reward-based, value-based or a policy-based way, and in turn can influence the learning process in a reward-based, value-based or policy-based shaping way .\nThis view contrasts with the standard flow of the advice-taking process, where advice is interpreted before being integrated into the learning process .\nIn fact in many works, interpretation and shaping happen simultaneously, sometimes by using the same mechanisms .\nUnder this perspective, we can extend the similarity between all forms of advice to include also other sources of information such as demonstration and reward functions.\nAt the end, even though these signals can sometimes contradict each other, they globally inform about one same thing, \\textit{i.e.,} the task .\nUntil recently, advice and demonstration have been mainly considered as two complementary but distinct approaches, \\textit{i.e.,} communication vs. action .\nHowever, these two approaches share many common aspects.\nFor example, the counterpart of interpreting advice in the LfD literature is the correspondence problem, which is the question of how to map the teacher's states an actions into the agent's own states and actions.\nWith advice, we also have a correspondence problem that consists in interpreting the raw advice signals.\nSo, we can consider a more general correspondence problem that consists in interpreting raw teaching signals, independently from their nature.\nSo far, the correspondence problem has been mainly addressed within the community of learning by imitation. \nImitation is a special type of social learning in which the agent reproduces what it perceives. \nSo, there is an assumption about the fact that what is seen has to be reproduced.\nAdvice is different from imitation in that the robot has to reproduce what is communicated by the advice and not what is perceived.\nFor instance, saying \"turn left\", requires from the robot to perform the action of turning left, not to reproduce the sentence \"turn left\".\nHowever, evidence from neuroscience gave rise to a new understanding of the emergence of human language as a sophistication of imitation throughout evolution .\nIn this view, language is grounded in action, just like imitation .\nFor example, there is evidence that the mirror neurons of monkeys also fire to the sounds of certain actions, such as the tearing of paper or the cracking of nuts , and that spoken phrases about movements of the foot and the hand activate the corresponding mirror-neuron regions of the pre-motor cortex in humans .\nSo, one challenging question it whether we could unify the problem of interpreting any kind of teaching signal under the scope of one general correspondence problem. \nThis is a relatively new research question, and few attempts have been made in this direction.\nIn , the authors proposed a mathematical framework for learning from different sources of information.\nThe main idea is to relax the assumptions about the meaning of teaching signals, by taking advantage of the coherence between the different sources of information.\nWhen comparing demonstrations with instructions, we mentioned that some demonstration settings could be considered as a way of providing continuous streams of contextual instructions, with the subtle difference that demonstrations are systematically executed by the robot. \nConsidering this analogy, the growing literature about interpreting instructions  could provide insights for designing new ways of solving the correspondence problem in imitation.\nUnifying all types of teaching signals under the same view is a relatively recent research question , and this survey aims at pushing towards this direction by clarifying some of the concepts used in the interactive learning literature and highlighting the similarities that exist between different approaches.\nThe computational questions covered in this survey extend beyond the boundaries of Artificial Intelligence, as similar research questions regarding the computational implementation of social learning strategies are also addressed by the Cognitive Neuroscience community .\nWe hope this survey will contribute in bridging the gap between both communities.", "cites": [4113, 4114], "cite_extract_rate": 0.05, "origin_cites_number": 40, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.8}, "insight_level": "high", "analysis": "The section demonstrates strong insight by synthesizing multiple forms of advice into a unified framework, drawing connections between different types of signals (e.g., instructions, feedback, demonstrations). It provides critical analysis by highlighting that many methods are developed in isolation and suggests opportunities for cross-method integration. The abstraction level is particularly high, as the section identifies broader patterns such as the correspondence problem and proposes a meta-level perspective on advice integration."}}
