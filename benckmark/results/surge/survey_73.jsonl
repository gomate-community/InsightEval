{"id": "c1e3d948-68a4-41aa-9453-ed02e0504669", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "347fe6bc-3506-4394-85f3-39521077022f", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nUndoubtedly, future 5G and beyond mobile communication networks will have to address stringent requirements of delivering popular content at ultra high speeds and low latency due to the proliferation of advanced mobile devices and data rich applications. In that ecosystem, edge-caching has received significant research attention over the last decade as an efficient technique to reduce delivery latency and network congestion especially during peak-traffic times or during unexpected network  congestion episodes by bringing popular data closer to the end users. One of the main reasons of enabling edge caching in the network is to reduce the number of requests that traverse the access and core mobile network as well as reducing the load at the origin servers that would have to, otherwise, respond to all requests directly in absence of edge caching. In that case popular content and objects can be stored and served from edge locations, which are closer to the end users. This operation is also beneficial from the end user perspective since edge caching can dramatically reduce the overall latency to access the content and increase in the sense overall user experience. It is also important to note that the notion of popular content means that the requests of top 10\\% of video content on the Internet account for almost 80\\% of all traffic; which relates to multiple requests from different end users of the same content\n.\nRecently, deep learning (DL) has attracted significant attention from both academia and industry and has been applied to diverse domains like self-driving, medical diagnosis, playing complex games such as Go . DL has also made their way into communication areas . In this paper, we pay attention to the application of DL in caching policy. Though there are some earlier surveys related to machine learning applications, they either focus on general machine learning techniques for caching , or concentrate on overall wireless applications . The work  provides a big picture of applying machine learning in wireless communications. In , the authors consider the machine learning on both caching and routing strategy. A comprehensive survey on machine learning applications for caching content in edge networks is provided in . The researchers  provide a survey about machine learning on mobile edge caching and communication resources. On the other hand,  overviews how artificial neural networks can be employed for various wireless network problems. The authors in  detail a survey on deep reinforcement learning (DRL) for issues in communications and networking.  presents a comprehensive on deep learning applications and edge computing paradigm. Our work can be distinguished from the aforementioned papers based on the fact that we focus on the deep learning techniques on content caching and both wired and wireless caching are taken into account. Our main contributions are listed as follows:\n\\begin{itemize}\n\t\\item We classify the content caching problem into Layer 1 Caching and Layer 2 Caching. Each layer caching consists of four tightly coupled subproblems: where to cache, what to cache, cache dimensioning and content delivery. Related researches are provided accordingly. \n\t\\item We present the fundamentals of DL techniques which are widely used in content caching, such as convolutional neural network, recurrent neural network, actor-critic model based deep reinforcement learning, etc.\n\t\\item We analyze a broad range of state-of-the-art literature which use DL to content caching. These papers are compared based on the DL structure, layer caching coupled subproblems and the objective of DL in each scenarios. Then we discuss research challenges and potential directions for the utilization of DL in caching. \n\\end{itemize}\n\\begin{figure}[htb]\n\t\\centering\n\t\\includegraphics[trim=0mm 0mm 0mm 0mm, clip, width=\\textwidth]{figure/paper.eps}\n\t\\caption{Survey Architecture}\n\t\\label{fig:organization}\n\\end{figure}\nThe rest of this survey is organized as follows (as illustrated in Figure \\ref{fig:organization} ). Section \\ref{sec:caching} presents the categories of content caching problem. Section \\ref{sec:dl} reviews typical deep neural network structures. In Section \\ref{sec:dl_caching}, we list state-of-the-art DL-based caching strategies and their comparison. Section \\ref{sec:challenges} debates challenges as well as potential research directions. In the end, Section \\ref{sec:conclusions} concludes this paper. For better readability, the abbreviations in this paper is listed as Table \\ref{tab:abbr} shows.\n\\begin{table}[htb]\n\t\\centering\n\t\\caption{\\label{tab:abbr} List of Abbreviations.}\n\t\\begin{tabular}{l|p{.38\\textwidth}|l|p{.38\\textwidth}}\n\t\t\\hline\n\t\t\\textbf{Abbr.} & \\textbf{Description} & \\textbf{Abbr.} & \\textbf{Description} \\\\\n\t\t\\hline\n\t\t3C & Computing, Caching and Communication & A3C & Asynchronous Advantage Actor-Critic \\\\\n\t\t\\hline\n\t\tBBU & Baseband Unit & CCN & Content-Centric Network \\\\\n\t\t\\hline\n\t\tCNN & Convolutional Neural Network & CoMP-JT & Coordinated Multi Point Joint Transmission \\\\\n\t\t\\hline\n\t\tCR & Content Router & C-RAN & Cloud-Radio Access Network \\\\\n\t\t\\hline\n\t\tCSI & Channel State Information & D2D & Device to Device \\\\\n\t\t\\hline\n\t\tDDPG & Deep Deterministic Policy Gradient & DL & Deep Learning \\\\\n\t\t\\hline\n\t\tDNN & Deep Neural Network & DQN & Deep Q Network \\\\\n\t\t\\hline\n\t\tDRL & Deep Reinforcement Learning & DT & Digital Twin \\\\\n\t\t\\hline\n\t\tED & End Device & ES & Edge Server \\\\\n\t\t\\hline\n\t\tETSI & \\multicolumn{3}{l}{European Telecommunication Standardization Institute} \\\\\n\t\t\\hline\n\t\tESN & Echo-State Network & FIFO & First In First Out\\\\\n\t\t\\hline\n\t\tFNN & Feedforward Neural Network & FBS & Femto Base Station \\\\\n\t\t\\hline\n\t\tICN & Information-Centric Network & LFU & Least Frequently Used \\\\\n\t\t\\hline\n\t\tLP & Linear Programming & LRU & Least Recently Used \\\\\n\t\t\\hline\n\t\tLSTM & Long Short-Term Memory & MAR & Mobile Augmented Reality \\\\\n\t\t\\hline\n\t\tMD & Mobile Device & MILP & Mixed Integer Linear Programming \\\\\n\t\t\\hline\n\t\tMBS & Macro Base Station & NFV & Network Function Virtualization\\\\\n\t\t\\hline\n\t\tPNF & Physical Network Function & PPO & Proximal Policy Optimization \\\\\n\t\t\\hline\n\t\tQoE & Quality of Experience & RL & Reinforcement Learning \\\\\n\t\t\\hline\n\t\tRNN & Recurrent Neural Network & RRH & Remote Radio Head \\\\\n\t\t\\hline\n\t\tSAE & Sparse Auto Encoder & SDN & Software Defined Network \\\\\n\t\t\\hline\n\t\tseq2seq & Sequence to Sequence & SNM & Shot Noise Model \\\\\n\t\t\\hline\n\t\tTRPO & Trust Region Policy Optimization & TTL & Time to Live \\\\\n\t\t\\hline\n\t\tVNF & Virtual Network Function & WSN & Wireless Sensor Network \\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}", "cites": [3705, 3582, 3704, 3706, 3420, 7633, 3703], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The introduction section provides a general overview of edge caching and deep learning applications, citing several related surveys. It briefly mentions the scope and contributions of the current survey but does not deeply synthesize, compare, or abstract the cited works. The section remains largely descriptive, listing prior surveys without significant analysis or integration into a novel framework."}}
{"id": "93519284-cc11-4b04-b82f-f74d3c6a42fc", "title": "Layer 1 Caching", "level": "subsection", "subsections": [], "parent_id": "77d13750-5344-4e36-b9ce-8573ab07d97a", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Data Caching Review"], ["subsection", "Layer 1 Caching"]], "content": "In Layer 1 Caching, the popular content is considered to be hosted in CRs. In the context of Information-Centric Network (ICN), CR plays dual roles both as a typical router (i.e. data flow forwarding) and content store (i.e. local area data caching facility). Generally, the CR is connected via wired networks. Layer 1 Caching consists of four tightly coupled problems: where to cache, what to cache, cache dimensioning and content delivery.\nWhere to cache focuses on selecting the proper CRs to host the content. For instance, in Figure \\ref{fig:caching}, contents replicas can be placed in lower hierarchical level CRs, such as router B and C, as the mean of reducing transmission cost but with extra cost pay for hosting contents; reversely, consolidating caching in CR A can be adopted to saving caching cost at the expense of more transmission cost and has the risk of expiring end users' delay requirement. Here the caching/hosting cost is the cost to deploy the content, which could be measured by space utilization, energy consumption or other metrics. The transmission cost represents the price for delivering the content from cached CR (or data server) to end user and is basically estimated via the number of hops. Where to cache problem usually has been modelled as a Mixed Integer Linear Programming (MILP):\n\\begin{subequations} \n\\label{fml:MILP}\n\\begin{align}\n\\label{MILP:obj}\n\\mathop{\\min}_{\\substack{x}}\\; & c^T x\\\\\n\\textrm{s.t.}\\quad \\label{MILP:con1}\n& Ax\\leq b \\\\\n\\label{MILP:con2}\n& x\\in\\{0,1\\} \\\\\n\\label{MILP:con3}\n\\text{or}\\quad & x\\geq 0\n\\end{align}\n\\end{subequations}\nwhere $x$ is the decision variable. Normally it is a binary variable indicating the CR assignment. In special cases, with the aim of modelling or linearization, some non-binary auxiliary variables are introduced as constraint \\eqref{MILP:con3} shows. If taking caching a part of a file not the complete into consideration, the decision variable $x$ is a continuous variable representing the segments host in the CR, then constraint \\eqref{MILP:con2} becomes $x\\in[0,1]$ and MILP model turns to linear programming (LP). There are many work allocating contents via MILP with different objectives and limitations. The authors in  propose a model to minimize the user delay and load balancing level of CRs with the satisfaction of cache space. The work in  considers a trade-off between caching and transmission cost with cache space, link bandwidth and user latency constraints. In , an energy efficient optimization model is constructed consisting of caching energy and transport energy.  provides more details of mathematical model and related heuristic algorithms in caching deployment of wired networks. \nWhat to cache concentrates on selecting the proper contents in CRs for the purpose of maximizing the cache hit ratio. Via exploiting the statistical patterns of user requests, the popularity of requested information and user preference can be forecasted and play a very significant role in determining caching content. On the one hand, from the view of aggregated request contents, researchers propose many different models and algorithms for popularity estimation. One widely used model in web caching is the Zipf model based the assumption that the content popularity is static and each users' request is independent. However, this method fails to reflect the temporal and spatial correlations of the content, where the temporal correlation reflects the popularity varies over time and the spatial correlation represents the content preference is different on the geographical area and social cultural media. A temporal model named the shot noise model (SNM) is built in  which enables users to estimate the content popularity dynamically. Inspired by SNM, the work in  considers both spatial and temporal characteristics during caching decision. On the other hand, from the view of a specific end user during a certain period, caching his/her preference content (may not be the popular in network) can also help to reduce the traffic flow. Many approaches in recommendation systems can be applied in this case. Another aspect of what to cache problem is the designing of cache eviction strategies when storage space faces the risk of overflow. Depending on the life of caching contents, these policies can be divided into two categories roughly: one is like first in first out (FIFO), least frequently used (LFU), least recently used (LRU) and randomized replacement, the contents would not be removed until no more memory is available; the other one is called time to live (TTL) strategy, where the eviction happens once the related timer expires.  presents analytic model for hit ratio in TTL-based cache requested by independent and identically distributed flows. It worth noting that in , the TTL-based cache policy is used for the consistency of dynamic contents instead of contents replacement. In , the authors introduce a TTL model for cache eviction and the timer is reset once related content cache hit happens.\nCache dimensioning highlights how much storage space to be allocated. Benefit from the softwarization and virtualization technologies, the cache size in each CR or edge cloud can be managed in a more flexible and dynamical way, which makes the cache dimensioning decisions become an important feature in data caching. Technically, the cache hit ratio rises with the increasing of cache memory, and consequently eases the traffic congestion in the core network. However, excessive space allocation would waste the resource like energy to support the caching function. Hence there is a trade-off between cache size cost and network congestion. Economically, taking such scenario into consideration: a small content provider wants to rent service from a CDN provider such as Akamai or Huawei Cloud, and there is also a balance between investment saving and network performance. In , the proper cache size of individual CR in Content-Centric Network (CCN) is investigated via exploiting the network topology. In , the authors consider the effect of network traffic distribution and user behaviours when designing cache size.\nContent delivery considers how to transform the caching content to the requested user. The delivery traffic embraces single cache file downloading and video content steaming and the metrics for these two scenarios vary. Regarding file downloading, the content cannot be consumed until the delivery is completed. Therefore the downloading time of the entire file is viewed as a metric to reflect the quality of experience (QoE). For video steaming, especially for those large video splitted into several chunks, the delay limitation only works on the first chunk. In that case, delivering the first chunk in time and keep the smooth transmission of the rest chunks are the key aims . Apart from those measuring metrics, another problem in content delivery is the routing policy. In CCN , one implementation of ICN architecture, employs a flooding-based name routing protocol to publish the request among cached CRs. On one hand, flooding strategy simplifies the designing complexity and reduce the maintaining cost particularly in an unstable scenario; on the other hand, it costly wastes bandwidth resources. In , the authors discuss the optimal radius in scoped flooding. The deliver route is often considered jointly with where to cache problem, in which the objective function \\eqref{MILP:obj} includes both deployment and routing cost.", "cites": [3709, 3707, 3710, 3708], "cite_extract_rate": 0.25, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of Layer 1 caching and its subproblems, referencing multiple papers to support each aspect. While it does integrate the cited works into a structured explanation of the problem areas, the integration is mostly additive rather than synthesizing novel connections. There is minimal critical analysis or identification of broader trends or limitations in the cited works, and the discussion remains largely at the level of specific models and algorithms without higher-level abstraction."}}
{"id": "a2d45d41-6ccf-4b15-99cb-2ea399aa1ef2", "title": "Layer 2 Caching", "level": "subsection", "subsections": [], "parent_id": "77d13750-5344-4e36-b9ce-8573ab07d97a", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Data Caching Review"], ["subsection", "Layer 2 Caching"]], "content": "Contrast to Layer 1 caching in wired connection, Layer 2 caching considers implementing caching techniques in wireless network. Though both of them need solve where to cache, what to cache, cache dimensioning and content delivery problems, wireless caching is more challenging and some mature strategies in wired caching cannot be migrated directly to wireless case. Some reasons come from the listed aspects: the resources in wireless environment, such as caching storage and spectrum, are limited compared with CRs in Layer 1 Caching; the mobility of end users and dynamic network typologies are also required to be considered during the design of caching strategies; moreover, the wireless channels are uncertain since they can be effected by fading and interference.\nIn wireless caching, where to cache focus on finding the proper candidates among MBS, FBS, ED, even BBU pool and RRU in C-RAN to host the content. Caching at MBS and FBS can alleviate backhaul congestion since end users obtain the requested content from BS directly instead of from CR via backhaul links. Compared with FBS, MBS has wider coverage and typically, there is no overlap among different MBSs . As mentioned above, the caching space in BSs is limited and it is impractical to cache all popular content. With the aim of improving cache-hit ratio, a MILP-modelled collaborative caching strategy among MBSs is proposed in . If the accessed MBS does not host the content, the request will be served by a neighbour MBS which cache the file rather than by the data server. For FBS caching, a distributed caching method is presented in  and the main idea is that the ED locating in the FBS coverage overlap is able to obtain contents from multiple hosters. Caching at ED can not only ease backhaul congestion but also improve the area spectral efficiency . When the end user requests a content, he/she would be severed by the local storage if the content is precached in his/her ED or by adjacent ED via D2D communication if the content is host accordingly. In , the authors model the cache-enabled D2D network as a Poisson cluster process, where end users are grouped into several clusters and the collective performance is improved. Individually, caching the interested contents for other users affects personal benefit. In , a Stackelberg game model is applied to formulate the conflict among end users and a related incentive mechanism is designed to encourage content sharing. For the case of cache-enabled C-RAN, caching at BBU can ease the traffic congestion in the backhaul while caching at RRH can reduce the fronthaul communication cost. On the other hand, caching all at BBU raises the signaling overhead of BBU pool while at RRH weakens the processing capability. Therefore, where to cache the content in C-RAN makes a substantial contribution to balancing the signal processing capability at the BBU pool and the backhaul/fronthaul costs . The work in  investigates caching at RRHs with jointly considering cell outage probability and fronthaul utilization. Due to the end users' mobility, the prediction/awareness of user moving behaviour also influence the proper hoster selection. There are some researches exploiting user mobility in cache strategy designing like  and .\nSimilar with Layer 1, what to cache decision as well as eviction policy of layer 2 depends on the accurate prediction on content popularity or user preference in proactive caching method. The content popularity contains the feature of temporal and spatial correlations, which has already been described in Layer 1 Caching. In Layer 2 caching, the proper spatial granularity in popular contents estimation needs to take special attentions . For example, the coverage of MBS and FBS are different, which makes the popularity in MBS and FBS are different as well. Because the former based on a large number of users' behaviors but the individual may prefer specific content categories. For small cells, the preference estimation requires more accurate information like historical data . In order to capture the temporal and spatial dynamics of user preference, many different deep learning based algorithms are proposed, which will be illustrated in Section \\ref{sec:dl_caching}.\nCache dimensioning in Layer 2 Caching has more complicated factors need to be considered, not only including the network topology and content popularity as Layer 1 Caching, but also containing backhaul transmission status and wireless channel features. The proper cache size assignment is studied in the scenario of backhaul limited cellular network . It also provides the closed-form boundary of minimum cache size in one cell case. In the case of dense wireless network, the work in  quantifies the minimum required cache to achieve the linear capacity scaling of network throughput. The authors of  also consider the scenario of dense networks. They derive the closed-form of the optimal memory size which can reduce the consumption of backhaul capacity as well as guarantee wireless QoS.\nAccording to the number of transmitters and receivers, we divide the content delivery in Layer 2 caching into three categories: one candidate serves one end user, such as unicast and D2D transmission; one candidate serves multiple users like multicast; and coordinated delivery including multiple transmitters serve one or more receivers like coordinated multi-point joint transmission (CoMP-JT). Once the requested content is cached locally, BS can serve the end user via unicast or the adjacent device shares the contents by implementing D2D transmission. Concurrent transmission has the risk of co-channel interference in dense deployed networks. In D2D network, link scheduling is introduced to select subsets of links to transmit simultaneously . With the aim  of improving the spectral efficiency, multicast is applied in content delivery when serving multiple requests simultaneously with the same content. Therefore there is a trade off between spectral efficiency and service delay. For the aim of serving more users in one transmission as well as higher spectral efficiency, the BS will wait to collect enough requirement for the same content which makes the first request a long waiting time. An optimal dynamic multicast scheduling is proposed in  to balance these two factors. Multicast can also serve multiple requests with different contents. In , the authors provides a coded caching scheme which requires the communication link is error free and each user caches a part of its own content and partial of other users. Then BS multicasts the coded data to all users. Each user can decode his own requested content by XOR operation between the received data and the precached other users' file. However, the coding complexity increases exponentially as the quantity of end users grows. The CoMP-JT can improve the spectral efficiency as well via sharing channel state information (CSI) and contents among BSs but it also needs high-capacity backhaul consumption for exchanging data. In C-RAN, the BBUs are centralized in the BBU pool, which makes communication among BSs very efficiency.  designs CoMP-JT in C-RAN for the purpose of minimizing power consumption with limitations of transmission energy, link capacity and requested QoS.", "cites": [3712, 3716, 8682, 3714, 3717, 3713, 8681, 3715, 3711], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to present a coherent narrative on Layer 2 caching challenges and strategies in wireless networks. It abstracts general issues such as mobility, backhaul limitations, and trade-offs in delivery methods. While it provides a clear analytical structure and highlights key differences between caching at various network components, it could offer a more in-depth critical evaluation of the limitations or drawbacks of the proposed methods."}}
{"id": "17a86b07-b7c3-41f4-9b7f-05a0e6ea6ae0", "title": "Feedforward Neural Network (FNN)", "level": "subsection", "subsections": [], "parent_id": "c0dca0bc-798e-46e6-903c-5acb8a815d31", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Feedforward Neural Network (FNN)"]], "content": "FNN is a kind of DNNs whose information propagation direction is forward and there is no cycle in neurons. In this paper, the term FNN is used to represent fully connected neural network, which indicates the connection between two adjacent layers is filled. According to the Universal Approximation Theorem, FNN has the ability to approximate any closed and bounded function with enough neurons in hidden layer . The hidden layer is applied to extract features of input vector, and then feed the output layer, which works as a classifier. Though FNN is very powerful, it gets into trouble when dealing with real-world task such as image recognition due to enormous weight parameters (because of fully connected) and lack of data augmentation.", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of FNNs and their properties, citing a general paper on deep learning. However, it lacks integration of deeper insights from the cited work and does not connect multiple sources or engage in comparative or critical analysis. The generalization is minimal, focusing only on the structure and limitations of FNNs without broader meta-level discussion."}}
{"id": "f6c49080-7dcd-4236-b8c7-a294adda5983", "title": "Convolutional Neural Network (CNN)", "level": "subsection", "subsections": [], "parent_id": "c0dca0bc-798e-46e6-903c-5acb8a815d31", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Convolutional Neural Network (CNN)"]], "content": "For the aim of overcoming the aforementioned drawback of FNN, CNN employs convolution and pooling operations, where the former applies sliding convolutional filters to the input vector and the later does down sampling, usually via maximum or mean pooling. Generally, CNN tends to contain deeper layers and smaller convolutional filters, and the structure becomes fully convolutional network , reducing the ratio of pooling layers as well as fully connected layers. Taxonomically, CNN belongs to FNN and has been broadly employed in image recognition, video analysis, natural language processing, etc. Including CNN, one of the limitations of FNN is that the output only depends on current input vectors. So it is hard to deal with sequential tasks.", "cites": [810], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of CNN, including its operations and applications, and references one paper to support the concept of fully convolutional networks. However, it lacks integration of broader ideas from the cited work and does not critically analyze or compare different approaches. The explanation remains at a surface level without identifying trends or principles in the application of CNN to edge caching."}}
{"id": "12cbfae2-b31a-489d-8357-cb9dedb5c4fa", "title": "Recurrent Neural Network (RNN)", "level": "subsection", "subsections": ["15615e34-4c33-4644-b6a2-80e552ca205d", "f028c925-baae-474d-ae9c-0ae54c3029f3", "bf6576fa-f9cd-4916-b753-6a9371a05fee"], "parent_id": "c0dca0bc-798e-46e6-903c-5acb8a815d31", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Recurrent Neural Network (RNN)"]], "content": "In order to deal with sequential tasks and using historical information, RNN employs neurons with self feedback in hidden layers. Unlike the hidden neuron in FNN, the output of recurrent neuron depends on both current output of precious layer and last hidden state. Compared with FNN approximates any continues functions, RNN with Sigmoid activation function can simulate a universal Turing Machine and has the ability to solve all computational problems . It is worth noting that RNN has the risk to suffer from long-term dependencies problem  including gradient exploding and vanishing. Additionally, RNN has more parameters waiting to be trained due to adding recurrent weights. In the following, we introduce some RNN variants as Figure \\ref{fig:RNN} shows.\n\\begin{figure}[htb]\n    \\centering\n    \\includegraphics[trim=0mm 0mm 35mm 0mm, clip, width=\\textwidth]{figure/RNN.eps}\n    \\caption{RNN Variants}\n    \\label{fig:RNN}\n\\end{figure}", "cites": [166], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of RNNs, their structure, and limitations, but it does not effectively synthesize or integrate information from the cited paper. There is minimal critical analysis of the works or identification of broader patterns or principles in the use of RNNs for data caching in edge networks."}}
{"id": "15615e34-4c33-4644-b6a2-80e552ca205d", "title": "Echo-State Network (ESN)", "level": "subsubsection", "subsections": [], "parent_id": "12cbfae2-b31a-489d-8357-cb9dedb5c4fa", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Recurrent Neural Network (RNN)"], ["subsubsection", "Echo-State Network (ESN)"]], "content": "As aforementioned, simple RNN contains more parameters in training step, where the recurrent weights and input weights are difficult to learn . The basic idea of ESN is fixing these two kinds of weights and only learn the output weights (as links highlighted in Figure \\ref{fig:RNN}). The hidden layer is renamed as reservoir in ESN, where the neurons are sparsely connected and the weights are randomly assigned. The recurrent weights keep constant so the information of previous moments is stored in the reservoir with constant weight like voice echoing.", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic, descriptive overview of Echo-State Networks (ESNs) without synthesizing insights from multiple papers. It merely paraphrases the definition and structure of ESNs, relying on a single general paper on deep learning without specific references to ESNs in the context of edge caching. There is no critical evaluation or abstraction to broader principles or trends in the field."}}
{"id": "f028c925-baae-474d-ae9c-0ae54c3029f3", "title": "Long Short-Term Memory (LSTM)", "level": "subsubsection", "subsections": [], "parent_id": "12cbfae2-b31a-489d-8357-cb9dedb5c4fa", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Recurrent Neural Network (RNN)"], ["subsubsection", "Long Short-Term Memory (LSTM)"]], "content": "Recently, an efficient way to cope with long-term dependencies in practical is employing gated RNN, including LSTM . Then we compare with the recurrent neuron in simple RNN: Internally LSTM introduces three gates to control signal propagation, where input gate $I$ decides the partition of input signal to be stored, forget gate $F$ controls ratio of last moment memory to be kept until next period (the name \"forget gate\" may be a little misleading because it actually represents the ratio to be remembered) and output gate $O$ influences the proportion of current state to be delivered; Externally LSTM has four inputs embracing one input signal and three control signals for three gates. All these four signals are derived via the calculation of current network input and last moment delivered state.", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of LSTM and its gate mechanisms but does not meaningfully synthesize or integrate the content from the cited paper. It lacks critical evaluation of the approaches or limitations, and no broader abstraction or meta-level insights are drawn. The content remains at a surface-level explanation of the technique."}}
{"id": "bf6576fa-f9cd-4916-b753-6a9371a05fee", "title": "Pointer Network", "level": "subsubsection", "subsections": [], "parent_id": "12cbfae2-b31a-489d-8357-cb9dedb5c4fa", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Recurrent Neural Network (RNN)"], ["subsubsection", "Pointer Network"]], "content": "A typical application of RNN is converting one sequence to another sequence (seq2seq) such as machine translation. Conventionally, the output of seq2seq architecture is a probability distribution of output dictionary. However, it cannot deal with the problem that the size of output relies on the length of input due to fixed output dictionary. In , the authors modify the output to be the distribution of input sequence, which is analogous to pointers in C/C++. Pointer network has been widely used in text condensation.", "cites": [167], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Pointer Network and its relevance to RNNs, but it lacks substantial synthesis of ideas beyond a single cited paper. There is minimal critical analysis or identification of broader patterns or principles in the use of Pointer Networks for data caching in edge networks."}}
{"id": "66cec1fd-1f07-4527-b02b-b98ccb6507c3", "title": "Auto Encoder", "level": "subsection", "subsections": [], "parent_id": "c0dca0bc-798e-46e6-903c-5acb8a815d31", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Auto Encoder"]], "content": "Auto Encoder is a stack of two NNs named encoder and decoder respectively, where the former tries to learn the representative characteristics of input and generate a related code, and the later reads the code and reconstructs the original input. In order to avoid the auto encoder simply copying the input, some restrictions are considered like the dimension of code is smaller than input vector . The quality of auto encoder can be measured via reconstruction error, which estimates the similarity between input and output. In most cases, the auto encoder is used for the proper representation of input vector so the decoder part is removed after unsupervised training. The code can be employed as input for further deep learning models.", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of autoencoders without effectively integrating the cited paper's content or making connections to broader themes in deep learning for edge caching. It lacks critical evaluation of the method or its applications, and offers no abstraction or generalization of patterns or principles from the cited work."}}
{"id": "d2c418b7-118c-42b8-b9d8-37f74087f743", "title": "DNN as Critic (Value-Based)", "level": "subsubsection", "subsections": [], "parent_id": "cbd77acf-a82e-43b7-9bdd-bef9313a0f73", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Deep Reinforcement Learning (DRL)"], ["subsubsection", "DNN as Critic (Value-Based)"]], "content": "In value-based method, DNN does not get involved with policy decision but estimates the policy performance. Two functions are introduced for the measurement: $V^\\pi(s)$ represents the reward expectation of policy $\\pi$ starting from state $s$; $Q^\\pi(s,a)$ illustrates the reward expectation of policy $\\pi$ starting from state $s$ and taking action $a$. In addition, $V^\\pi(s)$ is the expected value of $Q^\\pi(s,a)$. If we can estimate $Q^\\pi(s,a)$, the policy $\\pi$ can also be improved by choosing the action $a^*$ hold $Q^\\pi(s,a^*)\\geq V^\\pi(s)$. So the DNN employed in agent is approximating function $Q^\\pi(s,a)$, where the inputs are state $s$ and action $a$ and output is the estimated value $Q^\\pi(s,a)$. There are some representative critic methods like Deep Q Networks (DQN)  and its variants Double DQN , Dueling DQN , etc.", "cites": [1409, 1408], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.5, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a basic description of value-based DRL methods and the role of DNN as a critic. It introduces key concepts like $V^\\pi(s)$ and $Q^\\pi(s,a)$ and mentions representative works such as DQN, Double DQN, and Dueling DQN. However, it lacks deeper synthesis between the cited papers, offers no critical evaluation of their strengths or weakn"}}
{"id": "4e5ada48-bb01-4d53-88dc-d7516f8cb54c", "title": "DNN as Actor (Policy-Based)", "level": "subsubsection", "subsections": [], "parent_id": "cbd77acf-a82e-43b7-9bdd-bef9313a0f73", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Deep Reinforcement Learning (DRL)"], ["subsubsection", "DNN as Actor (Policy-Based)"]], "content": "In policy-based method, DNN gets involved in the action selection directly instead of via $Q^\\pi(s,a)$. The policy can be viewed as an optimization problem, where the objective function is maximizing reward expectation and the search space is policy space. The input of DNN is current state and output is the probability distribution of potential actions. By employing gradient ascent, we can update the DNN to provide better action then maximize total reward. Some popular algorithms include Trust Region Policy Optimization (TRPO) , Proximal Policy Optimization (PPO) .", "cites": [2219, 1391], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of policy-based DRL methods and their use of DNNs, mentioning TRPO and PPO as examples. It integrates minimal information from the cited papers without elaborating on their unique contributions or comparing them. There is little abstraction or critical evaluation, and the narrative remains largely factual without deeper insights or synthesis of broader trends."}}
{"id": "92185b95-3c92-401c-8a45-161c99f6f228", "title": "Actor-Critic Model", "level": "subsubsection", "subsections": [], "parent_id": "cbd77acf-a82e-43b7-9bdd-bef9313a0f73", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning Outline"], ["subsection", "Deep Reinforcement Learning (DRL)"], ["subsubsection", "Actor-Critic Model"]], "content": "Generally, compared with policy-based approach, the value-based method is less stable and suffer from poor convergence since the policy is derived based on $Q^\\pi(s,a)$ approximation. But value-based method is more sample efficient, while policy-based method is easier to fall into local optimal solution because the search space is vast. The actor-critic model combines these two approaches, i.e. the agent contains two DNNs named actor and critic respectively. In each training iteration, the actor considers current state $s$ and policy $\\pi$ for deciding action $a$. Then the environment changes to state $s'$ and returns reward $r$. The critic updates its own parameters based on the feedback from environment and output a mark for the actor's action. The actor updates the policy $\\pi$ depending on critic's mark. Some typical algorithms are proposed recent years like Deep Deterministic Policy Gradient (DDPG)  and Asynchronous Advantage Actor-Critic (A3C) .", "cites": [1390], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the actor-critic model within DRL, contrasting value-based and policy-based approaches. It integrates key concepts from the cited paper (A3C) and generalizes the role of the actor and critic components. However, the synthesis is limited to one paper, and the critical analysis remains surface-level without deeper evaluation of limitations or trade-offs."}}
{"id": "3f275de1-b1c2-48e0-ae07-49947668a760", "title": "Deep Learning for Data Caching", "level": "section", "subsections": ["64ba4f69-8c92-44ec-9c4d-6de443abc8ba", "8a40aabe-271f-491f-92cb-1da70950927c", "67ee9973-0fe6-40d1-b44e-d9d39d452c5f", "c63a6eb6-9774-4886-b68b-d0543cf7f42c"], "parent_id": "347fe6bc-3506-4394-85f3-39521077022f", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning for Data Caching"]], "content": "\\label{sec:dl_caching}\nWe divide the studies regarding deep learning for data caching in edge networks into four categories depending on the DL tools employed: FNN and CNN; RNN; Auto Encoder; DRL. Recently many works utilize more than one DL techniques for jointly considered caching problems. For instance, at the beginning we applies a RNN to predict content popularity, and then a DRL to find suboptimal solutions of content placement for the purpose of reducing time complexity. In such case, we classify the related work into DRL since it represents the caching allocation policy. Unless mention the caching location (such as CRs, MBSs, FBSs, EDs and BBUs) otherwise, the approaches in this section can be utilized for both Layer 1 and Layer 2 caching. Table \\ref{tab:summary} summarize some studies of DL for caching.\n\\begin{comment}\n\\begin{longtable}[c]{p{.12\\textwidth}|l|p{.2\\textwidth}|p{.23\\textwidth}|p{.26\\textwidth}}\n    \\caption{Summary of Deep Learning for Data Caching in Edge Network}\n    \\label{tab:summary1}\\\\\n    \\hline\n    \\textbf{DL method} & \\textbf{Study} & \\textbf{Caching Problem} & \\textbf{Caching Objective} & \\textbf{DL Objective} \\\\\n    \\endfirsthead\n    \\hline\n    \\textbf{DL method} & \\textbf{Study} & \\textbf{Caching Problem} & \\textbf{Caching Objective} & \\textbf{DL Objective} \\\\\n    \\endhead\n        \\hline\n        CNN &  & L1 where to cache \\& Content Delivery & minimize caching \\& transmission cost & nominate proper CRs for caching \\\\\n        \\hline\n        CNN &  & L1 where to cache \\& Content Delivery & minimize caching \\& transmission cost & reduce feasible region of CRs for caching \\\\\n        \\hline\n        CNN\\& FNN &  & L2 content delivery & minimize transmission time/energy & reduce feasible region of time slot allocation for delivery \\\\\n        \\hline\n        FNN &  & L2 where to cache \\& content delivery & minimize energy cost & determine proper MBSs for caching \\& time duration for delivery \\\\\n        \\hline\n        Auto-encoder &  & L1 \\& L2 what to cache & improve cache hit ratio  & predict content popularity \\\\\n        \\hline\n        ESN &  & L2 what to cache \\& content delivery & minimize transmit power & predict content popularity \\& user mobility \\\\\n        \\hline\n        DRL &  & L2 what to cache \\& content delivery & minimize service latency & decide content caching, computing offloading and radio resource allocation \\\\\n        \\hline\n        LSTM &  & L2 what to cache & improve cache hit ratio & predict content popularity \\\\\n        \\hline\n        FNN &  & L2 what to cache & save traffic data & predict visiting content \\\\\n        \\hline\n        RNN \\& DRL &  & L2 what to cache \\& where to cache & minimize service latency & predict popularity \\& decide content caching and offloading \\\\\n        \\hline\n        FNN &  & L1 what to cache & improve cache hit ratio & predict content popularity \\\\\n        \\hline\n        Auto-encoder \\& RNN &  & L2 content delivery & minimize transmission delay & reduce traffic load \\& select optimal BS subset\\\\\n        \\hline\n         CNN &  & L2 what to cache & minimize service latency & predict visiting content \\\\\n        \\hline\n         ESN \\& LSTM \\& DRL &  & L2 where to cache \\& what to cache \\& content delivery & minimize delay \\& power consumption & predict user mobility, content popularity \\& determine D2D link \\\\\n        \\hline\n        DRL &  & L2 where to cache \\& content delivery & minimize content access cost & decide content caching \\& bandwidth allocation \\\\\n        \\hline \n        DRL &  & L2 where to cache \\& what to cache & maximize cache hit ratio \\& minimize transmission delay & decide cache replacement \\\\\n        \\hline\n        auto-encoder \\& DRL &  & L2 content delivery & minimize average delay and power consumption & decide multicast scheduling and caching jointly\\\\\n        \\hline\n        DRL &  & L2 what to cache & maximize traffic load served by FBSs & decide which and how many contents be cached\\\\\n        \\hline\n        DRL &  & L2 what to cache & minimize transmission power & predict users' preference and content popularity \\\\\n        \\hline\n        DRL &  & L2 what to cache & maximize network operator's utility & decide content replacement \\\\\n        \\hline\n        DRL &  & L2 what to cache & minimize service latency & decide cache replacement \\& power allocation \\\\\n        \\hline\n        RNN &  & L2 what to cache & improve cache hit ratio & predict content popularity \\\\\n        \\hline\n        DRL &  & L2 what to cache & maximize overall content quality & decide cache replacement \\\\\n        \\hline \n        DRL &  & L2 what to cache & maximize cache hit ratio & decide caching strategy \\\\\n        \\hline\n        DRL &  & L2 what to cache & maximize energy efficient & decide which content to be cached \\\\\n        \\hline\n        CNN \\& RNN \\& DRL &  & L2 what to cache & maximize cost reduction of operators & predict popularity \\& searching best NN model \\\\\n        \\hline\n        LSTM &  & L2 what to cache & maximize cache hit ratio & decide content replacement \\\\\n        \\hline\n        DRL &  & L2 where to cache & maximize QoE & decide content placement \\\\\n        \\hline\n        DRL &  & L2 what to cache & improve cache hit ratio & decide content update \\\\\n        \\hline\n        DRL &  & L2 what to cache & minimize transmitting cost & decide cache replacement \\\\\n        \\hline\n        auto-encoder &  & L2 what to cache & improve backhaul offloading \\& user satisfaction & predict content popularity \\\\\n        \\hline\n        DRL &  & L2 what to cache & minimize cumulative cost & determine caching content \\\\\n        \\hline \n        DRL &  & L2 what to cache & maximize chunk hit ratio \\& minimize service time & determine caching chunks \\\\\n        \\hline\n\\end{longtable}\n\\end{comment}\n\\begin{table}[htb]\n\\caption{Summary of Deep Learning for Data Caching}\n\\label{tab:summary}\n\\begin{tabular}{p{.07\\textwidth}|p{.1\\textwidth}|p{.28\\textwidth}|p{.43\\textwidth}}\n    \\hline\n    \\textbf{method} & \\textbf{Study} & \\textbf{Caching Problem} & \\textbf{DL Objective}\\\\\n    \\hline\n    \\multirow{8}{.07\\textwidth}{FNN and CNN} &  & content delivery & reduce feasible region of time slot allocation\\\\\n    \\cline{2-4}\n    &  & where to cache, content delivery & determine MBSs for caching \\& delivery duration \\\\\n    \\cline{2-4}\n    &  & where to cache, content delivery & nominate proper CRs for caching \\\\\n    \\cline{2-4}\n    &  & where to cache, content delivery & reduce feasible region for caching \\\\\n    \\cline{2-4}\n    &  & what to cache & extract video features\\\\\n    \\cline{2-4}\n    &  & what to cache & predict requested content \\& frequency \\\\\n    \\cline{2-4}\n    &  & what to cache & predict requested content \\\\\n    \\cline{2-4}\n    &  & what to cache & predict content popularity\\\\\n    \\hline\n    \\multirow{4}{.07\\textwidth}{RNN} &  & what to cache & predict requested content \\& user mobility \\\\\n    \\cline{2-4}\n    &  & what to cache & predict content popularity \\\\\n    \\cline{2-4}\n    &  & content delivery & reduce traffic load, select optimal BS subset\\\\\n    \\hline\n    \\multirow{3}{.07\\textwidth}{Auto Encoder} &  & what to cache & predict content popularity \\\\\n    \\cline{2-4}\n    &  & what to cache & predict top popular contents \\\\\n    \\hline\n    \\multirow{17}{.07\\textwidth}{DRL}  & & what to cache & decide cache placement \\\\\n    \\cline{2-4}\n    &  & what to cache & decide cache replacement \\& power allocation \\\\\n    \\cline{2-4}\n    &  & what to cache & predict popularity \\& searching best NN model \\\\\n    \\cline{2-4}\n    &  & where to cache & decide cache location \\\\\n    \\cline{2-4}\n    &  & content delivery & users grouping\\\\\n    \\cline{2-4}\n    &  & where to cache, content delivery & decide BS connection, computation offloading \\& caching location\\\\\n    \\cline{2-4}\n    &  & what to cache, content delivery & decide caching \\& bandwidth allocation \\\\\n    \\cline{2-4}\n    & & what to cache, content delivery & decide caching, computing offloading \\& radio resource allocation \\\\\n    \\cline{2-4}\n    &  & what to cache, content delivery & decide multicast scheduling \\& caching replacement\\\\\n    \\cline{2-4}\n    &  & where \\& what to cache & predict popularity, decide caching \\& task offloading \\\\\n    \\cline{2-4}\n    &  & where \\& what to cache, content delivery & predict user mobility \\& content popularity, determine D2D link\\\\\n    \\hline\n\\end{tabular}\n\\end{table}", "cites": [3721, 7746, 3718, 3719, 3723, 3722, 3720], "cite_extract_rate": 0.16279069767441862, "origin_cites_number": 43, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of deep learning methods used in edge caching by listing DL techniques and their associated caching objectives. It shows minimal synthesis by grouping studies based on DL types but lacks deeper integration of ideas across papers. There is little to no critical evaluation of the approaches or their limitations, and abstraction is limited to identifying broad DL categories without offering higher-level insights or principles."}}
{"id": "64ba4f69-8c92-44ec-9c4d-6de443abc8ba", "title": "FNN and CNN", "level": "subsection", "subsections": [], "parent_id": "3f275de1-b1c2-48e0-ae07-49947668a760", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning for Data Caching"], ["subsection", "FNN and CNN"]], "content": "In , the content delivery problem in wireless network is formulated as two MILP optimization models with the aims of minimum delivery time slot and energy consumption respectively. Both models consider the data rate for content delivery. Considering the computational complexity of solving MILP, a CNN is introduced to reduce the feasible region of decision variables, where the input is channel coefficients matrix. The FNN in paper  plays a similar role as  to simplify the searching space of the content delivery optimization model. \nFor resource allocation problem, the authors of  model it as linear sum assignment problems then utilize CNN and FNN to solve the model. The idea is extended in  and , where the authors consider where to cache problem among potential CRs and content delivery jointly, which is modeled as MILP with the aim of balancing caching and transmission cost by considering the user mobility, space utilization and bandwidth limitations. The cache allocation is viewed as multi-label classification problem and is decomposed into several independent sub-problems, where each one correlates with a CNN to predict assignment. The input of CNN is a grey-scale image which combines the information of user mobility, space and link utilization level. In , a hill climbing local search algorithm is provided to improve the performance of CNN while in , the prediction of CNN is used to feed a smaller MILP model. \nFor these above works , the FNN or CNN input is extracted from the optimization model. The work in  trains a CNN via original graph instead of parameters matrix/image, which makes the process human recognizable and interpretable. Though the authors take traveling salesman problem not data caching as an example, the method can be viewed as a potential research direction. \nIn , an ILP model is proposed to minimize the backhaul video-data type load by determining the portion of cached content in BSs. Considering the fact that the mobile users covered by a BS change frequently, therefore predicting user preference is unnecessary. Instead, the authors concentrate on the popular content in general. At the beginning, a 3D CNN is introduced to extract spatio-temporal features of videos. The popularity of new contents without historical information is determined via comparing similar video features. The authors of  also considers the spatio-temporal features among visiting contents in a mobile bus WiFI environment. By exploiting the previous 9 days collecting data, the content that the user may visit on the last day and corresponding visiting frequency can be forecast. The social property is taken into account in . By observing users interests on tweets during 2016 U.S. election, a CNN based predicted model can foresee the content category that is most likely to be requested. Such kind of content would be cached in MBSs and FBSs.\nThe work of  examines the role of DNN in caching from another aspect. The authors propose a FNN to predict content popularity as a regression problem. The results show that FNN outperforms RNN, though the later is believed to be effective to solve sequential predictions. Moreover, replacing the FNN by a linear estimator does not devalue the performance significantly. The author provides explanation that FNN would work better than linear predictor in the case of incomplete information, and RNN has more advantages to model the popularity prediction as a classification rather than a regression problem.", "cites": [3723, 3721], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present how FNN and CNN are used in data caching optimization, making connections between different problem formulations and methodologies. It includes some critical evaluation, such as noting that FNN outperforms RNN in a specific context and that replacing FNN with a linear estimator does not degrade performance much. However, the abstraction remains limited to general observations about spatio-temporal features and caching strategies without deeper conceptual frameworks."}}
{"id": "8a40aabe-271f-491f-92cb-1da70950927c", "title": "RNN", "level": "subsection", "subsections": [], "parent_id": "3f275de1-b1c2-48e0-ae07-49947668a760", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning for Data Caching"], ["subsection", "RNN"]], "content": "Considering RNN is superior in dealing with sequential tasks, the work  applies a bidirectional RNN for online content popularity prediction in mobile edge network. Simple RNN's output depends on previous and current storage, but the bidirectional can also take future information into account. The forecast model consists three blocks cascadingly: a CNN reads user requests and extract features; bidirectional LTSM learns association of requests over time step; FNN is added in the end to improve the prediction performance. Then content eviction is based on the popularity prediction. \nThe authors in  utilize ESN to predict both content request distribution and end user mobility pattern. The user's preference is viewed as context which links with personal information combining gender, age, job, location, etc.. For the request prediction, the input of ESN is user's information vector and the output represent the probability distribution of content. For mobility prediction, the input includes historical and present user's location and the output is the expected position for next time duration. Eventually, the prediction influences the caching content decisions in BBUs and RRHs for the purpose of minimizing traffic load and delay in CRAN. The authors extend their work in  by introducing conceptor-based ESN which can split users' context into different patterns and learn them independently. Therefore a more accurate prediction is achieved. \nIn , a caching decision policy named PA-Cache is proposed to predict time-variant video popularity for cache eviction when the space is full. The temporal content popularity is exploited by attaching every hidden layer representation of RNN to an output regression. In order to improve the accuracy, hedge backpropagation is introduced during training process which decides when and how to adapt the depth of the DNN in an evolving manner. Similarly, the work in  also considers caching replacement of video content. A deep LSTM network is utilized for popularity prediction consisting of stacking multiple LSTM layers and one softmax layer, where the input of the network is request sequence data (device, timestamp, location, title of video) without any prepossessing and the output is estimated content popularity. Another work concentrates on prediction and interactions between user mobility and content popularity can be found .\nThe work of  recognizes the popularity prediction as a seq2seq modeling problem and proposes LSTM Encoder-Decoder model. The input vector consists of past probabilities where each vectors are calculated during a predefined time window. In , the authors focus on caching content delivery with the aim of minimizing BSs to cover all requested users, i.e. set cover problem, via coded caching. Unlike , an auto encoder is introduced in coded caching stage for file conversion to reduce transmission load. In addition, a RNN model is employed to select BSs for broadcasting. \nThe paper  shows the potential of RNN in solving where to cache problem. In , a task allocation model is formulated as a knapsack problem and the decision variables represent the task is processed locally in mobile devices (MDs) or remotely in edge servers (ESs). The authors design a multi-pointer network structure of 3 RNNs, where 2 encoders encode MDs and ESs respectively, 1 decoder demonstrates ES and MD pairing. Considering the similarity of where to cache optimization model and knapsack problem, the multi-pointer network can be transferred for caching location decision after according parameter modifications.", "cites": [3720], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of how RNNs and related variants are used in edge caching research. It connects different papers by mentioning their use of RNNs for popularity prediction and caching decisions, but lacks deeper synthesis or a unifying framework. There is minimal critical analysis or abstraction to broader patterns or principles."}}
{"id": "c63a6eb6-9774-4886-b68b-d0543cf7f42c", "title": "DRL", "level": "subsection", "subsections": [], "parent_id": "3f275de1-b1c2-48e0-ae07-49947668a760", "prefix_titles": [["title", "A Survey of Deep Learning for Data Caching in Edge Network"], ["section", "Deep Learning for Data Caching"], ["subsection", "DRL"]], "content": "The work in  focuses on the cooperative caching policy at FBSs with maximum distance separable coding in ultra dense networks. A value-based model is utilized to determine caching categories and the content quantity at FBSs during off peak duration. The authors in  study the problem of caching 360\\degree videos and virtual viewports in FBSs with unknown content popularity. The virtual viewport represents the most popular tiles of a 360\\degree video over users' population. A DQN is introduced to decide which tiles of a video to be hosted and in which quality. Additionally,  employs DQN for content eviction decision offering a satisfactory quality of experience and  is for the purpose of minimizing energy consumption. In , the authors also apply DQN to decide cache eviction in a single BS. Moreover, the critic is generated with stacking LSTM and FNN to evaluate Q value and an external memory is added for recording learned knowledge. For the purpose of improving the prediction accuracy, the Q value update is determined by the similarity of estimated value of critic and recording information in the external memory, instead of critic domination. The paper  puts forth DQN a two-level network caching, where a parent node links with multiple leaf nodes to cache content instead of a single BS. In , a DRL framework with Wolpertinger architecture  is presented for content caching at BSs. The Wolpertinger architecture is based on actor-critic model and performs efficiently in large discrete action space.  employs two FNNs working as actor and critic respectively, where the former determines requested content is cached or not and the later estimates the reward. The whole framework consists two phases: in offline phase, these two FNNs are trained in supervised learning; in online phase, the critic and actor update via the interaction with environment. The authors extend their work to a multi agent actor-critic model for decentralized cooperative caching at multiple BSs . In , an actor-critic model is used for solving cache replacement problem, which balance the data freshness and communication cost. The aforementioned papers put attention on the network performance while ignore the influence of caching on information processing and resource consumption. Therefore, authors of  design cache policy considering both network performance during content transmission and processing efficiency during data consumption. A DQN is employed to determine the number of chunks of the requested file to be updated. The paper  investigates a joint cache replacement and power allocation optimization problem to minimize latency in a downlink F-RAN. A DQN is proposed for finding a suboptimal solution. Though  is regarded as solving what to cache problem like , the reinforcement learning approach plays a different role. In , a DNN is utilized for content popularity prediction and then a RL is used for DNN hyperparameters tuning instead of determining caching content. Therefore the action space consists of choosing model architectures (i.e. CNN, LSTM, etc.), number of layers and layer configurations.\nIn , the authors generate an optimization model with the aim of maximizing network operator's utility in mobile social networks under the framework of mobile edge computing, in network caching and D2D communications (3C). The trust value which if estimated through social relationships among users are also considered. Then a DQN model is utilized for solving optimization problem, including determine video provider and subscriber association, video transcoding offloading and the video cache allocation for video providers.. The DQN employs two CNNs for training process, where one generates target Q value and the other is for estimated Q value. Unlike the conventional DQN, the authors in  introduces a dueling structure, i.e. the Q value is not computed in the final fully connected layer, but is decomposed into two components and use the summary as estimated Q value, which helps achieve a more robust result. The authors also consider utilizing dueling DQN model in different scenarios like cache-enabled opportunistic interference alignment  and orchestrating 3C in vehicular network . The work  provides a DDPG model to cope with continuous valued control decision for 3C in vehicular edge networks, which is combined with the idea of DQN and actor-critic model. The DDPG structure can be divided into two parts as DQN, one is for estimated Q value and the other for target Q value. Each part consists of two DNNs, which play the role of actor and critic respectively. The critic updates its parameters like DQN while the actor learns policy via deterministic policy gradient approach. The proposed DRL is used for deciding content caching/replacement, vehicle organization and bandwidth resource assignment on different duration. \nThe paper  provides an optimization model which takes what to cache and content delivery into consideration in the fog-enabled IoT network in order to minimize service latency. Since the wireless signals and user requests are stochastic, a actor-critic model is engaged where the actor makes decision for requesting contents while critic estimates the reward. Specially, the action space $S$ consists of decision variables and reward function is a variant of the objective function. A caching replacement strategy and dynamic multicast scheduling strategy are studied in . In order to get a suboptimal result, an auto encoder is used to approximate the state. Further, a weighted double DQN scheme is utilized for avoiding overestimation of Q value.  applies a RNN to predict content popularity by collecting historical requests and the output represents the popularity in the near future. Then the prediction is employed for cooperative caching and computation offloading among MEC servers, which is modelled as a ILP problem. For the purpose of solving it efficiently, a multi-agent DQN is applied where each user is viewed as an agent. The action space consists of task local computing and offloading decision as well as local caching and cooperative caching determination. The reward is measured by accumulated latency. The agent choose its own action based on current state without cooperation. The where to cache, what to cache and content delivery decision of D2D network are jointly modelled in . Two RNNs, ESN and LSTM, are considered to predict mobile users' location and requested content popularity. Then the prediction result is used for determining content categories and cache locations. The content delivery is formulated as the actor-critic based DRL framework. The state spaces include CSI, transmission distances and communication power between requested user and other available candidates. The function of DRL is determining the communication link among users with the aim of minimizing power consumption and content delay.\nWe notice that most papers prefer to use value-based model (critic) and value-policy-based (actor-critic) model in DRL framework, but rare paper considers only policy-based model to solve data caching problem. One proper reason is that the search space of caching problem is enormous so policy-based model is easier to fall into local optimal solution, resulting in poor performance. Though the value-based model is less stable, some variant structures are utilized like Double DQN in  to avoid value overestimation and dueling DQN in  to improve robust.", "cites": [7746, 3718, 3719, 3724, 3722], "cite_extract_rate": 0.23809523809523808, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple DRL-based caching strategies, connecting methods like DQN, actor-critic, and dueling DQN across different network scenarios. It provides critical insights, such as the limited use of policy-based models and the reasons behind this trend, while also highlighting variants like Double DQN and Wolpertinger architecture. The abstraction level is strong as it identifies broader patterns in model choices and their implications for performance and stability in caching problems."}}
