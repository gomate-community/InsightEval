{"id": "6019d392-fb6b-4290-8c58-a72268a80ff9", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{W}{eakly} supervised learning (WSL) has recently received much attention in computer vision community.\nA plethora of methods on this topic have been proposed in the past decade to address the challenging computer vision tasks including semantic segmentation~, object detection~, and 3D reconstruction~, to name a few.\nAs shown in Fig.~\\ref{illustration}, a WSL problem is defined as the learning process when some partial information regarding the task (e.g., class label or object location) on a small subset of the data points is at our disposal.\nCompared to the conventional learning framework, e.g., fully supervised learning approaches, the WSL framework needs to operate on the small amount of weakly-labelled training data to learn the target model, which alleviates a huge amount of human labor to annotate training samples.\nIt can also facilitate the learning process when the fine-grained annotation\nis extremely labor intensive and time consuming to even obtain the whole labeled data required by the fully-supervised approaches.\nWhile a plethora of WSL-based vision methods have been developed, this survey mainly focuses on the task of weakly supervised object localization and detection, which is shown as the red dot in Fig.~\\ref{illustration}.\nIt is well-known that object localization and detection is a fundamental research problem in computer vision.\nLearning object localization and detection models under weak supervision has attracted much attention in the past decades.\nWhile existing methods treat weakly supervised object localization (WSOL) and weakly supervised object detection (WSOD) as two different tasks\\footnote{ {The difference between WSOL and WSOD mainly lies in that WSOL mainly aims at localizing a single known (or unknown) object from each given image scene. The goal of WSOD is to instead detect every possible object instance from the given image scene. This makes WSOD a little more difficult than WSOL.}}, we consider these as a common task due to several reasons: 1) these tasks learn with the same image-level human annotation; 2) these two tasks need certain supervision as input and usually aim to localize objects on the bounding-box level as output; 3) WSOD task can be accomplished by directly training off-the-shelf fully supervised object detectors on the object locations obtained from WSOL.\nDuring the last decade, considerable efforts have been made to develop various approaches for learning object detectors with weak supervision.\nSome of the existing algorithms only learn weakly supervised object detectors for one or several certain object categories, such as vehicles~, traffic signs~, pedestrians~, faces~, tuberculosis bacilli~, aircrafts~, and human actions~.\nWhile other approaches, e.g., , focus on developing weakly supervised learning frameworks for unconstrained object categories, i.e., learning frameworks can be extended to learn object detector for the given category-specific weakly-labelled training images.\nAs enormous methods have been developed for these important tasks, a comprehensive review of the literature concerning weakly supervised object localization and detection is of great importance.\nAs weakly supervised object localization and detection methods mainly exploit the image-level manual annotation, the learning frameworks not only need to address the typical issues, such as the intra-class variations in appearance, transformation, scale and aspect ratio, encountered in conventional fully supervised object localization and detection task, but also the \\textbf{learning under uncertainty} challenges caused by the inconsistency between human annotations and real supervisory signals.\nIn weakly supervised object localization and detection, the accuracy of object locations and learning processes are closely related.\nThe key is to propagate the image-level supervisory signals to the instance-level (bounding-box-level) training data for the learning processes.\nAs each training image can be labeled by numerous bounding boxes of different accuracy, propagating such weak supervision inevitably involves a large amount of ambiguous and noisy information as each training instance.\nMore specifically, the \\textbf{learning under uncertainty} issue would cause the following challenges that make the weakly supervised learning process challenging:\n\\begin{itemize}\n \\item \\textbf{Learning with inaccurate instance locations:} This issue is mainly caused by the definition ambiguity in object parts and context.\n Without precise annotation or definition, it is difficult for a learner to decide whether an object category label associates with a discriminative object part, the whole object region, or the object with a certain context region.\n As a result, the bounding-box instance locations inferred by the learner may contain many inaccurate samples including the ones with local object parts or undesired contextual regions.\n These samples would negatively affect the performance of WSL-based detectors.\n \\item \\textbf{Learning with noisy samples:} Even when the bounding-box locations can be precisely labeled, the training examples enclosed by bounding-boxes may still be noisy as background pixels are usually included.\n As there is no additional information to separate foreground objects from the background, the learner may tag a ``background'' label to an object region when it fails to recognize the object category.\n In addition, the learner may mistakenly label a bounding-box that contains a bicycle as a motorcycle, as these two object categories share many similar features.\n \\item \\textbf{Learning with domain shifts:} For a certain object category, the image regions localized during the learning process may only contain samples with limited diversity in object shape, appearance, scale, and view angle.\n This makes the subsequent learning process biased to limited knowledge of the object category and does not generalize well for test samples.\n For instance, a learner can hardly localize or detect a flying swan when all the training samples contain the swimming ones on lakes.\n This issue happens frequently among the weakly supervised learning process when there is a large gap between the training and testing domains.\n \\item \\textbf{Learning with insufficient instance samples:}\n Similar to the issues in conventional learning methods, it is difficult to train effective object detectors under the weakly-supervised setting when the amount of training samples is limited.\n In addition, the number of positive samples is usually much smaller than that of negative samples for binary classes.\n Furthermore, the data distributions for a large number of categories is usually long-tailed.\n This issues are significantly exaggerated for the WSL-based methods using deep learning.\n\\end{itemize}\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1\\linewidth]{illustration.jpg}\n \\caption{Illustration of the weakly supervised learning tasks in computer vision community. The blue area in the top block indicates the conventional fully-supervised learning tasks, while the red area in the top block indicates the weakly supervised learning tasks. The coordinate axes shows different levels of human annotation or supervision requirement, from low cost to high cost. Notice that the high cost annotation can be transformed to low cost annotation easily, e.g., from bounding-box level to image level, whereas the low cost annotation is hard to be transformed to high cost annotation. In the bottom block, we also show the label cost, in terms of annotation time, and the examples of different type of annotations. In this survey, we mainly focus on reviewing the research progress in weakly supervised object localization and detection, i.e., the red dot in the top block.}\n \\label{illustration}\n\\end{figure}\nTo address the above-mentioned issues in learning weakly supervised object detectors, existing methods are usually constructed based on two steps: initialization and refinement.\nThe initialization stage is used to leverage certain prior knowledge to propagate image-level annotation into instance level, and thus can generate instance-level annotation (but with label noise, sample bias, and limited quality in location accuracy) for the learning process.\nThe refinement stage is used to leverage new instance samples obtained from the first stage to mine truthful knowledge about the objects of interest gradually and finally obtain the desired object models for localization and detection.\nThese two learning stages need to collaborate to address the aforementioned five-fold challenges.\nIn initialization stage, efforts should be made to improve the annotation quality as much as possible to generate training instances with proper locations, accurate labels, high diversity, and high recall rate.\nAs the annotation quality obtained in the learning stage cannot be perfect, in the refinement stage, further efforts should be made to improve the learner's robustness to cope with the inaccurate instance location, noisy examples, biased instance sample, insufficient instance sample issues as well as the capacity to take advantage of the unlabelled instance samples.\nWhen properly addressing the problems in each learning stage,\ngood weakly supervised object detectors can be learned.\nIn this work, we review the existing weakly supervised object localization and detection approaches\\footnote{Some early methods, such as~, learn to localize category-wise key points under the weak supervision, while this survey mainly focuses on the methods for localizing instances with bounding-boxes.}, which are divided into three main categories and eight subcategories.\nThese three main categories are based on classic approaches, feature representations from off-the-shelf deep models, and deep learning frameworks.\nThe eight subcategories include approaches for initialization, refinement, initialization and refinement, pre-trained deep features, inherent cues in deep models, fine-tuned deep models, single-network training, and multi-network training.\nWe further discuss the relationship between the approaches in different categories.\nIn addition, we also discuss open problems and challenges of current studies and propose several promising research directions in the future for constructing more effective weakly supervised object localization and detection frameworks.\n\\begin{figure*}[t]\n \\centering\n \\includegraphics[width=1\\linewidth]{category2.jpg}\n \\caption{In the left block, taxonomy of the existing approaches for weakly supervised object localization and detection, which includes three main\n categories and eight subcategories.\n In the right block, the relationships between the approaches in different categories are shown.\n}\n \\label{taxonomy}\n\\end{figure*}\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=1\\linewidth]{history.JPG}\n \\caption{Developments of weakly supervised localization and detection methods. The yellow histogram shows the number of publications in this research field in each year, and the curves show the number of proposed methods each year for a particular category of approach.}\n \\label{history}\n\\end{figure}", "cites": [2259, 209, 2260], "cite_extract_rate": 0.2, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The introduction section provides a clear overview of weakly supervised learning in computer vision and identifies key challenges in weakly supervised object localization and detection. It begins to synthesize the field by categorizing tasks and discussing common issues. However, the cited papers are not deeply integrated into the narrative, and there is minimal critical evaluation or comparison of their approaches. The section offers some abstraction by framing the problem around general challenges like learning under uncertainty, but it does not yet establish a novel or comprehensive framework."}}
{"id": "a3e94559-bb78-4175-a692-379986435e13", "title": "Taxonomy", "level": "section", "subsections": [], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Taxonomy"]], "content": "\\label{Taxonomy}\nIn the last decade, a plethora of methods have been developed for weakly supervised object localization and detection.\nWe can generally categorize existing methods based on classic formulations, feature representations from off-the-shelf deep models, and deep weakly supervised learning algorithms.\nWhile inside each main category, we further divide the approaches into two or three subcategories.\nFig.~\\ref{taxonomy} shows our taxonomy of the studies in the research field of weakly supervised object localization and detection.\nIn addition, Fig. \\ref{history} reviews the development history of each of the main category as well as that of the whole research field.\nA few approaches based on classic formulations appeared around 2002.\nFrom 2002 to 2009, the research in this field went through a very slow pace.\nSince 2014, numerous approaches based on both classic formulations and learned feature representations from deep models have been developed and received much attention.\nWhile in the last few years, more approaches solely based on deep learning have become the main stream to address the problems of weakly supervised object localization and detection.\nWhile a plethora of methods have been developed to address different aspects of these problems in the past decades, this field is gaining increasing attention.\n\\renewcommand{\\multirowsetup}{\\centering}\n \\begin{table*} [t]\n \\tiny\n \\caption{Summary of the approaches for initialization, which is a subcategory in the weakly supervised object localization and detection approaches that learn by classic models. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n \\label{table initialization}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2.5cm}<{\\centering}m{1.2cm}<{\\centering}m{1.5cm}<{\\centering}m{3.1cm}<{\\centering}m{1.8cm}<{\\centering}m{1.5cm}<{\\centering}m{3.5cm}<{\\centering}m{1.8cm}<{\\centering}}\n \\hline\n \\multirow{2}{*}{\\textbf{Methods}} & \\multirow{2}{*}{\\textbf{Detector}} & \\multirow{2}{*}{\\textbf{Descriptor}} & \\multirow{2}{*}{\\textbf{Prior knowledge}} & \\multirow{2}{*}{\\textbf{Extra training data}} & \\multirow{2}{*}{\\textbf{Learning model}} & \\multirow{2}{*}{\\textbf{Learning strategy}} & \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline \\hline\nCao-PR-2017 & SVM & HOG+PCA & Road map prior + density prior & None & SVM & MIL with density estimation & Vehicle in satellite imagery \\\\\nShi-TPAMI-2015 & DPM & SIFT+Lab+LBP, BOW & Appearance prior + geometry prior & None & Topic model & Bayesian inference & General objects \\\\\nTang-ICIP-2014 & DPM & HOG & Saliency (objectness score) & None & DPM & Select initial boxes + DPM training & General objects \\\\\n Xie-VCIP-2013 & None & SIFT & Intra-class consistency & None & None & Low-rank and sparse coding & General objects \\\\\nSiva-CVPR-2013 & DPM & Lab+SIFT & Saliency & Labelme + PASCAL07, 12 (unlabelled) &None & None & General objects \\\\\n Shi-ICCV-2013 & DPM & SIFT & Appearance prior (spatial distribution, size, saliency) & None & Topic model & Bayesian inference & General objects \\\\\n Sikka-FG-2013 & None & SIFT, LLC, BOW & None & None & MilBoost & Generating multiple segments for initialization and use Milboost for learning & Pain (on face) \\\\\n Siva-ECCV-2012& None & SIFT+BOW & Iter-class variance + saliency & None & None & Negative mining & General objects \\\\\n Shi-BMVC-2012 & None & BOW & Mapping relationship between the box overlap and appearance similarity & Part of PASCAL 07 (box annotation) & RankSVM & Transfer learning by ranking & General objects \\\\\n Khan-AAPRW-2011 & DPM & Phog/phow & None & Internet image (weakly annotated) & MIL & Learning from internet image & Pascal@8 \\\\\nZhang-BMVC-2010 & SVM & IHOF & Co-occurrence & None & SVM & High order feature learning by exploring co-occurrence & General objects \\\\\n\\hline\n \\end{tabular}}\n\\end{table*}\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n \\tiny\n\t\\caption{Summary of the approaches for refinement, which is a subcategory in the weakly supervised object localization and detection approaches that learn by classic models.\n\tHere, * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n\t \\label{table refinement} \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2.5cm}<{\\centering}m{1.5cm}<{\\centering}m{3cm}<{\\centering}m{2cm}<{\\centering}m{2cm}<{\\centering}m{2cm}<{\\centering}m{3.1cm}<{\\centering}m{2.5cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\n Wang-TMI-2018 & None & Color & None & None & Low-rank model & Low-rank Factorization & General lesion \\\\\n Wang-TC-2017 & None & SIFT+LAB & None & None & Probability model & BOW learning+instance labeling & General objects \\\\\n Cholakkal-CVPR-2016 & None & SIFT & Saliency & None & SVM* & ScSPM-based top down saliency & Salient objects \\\\\n Zadrija-GCPR-2015 & None & SIFT+FV & None & None & GMM + linear classifier & Patch-level spatial layout learning & Traffic sign \\\\\n Krapac-ICCVTA-2015 & None & SIFT+FV & None & None & Sparse logistic regression & Sparse classification & Traffic sign \\\\\n Cinbis-CVPR-2014 & SVM & FV & Center prior & None & SVM & Multi-fold MIL & General objects \\\\\n Wang-ICIP-2014 & SVM + graph model & SIFT & None & None & SVM + graph model & Maximal entropy random walk & Car, dog \\\\\n Wang-ICIP-2014 & SVM & SIFT & None & None & SVM & Clustering for window mining & General objects \\\\\n Tang-CVPR-2014 & None & SIFT & Saliency & None & Boolean constrained quadratic program & Mine similarity and discriminativeness both for image and box & General objects \\\\\n Hoai-PR-2014 & SVM & SIFT,BOW & None & None & SVM* & Localization-classification SVM & Face,car, human motion \\\\\n Wang-WACV-2013 & Task-specific detectors & HOG/SC & Background\\ saliency & None & MIL+AdaBoost* & Soft-label Boosting after MIL & Vehicle, pedestrain\\\\\n Kanezaki-MM-2013 & Linear classifiers & 3D voxel feature (color+C3HLAC +Intensity, texture, GRSD) & None & None & Linear classifiers & Multi-class MIL & Balls, tools \\\\\n Pandey-ICCV-2011 & DPM* & HOG & None & None & DPM* & Learning DPM with fully latent variable & General objects \\\\\nBlaschko-NIPS-2010 & None & BOW/HOG & None & None & SVM* & Learning SVM with structured output ranking objective & Cat, pedestrian \\\\\nHoai-ICCV-2009 & SVM & SIFT,BOW & None & None & SVM* & Localization-classification SVM & Face,car, human motion \\\\\n Galleguillos-ECCV-2008 & None & SIFT+BOW & None & None & MilBoost & Train MIL classifier for localization & Landmarks, faces, airplanes, leopard, motorbike, car \\\\\n Rosenberg-BMVC-2002 & GMM & Orientation deriviate filters & Exampler prior & Training exampler (box annotation) & GMM & Learning from exampler training data to weakly labelled training data & Telephone \\\\\n\\hline\n \\end{tabular}}\n\\end{table*}\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n \\tiny\n\t\\caption{Approaches for both initialization and refinement, which is a subcategory in the weakly supervised object localization and detection approaches that learn by classic models. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n\t \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{1.8cm}<{\\centering}m{1.2cm}<{\\centering}m{1.5cm}<{\\centering}m{2cm}<{\\centering}m{1.8cm}<{\\centering}m{1.5cm}<{\\centering}m{3cm}<{\\centering}m{1.5cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\n Wang-cvpr-2013 & None & Color+SIFT & None & None & HST+SVM & Joint parsing and attribute localization & Scene attributes \\\\\n Deselaers-IJCV-2012 & DPM & GIST+CH+BOW+HOG & Generic knowledge & Meta-training data with box annotation & CRF+DPM & Learning appearance model by trainsferring genearic knowledge & General objects \\\\\n Siva-ICCV-2011 & DPM & SIFT+BOW+HOG & Inter-class prior + intra-class prior & None & DPM & Model drift learning & General objects \\\\\nDeselaers-ECCV-2010 & DPM & GIST+CH+BOW+HOG & Generic knowledge & Meta-training data with box annotation & CRF+DPM & Learning appearance model by transferring generic knowledge & General objects \\\\ \\hline\n \\end{tabular}}\n\\end{table*}\nAs shown in the right block of Fig.~\\ref{taxonomy}, methods in\nmain categories are related in several aspects.\nNumerous methods are developed based on the classic formulations with the advances of feature representations from deep models.\nSimilarly, a number of methods based solely on deep models are end-to-end trainable by considering classic formulations and feature extraction schemes.\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=7 cm]{subcategory1.jpg}\n \\caption{Flowchart of the weakly supervised object localization and detection approaches using classic models.}\n \\label{nondeep flowchart}\n\\end{figure}", "cites": [2261, 2262, 2263], "cite_extract_rate": 0.1, "origin_cites_number": 30, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily presents a descriptive overview of various weakly supervised object localization and detection methods by listing them in tabular form with attributes such as detector, descriptor, prior knowledge, and learning strategy. It includes basic categorization and chronological information but does not synthesize or connect ideas across papers in a meaningful way. There is minimal critical evaluation or abstraction beyond the specific methods mentioned."}}
{"id": "bba63d48-815b-4407-9de8-7df72823a004", "title": "Classic Models", "level": "section", "subsections": ["bfae7607-5833-4424-b0e8-9e0a00e92605", "8f1d3bcb-1c87-4161-b6dc-8cbe17a169ff", "31104e17-5d59-42f1-88ae-903542c63cd2", "19588196-93c9-4245-8a54-e9fb29d5fd1e"], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Classic Models"]], "content": "\\label{Classic Models}\nIn this section, we review the classic approaches that learn weakly supervised object localizer or detector without using deep features.\nThese methods typically consist of one initialization module followed by one refinement process as shown in\nFig.~\\ref{nondeep flowchart}.\nIn , the detector is based on the deformable part model (DPM)~.\nIn other approaches , the detector is based on the support vector machine (SVM) classifier.\nThe features used by these approaches are hand-crafted feature descriptors, such as HOG in , SIFT in , and Lab color in , which are sometimes used to build higher level representation such as bag-of-words (BOW) in , ,,,, Fisher vector representation in , and subspace-based representation in .\nIn the following, we divide these approaches for initialization and refinement process.", "cites": [2261, 2262, 2263], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of classic models in weakly supervised object localization and detection. It mentions key components like initialization and refinement processes, and lists commonly used features and classifiers without establishing meaningful connections between the cited papers. There is little critical evaluation or abstraction beyond the specific methods."}}
{"id": "bfae7607-5833-4424-b0e8-9e0a00e92605", "title": "Initialization", "level": "subsection", "subsections": [], "parent_id": "bba63d48-815b-4407-9de8-7df72823a004", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Classic Models"], ["subsection", "Initialization"]], "content": "Numerous methods have been developed to mine reliable instance samples, using prior knowledge, as weak supervision for the following processes.\nA brief summary of these approaches are shown in Table \\ref{table initialization}.\nZhang et al.~ leverage the prior-knowledge of object co-occurrence to identify translation and scale invariant high order features for weakly supervised object localization.\nIn~ Shi et al. propose a transfer learning paradigm to first use a RankSVM to learn the mapping relationship between the box overlap and appearance similarity from an auxiliary training data (with bounding-box level annotation) and then transfer the learned prior-knowledge for localizing objects of interest in the given weakly labeled images.\nA simple yet effective approach, named as negative mining, is developed by Siva et al. to explore the inter-class variance among the object regions in weakly labeled training images.\nThe final object locations are obtained by\nusing a linear combination of the inter-class variance and saliency prior.\nSimilarly, Tang et al.~ and Xie et al.  use the saliency prior and intra-class consistency to mine the initial object locations, respectively.\nIn , Shi et al. explore the appearance prior and geometry prior in their topic model to build a Bayesian joint modeling framework for weakly supervised object localization.\nOn the other hand, Cao et al.~ exploit the road map prior and density prior to mine the initial vehicle locations from the weakly labeled satellite images and then trained the vehicle detector under a modified multiple-instance learning (MIL) model.\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n \\tiny\n\t\\caption{Summary of the approaches using pre-trained feature representations, which is a subcategory in the weakly supervised object localization and detection approaches based on the off-the-shelf deep models. * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n \\label{table off-the-shelf feature}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2cm}<{\\centering}m{1cm}<{\\centering}m{1.6cm}<{\\centering}m{2.8cm}<{\\centering}m{3.8cm}<{\\centering}m{1.2cm}<{\\centering}m{3.5cm}<{\\centering}m{2.2cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\n Gonthier-arxiv-2018 & None & CNN & Supervised objectness score (Fast R-CNN(Resnet)) & ImageNet(tag label) & SVM & MIL & Objects in art (watercolor2K, people-art) \\\\\nZadrija-CVIU-2018 & None & VGG19 Conv 5\\_4. SIFT, Fisher vector & None & ImageNet(tag label) & Sparse model & None & Zebra crossings, traffic signs \\\\\n Cinbis-TPAMI-2017 & SVM* & FV+CNN & Center prior & ImageNet(tag label) & SVM* & Multi-fold MIL & General objects \\\\\nWei-IJCAI-2017 & None & CNN & None & ImageNet(tag label) & None & Deep descriptor transforming & General objects \\\\\n Zhang-IJCAI-2016 & SVM & CNN & Saliency prior & ImageNet(tag label) & SVM & Easy-to-hard(SPL+CL) & General objects \\\\\n Li-ECCV-2016 & None & FC6 & Strong detector prior (sparsity) & ImageNet(tag label) & SVM & Regularizing score distribution & General objects \\\\\n Ren-TPAMI-2016 & SVM* & FC6 & None & ImageNet(tag label) & SVM* & MIL+bag splitting & General objects \\\\\n Wan-ICIP-2016 & SVM* & FC7 & None & ImageNet(tag label) & SVM* & Correlation suppression+part suppression & General objects \\\\\n Rochan-IVC-2016 & None & Color histogram +CNN & Saliency & PASCAL (edge box), ImageNet & SVM & None & General objects \\\\\n Shi-ECCV-2016 & SVM & FC7(Alexnet) & Size prior & ImageNet(tag label), PASCAL2012 (object size) & SVM & Easy-to-hard(curriculum) & General objects \\\\\n Wang-TIP-2015 & SVM & FC6 & None & ImageNet(tag label) & pLSA, SVM & Online latent category learning & General objects \\\\\nRochan-CVPR-2015 & SVM & CNN & Objectness score, word embedding prior & YouTube-Objects (for parameter validation), Familiar object categories(detector) & SVM, Sparse reconstruction & Appearance transfer from text representation & General objects \\\\\nBilen-CVPR-2015 & LSVM & FC7+spatial features & None & ImageNet(tag label) & LSVM & Convex clustering & General objects \\\\\nWang-ICCV-2015 & None & FC6 & None & PASCAL(edge box), ImageNet & SVM* & Relaxed multiple-Instance SVM & General objects \\\\\nZhou-ICMBD-2015 & SVM & FC7 & Saliency prior & ImageNet(tag label) & SVM & Negative Bootstrapping & Airplanes in remote sensing \\\\\nHan-TGRS-2015 & SVM & DBM & Saliency, intra-class compactness, inter-class separability & None & DBM + SVM & Bayesian framework for initialization + refinement detector training & Objects in remote sensing \\\\\nMathe-Arxiv-2014 & Sequential detector & FC6 & Human fixation & ImageNet(tag label) & MIL*+RL & Constrained multiple instance SVM learning + reinforcement learning of detector & Human actions \\\\\n Wang-ECCV-2014 & SVM & FC6 & None & ImageNet(tag label) & pLSA, SVM & Online latent category learning & General objects \\\\\n Bilen-BMVC-2014 & LSVM & DeCAF & None & ImageNet(tag label) & LSVM* & LSVM with posterior regularization on symmetry and mutual exclusion & General objects \\\\\nSong-NIPS-2014 & DPM & FC7 & Objectness score & ImageNet(tag label) & LSVM & Frequent configuration mining+detector training & General objects \\\\\n Song-ICML-2014 & LSVM & DeCAF & None & ImageNet(tag label) & Graph model+LSVM* & Initialization via discriminative submodular cover+smoothed LSVM learning & General objects \\\\\n\\hline\n \\end{tabular}}\n\\end{table*}", "cites": [2261, 2262, 2267, 2263, 2269, 2270, 2266, 2264, 2265, 8537, 2268], "cite_extract_rate": 0.4074074074074074, "origin_cites_number": 27, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of initialization methods in weakly supervised object localization, citing multiple papers and briefly describing their approaches. However, it lacks a deeper synthesis of ideas or comparative analysis between methods. There is minimal abstraction or discussion of overarching patterns, and the narrative remains largely descriptive."}}
{"id": "8f1d3bcb-1c87-4161-b6dc-8cbe17a169ff", "title": "Refinement", "level": "subsection", "subsections": [], "parent_id": "bba63d48-815b-4407-9de8-7df72823a004", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Classic Models"], ["subsection", "Refinement"]], "content": "After potential object instances are obtained, these hypotheses are verified in the following refinement processes.\nThe goals of these approaches are to design learning objective functions, optimization strategies, or learning mechanisms to gradually determine objects of interest from the extracted initial instance training samples.\nA brief summary of these approaches is shown in Table \\ref{table refinement}.\n Hoai et al.  propose an approach which localizes the instances of the positive class and learns a sub-window classifier to recognize the corresponding object class.\nBlaschko et al.  use a structured output SVM to learn a regressor from the weakly labeled training images to object locations that are parameterized by the coordinates of the bounding boxes.\nThe object locations were treated as latent variables, while the image-level annotation was used to constrain the set of values the latent variable can take.\nSimilarly, Pandey et al.  learn weakly supervised object detectors by using DPMs with latent SVM training.\nIn , a soft-label boosting approach is developed to exploit the soft labels that are estimated during the MIL process to train object detectors based on Boosting algorithm.\nIn , Tang et al. treat the weakly supervised object localization problem as an object co-localization task, and present a joint image-box formulation to mine reliable object locations via a Boolean constrained quadratic program.\nThis approach can handle noisy labels in the image-level annotations.\nTo address the property that the MIL process may converge to poor local optima after the initialization, Cinbis et al.  design a multi-fold MIL training paradigm.\nThis method divides the whole weakly labelled training images into multiple folds and implements the detector training process and object re-localization process in different folds, thereby alleviating the issue with convergence of poor local optima.\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n \\tiny\n\t\\caption{Summary of the approaches using visual cues, which is a subcategory in the weakly supervised object localization and detection approaches based on the off-the-shelf deep models. * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n\t \\label{table pre-learning} \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2.5cm}<{\\centering}m{1.5cm}<{\\centering}m{1.5cm}<{\\centering}m{2cm}<{\\centering}m{3cm}<{\\centering}m{1.5cm}<{\\centering}m{3.1cm}<{\\centering}m{1.8cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\n Li-ISPRS-2018 & CAM* (VGG-F) & CNN & None & ImageNet(tag label) & VGG-F* & Learning learning + CAM learning (patch level) & Remote sensing objects \\\\\n Wilhelem-DICTA-2017 & None & CNN & None & ImageNEt(tage label) & CAM & CAM+KDE refine & General objects \\\\\n Tang-TMM-2017 & DPM & CNN & Saliency + objectness score & ImageNet(tag label) & DPM+ CNN & Region initialization+DPM and feature learning+bounding box modification & General objects \\\\\n Kolesnikov -BMVC-2016 & None & CNN & Human feedback annotation & ImageNet(tag label) & CAM & Active learning for identifying object cluster & General objects \\\\\n Bency-ECCV-2016 & None & CNN & None & ImageNet(tag label) & VGG16 & Beam-search based on CNN classifier & General objects \\\\\nZhou-MSSP-2016 & SVM & FC7 & Saliency prior & ImageNet(tag label), remote sensing data(unlabelled) & CNN (AlexNet), SVM & Deep feature transfer +MIL & Remote sensing objects (airplane, car, airport) \\\\\nBergamo-WACV-2016 & SVM & CNN & None & ImageNEt(tage label) & CNN, SVM & Mask out initialization + SVM detector training & General objects \\\\\nHoffman-CVPR-2015 & SVM & FC7 & Detector prior+ representation prior & ImageNet(tag label), ILSVRC13 validation subset(box annotation) & CNN, Latent SVM & Transferring detectors and representation from auxiliary data & General objects \\\\\n\\hline\n \\end{tabular}}\n\\end{table*}", "cites": [2272, 2271, 2274, 2273], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes individual methods and their technical details without clearly synthesizing or connecting the underlying principles across the cited papers. It lacks critical evaluation of the approaches' strengths, weaknesses, or trade-offs, and offers minimal abstraction or generalization beyond the specific systems discussed. The content appears to be a factual summary rather than a deeper analysis."}}
{"id": "ce302070-cf4f-4d53-a26c-8a51743b26f9", "title": "Off-the-shelf Deep Models", "level": "section", "subsections": ["72d7b986-53df-40a6-a211-40fcbb5e65ab", "a62482ff-4ed2-4ba2-b72d-45432f53b8f2", "eab65442-451f-481c-9954-9b19dd3d9a08", "0873c3d1-8648-437a-a297-c566cfb8420e"], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Off-the-shelf Deep Models"]], "content": "In this section, we review the approaches that learn weakly supervised object localizer or detector based on classic formulations and feature representations based on the deep neural networks, either pre-trained from the ImageNet dataset  (with image tag annotation) or further fine-tuned on the weakly supervised training images in the target domain.\nThe feature representations are based on the widely used deep models for image classification, such as AlexNet  and VGG .\nThe detectors are constructed based on classic formulations such as DPM and SVM~, or recent models such as RCNN  and fast RCNN  .\nWe further divide these approaches into three subcategories using pre-trained deep features, inherent cues in deep models, and fine-tuned deep models as shown in Fig. \\ref{off-the-shelf-flowchart}.\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=8.5 cm]{subcategory.jpg}\n \\caption{Illustration of different usages of the off-the-shelf deep neural networks by the weakly supervised object localization and detection approaches based on the off-the-shelf deep models. }\n \\label{off-the-shelf-flowchart}\n\\end{figure}\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n \\tiny\n\t\\caption{Summary of the approaches with fine-tuned deep models, which is a subcategory in the weakly supervised object localization and detection approaches based on the off-the-shelf deep models. * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. }\n\\label{table prepost-learning}.\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2cm}<{\\centering}m{2cm}<{\\centering}m{1.6cm}<{\\centering}m{2.8cm}<{\\centering}m{2.5cm}<{\\centering}m{1.5cm}<{\\centering}m{3.7cm}<{\\centering}m{1.5cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\n Zhang-IJCV-2019 & Fast RCNN (VGG16) & Pre-trained FC7 & Tag number + mask out prior(AlexNet) & ImageNet(tage label) & SVM & Easy-to-hard & General objects \\\\\n Uijlings-CVPR-2018 & None & CNN & Semantic objectness(SSD*) & ImageNet(tage label), ILSVRC(full annotation) & SSD*+SVM+ Fast RCNN & MIL+knowledge transfer & General objects \\\\\n Jie-CVPR-2017 & Fast RCNN (VGG16) & CNN & Image-to-object transfer prior & ImageNet(tage label) & Fast RCNN (VGG16) & Initialization based on classification network and subgraph discovery + iterative Fast RCNN learning & General objects \\\\\nShi-ICCV-2017 & Fast RCNN & CNN & Things and stuff prior & ImageNet(tage label), PASCAL Context (full annotation) & FCN,Fast RCNN & Localizing objects based on things and stuff prior and training Fast TCNN iteratively & General objects \\\\\nSingh-CVPR-2016 & Fast RCNN & CNN & Tracking prior & ImageNEt(tage label), Youtube-objects (unlabelled) & Fast RCNN & Discriminative region minning+transferring tracking object pattern + learn object detector & General objects \\\\\nLi-CVPR-2016 & VGG* & CNN & Mask out prior(AlexNet) & ImageNEt(tage label) & VGG*, SVM & Progressive Domain Adaptation & General objects \\\\\nLiang-ICCV-2015 & CNN & CNN & Instance example, motion prior & ImageNEt(tage label) & CNN,R-CNN & Seed selection based on instance example and instance tracking & General objects \\\\\nChen-ICCV-2015 & RCNN & CNN & Online data type & Web data (weak label) & BLVC net +E-LDA + RCNN & Simple image initialization + graph-based representation adaptation on hard image & General objects \\\\\nZhou-CVPR-2015 & RCNN & FC7 & None & ImageNet(tag label) & SVM, R-CNN & .Max-margin visual concept discovery + Domain-specific detector selection & General objects\\\\\n\\hline\n \\end{tabular}}\n\\end{table*}", "cites": [1223, 514, 2272, 8429, 2275, 895, 2278, 2264, 2273, 2276, 2279, 2277], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes different approaches using off-the-shelf deep models, organizing them into a table with method-specific details. While it categorizes the methods into subcategories (pre-trained, inherent cues, fine-tuned), it lacks deeper synthesis or discussion of how these categories relate or influence each other. There is minimal critical evaluation or abstraction to broader principles, making the section largely descriptive in nature."}}
{"id": "72d7b986-53df-40a6-a211-40fcbb5e65ab", "title": "Pre-trained Deep Features", "level": "subsection", "subsections": [], "parent_id": "ce302070-cf4f-4d53-a26c-8a51743b26f9", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Off-the-shelf Deep Models"], ["subsection", "Pre-trained Deep Features"]], "content": "The methods of this category replace the hand-crated feature representations with the pre-trained deep features typically from as AlexNet and VGG.\nA brief summary of these approaches is shown in Table \\ref{table off-the-shelf feature}.\nSong et al.  determine discriminative feature configurations of an object class via graph modeling, and train object detectors within the multiple-instance learning paradigm.\nThe deep features of this work are extracted based on the DeCAF scheme~ and AlexNet~.\nBy using the deep features and spatial features to represent each proposal region, Bilen et al.  propose a convex clustering process for learning the object models under the weak supervision.\nThe learning objective is able to enforce the similarity among the selected proposal windows.\nIn , Shi and Ferrari develop a curriculum learning strategy to feed training images into the MIL loop in a pre-defined order, where images containing larger objects are learned at the early stages while images containing smaller objects are learned at later stages.\nRen et al.  present a bag-splitting-based MIL mechanism that iteratively generated new negative bags from the positive ones.\nThis algorithm can gradually reduce the ambiguity in positive images and thus facilitate the learning of more reliable training instance samples.\nIn , Wei et al. leverage the pre-trained CNN model to implement a Deep Descriptor Transforming process, which can obtain the category-consistent image regions via evaluating the correlations of the descriptors in the convolutional activations of the CNN model.\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n  \\tiny\n \t\\caption{A brief summary of the approaches using single-network training scheme, which is a subcategory in the weakly supervised object localization and detection approaches with deep weakly supervised learning algorithms. * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n  \\label{table endtoend}\n  \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2.5cm}<{\\centering}m{1.5cm}<{\\centering}m{1cm}<{\\centering}m{1cm}<{\\centering}m{2.5cm}<{\\centering}m{1.5cm}<{\\centering}m{2.8cm}<{\\centering}m{1.8cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n & & & & & & &\\\\\n\\hline\n\\hline\nHuang-NIPS-2020&Faster RCNN &CNN &None  &ImageNet(tag label) &Faster RCNN*(VGG16/ResNet50) &Proposal attention aggregation and distillation & General objects \\\\\nShen-CVPR-2020&Faster RCNN* &CNN &None  &ImageNet(tag label) + Flickr &Faster RCNN*(VGG16) &bagging-mixup + background noise decomposition + clean dat modelling& General objects \\\\\nMai-CVPR-2020&None &CNN &None  &ImageNet(tag label) &VGG/InceptionV3 &Integrating discriminative region mining and adversarial erasing & General objects \\\\\nZhang-ECCV-2020 &None &CNN &Cross-image consistency  &ImageNet(tag label) &VGG/InceptionV3/ ResNet50 &Inter-image stochastic consistency and global consistency & General objects \\\\\nYang-WACV-2020&None &CNN &None  &ImageNet(tag label) &VGG &Weighted classification activation map combination & General objects \\\\\nYang-ICCV-2019 & Faster RCNN* (VGG16) & CNN & None & ImageNet(tag label) & Faster RCNN* (VGG16) + CAM & Online classifier learning with bounding box regression & General objects \\\\\n Wan-CVPR-2019 & Faster RCNN* (VGG16) & CNN & None & ImageNet(tag label) & Faster RCNN* (VGG16) & Continuation MIL & General objects \\\\\nShen-CVPR-2019 & WSDDN* (VGG16) & CNN & None & ImageNet(tag label) & Two-stream CNN (WSDDN+DeepLab) & Joint detection and segmentation with cyclic guidance & General objects \\\\\nWan-CVPR-2019 & Fast RCNN & CNN & None & ImageNet(tag label) & Two-stream CNN & continuation instance selection and detector estimation & General objects \\\\\nChoe-CVPR-2019 & None & CNN & None & ImageNet(tag label) & CNN & Feature learning by attention-based dropout & General objects \\\\\nJiang-ICCV-2019 & None &CNN & None & ImageNet(tag label) & VGG16/Resnet101 & Online attention accumulation on CAM & General objects\\\\\nSangineto-TPAMI-2018 & Fast RCNN (VGG16) & CNN & None & ImageNet(tag label) & Fast RCNN (VGG16) & Easy-to-hard & General objects \\\\\nInoue-CVPR-2018 & SSD & CNN & None & PASCAL (full label as source domain),ImageNet & SSD & Domain transfer + pseudo-labeling & Cartoon objects \\\\\n Wan-CVPR-2018 & Faster RCNN* (VGG16) & CNN & None & ImageNet(tag label) & Faster RCNN* (VGG16) & Min-entropy latent modeling & General objects \\\\\nShen-TNNLS-2018 & VGG16* & CNN & None & ImageNet (tag label) & vgg16* & Object-specific pixel gradient mapping+Iterative component mining & General objects \\\\\nTang-TPAMI-2018 & Fast RCNN* (model ensemble) & CNN & None & ImageNet(tag label) & Fast RCNN* (model ensemble) & MIL+oicr+multi-scale+proposal cluster learning & General objects \\\\\n Zhang-CVPR-2018 & None & CNN & None & ImageNet(tag label) & VGG16* & Adversarial complementary erasing & General objects (for ILSVRC) \\\\\n Choe-BMVC-2018 & ResNet & CNN & None & Tiny ImageNet(tag label) & ResNet & GoogLeNet Resize (GR) augmentation & General objects (for Tiny ImageNet) \\\\\n Zhang-ECCV-2018 & Inception-v3+CAM & CNN & None & ImageNet(tag label) & Inception-v3+CAM & Self-produced guidance learning & General objects (for ILSVRC and CUB) \\\\\n Gao-ECCV-2018 & Fast RCNN* & CNN & Count (human label) & ImageNet(tag label) & Fast RCNN*+Fast RCNN & WSL with count-based region selection & General objects \\\\\n Singh-ICCV-2017 & CAM* (GoogLeNet)& CNN & None & ImageNet (tag label) & CAM* (GoogLeNet) & Random hidding patches & General objects (for ILSVRC) \\\\\n Zhu-ICCV-2017 & None & CNN & None & ImageNet(tag label) & GoogLeNet* & Soft proposal layer+CAM & General objects \\\\\nWan-ICIP-2017 & None & CNN & None & ImageNet(tag label) & CAM*(VGG) & CAM with spatial pyramid pooling layer & General objects \\\\\n Durand-CVPR-2017 & None & CNN & None & ImageNet(tag label) & CAM* (ResNet101) & CAM with multi-map transfer layer & General objects \\\\\nTang-CVPR-2017 & Fast RCNN* (model ensemble) & CNN & None & ImageNet(tag label) & Fast RCNN*+Fast RCNN & MIL+oicr+multi-scale & General objects \\\\\n Jiang-CVPR-2017 & Fast RCNN* (AlexNEt) & CNN & None & PASCAL (edge box),ImageNet & AlexNet+ ROIpool & Region calssification+region selection+multi-scale & General objects \\\\\nDiba-CVPR-2017 & Faster RCNN* (VGG16) & CNN & None & ImageNet(tag label) & Multi-stream CNN & Cascading LocNet (CAM), SegNet, and MILNet+multi-scale & General objects \\\\\nSelvaraju-ICCV-2017 & None & CNN & None & ImageNet(tag label) & VGG* & Gradient-based class activation mapping & General objects \\\\\nTang-PR-2017 & None & CNN & None & ImageNet(tag label) & Fast RCNN* (VGG-16) & SPP with discovery block and classification block & General objects \\\\\nGudi-BMVC-2017 & None & CNN & None & ImageNet(tag label) & CAM* (VGG-16) & CAM with Spatial Pyramid Averaged Max (SPAM) Pooling & General objects\\\\\n Bilen-CVPR-2016 & Fast RCNN* (model ensemble) & CNN & None & PASCAL (edge box),ImageNet & Fast RCNN* & MIL+multi scale & General objects \\\\\n Kantorov-ECCV-2016 & Fast RCNN* (VGG-F) & CNN & Context & ImageNet(tag label) & Fast RCNN* & MIL+multi-scale & General objects \\\\\n Teh-BMVC-2016 & CNN & CNN & None & PASCAL (edge box),ImageNet & CNN & Proposal attention learning & General objects\\\\\n Durand-CVPR-2016 & None & CNN & None & ImageNet(tag label) & CNN & Feature extraction network+weakly supervised prediction module & General objects \\\\\n Zhou-CVPR-2016 & None & CNN & None & ImageNet(tag label) & GoogLeNet* & Class activation mapping & General objects\\\\\n Oquab-CVPR-2015 & None & CNN & None & ImageNet(tag label) & CNN & Fully convolutional CNN with global max pooling & General objects \\\\\n Wu-CVPR-2015 & None & CNN & None & PASCAL (BING),ImageNet & CNN & Deep multiple instance learning network & General objects\\\\\n\\hline\n \\end{tabular}}\n\\end{table*}", "cites": [2282, 2297, 2288, 2289, 2291, 2286, 2294, 7089, 2285, 2293, 2287, 2283, 2269, 2299, 2259, 2296, 2284, 2292, 2264, 2265, 737, 8537, 2295, 2298, 2281, 2290, 2280], "cite_extract_rate": 0.6136363636363636, "origin_cites_number": 44, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily lists various methods that use pre-trained deep features for weakly supervised object detection, describing each with minimal context. While it attempts to connect papers through shared strategies (e.g., MIL, CAM), the integration is superficial and lacks a deeper synthesis or critical evaluation of the approaches. There is little abstraction or identification of overarching trends or principles beyond the specific techniques."}}
{"id": "a62482ff-4ed2-4ba2-b72d-45432f53b8f2", "title": "Inherent Cues in Deep Models", "level": "subsection", "subsections": [], "parent_id": "ce302070-cf4f-4d53-a26c-8a51743b26f9", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Off-the-shelf Deep Models"], ["subsection", "Inherent Cues in Deep Models"]], "content": "Instead of using the pre-trained deep models as feature extractor, the methods of this category obtain useful information cues (such as the activations in the intermediate network layers and the semantic scores in the output network layer) from the pre-trained deep neural networks to facilitate the weakly supervised learning process.\nThe focus of these approaches mainly lies in the initialization stage of the weakly supervised learning process.\nA brief summary of these approaches is shown in Table \\ref{table pre-learning}.\nBergamo et al.  propose a self-taught deep learning approach for localizing objects of interest under weak supervision.\nIn the initialization stage, they design a mask-out strategy based on the deep semantic cues from a pre-trained classification network.\nSpecifically, this method first calculates the degeneration of the image-level classification scores when masking out a certain object proposal region and then selects those with large differences as the interested object regions.\nAfter the initialization stage, this method trains an SVM-based object detector in the subsequent refinement stage for final object localization.\nSimilar to , Bency et al.  propose a beam search algorithm to leverage the activation maps of a pre-trained classification network to localize the objects of interest.\nThis method is based on the observation that when image regions centered around objects of interest are classified by a pre-trained DNN, they obtain higher semantic scores than other image regions.\nHoffman et al.  develop a transfer learning-based algorithm, where the deep neural network is first trained on both the weakly labeled auxiliary training data and the strongly labeled training data to obtain the background detector.\nThen, an SVM-MIL model is adopted to learn the object detectors based on the potential foreground regions that are obtained by using the pre-trained DNN to screen the image background regions.\nTo overcome the problem that the objects of interest would sometimes co-occur with the distracting image background, Kolesnikov et al.  {propose a user-guided weakly supervised learning framework to improve the localization capacity.}\nThis approach first trains a classification network under the image level annotation.\nFor each image, the intermediate feature maps of the pre-trained network are clustered, and the object clutter is identified by a user.", "cites": [2271, 2273, 2274, 2272], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methods that use inherent cues from pre-trained deep models for weakly supervised object localization. While it mentions several approaches and their general strategies, it lacks substantial synthesis of ideas across papers, critical evaluation of their strengths and weaknesses, or abstraction to broader principles. The content remains at the level of individual method descriptions without deeper analysis or integration."}}
{"id": "eab65442-451f-481c-9954-9b19dd3d9a08", "title": "Fine-tuned Deep Models", "level": "subsection", "subsections": [], "parent_id": "ce302070-cf4f-4d53-a26c-8a51743b26f9", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Off-the-shelf Deep Models"], ["subsection", "Fine-tuned Deep Models"]], "content": "The methods of this category fine-tune the off-the-shelf DNN models during the weakly supervised learning process to obtain strong object detectors .\nA brief summary of these approaches is shown in Table \\ref{table prepost-learning}.\nChen et al.  first train CNNs from the web image data via an easy-to-hard learning scheme.\nThe learned deep features are used to mine object locations by using the exemplar-LDA detector .\nThe off-the-shelf RCNN detector is then adopted to learn object models based on their localization results.\nIn , Li et al. first use the mask-out strategy based on the pre-trained classification network to obtain the class-specific object proposals.\nAn SVM-based MIL process is used to localize object instances and\nthe classification network is further fine-tuned on the localized object instances for better performance.\nShi et al.  propose to transfer the prior knowledge of \\textit{things} and \\textit{stuff} to help the weakly supervised learning process.\nA semantic segmentation network is trained from the source set (with available bounding-box annotations) to generate the stuff map and thing map for the weakly labeled images in the target set.\nThese maps are used to obtain potential object locations, and the fast RCNN model is adopted in a Deep MIL scheme to train the object detectors.\nRecently,  revisits knowledge transfer for training the weakly supervised object detector.\nIn this method, a DNN-based proposal extractor is learned from the source data firstly.\nThe DNN is designed based on the SSD  architecture and trained with a semantic hierarchy.\nThe network is then used to provide proposals and other prior knowledge for the weakly labeled images in the target set.\nAn MIL process is used to determine the proposals that cover the objects of interest\nbased on which the fast RCNN model is adopted to learn the final object detectors.\nIn , Zhang et al. first learn to localize the objects of interest via a collaborative self-paced curriculum learning mechanism based on pre-trained deep features.\nThe fast RCNN model is applied to learn object detectors.\n\\renewcommand{\\multirowsetup}{\\centering}\n\\begin{table*} [t]\n  \\tiny\n\t\\caption{A brief summary of the approaches with multi-network training, which is a subcategory in the weakly supervised object localization and detection approaches with deep weakly supervised learning algorithms. * indicates a certain variation of the corresponding model. An approach is considered for general object category when it is tested for detecting more than five object categories in the corresponding literature. The approaches with None detector indicate the weakly supervised object localization approaches.}\n\t\\label{table multinet} \\resizebox{\\linewidth}{!}{\\begin{tabular}{m{2cm}<{\\centering}m{1.5cm}<{\\centering}m{1.5cm}<{\\centering}m{1.8cm}<{\\centering}m{3cm}<{\\centering}m{2.6cm}<{\\centering}m{3cm}<{\\centering}m{1.5cm}<{\\centering}}\n\\hline\n \\multirow{2}{*}{\\textbf{Methods}} &\n \\multirow{2}{*}{\\textbf{Detector}} &\n \\multirow{2}{*}{\\textbf{Descriptor}} &\n \\multirow{2}{*}{\\textbf{Prior knowledge} } &\n \\multirow{2}{*}{\\textbf{Extra training data}} &\n \\multirow{2}{*}{\\textbf{Learning model}} &\n \\multirow{2}{*}{\\textbf{Learning strategy}} &\n \\multirow{2}{*}{\\textbf{Object category}} \\\\\n  & & & & & & &\\\\\n\\hline\n\\hline\nZhang-CVPR-2020 &None &CNN &Common object co-localization &ImageNet(tag label) &VGG/InceptionV3/ResNet50/ DenseNet161 &Classification + pseudo supervised object localization & General objects \\\\\nZhong-ECCV-2020 &Faster RCNN &CNN &Location prior &ImageNet(tag label) + COCO (box label) &Oneclass universal detector + MIL classifier (on ResNet50) &Progressive knowledge transfer & General objects \\\\\nKosugi-ICCV-2019 & Fast RCNN* &CNN & Mask-out prior & ImageNet(tag label) & Mask-out net + OICR* & Mask-our prior-guided label refinement & General objects \\\\\nSingh-CVPR-2019 & Fast RCNN* & CNN & Motion prior & ImageNet(tag label), videos & RPN+WSDDN/OICR (VGG16) & Training RPN using weakly-labeled videos for WSOD & General objects \\\\\nArun-CVPR-2019 & Fast RCNN & CNN & None & ImageNet(tag label) & Fast RCNN (VGG16) + Fast RCNN* (VGG16) & Employ dissimilarity coefficient for modeling uncertainty & General objects \\\\\nLi-TPAMI-2019 & Faster RCNN* (VGG16) & CNN & Objectness (classifier) prior & ImageNet(tag label), ILSVRC2013(box label for unseen categories) & Faster RCNN* (VGG16) *2 & Objectness transfer+MIL +multi-scale & General objects \\\\\n Zhang-ECCV-2018 & fast RCNN* (VGG16) & CNN & None & PASCAL (edge box), ImageNet(tag label) & Multi-view WSDDN+multi view Fast RCNN & Two phase multi-view learning & General objects \\\\\nShen-CVPR-2018 & SSD & CNN & None & ImageNet(tag label) & SSD+Fast RCNN* & MIL+GAN & General objects \\\\\n Zhang-CVPR-2018 & Faster RCNN (VGG16) & CNN & None & ImageNet(tag label) & MIDN+Faster RCNN & WSOD+Pseudo Ground-truth Mining+FOD & General objects \\\\\nZhang-CVPR-2018 & Fast RCNN* (VGG16) & CNN & None & PASCAL (edge box), ImageNet & WSDDN + Fast RCNN* & WSDDN+easy-to-hard FOD & General objects \\\\\nTang-ECCV-2018 & Fast RCNN* (VGG16) & CNN & None & ImageNet(tag label) & Fast RCNN* (VGG16) & Alternating training of WSRPN and WSOD+multi-scale & General objects \\\\\nTao-TMM-2018 & Fast RCNN* (VGG16) & CNN & Web image & Web dataset(weak label),imageNet(tag label) & Midn & Easy-to-hard & General objects \\\\\n Wang-IJCAI-2018 & Faster RCNN (VGG16) & CNN & Model consistency & ImageNet(tag label) & Faster RCNN+Fast RCNN* & MIL+collaborative learning & General objects \\\\\nWei-ECCV-2018 & Faster RCNN* (VGG16) & CNN & Shape prior+ context prior & ImageNet(tag label) & MIDN+CAM+ Deeplab & Tight Box Mining+MIL +OICR+multi-scale & General objects \\\\\n Ge-CVPR-2018 & Faster RCNN (VGG16) & CNN & Local objectness and global attention & ImageNet(tag label) & MIDN,TripNet, GoogleNet, FCN, Fast RCNN & Multi evidence fusion+ outlier filtering +pixel label preidction +box generation+multi-scale & General objects \\\\\nDong-MM-2017 & Fast RCNN* & CNN & None & ImageNet(tag label) & Fast RCNN*+R-FCN & Easy-to-hard & General objects \\\\\n Li-BMVC-2017 & Fast RCNN* & CNN & Shape prior & ImageNet(tag label) & Fast RCNN* +CAM+DeepLab & Easy-to-hard & General objects \\\\\nWang-CVPR-2017 & CNN & CNN & None & ImageNet(tag label) & CAM* & Image-level training+pixel-level fine tuning & Salient objects \\\\\n Sun-CVPR-2016 & None & CNN & None & ImageNet(tag label) & Multi-scale FCN+ CNN(vgg16) & Cascade localization and recognition & General objects \\\\\n Zhang-TGRS-2016 & CNN & CNN & None & ImageNet(tag label), auxiliary data(image label) & CPRNet+LocNet & Alternative training CPRNet and LocNet & Aircraft \\\\\n\\hline\n \\end{tabular}}\n\\end{table*}", "cites": [8538, 2306, 2303, 2301, 802, 2302, 2309, 2300, 2304, 2308, 2276, 2307, 2279, 2310, 2305, 2277], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 28, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methods that fine-tune off-the-shelf deep models for weakly supervised object detection but lacks synthesis of ideas and deeper analysis. It lists techniques from various papers without critically evaluating their strengths, weaknesses, or comparing them in a structured way. The table is detailed but does not reveal overarching patterns or insights."}}
{"id": "930ae35a-10a8-4a65-83e8-a12f89f6fe36", "title": "Deep Weakly Supervised Learning", "level": "section", "subsections": ["352d0dfb-6a12-4bf2-b2bf-5544ab013008", "65da26f9-ce63-494b-9e38-4a72e74d3aa0", "c9fa8b67-133f-4688-92de-bc8e20c858b9"], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Deep Weakly Supervised Learning"]], "content": "In this section, we review the methods that learn weakly supervised object localizers or detectors by designing novel deep weakly supervised learning frameworks\\footnote{Notice that here we mainly indicate that the core learning blocks used in the learning frameworks are based on deep learning, while some minor computational components in pre-processing or post-processing, such as proposal extraction and bounding-box modification, et al., are not necessarily be implemented by deep learning.}.\nDifferent from the approaches discussed in previous sections, both the feature representations and the object detectors of the approaches in this category are learned by newly-designed deep neural networks.\nThe whole weakly supervised learning framework may be designed in a compact network model, such as in , or contain several function-distinct DNN components, such as in .\nWe categorize these approaches into two groups using single-network training and multi-network training, respectively.", "cites": [2282, 2291, 7089, 2300, 2312, 2284, 2304, 2311, 2303], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of deep weakly supervised learning methods but does not deeply synthesize or connect the cited papers into a cohesive narrative. It introduces a categorization (single-network vs. multi-network training) but offers limited comparative or critical evaluation of the approaches. The language is primarily descriptive, with minimal abstraction or meta-level insights."}}
{"id": "352d0dfb-6a12-4bf2-b2bf-5544ab013008", "title": "Single-Network Training", "level": "subsection", "subsections": [], "parent_id": "930ae35a-10a8-4a65-83e8-a12f89f6fe36", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Deep Weakly Supervised Learning"], ["subsection", "Single-Network Training"]], "content": "The methods of this category are designed with a single deep neural network using the training images (or together with the extracted object proposals) as inputs and the image-level classification labels as the outputs.\nThese approaches do not usually rely on meticulously designed initialization processes to obtain the potential object regions.\nInstead, these methods discover the interested object regions solely based on the end-to-end learning process of the designed DNN models.\nThe DNN models used in these approaches usually have similar feature learning layers as the conventional image classification network, e.g., AlexNet, VGG, GoogleNet, and ResNet, followed by the instance label inferring and image label propagation layers to inherently predict the labels of each proposal region and generate the final image labels from the predicted proposal labels, respectively. Some of the methods contain multiple network streams for online inferring multiple informative cues.\nA brief summary of these approaches are\nshown in Table \\ref{table endtoend}.\nIn the DNN models proposed by Wu et al.  and Bilen et al. , the network inputs are the training images and extracted object proposal regions while the outputs are the image-level semantic scores.\nThe first parts of these networks extract the features and infer the labels for each proposal region, and the second parts propagate the proposal scores to the image-level via the max pooling scheme  or the two-stream score regularization method .\nZhou et al.  present an end-to-end weakly supervised deep learning based on the class activation mapping (CAM).\nThe weights of the feature maps in the intermediate layers are inferred based on the correspondence between the feature map and a certain object category.\nThe feature maps are then combined to form the class activation maps based on the inferred weights, which highlight the locations of the objects of interest. {Notably, this method is determined to be highly efficient in recent work . }\nBuilt on CAM , Durand et al.  introduce the multi-map transfer layer and the WILDCAT pooling layer to facilitate the more accurate deep MIL process.\nRecently, a number of two-branch MIL models have been developed in which one is based on a typical deep network and the other one is introduced for weakly supervised learning.\nBased on the WSDDN , Tang et al.  integrate MIL branch and the instance classifier refinement branch into a unified deep learning framework such that more accurate online instance classifier learning is realized under the weak supervision.\nIn , Diba et al. propose a weakly supervised cascaded convolutional network, which contains three branches.\nThe first branch adopts the CAM module to generate the class activation maps. \nThe second branch uses the generated class activation maps as the supervision signal to learn a segmentation module to generate the segmentation masks of the objects of interest.\nUsing the candidate object proposals selected based on the obtained segmentation masks as supervision, the third network branch uses a MIL process to mine accurate object locations from the candidate object regions.\nIn , Zhang et al. present a CAM-based network architecture which contains a classification branch and a counterpart classifier branch for object localization.\nSpecifically, the classification branch is used to localize the discriminative object regions, which drives the counterpart classifier branch to discover new and complementary object regions by erasing its discovered regions from the feature maps.\nWan et al.  propose a min-entropy latent model (MELM) for weakly supervised object detection based on the assumption that minimizing entropy results in minimum randomness of a system.\nThe network architecture is similar to , but global min-entropy and local min-entropy losses are introduced to train a DNN model to select the proposal cliques of largest object probability and mine truthful object locations from the selected proposal cliques.\nZhang et al.  develop a self-generated guidance method for weakly supervised object localization.\nIn this work, a self-generated guidance map is derived from a CAM {layer to help learning features and object location masks from the previous network layers.}\n {More recently, Gao et al. propose a token semantic coupled attention mapping for WSOL, which models the long-range visual dependency of the image regions and thus avoid partial activation.  Ren et al. introduce the instance-associative spatial diversification constraints and build the parametric spatial dropout block to address the instance ambiguity and incomplete localization problems. Besides, they additionally adopt a sequential batch back-propagation algorithm, which enables their model to use a large ResNet as the backbone\\footnote{{As discussed in , it is non-trivial to introduce the deeper network backbones, e.g, the deep residual network, into the weakly supervised object detection frameworks as it would encounter dramatic deterioration of detection accuracy and training non-convergence. }}. Although there are other methods that use ResNet as the backbone , there is very limited exploration of using more recent backbone architectures, e.g., DesNet  and Res2net , in both WSOL and WSOD frameworks. }", "cites": [2259, 2282, 2298, 2290, 2314, 2280, 2315, 737, 107, 7090, 2286, 687, 2313, 2295], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple deep learning-based WSOL/WSOD methods into a coherent narrative, linking the role of CAM, multi-branch architectures, entropy minimization, and self-guidance techniques. It offers critical insights, such as the limitations of deeper backbone networks in weakly supervised settings and the issue of partial activation. The abstraction is strong, identifying overarching design principles like end-to-end learning, use of attention mechanisms, and the integration of multiple streams for improved localization."}}
{"id": "65da26f9-ce63-494b-9e38-4a72e74d3aa0", "title": "Multi-Network Training ", "level": "subsection", "subsections": [], "parent_id": "930ae35a-10a8-4a65-83e8-a12f89f6fe36", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Deep Weakly Supervised Learning"], ["subsection", "Multi-Network Training "]], "content": "The methods in this school collaborate multiple networks, either in one training stage or in multiple training stages, to accomplish the weakly supervised object localization or detection task.\nThe approaches of this category usually train a network to mine the initial object regions  and another network for the detection task under the MIL framework .\nAn additional object detection network, e.g., Fast RCNN, may also be used to train the final object detectors .\nBy integrating multiple networks, these methods tend to achieve better performance both in object localization and detection.\nA brief summary of these approaches is shown in Table \\ref{table multinet}.\nLi et al.  propose a multiple instance curriculum learning method, where a network based on the WSDDN  model is used to mine candidate object proposals and another one based on the CAM  algorithm to generate saliency maps from the selected proposals.\nA curriculum is designed to select confident training examples based on the consistency between the regions outputted by the two networks.\nThe object detectors are then trained by using the confident training examples iteratively.\nDong et al.  present a dual-network progressive approach for weakly supervised object detection, where a positive instance selection network and a region refinement network are adopted to minimize the classification error and modify object localization, respectively.\nThese two networks are worked under a co-training paradigm.\nIn , Ge et al. first obtain intermediate object localization and pixel labeling results using a classification network.\nA triplet-loss network and an instance classification network are then constructed to detect outlier and filter object instances.\nFinally, the filtered object instances are used as the supervision to train another Fast RCNN-based detection network.\n {In order to overcome the limitations brought by the imprecise of the extracted object proposals,} Wei et al.  propose to mine object proposals with tight boxes to learn weakly supervised object detector. {The assumption is that the proposals with tight boxes are more likely to contain the objects of interest thus mining such kind of proposals would help screen the cluttered background regions. In their approach, }\na semantic segmentation network is first learned using the object localization map generated by CAM as the pseudo ground-truth.\nThe predicted segmentation masks are used to mine object proposals with tight boxes, and fed into the online instance classifier refinement (OICR) network to learn weakly supervised object detector.\n {With the same motivation as , Tang et al.  propose to combine a two-stage region proposal network with an OICR network to learn the weakly supervised object detectors. Instead of mining proposals based on the semantic segmentation cue, Tang et al. use both the low-level cues (feature maps in shallow layers) and the high-level cues (semantic scores in deep layers) to mine reliable proposals in the two stages, respectively. The parameters of the region proposal network are obtained based on the network trained by .}\n {Zhang et al. explore the reliability of each training image by evaluating the image difficulty and then feed the images into the learning procedure in an easy-to-hard order. Specifically, the image difficulty is evaluated by diagnosing the localization outputs of the pre-trained WSDDN model based on the concentrateness of the high-scored proposal locations. }\nIn , Zhang et al. use three networks to learn weakly supervised object detectors.\nFirst, an OICR network is trained to generate the initial object regions.\nThen, they train an RPN  based on {the pseudo ground-truth boxes obtained after implementing a post-process on the initial object regions, and use the learned RPN} to generate more accurate object locations.\nFinally, fully supervised object detectors  are trained based on the obtained object locations.", "cites": [2259, 2298, 2306, 737, 2301, 2304, 2310, 209, 8429, 2309], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by describing how different multi-network training strategies are used within the weakly supervised learning framework, linking methods like WSDDN, CAM, and RPN across the cited works. It provides some critical insight by explaining motivations behind techniques (e.g., addressing false positives, mining tight boxes), but does not deeply evaluate trade-offs or limitations. The section identifies patterns such as the use of pseudo-ground truth and progressive training, but these insights are not elevated to a meta-level framework."}}
{"id": "79d342a2-5acc-4290-aaff-2761ce99e494", "title": "Datasets and Evaluation Metrics", "level": "section", "subsections": [], "parent_id": "3a2d911b-3708-4f3a-880a-851136519e79", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Datasets and Evaluation Metrics"]], "content": "During the last decades, significant efforts have been made to develop various methods for learning weakly supervised object localizer or detector.\nFor fair performance evaluation, it is of great importance to introduce some publicly available benchmark datasets and evaluation metrics.\n\\begin{table}[t]\n  \\centering\n  \\caption{ {Brief summarization of the characteristics of the datasets. The top three are the datasets usually used for the WSOD task, while the bottom two are the common datasets for the WSOL task. \\#Categories indicates the number of object categories. \\#Images indicates the number of images. ``GO'' is short for Generic Objects. }}\n    \\begin{tabular}{ccccc}\n    \\hline\n    Dataset & \\multicolumn{1}{c}{Content} & \\multicolumn{1}{c}{\\#Categories} & \\multicolumn{1}{c}{\\#Images} & \\multicolumn{1}{c}{Metrics} \\\\\n    \\hline\\hline\n    PASCAL VOC 07 & \\multirow{3}[2]{*}{GO} & \\multirow{3}[2]{*}{20} & 9,962 & \\multirow{3}[2]{*}{mAP, CorLoc} \\\\\n    PASCAL VOC 10 &       &       & 21,738 &  \\\\\n    PASCAL VOC 12 &       &       & 22,531 &  \\\\\n    \\hline\n    CUB-200-2011   & Birds & 200   & 11,788 & Top-1/5 Loc \\\\\n    ILSVRC 2016 & GO    & 1000  & 1.2 M & GT Loc \\\\\n    \\hline\n    \\end{tabular}\n  \\label{tab:dataset}\n\\end{table}\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=9.5 cm]{dataset.jpg}\n \\caption{ {Illustration of examples from the PASCAL VOC (top block), CUB-200-2011 (bottom-left block), and ILSVRC 2016 (bottom-right block) datasets.}}\n \\label{dataset}\n\\end{figure}\nExisting weakly supervised object detection methods are usually evaluated on the PASCAL VOC datasets, including the PASCAL VOC 2007, 2010, and 2012 sets.\nThe PASCAL VOC 2007 , PASCAL VOC 2010  and PASCAL VOC 2012  contain 9,962, 21,738, and 22,531 images of 20 object classes.\nThese three datasets are divided into train, val, and test sets, where the trainval set (5,011 images for PASCAL VOC 2007, 10,869 images for PASCAL VOC 2010, and 11,540 images for PASCAL VOC 2012) are used to train the weakly supervised object detector, and the rest for evaluation.\nThe mean of AP (mAP) metric is used to measure the performance where one object is successfully detected if the intersection over union (IoU) between the ground-truth and predicted boxes is more than 50 percentage.\nThe weakly supervised object localization performance is usually evaluated on the PASCAL VOC, ILSVRC, and CUB datasets.\nOn the PASCAL VOC datasets , weakly supervised object localization methods only use the trainval sets, which are different from the setting in weakly supervised object detection.\nThat is, both the weakly supervised learning process and localization process are implemented on the same image data.\nTo evaluate the localization performance on the PASCAL VOC datasets, the correct localization metric (CorLoc) is adopted, where the bounding-box with the highest class-specific score from each image is examined to be whether correct (with more than 50\\% overlap with the ground-truth box) or not.\nIn addition to the PASCAL VOC datasets, the ILSVRC 2016 dataset \\cite {russakovsky2015imagenet} (i.e., the ImageNet) and CUB-200-2011 dataset  are also widely used for performance evaluation.\nThe ILSVRC 2016 dataset contains more than 1.2 million images of 1,000 classes for training, while the validation set, which contains 50,000 images, is used for testing.\nThe CUB-200-2011 dataset contains 11,788 images of 200 categories with 5,994 images for training and 5,794 for testing.\nFor these two datasets, {The commonly-used evaluation metrics are GT-known localization accuracy (GT Loc), Top-1 localization accuracy (Top-1 Loc), and Top-5 localization accuracy (Top-5 Loc). Specifically, GT Loc judges the localization results as correct when the intersection over union (IoU) between the ground-truth bounding box and the estimated box is no less than 50\\%, while Top-1 Loc considers the localization results as correct when the class predicted with the highest score is equal to the ground-truth class and the estimated bounding box has no less than 50\\% IoU with the ground-truth bounding box . Top-5 Loc differs from Top-1 Loc in that it checks if the target label is one of the top 5 predictions. As can be seen, the Top-1 Loc is a harder metric than the GT Loc as it needs to additionally predict the class label correctly. This will dramatically increase the task difficulty when performing under very large or fine-grained semantic spaces. The difficulty of Top-5 Loc is between Top-1 Loc and GT Loc as it requires the model to predict the class label but does not restrict the prediction to be perfectly correct.}\n {We provide a brief summarization of the characteristics of the aforementioned datasets in Table \\ref{tab:dataset}. We additionally show some examples from each dataset to illustrate the bias of image content in different datasets. As displayed in Fig. \\ref{dataset}, the PASCAL datasets contain relatively more complex image content, where multiple object instances and categories may appear in a single image and different images would contain objects with significant scale variations. Although it only contains 20 object categories, its category diversity is higher than the CUB-200-2011 dataset as all the 200 categories in the CUB-200-2011 dataset are related to birds. ILSVRC 2016 contains far more images and categories than the PASCAL datasets. However, the content of the images from the ILSVRC 2016 dataset tends to be simpler than that of the PASCAL datasets---each of the images from the ILSVRC 2016 dataset typically contains only one single object and the objects have more consistent sizes and are placed in clearer background context relative to those from the PASCAL datasets. }\\\\\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=9.5 cm]{videoapp.jpg}\n \\caption{Weakly supervised object localization or detection methods for video understanding. The examples are from . }\n \\label{videoap}\n\\end{figure}", "cites": [2286], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of benchmark datasets and evaluation metrics used in weakly supervised object localization and detection. It includes a table and figure to illustrate dataset characteristics and example images, but it lacks meaningful synthesis across cited works, critical evaluation of their strengths and limitations, and abstraction to broader trends or principles in the field. The only cited paper in this section is not deeply integrated into the narrative."}}
{"id": "2093fbc7-1f16-499f-9309-a24b9da6de5b", "title": "Video Understanding", "level": "subsection", "subsections": [], "parent_id": "5cef946e-16aa-4550-aeb5-c885a8308ec2", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Applications"], ["subsection", "Video Understanding"]], "content": "As it is time-consuming to obtain object-level annotations for each video frame, weakly supervised object localization and detection methods have also been applied in the field of video understanding  (see Fig. \\ref{videoap}).\nFor example, Chanda et al.  build a two-stream learning framework, which adapts the information from the labeled images (source domain) to the weakly labeled videos (target domain).\nIn  Zhang et al. propose a self-paced fine-tuning network for learning two network heads to localize and segment the object of interest from the weakly labeled training videos.\nThe network is equipped with the multi-task self-paced learning function which can integrate confident knowledge from each single task (localization or segmentation) and use it to build stronger deep feature representation for both tasks.\nOn the other hand,  develop methods to localize temporal actions in the given untrimmed videos, where the main goal is to predict the temporal boundary of each action instance contained in the weakly labeled training videos.\n {Essentially, such a weakly supervised action localization (WSAL) task is an emerging, yet rapidly developing topic in recent years, and the methods for solving this task are highly related to the weakly supervised object detection and localization methods. The additional challenges are: (i) the duration of the interesting action has very large variation, i.e., from a few seconds to thousands of seconds; and (ii) the features extracted to represent the interesting action would be entangled with those of the complex scenes of the video frame. }\n {Notice that when applying to video understanding, there are strong correlations among adjacent video frames. So, additional informative constraints can be introduced to facilitate the weakly supervised object detection or localization under this scenario. }", "cites": [2319, 2317, 2316, 2318, 8539], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key video understanding approaches using weak supervision by grouping methods around tasks such as temporal action localization and object segmentation. It introduces abstract ideas like multi-task self-paced learning and attention-based sparse pooling. However, the critical analysis is limitedfew limitations or evaluations of the cited methods are discussed. The narrative is coherent but not particularly novel in its synthesis."}}
{"id": "5bcd51f6-c4c7-4e80-b0c3-9cec6a4e37d4", "title": "Art Image Analysis", "level": "subsection", "subsections": [], "parent_id": "5cef946e-16aa-4550-aeb5-c885a8308ec2", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Applications"], ["subsection", "Art Image Analysis"]], "content": "One interesting application of the weakly supervised object localization and detection techniques is the analysis of art images (see Fig. \\ref{art}).\nInoue et al.  propose a cross-domain weakly-supervised object detection framework for learning the object detectors from weakly labeled watercolor images.\nA progressive domain adaptation method to transfer the style of the fully-labeled data from the source domain (the normal RGB domain) to the target domain (the watercolor domain) is developed.\nIn  Gonthier et al. propose a weakly supervised learning algorithm for detecting objects in paintings.\nThe IconArt database which contains object classes that are absent from the photographs in daily life is developed for performance evaluation.\nIn addition, Crowley\nand Zisserman  adopt a weakly supervised object localization scheme for automatically annotating images of gods and animals in decorations on classical Greek vases.\n {When applying to art image analysis, a key challenge arises due to the distinctiveness of the content domain---even the same semantics and image scenes would be presented differently to those in the natural environment. Under this scenario, models with stronger self-domain adaptation capacity would be required for the task.}\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=9.5 cm]{artapp.jpg}\n \\caption{Examples of the application of weakly supervised object localization or detection approaches for the analysis of art images.\n The examples are from , where detection results in different colors in the painting images indicate different types of objects. }\n \\label{art}\n\\end{figure}", "cites": [2267, 2299], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates two different papers by connecting their approaches to the broader theme of art image analysis under weak supervision. It identifies a domain-specific challenge in art images and suggests the need for models with self-domain adaptation capacity, showing a moderate level of synthesis and abstraction. While it does not offer in-depth evaluation or comparison of the methods, it moves beyond mere description by highlighting the importance of adaptation in art domain tasks."}}
{"id": "6c796257-03d1-4c11-b23e-711e3eea5646", "title": "Medical Imaging", "level": "subsection", "subsections": [], "parent_id": "5cef946e-16aa-4550-aeb5-c885a8308ec2", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Applications"], ["subsection", "Medical Imaging"]], "content": "As shown in Fig. \\ref{bio}, medical image analysis is one area where the weakly supervised object localization and detection methods are of critical importance as only few annotations of target objects by trained experts in bio-image (e.g., organ or tissues).\nTo alleviate this problem, Hwang and Kim  develop a two-stream DNN model to localize the tuberculosis regions from the chest X-ray images.\n {Considering that the medical image-based applications usually do not have the pre-trained networks, this work proposes a weakly supervised learning scheme without requiring any pre-trained network parameters. The proposed network contains a fully connected layer-based classification branch and a CAM-based localization branch with shared convolutional layers for feature extraction. Both of the two branches are supervised by image label annotation, where a weighting parameter is introduced to dynamically control the relative importance between them to gradually switch the focus of the learning process from the classification branch to the localization branch. The authors demonstrate that the features learned from the classification layer at the early stage can provide informative cues to learn the localization branch at the late stage.}\n {For detecting a general type of lesions, Wang et al.  model the normal image as the combination of background and noise, while modeling the abnormal images as the combination of background, blood vessels, and noise. With the assumption that the noise for the normal image and abnormal image is the unified distribution, the image data can then be decomposed by the low-rank subspace learning technique to obtain the vessel areas. }\nIn , Gondal et al. apply the weakly supervised object detection network on the retina images and achieve few false positives with high sensitivity on the lesion-level prediction.\nLi et al.  apply a sparse coding-based weakly supervised learning method for localizing actinophrys in microscopic images.\nDubost et al.  propose weakly supervised regression neural networks for detecting brain lesions. Besides, some recent works also show great research interests in weakly supervised learning-based brain image analysis, such as brain disease prognosis , brain tumor or lesion segmentation , brain structure estimation , etc.\n {Notice that compared to common images, medical imaging data usually suffers from issues of low contrast and limited texture. Fortunately, some spatial priors could be obtained for different organs or lesions. These priors can be used to guide the weakly supervised learning process on medical imaging data.}\n\\begin{figure}[t]\n \\centering\n \\includegraphics[width=9.5 cm]{bioimage.jpg}\n \\caption{Examples of the application of weakly supervised object localization or detection approaches in medical image analysis. The examples are from . }\n \\label{bio}\n\\end{figure}", "cites": [2321, 2322, 2320], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers by connecting their approaches and common goals in the medical imaging domain. It highlights shared challenges (e.g., limited annotations, low contrast) and how different methods attempt to address them. However, it lacks deeper comparative or critical analysis of the strengths and weaknesses of these methods, and while it identifies some patterns (e.g., use of spatial priors), it does not generalize to a broader theoretical framework."}}
{"id": "e95954d7-ff46-4ac2-807f-3d34b6e59526", "title": "Multiple Instance Learning", "level": "subsection", "subsections": [], "parent_id": "d8feaee7-5d65-4327-96a9-38b968bfe1c4", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Future Directions"], ["subsection", "Multiple Instance Learning"]], "content": "Weakly supervised object localization or detection methods can be easily formulated within the MIL framework.\nEarly methods in this filed usually add prior knowledge  or post regularization  on the classic MIL models, such as LSVM , while the current research obtains the breakthrough by building deep MIL models . To further improve the weakly supervised learning performance, efforts should be made to introduce the most advanced ideas and techniques in the research filed of MIL, such as the set-level problem  the key instance shift issue  and the scalable issue  in MIL.\nFurther research towards more advanced MIL techniques would also bring helpful insights for the WSOL and WSOD in the future.", "cites": [2269, 2323, 737], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abtraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers by connecting the evolution of MIL-based approaches in weakly supervised object localization and detection. It identifies broader trends such as the shift from classic MIL models with prior knowledge to deep MIL models. However, the critical analysis is limited, and while it abstracts some general issues in MIL (e.g., scalability and key instance shift), it does not provide a deep, meta-level evaluation of the field."}}
{"id": "abffd2fa-2c1a-4f41-8253-4df7d27f9686", "title": "Multi-Task Learning", "level": "subsection", "subsections": [], "parent_id": "d8feaee7-5d65-4327-96a9-38b968bfe1c4", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Future Directions"], ["subsection", "Multi-Task Learning"]], "content": "Another future direction is to combine multiple weakly supervised learning tasks into a unified learning framework. These tasks may include object detection , semantic segmentation , instance segmentation , 3D shape reconstruction , and depth estimation .\nEssentially, efforts for simultaneously accomplishing multiple aforementioned tasks have been made in the conventional fully supervised learning scenario , which have demonstrated that such learning mechanism can bring helpful information from one task to the other ones.\nThe methods proposed by Zhang et al.  is an early attempt to implement such a weakly supervised multi-task learning mechanism and the experimental results show that training object segmentation and 3D shape reconsecration models jointly indeed benefits the both weakly supervised learning tasks. With similar spirits to , Zhang et al.  and Shen et al.  establish a self-paced fine-tuning network and a cyclic guidance network to jointly learn object localization and segmentation models under the weak supervision, respectively. {Under the multi-task weakly supervised learning scenario, one key problem is that the learning ambiguity of each individual task might be aggregated and the imprecise prediction on one task might affect the learning on other tasks. To deal with this problem, one needs to disentangle the complex multi-task learning, separately learning each individual task first, and then leveraging the confidence knowledge from each task to provide informative priors to guide the learning processes of the other tasks. }", "cites": [7091, 2260, 2324, 1751, 8429, 8540], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of multi-task learning in the context of weakly supervised object localization and detection, integrating key concepts from several cited works to form a coherent narrative. It mentions how different tasks can benefit from joint training, referencing specific papers like Zhang et al. and Shen et al. that propose novel frameworks. However, the critical analysis is somewhat limited, with only a brief mention of challenges like aggregated learning ambiguity. The abstraction level is moderate, as it generalizes the concept of multi-task learning under weak supervision but does not provide a deep meta-level insight."}}
{"id": "56c3e88b-fc74-49e6-aa73-cff57609f8ef", "title": "Robust Learning Theory", "level": "subsection", "subsections": [], "parent_id": "d8feaee7-5d65-4327-96a9-38b968bfe1c4", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Future Directions"], ["subsection", "Robust Learning Theory"]], "content": "To address the learning under uncertainty issue that is inherently existed in the weakly supervised learning process, robust learning strategy will become one of the key techniques in the future. The goal is to alleviate the influence of the noisy samples during the learning process. In implementation, such learning strategy is usually achieved by selecting easy and confident training samples in the early learning stages while using hard and more ambiguous training samples in the late learning stages. Essentially, a number of recent methods have already introduced the robust learning strategies into their learning frameworks. For example, Shi and Ferrari  propose a curriculum learning strategy to feed training images into the WSOL learning loop in order from images containing bigger objects down to smaller ones. The training order is determined by the size of the object, which is estimated based on a regression model. Similarly, Zhang et al.  design a zigzag learning strategy, where they first develop a criterion to automatically rank the localization difficulty of an image, and then learn the detector progressively by feeding examples with increasing difficulty. As can be seen, these methods are just intuitive ways to introduce robust learning strategy into the weakly supervised object localization and detection frameworks, while they have already achieved obvious performance gains when compared with the conventional learning strategy. Along this line, Zhang et al.  propose a self-paced curriculum learning framework for weakly supervised object detection. By integrating the curriculum learning  with the self-paced learning , the established learning framework provides a more theoretical-sounded way to improve the learning robustness. However, the solid robust learning theory is still lack in this research field.", "cites": [2306, 8537], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes two distinct approaches (Shi and Ferrari, Zhang et al.) and connects them under the broader theme of robust learning strategies in weakly supervised object detection. It provides critical evaluation by noting that current methods are 'intuitive' and lack solid theoretical grounding. The abstraction is strong as it generalizes the concept of robust learning and positions it as a future direction, identifying a common trend and a theoretical gap in the field."}}
{"id": "47c758a2-7983-451c-ad2b-fa45ee0b2b84", "title": "Reinforcement and Adversarial Learning", "level": "subsection", "subsections": [], "parent_id": "d8feaee7-5d65-4327-96a9-38b968bfe1c4", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Future Directions"], ["subsection", "Reinforcement and Adversarial Learning"]], "content": "Besides the conventional CNN models, it is also worth trying to apply some more advanced learning models into the learning process of the weakly supervised object detector. Here we give two examples. The first one is the deep reinforcement learning.\nAccording to~, biological vision systems are believed to have a sequential process with changing retinal fixations that gradually accumulate evidence of certainty when searching or localizing objects. Several existing methods~ have also demonstrated that designing deep reinforcement learning frameworks to model such a sequential searching process can indeed help to address the object localization, detection, and tracking problems in the computer vision community. Thus, it is highly desirable, both biologically and computationally, to explore deep reinforcement learning models that facilitate the weakly supervised object localization and detection systems in such a sequential searching process .\nThe second one is the generative adversary learning. As we know, generative adversary learning has been demonstrated to have advantages in unsupervised and semi-supervised learning scenarios . It can generate the desired data distribution based on very weak supervision, i.e, ``real'' or ``fake''. Such capacity endows generative adversary learning very large potential in solving the weakly supervised object localization and detection problems. Although existing methods, such as , have already made efforts to introduce such a learning mechanism into the weakly supervised object localization and detection, there is still much room for improvement along this research direction.", "cites": [2329, 2328, 2326, 2280, 2325, 2330, 2327, 8541], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the core ideas of reinforcement and adversarial learning in the context of weakly supervised object localization and detection, using multiple cited papers to support the argument. It abstracts these methods into general research directions, highlighting their potential and how they align with biological and computational goals. However, the critical analysis is limited, as it mainly points out potential for improvement without delving into specific limitations or trade-offs of the cited approaches."}}
{"id": "8b5b77db-9af6-4869-b487-c1496c6cc1b1", "title": "Prior-guided Deep MIL", "level": "subsection", "subsections": [], "parent_id": "d8feaee7-5d65-4327-96a9-38b968bfe1c4", "prefix_titles": [["title", "Weakly Supervised Object Localization and Detection: A Survey"], ["section", "Future Directions"], ["subsection", "Prior-guided Deep MIL"]], "content": "From Table \\ref{table endtoend} and Table \\ref{table multinet}, we can observe that most of the current deep weakly supervised object detection methods have not introduced any prior knowledge into their learning frameworks. However, from our review on classic models (see Sec. \\ref{Classic Models}), prior knowledges actually play important roles in avoiding the weakly supervised learning process from drifting to trivial solutions. Considering this issue, some recent works utilize prior knowledges of saliency , objectness , shape , count , human action , human object interaction , mask-out scoring  in their frameworks. However, research towards building effective deep MIL frameworks (such as the one with prior knowledge distillation  or cross domain adaptation ) to embed helpful prior knowledge into the weakly supervised learning process needs to be further explored in the future. In addition, the co-occurring patterns mined in co-saliency detection  and object co-localization  approaches can also be used as informative priors to guide the deep multiple instance learning process in weakly supervised object localization and detection.", "cites": [2332, 2289, 8538, 2334, 2307, 2331, 2333, 2309], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers by connecting prior knowledge concepts such as saliency, objectness, and count to the broader context of deep MIL in weakly supervised object detection. It identifies a trend of incorporating diverse priors and highlights a critical gap in existing deep methods: lack of explicit prior integration. The discussion abstracts these ideas into a general direction for future research, emphasizing the potential of prior-guided frameworks and cross-domain adaptation."}}
