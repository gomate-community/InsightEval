{"id": "d21f0515-b2eb-4237-8978-945c5fb8680c", "title": "Introduction", "level": "section", "subsections": ["5d767563-e330-4082-a329-688374a9114c", "8523bc62-bac3-41c2-8069-03d1b6db759d", "70940289-1203-4a10-9fe0-d8c2db8ffc29"], "parent_id": "cbdb372b-cea7-411a-9f50-8d818e9f2d0d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{Fig1.pdf} \\vspace {-0.5cm}\n    \\caption{RGB-D based salient object prediction on a sample image using two classic non-deep models (\\ie, DCMC  and SE ) and seven state-of-the-art deep models (\\ie, D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net ). \n    }\\label{fig_00}\n\\end{figure}\nSalient object detection (SOD) aims to locate the most visually prominent object(s) in a given scene~. SOD plays a key role in a range of real-world applications, such as stereo matching~, \nimage understanding , co-saliency detection~, action recognition , video detection and segmentation , semantic segmentation , medical image segmentation~, object tracking , person re-identification , camouflaged object detection~, image retrieval~, \\etc. Although significant progress has been made in the SOD field over the past several years , there is still room for improvement when faced with challenging factors, such as complicated background or different lighting conditions in the scenes. One way to overcome these challenges is to employ depth maps, which provide complementary spatial information for RGB images and have become easier to capture due to the large availability of depth sensors (\\eg, Microsoft Kinect).\nRecently, RGB-D based SOD has gained increasing attention and various methods have been developed . Early RGB-D based SOD models tended to extract handcrafted features and then fuse RGB image and depth maps. For example, Lang \\etal~, the first work on RGB-D based SOD, utilized Gaussian mixture models to model the distribution of depth-induced saliency. Ciptadi \\etal~ extracted 3D layout and shape features from depth measurements. Besides, several methods  measure depth contrast using the depth difference between different regions. In , a multi-contextual contrast model including local, global, and background contrast was developed to detect salient objects using depth maps. More importantly, however, this work also provided the first large-scale RGB-D dataset for SOD. Despite the effectiveness achieved by traditional methods using handcrafted features, they tend to suffer from a limited generalization ability for low-level features and lack the high-level reasoning required for complex scenes. To address these limitations, several deep learning-based RGB-D SOD methods  have been developed, showing improved performance. DF  was the first model to introduce deep learning technology into the RGB-D based SOD task. More recently, various deep learning-based models  have focused on exploiting effective multi-modal correlations and multi-scale/level information to boost SOD performance. To more clearly describe the progress in the RGB-D based SOD field, we provide a brief chronology in Fig.~\\ref{fig_0}. \n\\begin{figure*}[t]\n\\centering\n\\begin{overpic}[width=0.99\\linewidth]{Fig2.pdf}\n\\put(7.7,23.7){\\footnotesize }  \n\\put(15.2,27.7){\\footnotesize }  \n\\put(21.3,33.2){\\footnotesize }      \n\\put(25.8,6.6){\\footnotesize }        \n\\put(31.1,29.9){\\footnotesize } \n\\put(41.7,25.0){\\footnotesize }     \n\\put(51.7,31.9){\\footnotesize }        \n\\put(62.4,9.0){\\footnotesize }        \n\\put(72.8,25.3){\\footnotesize }  \n\\put(74.6,34.4){\\footnotesize }     \n\\put(76.6,3.4){\\footnotesize }      \n\\put(83.4,29.3){\\footnotesize }       \n\\put(86.9,9.3){\\footnotesize }           \n\\put(89.0,24.9){\\footnotesize }           \n\\end{overpic}\\vspace{-0.25cm}\n\\caption{ A brief chronology of RGB-D based SOD. The first early RGB-D based SOD work was the DM  model, proposed in 2012. Deep learning techniques have been widely applied to RGB-D based SOD since 2017. More details can be found in \\secref{sec:models}.}\\label{fig_0}\n\\end{figure*}\nIn this paper, we provide a comprehensive survey on RGB-D based SOD, aiming to thoroughly cover various aspects of the models for this task and provide insightful discussions on the challenges and open directions for future work. We also review another related topic, \\ie, light field SOD, in which the light field can provide more information (including focal stack, all-focus images, and depth maps) to boost the performance of salient object detection. Further, we provide a comprehensive comparison to evaluate existing RGB-D based SOD models and discuss their main advantages.", "cites": [6226, 6219, 6223, 8141, 6218, 8988, 6229, 6222, 6221, 6224, 6230, 6228, 6227, 6220, 6231, 6225, 8140, 6217], "cite_extract_rate": 0.3103448275862069, "origin_cites_number": 58, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of RGB-D salient object detection (SOD), listing examples of both non-deep and deep learning-based methods. It cites several relevant papers but does not deeply synthesize or connect their core ideas in a coherent framework. While it highlights the shift from handcrafted features to deep learning, it lacks critical evaluation of the cited works and offers minimal abstraction beyond specific methods."}}
{"id": "5d767563-e330-4082-a329-688374a9114c", "title": "Related Reviews and Surveys", "level": "subsection", "subsections": [], "parent_id": "d21f0515-b2eb-4237-8978-945c5fb8680c", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Introduction"], ["subsection", "Related Reviews and Surveys"]], "content": "There are several surveys that are closely related to salient object detection. For example,  Borji \\etal~ provided a \nquantitative evaluation of 35 state-of-the-art non-deep saliency detection methods. Cong \\etal~ reviewed several different saliency detection models, including RGB-D based SOD, co-saliency detection, and video SOD. Zhang \\etal~  provided an overview of co-saliency detection and reviewed its history, and summarized several benchmark algorithms in this field. Han \\etal~  reviewed the recent progress in SOD, including models, benchmark datasets, and evaluation metrics, as well as discussed the underlying connection among general object detection, SOD, and category-specific object detection. Nguyen \\etal~ reviewed various works related to saliency applications and provided insightful discussions on the role of saliency in each. Borji \\etal~ provided a comprehensive review of recent progress in SOD and discussed some related works, including generic scene segmentation, saliency for fixation prediction, and object proposal generation. Fan \\etal~ provided a comprehensive evaluation of several state-of-the-art CNNs-based SOD models, and proposed a high quality SOD dataset, termed \\textbf{SOC} (details can be found at: \\href{http://dpfan.net/socbenchmark/}{http://dpfan.net/socbenchmark/}). Zhao \\etal~ reviewed various deep learning-based object detection models and algorithms in detail, as well as various specific tasks, including SOD works. Wang \\etal~ focused on reviewing deep learning-based SOD models. Different from previous SOD surveys, in this paper, we focus on reviewing the existing RGB-D based SOD models and benchmark datasets.", "cites": [6233, 6235, 6232, 6234, 6225, 8607], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of related surveys and their focuses but lacks deep synthesis or integration of ideas across them. It briefly mentions the scope of each cited work, including RGB-D and deep learning aspects, but does not critically analyze their strengths or limitations. There is minimal abstraction or identification of broader trends beyond listing the works."}}
{"id": "b7808ac0-c2fe-488b-b432-aa33a9ddd22d", "title": "RGB-D based SOD Models", "level": "section", "subsections": ["aadad591-3c55-4cb0-8811-fb74561275e8", "99f631cc-35ef-4d37-abd4-33e889f4b427", "c44c1568-3d6f-4d1f-89f7-e8d27f9dbd19", "97d83e3a-57b6-481b-918b-eb7f21312fe1", "0f10411b-bb23-4247-8c54-2ec983151c68"], "parent_id": "cbdb372b-cea7-411a-9f50-8d818e9f2d0d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D based SOD Models"]], "content": "\\label{sec:models}\nOver the past few years, several RGB-D based SOD methods have been developed and obtained promising performance. These models are summarized in Tables ~\\ref{tab:001}, \\ref{tab:002}, \\ref{tab:003} and \\ref{tab:004}. The complete benchmark can be found at \\href{http://dpfan.net/d3netbenchmark/}{http://dpfan.net/d3netbenchmark/}.\nTo review these RGB-D based SOD models in detail,we introduce them from different perspectives as follows. (1) \\textbf{Traditional/deep models}: they are viewed from the perspective of feature extraction, that is using the manual features or  deep features. It is convenient for follow-up researchers to grasp the historical development trends of RGB-D SOD models. (2) \\textbf{Fusion-wise models}: it is critical to effectively fuse RGB and depth images in this task, thus we review different fusion strategies to understand their effectiveness. (3) \\textbf{Single-stream/multi-stream models}: we consider this problem from the perspective of model parameters. Single stream can save parameters, but the final result may not be optimal, and multiple streams may require more parameters. Thus, it is helpful to understand the balance between the amount of calculation and accuracy of different models. (4) \\textbf{Attention-aware models}: attention mechanisms have widely been applied in various visual tasks including SOD. We review related works on RGB-D SOD to analyze how do different models use attention. Thus, it is an alternative to design attention modules for future works. \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Summary of RGB-D based SOD methods (published from 2012 to 2016).}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:001}\n    \\setlength{\\tabcolsep}{3.5pt}\n    \\begin{tabular}{|p{0.4cm}|p{0.6cm}<{\\centering}|r|p{1.0cm}<{\\centering}|p{1.8cm}|p{1.2cm}|p{9.0cm}|}\n        \\hline\n         \\# & Year & Method & Pub.  & Training Set & Backbone & Description \\\\ \\hline\\hline\n    1 & 2012 & DM   & ECCV    & Without & Without & Models the correlation between saliency and depth by approximating the joint density using Gaussian mixture models \\\\ \\hline \n    2 & 2012 & RCM  & ICCSE  & Without  & Without & Develops a region contrast based SOD model with depth cues  \\\\ \\hline \n    3 & 2013 & LS  & BMVC  & Without  & Without & Extends the dissimilarity framework to model the joint interaction between  depth cues and RGB images \\\\ \\hline\n    4 & 2013 & RC  & BMVC  & Withoutt  & Without & Derives RGB-D saliency by formulating a 3D saliency model based on the region contrast of the scene and fuses it using SVM \\\\ \\hline \n    5 & 2013 & SOS  & NEURO  & Without  & Without & Incorporates depth cues for salient object segmentation by suppressing background regions \\\\ \\hline \n    6 & 2014 & SRDS  & ICDSP  & Without  & Without & Integrates depth and depth weighted color contrast with spatial compactness of color distribution  \\\\ \\hline  \n    7 & 2014 & LHM  & ECCV  & Without  & Without & Uses a multi-stage RGB-D algorithm to combine both depth and appearance cues to segment salient objects  \\\\ \\hline   \n    8 & 2014 & DESM  & ICIMCS  & Without  & Without & Combines three saliency cues: color contrast, spatial bias, and depth contrast  \\\\ \\hline  \n    9 & 2014 & ACSD  & ICIP  & Without  & Without & Measures a point's saliency by how much it stands out from the surroundings, and has two priors (regions nearer to viewers are more salient and salient objects tend to be located at the center) \\\\ \\hline \n    10 & 2015 & GP  & CVPRW  & Without & Without & Explores orientation and background priors for detecting salient objects, and uses PageRank and MRFs to optimize the saliency maps \\\\ \\hline  \n    11 & 2015 & SFP  & ICIMCS  & Without  & Without & Develops a RGB-D based SOD approach using saliency fusion and propagation\\\\ \\hline  \n    12 & 2015 & DIC  & TVC  & Without  & Without & Fuses the saliency maps from color and depth to generate a noise-free salient patch, and utilizes random walk algorithm to infer the object boundary \\\\ \\hline  \n    13 & 2015 & SRD  & ICRA  & Without  & Without & Designs a graph-based segmentation to identify homogeneous regions using color and depth cues \\\\ \\hline  \n    14 & 2015 & MGMR  & ICIP  & Without  & Without & Designs a mutual guided manifold ranking strategy to achieve SOD  \\\\ \\hline  \n    15 & 2015 & SF  & CAC  & Without  & Without & Proposes to automatically select discriminative features using decision trees for better performance   \\\\ \\hline  \n    16 & 2016 & PRC  & ACCESS  & Without  & Without & Saliency fusion and progressive region classification are used to optimize depth-aware saliency models \\\\ \\hline \n    17 & 2016 & LBE  & CVPR  & Without  & Without & Uses a local background enclosure to capture the spread of angular directions \\\\ \\hline \n    18 & 2016 & SE  & ICME  & Without  & Without & Utilizes cellular automata to propagate the initial saliency map and then generate the final saliency prediction result \\\\ \\hline \n    19 & 2016 & DCMC  & SPL  & Without  & Without & Develops a new measure to evaluate the reliability of depth maps for reducing the influence of poor-quality depth maps on saliency detection.\n    \\\\ \\hline  \n    20 & 2016 & BF  & ICPR  & Without  & Without & Fuses contrasting features from RGB and depth images with a Bayesian framework\n    \\\\ \\hline  \n    21 & 2016 & DCI  & ICASSP  & Without  & Without & Adopts the original depth map to subtract the fitted surface for generating a contrast increased map \\\\ \\hline    \n    22 & 2016 & DSF  & ICASSP  & Without  &Without  & Develops a multi-stage depth-aware saliency model for SOD \\\\ \\hline    \n    23 & 2016 & GM  & ACCV    & Without  &Without  & Combines color and depth-based contrast features using a generative mixture model \\\\ \\hline    \n    \\hline\n    \\end{tabular}\n\\end{table*}\n\\renewcommand\\arraystretch{1.1}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Summary of RGB-D based SOD methods (published from 2017 to 2018).}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:002}\n    \\setlength{\\tabcolsep}{3.5pt}\n    \\begin{tabular}{|p{0.4cm}|p{0.6cm}<{\\centering}|r|p{1.0cm}<{\\centering}|p{2.0cm}|p{1.2cm}|p{9.0cm}|}\n        \\hline\n         \\# & Year & Method & Pub.  & Training Set & Backbone & Description \\\\ \\hline\\hline\n    24 & 2017 & HOSO  & DICTA    & Without & Without & Combines surface orientation distribution contrast with color and depth contrast  \\\\ \\hline \n    25 & 2017 & M{$^3$}Net  & IROS  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Designs a multi-path multi-modal fusion strategy to integrate RGB and depth images in a task-motivated and adaptive way \\\\ \\hline \n    26 & 2017 & MFLN  & ICCVS  & NLPR(0.65K), NJUD(1.4K) & AlexNet & Leverages a CNN to learn high-level representations for depth maps, and uses a multi-modal fusion network to integrate RGB and depth representations for RGB-D based SOD \\\\ \\hline \n    27 & 2017 & BED  & ICCVW  & NLPR(0.6K), NJUD(1.2K) & GoogleNet& Uses a CNN to integrate top-down and bottom-up information for RGB-D based SOD, and uses a mid-level feature representation to capture background enclosure\\\\ \\hline \n    28 & 2017 & CDCP  & ICCVW  & Without & Without & Proposes a novel RGB-D SOD algorithm using a center dark channel prior to boost performance \\\\ \\hline \n    29 & 2017 & TPF  & ICCVW  & Without & Without & Leverages stereopsis to generate optical flow, which can provide an additional cue (depth cue) for producing the final detection result \\\\ \\hline    \n    30 & 2017 & MFF  & SPL  & Without & Without & Uses a multistage fusion framework to integrate multiple visual priors from the RGB image and depth cue for SOD  \\\\ \\hline \n    31 & 2017 & MDSF  & TIP  & NLPR(0.5K), NJUD(1.5K) & Without & Proposes a RGB-D SOD framework via a multi-scale discriminative saliency fusion strategy, and utilizes bootstrap learning to achieve the SOD task \\\\ \\hline\n    32 & 2017 & DF  & TIP  & NLPR(0.75K), NJUD(1.0K) & Without & Feeds RGB and depth features into a CNN architecture to derive the saliency confidence value, and uses Laplacian propagation to produce the final detection result \\\\ \\hline \n    33 & 2017 & MCLP  & TCYB & Without & Without & Utilizes the additional depth maps and employs the existing RGB saliency map as an initialization using a refinement-cycle model to obtain the final co-saliency map \\\\ \\hline \n    34 & 2018 & ISC  & SIVP & Without & Without & Fuses salient features using both bottom-up and top-down saliency cues \\\\ \\hline \n    35 & 2018 & HSCS  & TMM  & Without & Without & Utilizes a hierarchical sparsity reconstruction and energy function refinement for RGB-D based co-saliency detection \\\\ \\hline  \n    36 & 2018 & ICS  & TIP  &  Without & Without & Exploits the constraint correlation among multiple images and introduces depth maps into the co-saliency model  \\\\ \\hline     \n    37 & 2018 & CTMF  & TCYB  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Transfers the structure of the deep color network to be applicable for the depth modality and fuses both modalities to produce the final saliency map  \\\\ \\hline  \n    38 & 2018 & PCF  & CVPR  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Designs the first multi-scale fusion architecture and a novel complementarity-aware fusion module to fuse both cross-modal and cross-level features \\\\ \\hline  \n    39 & 2018 & SCDL  & ICDSP  & NLPR(0.75K), NJUD(1.0K) & VGG-16 & Designs a new loss function to increase the spatial coherence of salient objects  \\\\ \\hline \n    40 & 2018 & ACCF  & IROS  & NLPR(0.65K), NJUD(1.4K)  & VGGNet & Adaptively selects complementary features from different modalities at each level, and then performs more informative cross-modal cross-level combinations\\\\ \\hline   \n    41 & 2018 & CDB  & NEURO  &Without & Without & Utilizes a contrast prior and depth-guided-background prior to construct a 3D stereoscopic saliency model \\\\ \\hline  \n    \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8143, 8142], "cite_extract_rate": 0.04878048780487805, "origin_cites_number": 41, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes RGB-D based SOD models using a categorization into four perspectives (traditional/deep, fusion-wise, single-stream/multi-stream, and attention-aware). However, it lacks deeper synthesis, critical evaluation, or abstraction. The cited papers are not explicitly analyzed or connected to the section's framework, and the text reads as a classification rather than an insightful analysis of trends or limitations."}}
{"id": "aadad591-3c55-4cb0-8811-fb74561275e8", "title": "Traditional/Deep Models", "level": "subsection", "subsections": [], "parent_id": "b7808ac0-c2fe-488b-b432-aa33a9ddd22d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D based SOD Models"], ["subsection", "Traditional/Deep Models"]], "content": "\\textbf{Traditional Models}. With depth cues, several useful attributes, such as boundary cues, shape attributes, surface normals, etc., can be explored to boost the identification of salient objects in complex scenes. Over the past several years, many traditional RGB-D models based on handcrafted features have been developed . For example, the early work  focused on modeling the interaction between layout and shape features generated from the RGB image and depth map. Besides, the representative work  developed a novel multi-stage RGB-D model, and constructed the first large-scale RGB-D benchmark dataset, termed NLPR. \n\\textbf{Deep Models}. However, the above-mentioned methods suffer from unsatisfactory SOD performance due to the limited expression ability of handcrafted features. To address this, several studies have turned to deep neural networks (DNNs) to fuse RGB-D data . These models can learn high-level representations to explore complex correlations across RGB images and depth cues for improving SOD performance. We review some representative works in detail as follows.\n$\\bullet$ \\textbf{DF}  develops a novel convolutional neural network (CNN) to integrate different low-level saliency cues into hierarchical features, for effectively locating salient regions in RGB-D images. This was the first CNN-based model for the RGB-D SOD task. However, it utilizes a shallow architecture to learn the saliency map. \n$\\bullet$ \\textbf{PCF}  presents a complementarity-aware fusion module to integrate cross-modal and cross-level feature representations. It can effectively exploit complementary information by explicitly using cross-modal/level connections and modal/level-wise supervision to decrease fusion ambiguity. \n$\\bullet$ \\textbf{CTMF}  employs a computational model to identify salient objects from RGB-D scenes, utilizing CNNs to learn high-level representations for RGB images and depth cues, while simultaneously exploiting the complementary relationships and joint representation. Besides, this model transfers the structure of the model from the source domain (\\emph{i.e.}, RGB images) to be applicable to the target domain (\\emph{i.e.}, depth maps). \n$\\bullet$ \\textbf{CPFP}  proposes a contrast-enhanced network to produce an enhanced map, and presents a fluid pyramid integration module to effectively fuse cross-modal information in a hierarchical manner. Besides, considering the fact that depth cues tend to suffer from noise, a feature-enhanced module is proposed to learn an enhanced depth cue for boosting the SOD performance. It is worth noting that this is an effective solution. \n$\\bullet$ \\textbf{UC-Net}  proposes a probabilistic RGB-D based SOD network via conditional variational autoencoders (VAEs) to model human annotation uncertainty. It generates multiple saliency maps for each input image by sampling in the learned latent space. This was the first work to investigate uncertainty in RGB-D based SOD, and was inspired by the data labeling process. This method leverages the diverse saliency maps to improve the final SOD performance. \n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{Fig3.pdf} \\vspace {-1mm}\n    \\caption{Comparison of three fusion strategies that explore the correlation between RGB images and depth maps for RGB-D based SOD. These include: 1) Early fusion; 2) Late fusion; 3) Multi-scale fusion. \n    }\\label{fig_1}\n\\end{figure*}\n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Summary of RGB-D based SOD models published in 2019 and 2020}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:003}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{|p{0.4cm}|p{0.6cm}<{\\centering}|r|p{0.95cm}<{\\centering}|p{2.0cm}|p{1.2cm}|p{8.0cm}|}\n        \\hline\n         No. & Year & Method & Pub.  & Training Set & Backbone & Description \\\\ \\hline\\hline\n    42 & 2019 & SSRC  & NEURO  &NLPR(0.65K), NJUD(1.4K) & VGG-16& Uses a single-stream recurrent convolutional neural network with a four-channel input and DRCNN subnetwork \\\\ \\hline  \n    43 & 2019 & MLF  & SPL  & NJUD(1.588K) & VGG-16 & Designs a salient object-aware data augmentation method to expand the training set \\\\ \\hline      \n    44 & 2019 & TSRN  & ICIP  & NJUD(1.387K) & VGG-16  & Designs a fusion refinement module to integrate output features from different modalities and resolutions \\\\ \\hline     \n    45 & 2019 & DIL  & MTAP  & NLPR(0.5K), NJUD(0.5K) & Without & Designs a consistency integration strategy to generate an image pre-segmentation result that is consistent with the depth distribution \\\\ \\hline     \n    46 & 2019 & CAFM  & TSMC  & NUS , NCTU  & VGG-16 & Utilizes a content-aware fusion module to integrate global and local information \\\\ \\hline     \n    47 & 2019 & PDNet  & ICME  & NLPR(0.5K), NJUD(1.5K) & VGG-16 & Adopts a prior-model guided master network to process RGB information, which is pre-trained on the conventional RGB dataset to overcome the limited size  \\\\ \\hline  \n    48 & 2019 & MMCI  & PR  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Improves the traditional two-stream architecture by diversifying the multi-modal fusion paths and introducing cross-modal interactions in multiple layers  \\\\ \\hline     \n    49 & 2019 & TANet  & TIP  & NLPR(0.65K), NJUD(1.4K)  & VGG-16 & Uses a three-stream multi-modal fusion framework to explore cross-modal complementarity in both the bottom-up and top-down processes \\\\ \\hline  \n    50 & 2019 & DCMF  & TCYB  & NLPR(0.65K), NJUD(1.4K)   & VGG-16 & Formulates a CNN-based cross-modal transfer learning problem for depth-induced SOD, and uses a dense cross-level feedback strategy to exploit cross-level interactions \\\\ \\hline  \n    51 & 2019 & DGT  & TCYB  & Without  & Without & Exploits depth cues and provides a general transformation model from RGB saliency to RGB-D saliency \\\\ \\hline \n    52 & 2019 & LSF  & arXiv  & NLPR(0.65K), NJUD(1.4K)  & VGG & Designs an RGB-D system with three key components, including modality-specific representation learning, complementary information selection, and cross-modal complements fusion\\\\ \\hline \n    53 & 2019 & AFNet  & ACCESS  & NLPR(0.65K), NJUD(1.4K) & VGG-16  & Learns a switch map that is used to adaptively fuse the predicted saliency maps from the RGB and depth modality \\\\ \\hline  \n    54 & 2019 & EPM  & ACCESS  & Without & Without & Develops an effective propagation mechanism for RGB-D co-saliency detection \\\\ \\hline  \n    55 & 2019 & CPFP  & CVPR  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Uses a contrast-enhanced network to obtain the one-channel enhanced map, and designs a fluid pyramid integration module to fuse cross-modal cross-level features in a pyramid style  \\\\ \\hline   \n    56 & 2019 & DMRA  & ICCV  & NLPR(0.7K), NJUD(1.485K) & VGG-19  & Designs a depth-induced multiscale recurrent attention network for SOD, including a depth refinement block and a recurrent attention module\\\\ \\hline \n    57 & 2019 & DSD  & JVCIR  & NLPR(0.5K), NJUD(1.5K) & VGG-16  & Uses a saliency fusion network to adaptively fuse both the color and depth saliency maps \\\\ \\hline\n    58 & 2020 & DPANet  & arXiv  & NLPR(0.65K), NJUD(1.4K), DUT(0.8K) & ResNet-50 & Uses a saliency-orientated depth perception module to evaluate the potentiality of depth maps and reduce effects of contamination \\\\ \\hline\n    59 & 2020 & SSDP  & arXiv  & NLPR(0.7K), NJUD(1.485K), DUT(0.8K)  & VGG-19 & Makes use of existing labeled RGB saliency datasets together with unlabeled RGB-D data to boost SOD performance\\\\ \\hline    \n    60 & 2020 & AttNet  & IVC  & NLPR(0.65K), NJUD(1.4K)  & VGG-16 & Deploys attention maps to boost the salient objects' location and pays more attention to the appearance information \\\\ \\hline        \n    61 & 2020 & ---  & NEURO  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Uses an adaptive gated fusion module via a GAN to obtain a better fused saliency map from RGB images and depth cues \\\\ \\hline     \n    62 & 2020 & CoCNN  & PR  & STERE, NJUD & VGG-16 & Fuses color and disparity features from low to high layers in a unified deep model \\\\ \\hline\n    63 & 2020 & cmSalGAN  & TMM  & NLPR(0.65K), NJUD(1.4K) & ResNet-50 & Aims to learn an optimal view-invariant and consistent pixel-level representation for both RGB and depth images using an adversarial learning framework\\\\ \\hline\n    64 & 2020 & PGHF  & ACCESS  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Leverages powerful representations learned from large-scale RGB datasets to boost the model ability\\\\ \\hline\n        \\hline\n    \\end{tabular}\n\\end{table*} \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Summary of RGB-D based SOD models published in 2020.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:004}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{|p{0.4cm}|p{0.6cm}<{\\centering}|r|p{0.75cm}<{\\centering}|p{2.0cm}|p{1.2cm}|p{8.5cm}|}\n        \\hline\n         No. & Year & Method & Pub.  & Training Set & Backbone & Description \\\\ \\hline\\hline\n    65 & 2020 & BiANet  & TIP  & NLPR(0.7K), NJUD(1.485K) & VGG-16 & Uses a bilateral attention module (BAM) to explore rich foreground and background information from depth maps \\\\ \\hline\n    66 & 2020 & ASIF-Net  & TCYB  & NLPR(0.65K), NJUD(1.4K) & VGG-16 & Integrates the attention steered complementarity from RGB-D images and introduces a global semantic constraint using adversarial learning \\\\ \\hline\n    67 & 2020 & Triple-Net  & SPL  &  Triple-Net & ResNe-18 & Uses a triple-complementary network for RGB-D based SOD \\\\ \\hline\n    68 & 2020 & ICNet  & TIP  &  Triple-Net & VGG-16 & Uses a novel information conversion module to fuse high-level RGB and depth features in an interactive and adaptive way \\\\ \\hline\n    69 & 2020 & SDF  & TIP  & NLPR,NJUD, DEC,LFSD(1.5K) & VGG-16 & Proposes a exemplar-driven method to estimate relatively trustworthy depth maps, and uses a selective deep saliency fusion network to effectively integrate RGB images, original depths, and newly estimated depths \\\\ \\hline\n    70 & 2020 & GFNet  & SPL  & NLPR(0.8K), NJUD(1.588K)& Res2Net & Designs a gate fusion block to regularize feature fusion \\\\ \\hline    \n    71 & 2020 & RGBS  & MTAP  &NLPR(0.65K), NJUD(1.4K) & VGG-16 & Utilizes a GAN to generate the saliency map\\\\ \\hline    \n    72 & 2020 & D$^{3}${Net}  & TNNLS  & NLPR(0.7K), NJUD(1.485K)& VGG-16 & Uses a depth depurator unit (DDU) and a three-stream feature learning module to employ low-quality depth cue filtering and cross-modal feature learning, respectively \\\\ \\hline\n    73 & 2020 & JL-DCF  & CVPR  & NLPR(0.7K), NJUD(1.5K) & VGG-16, ResNet-101 & Uses a joint learning strategy and a densely-cooperative fusion module to achieve better SOD performance \\\\ \\hline\n    74 & 2020 & A2dele  & CVPR  & NLPR(0.7K), NJUD(1.485K)  & VGG-16 & Employs a depth distiller to explore ways of using network prediction and attention as two bridges to transfer depth knowledge to RGB images \\\\ \\hline   \n    75 & 2020 & SSF  & CVPR  &NLPR(0.7K), NJUD(1.485K), DUT(0.8K) & AGG-16  & Designs a complimentary interaction module to select useful representations from the RGB and depth images and then integrate cross-modal features \\\\ \\hline\n    76 & 2020 & S${^2}$MA  & CVPR  &NLPR(0.65K), NJUD(1.4K)  & VGG-16 & Fuses multi-modal information via self-attention and each otherâ€™s attention strategies, and reweights the mutual attention term to filter out unreliable information \\\\ \\hline\n    77 & 2020 & UC-Net  & CVPR  & NLPR(0.7K), NJUD(1.5K) & VGG-16 & Uses a probabilistic RGB-D saliency detection network via a conditional VAE to generate multiple saliency maps  \\\\ \\hline\n    78 & 2020 & CMWNet  & ECCV  & NLPR(0.65K), NJUD(1.4K)  & VGG-16 & Exploits feature interactions using three cross-modal cross-scale weighting modules to improve SOD performance \\\\ \\hline    \n    79 & 2020 & HDFNet  & ECCV  & NLPR(0.7K), NJUD(1.485K), DUT(0.8K)  & VGG-16 & Designs a hierarchical dynamic filtering network to effectively make use of cross-modal fusion information \\\\ \\hline      \n    80 & 2020 & CAS-GNN  & ECCV  & NLPR(0.65K), NJUD(1.4K)   & VGG-16 & Designs cascaded graph neural networks to exploit useful knowledge from RGB and depth images for building powerful feature embeddings \\\\ \\hline      \n    81 & 2020 & CMMS  & ECCV  & NLPR(0.7K), NJUD(1.485K) & VGG-16   & Proposes a cross-modality feature modulation module to enhance feature representations and an adaptive feature selection module to gradually select saliency-related features \\\\ \\hline    \n    82 & 2020 & DANet  & ECCV  & NLPR(0.65K), NJUD(1.4K) & VGG-16, VGG-19   & Develops a single-stream network combined with a depth-enhanced dual attention to achieve real-time SOD \\\\ \\hline    \n    83 & 2020 & CoNet  & ECCV  & NLPR(0.7K), NJUD(1.485K), DUT(0.8K) & ResNet & Develops a collaborative learning framework for RGB-D based SOD. Three collaborators (edge detection, coarse salient object detection and depth estimation) are utilized to jointly boost the performance \\\\ \\hline   \n    84 & 2020 & BBS-Net  & ECCV  & NLPR(0.65K), NJUD(1.4K)  & VGG-16, VGG-19, ResNet-50  & Uses a bifurcated backbone strategy to learn teacher and student features, and utilizes a depth-enhanced module to excavate informative parts of depth cues \\\\ \\hline    \n    85 & 2020 & ATSA  & ECCV  & NLPR(0.7K), NJUD(1.485K), DUT(0.8K)  & VGG-19 & Proposes an asymmetric two-stream architecture taking account of the inherent differences between RGB and depth data for SOD \\\\ \\hline    \n    86 & 2020 & PGAR  & ECCV  & NLPR(0.7K), NJUD(1.485K)   & VGG-16 & Propose a progressively guided alternate refinement network to produce a coarse initial prediction using a multi-scale residual block \\\\ \\hline \n    87 & 2020 & MCINet  & arXiv  & NLPR(0.65K), NJUD(1.4K)  & ResNet-50  & Develops a novel multi-level cross-modal interaction network for RGB-D SOD  \\\\ \\hline \n    88 & 2020 & DRLF  & TIP  & NLPR(0.65K), NJUD(1.4K)   & VGG-16 & Develops a channel-wise fusion network to conduct multi-net and multi-level selective fusion for RGB-D SOD  \\\\ \\hline \n    89 & 2020 & DQAM  & arXiv  & NLPR(0.65K), NJUD(1.4K) & Without  & Proposes a depth quality assessment solution to conduct ``quality-aware\" SOD for RGB-D images  \\\\ \\hline \n    90 & 2020 & DQSD  & TIP  & NLPR(0.65K), NJUD(1.4K)  & VGG-19 &  Integrates a depth quality aware subnet into a bi-stream structure to assess the depth quality before conducting RGB-D fusion \\\\ \\hline \n    91 & 2020 & DASNet  & ACM MM  & NLPR(0.7K), NJUD(1.5K) & ResNet-50  & Proposes a new perspective of containing the depth constraints in the learning process rather than using depths as inputs  \\\\ \\hline \n    92 & 2020 & DCMF  & TIP  & NLPR(0.65K), NJUD(1.4K) & VGG-16, ResNet-50  & Designs a disentangled\ncross-modal fusion network to expose structural and content representations from RGB and depth images \\\\ \\hline \n        \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8146, 8989, 8141, 6218, 6245, 6236, 6244, 6242, 8145, 6241, 6240, 6238, 6243, 6239, 8144, 6220, 6237], "cite_extract_rate": 0.2236842105263158, "origin_cites_number": 76, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes a range of traditional and deep RGB-D SOD models, highlighting their architectural choices and approaches to cross-modal fusion. It includes critical points, such as limitations of handcrafted features and depth quality issues, and identifies trends in techniques like hierarchical fusion and adaptive modules. While it provides meaningful comparisons and patterns, it stops short of offering a fully novel framework or deep meta-level analysis."}}
{"id": "99f631cc-35ef-4d37-abd4-33e889f4b427", "title": "Fusion-wise Models", "level": "subsection", "subsections": [], "parent_id": "b7808ac0-c2fe-488b-b432-aa33a9ddd22d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D based SOD Models"], ["subsection", "Fusion-wise Models"]], "content": "For RGB-D based SOD models, it is important to effectively fuse RGB images and depth maps. The existing fusion strategies can be grouped into three categories, including 1) early fusion, 2) multi-scale fusion, and 3) late fusion. We provide details for each fusion strategy as follows.\n\\textbf{Early Fusion}. Early fusion-based methods can follow one of two veins: 1) RGB images and depth maps are directly integrated to form a four-channel input . This is denoted as ``input fusion\" (shown in Fig.~\\ref{fig_1}); 2) RGB and depth images are first fed into each independent network and their low-level representations are combined as joint representations, which are then fed into a subsequent network for further saliency map prediction . This is denoted as ``early feature fusion\" (shown in Fig.~\\ref{fig_1}).  \n\\textbf{Late Fusion}. Late fusion-based methods can also be further divided into two families: 1) Two parallel network streams are adopted to learn high-level features for RGB and depth data, respectively, which are concatenated and then used for generating the final saliency prediction . This is denoted as ``later feature fusion\" (shown in Fig.~\\ref{fig_1}). 2) Two parallel network streams are used to obtain the independent saliency maps for RGB images and depth cues, and then the two saliency maps are concatenated to obtain a final prediction map . This is denoted as ``late result fusion\" (shown in Fig.~\\ref{fig_1}).\n\\textbf{Multi-scale Fusion}. To effectively explore the correlations between RGB images and depth maps, several methods propose a multi-scale fusion strategy . These models can be divided into two categories. The first category learn the cross-modal interactions and then fuse them into a feature learning network. For example, Chen \\etal~ developed a multi-scale multi-path fusion network to integrate RGB images and depth maps, with a cross-modal interaction (termed MMCI) module. This method introduces cross-modal interactions into multiple layers, which can empower additional gradients for enhancing the learning of the depth stream, as well as enable complementarity across low-level and high-level representations to be explored. The second category fuse the features from RGB images and depth maps in different layers and then integrate them into a decoder network (\\eg, skip connection) to produce the final saliency detection map (as shown in Fig.~\\ref{fig_1}). Some representative works are briefly discussed as follows.\n$\\bullet$ \\textbf{ICNet}  proposes an information conversion module to convert high-level features in an interactive manner. In this model, a cross-modal depth-weighted combination (CDC) block is introduced to enhance RGB features with depth features at different levels.\n$\\bullet$ \\textbf{DPANet}  uses a gated multi-modality attention (GMA) module to exploit long-range dependencies. The GMA module can extract the most discriminative features by utilizing a spatial attention mechanism. Besides, this model controls the fusion rate of the cross-modal information using a gate function, which can reduce some effects brought by the unreliable depth cues.\n$\\bullet$ \\textbf{BiANet}  employs a multi-scale bilateral attention module (MBAM) to capture better global information in multiple layers. \n$\\bullet$ \\textbf{JL-DCF}  treats a depth image as a special case of a color image and employs a shared CNN for both RGB and depth feature extraction. It also proposes a densely-cooperative fusion strategy to effectively combine the learned features from different modalities. \n$\\bullet$ \\textbf{BBS-Net}  uses a bifurcated backbone strategy (BBS) to split the multi-level feature representations into teacher and student features, and develops a depth-enhanced module (DEM) to explore informative parts in depth maps from the spatial and channel views. \n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{Fig4.pdf} \\vspace {-1mm}\n    \\caption{Examples of images, depth maps and annotations in nine RGB-D dataset, including (a) STERE , (b) NLPR  , (c) SSD , (d) GIT , (e) DES  , (f) LFSD , (g) NJUD , (h) DUT-RGBD , and (i) SIP . In each dataset, the RGB image, depth map and annotation are shown from left to right.\n    }\\label{fig_03}\n\\end{figure*}", "cites": [8989, 8141, 8145, 6241, 8144], "cite_extract_rate": 0.2, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section organizes the cited papers into fusion strategies (early, late, multi-scale), which shows some synthesis. However, it primarily describes the methods without in-depth analysis or comparison of their strengths and weaknesses. There is minimal abstraction, as it does not generalize broader principles or provide a meta-level understanding of the fusion approaches."}}
{"id": "c44c1568-3d6f-4d1f-89f7-e8d27f9dbd19", "title": "Single-stream/Multi-stream Models", "level": "subsection", "subsections": [], "parent_id": "b7808ac0-c2fe-488b-b432-aa33a9ddd22d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D based SOD Models"], ["subsection", "Single-stream/Multi-stream Models"]], "content": "\\textbf{Single-stream Models}. Several RGB-D based SOD works  focus on a single-stream architecture to achieve saliency prediction. These models often fuse RGB images and depth information in the input channel or feature learning part. For example, MDSF  employs a multi-scale discriminative saliency fusion framework as the SOD model, in which four types of features in three levels are computed and then fused to obtain the final saliency map. BED  utilizes a CNN architecture to integrate bottom-up and top-down information for SOD, which also incorporates multiple features, including background enclosure distribution (BED) and low level depth maps (\\eg, depth histogram distance and depth contrast) to boost the SOD performance. PDNet  extracts depth-based features using a subsidiary network, which makes full use of depth information to assist the main-stream network.\n\\textbf{Multi-stream Models}. Two-stream models  consist of two independent branches that process RGB images and depth cues, respectively, and often generate different high-level features or saliency maps and then incorporate them in the middle stage or end of the two streams. It is worth noting that most recent deep learning-based models  utilize this two-stream architecture with several models capturing the correlations between RGB images and depth cues across multiple layers. Moreover, some models utilize a multi-stream structure  and then design different fusion modules to effectively fuse RGB and depth information in order to exploit their correlations.", "cites": [8144, 6220], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of single-stream and multi-stream models in RGB-D SOD, mentioning some representative works and their general mechanisms. It includes minimal synthesis by grouping models into these two categories but does not deeply connect or integrate the ideas across papers. There is little critical analysis or identification of broader principles or trends, making it primarily descriptive in nature."}}
{"id": "0f10411b-bb23-4247-8c54-2ec983151c68", "title": "Open-source Implementations", "level": "subsection", "subsections": [], "parent_id": "b7808ac0-c2fe-488b-b432-aa33a9ddd22d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D based SOD Models"], ["subsection", "Open-source Implementations"]], "content": "We summarize the open-source implementations of RGB-D based SOD models reviewed in this survey. The implementations and hyperlinks of the source codes of these models are provided in Tab~\\ref{tab:04}. More source codes will be updated at: \\href{https://github.com/taozh2017/RGBD-SODsurvey}{https://github.com/taozh2017/RGBD-SODsurvey}.\n\\begin{table*}[t!]\n    \\centering\n    \\begin{threeparttable}\n    \\renewcommand\\arraystretch{1.0}\n    \\caption{A summary of RGB-D based SOD models with open-source implementations.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:04}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{p{1.0cm}||p{2.0cm}|p{2.0cm}|p{11.0cm}}\n        \\hline\\hline\n         Year & Model & Implementation & Code link \\\\ \\hline\\hline\n\\multirow{2}{*}{2014}\n & LHM       & Matlab  & \\href{https://sites.google.com/site/rgbdsaliency/code}{https://sites.google.com/site/rgbdsaliency/code} \\bigstrut\\\\\\cline{2-4}\n & DESM      & Matlab  & \\href{https://github.com/HzFu/DES\\_code}{https://github.com/HzFu/DES\\_code} \\\\ \\hline\n2015 & GP     & Matlab  & \\href{https://github.com/JianqiangRen/Global\\_Priors\\_RGBD\\_Saliency\\_Detection}{https://github.com/JianqiangRen/Global\\_Priors\\_RGBD\\_Saliency\\_Detection} \\\\ \\hline\n\\multirow{2}{*}{2016}\n &DCMC    & Matlab  & \\href{https://github.com/rmcong/Code-for-DCMC-method}{https://github.com/rmcong/Code-for-DCMC-method} \\bigstrut\\\\\\cline{2-4}\n &LBE    & Matlab \\& C++  & \\href{http://users.cecs.anu.edu.au/~u4673113/lbe.html}{http://users.cecs.anu.edu.au/~u4673113/lbe.html} \\\\ \\hline\n\\multirow{5}{*}{2017}\n &BED      & Caffe  & \\href{https://github.com/sshige/rgbd-saliency}{https://github.com/sshige/rgbd-saliency} \\bigstrut\\\\\\cline{2-4}\n & CDCP   & Matlab  & \\href{https://github.com/ChunbiaoZhu/ACVR2017}{https://github.com/ChunbiaoZhu/ACVR2017} \\bigstrut\\\\\\cline{2-4}\n& MDSF       & Matlab  & \\href{https://github.com/ivpshu}{https://github.com/ivpshu} \\bigstrut\\\\\\cline{2-4}\n& DF  & Matlab  & \\href{https://pan.baidu.com/s/1Y-PqAjuH9xREBjfl7H45HA}{https://pan.baidu.com/s/1Y-PqAjuH9xREBjfl7H45HA} \\\\ \\hline\n\\multirow{3}{*}{2018}\n& CTMF  & Caffe  & \\href{https://github.com/haochen593/CTMF}{https://github.com/haochen593/CTMF} \\bigstrut\\\\\\cline{2-4}\n& PCF  & Caffe  & \\href{https://github.com/haochen593/PCA-Fuse\\_RGBD\\_CVPR18}{https://github.com/haochen593/PCA-Fuse\\_RGBD\\_CVPR18} \\bigstrut\\\\\\cline{2-4}\n& PDNet    & TensorFlow &  \\href{https://github.com/cai199626/PDNet}{https://github.com/cai199626/PDNet} \\\\ \\hline\n\\multirow{4}{*}{2019}\n& AFNet    & TensorFlow &  \\href{https://github.com/Lucia-Ningning/Adaptive_Fusion\\_RGBD\\_Saliency\\_Detection}{https://github.com/Lucia-Ningning/Adaptive\\_Fusion\\_RGBD\\_Saliency\\_Detection} \\bigstrut\\\\\\cline{2-4}\n& CPFP  & Caffe & \\href{https://github.com/JXingZhao/ContrastPrior}{https://github.com/JXingZhao/ContrastPrior} \\bigstrut\\\\\\cline{2-4}\n &DMRA   & PyTorch & \\href{https://github.com/jiwei0921/DMRA}{https://github.com/jiwei0921/DMRA} \\bigstrut\\\\\\cline{2-4}\n& DGT   & Matlab & \\href{https://github.com/rmcong/Code-for-DTM-Method}{https://github.com/rmcong/Code-for-DTM-Method} \\\\ \\hline\n\\multirow{24}{*}{2020}\n&ICNet  & Caffe & \\href{https://github.com/MathLee/ICNet-for-RGBD-SOD}{https://github.com/MathLee/ICNet-for-RGBD-SOD} \\bigstrut\\\\\\cline{2-4}\n &JL-DCF  & Pytorch, Caffe & \\href{https://github.com/kerenfu/JLDCF}{https://github.com/kerenfu/JLDCF} \\bigstrut\\\\\\cline{2-4}\n &A2dele  & PyTorch & \\href{https://github.com/OIPLab-DUT/CVPR2020-A2dele}{https://github.com/OIPLab-DUT/CVPR2020-A2dele} \\bigstrut\\\\\\cline{2-4}\n& SSF  & PyTorch & \\href{https://github.com/OIPLab-DUT/CVPR\\_SSF-RGBD}{https://github.com/OIPLab-DUT/CVPR\\_SSF-RGBD} \\bigstrut\\\\\\cline{2-4}\n& ASIF-Net  & TensorFlow & \\href{https://github.com/Li-Chongyi/ASIF-Net}{https://github.com/Li-Chongyi/ASIF-Net} \\bigstrut\\\\\\cline{2-4}\n& S${^2}$MA  & PyTorch & \\href{https://github.com/nnizhang/S2MA}{https://github.com/nnizhang/S2MA} \\bigstrut\\\\\\cline{2-4}\n&UC-Net    & PyTorch &  \\href{https://github.com/JingZhang617/UCNet}{https://github.com/JingZhang617/UCNet} \\bigstrut\\\\\\cline{2-4}\n & D$^{3}${Net}  & PyTorch & \\href{https://github.com/DengPingFan/D3NetBenchmark}{https://github.com/DengPingFan/D3NetBenchmark} \\bigstrut\\\\\\cline{2-4}\n  & CMWNet  & Caffe & \\href{https://github.com/MathLee/CMWNet}{https://github.com/MathLee/CMWNet} \\bigstrut\\\\\\cline{2-4}\n  & HDFNet  & PyTorch & \\href{https://github.com/lartpang/HDFNet}{https://github.com/lartpang/HDFNet} \\bigstrut\\\\\\cline{2-4}\n  & CMMS  & TensorFlow & \\href{https://github.com/Li-Chongyi/cmMS-ECCV20}{https://github.com/Li-Chongyi/cmMS-ECCV20} \\bigstrut\\\\\\cline{2-4}\n  & CAS-GNN  & PyTorch & \\href{https://github.com/LA30/Cas-Gnn}{https://github.com/LA30/Cas-Gnn} \\bigstrut\\\\\\cline{2-4}\n  & DANet  & PyTorch & \\href{https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency}{https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency} \\bigstrut\\\\\\cline{2-4}\n  & CoNet  & PyTorch & \\href{https://github.com/jiwei0921/CoNet}{https://github.com/jiwei0921/CoNet} \\bigstrut\\\\\\cline{2-4}   \n  & DASNet  & PyTorch & \\href{http://cvteam.net/projects/2020/DASNet/}{http://cvteam.net/projects/2020/DASNet/} \\bigstrut\\\\\\cline{2-4}   \n  & BBS-Net  & PyTorch & \\href{https://github.com/DengPingFan/BBS-Net}{https://github.com/DengPingFan/BBS-Net}\\bigstrut\\\\\\cline{2-4}   \n  & ATSA  & PyTorch & \\href{https://github.com/sxfduter/ATSA}{https://github.com/sxfduter/ATSA} \\bigstrut\\\\\\cline{2-4}   \n  & PGAR  & PyTorch & \\href{https://github.com/ShuhanChen/PGAR\\_ECCV20}{https://github.com/ShuhanChen/PGAR\\_ECCV20} \\bigstrut\\\\\\cline{2-4}   \n  & FRDT  & PyTorch & \\href{https://github.com/jack-admiral/ACM-MM-FRDT}{https://github.com/jack-admiral/ACM-MM-FRDT} \\\\ \\hline\n  \\hline\\hline\n\\end{tabular}\n\\end{threeparttable}\n\\end{table*}    \n\\renewcommand\\arraystretch{1.1}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Statistics of nine RGB-D benchmark datasets in terms of year (Year), publication (Pub.), dataset size (Size), number of objects in the images (\\#Obj.), type of scene (Types), depth sensor (Sensor), and resolution (Resolution). See \\secref{sec:dataset} for more details on each dataset. These datasets can be downloaded from our website: \\href{ http://dpfan.net/d3netbenchmark/}{http://dpfan.net/d3netbenchmark/}.\n    }\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:03}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{|p{0.2cm}||p{2.3cm}|p{0.5cm}|p{0.95cm}|p{0.5cm}|p{1.0cm}|p{2.5cm}|p{3.4cm}|p{3.0cm}|}\n        \\hline\n         \\# & Dataset & Year & Pub. & Size & \\#Obj. & Types& Sensor & Resolution \\\\ \\hline\\hline\n         1 & \\textbf{STERE}  &2012 & CVPR & 1,000 & $\\sim${One} &{Internet}         & Stereo camera+sift flow & \n         $[251\\sim 1200]\\times[222\\sim 900]$ \\\\ \\hline\n         2 & \\textbf{GIT}    &2013 & BMVC   & 80 &Multiple &Home environment & Microsoft Kinect        & $640\\times 480$ \\\\ \\hline\n         3 & \\textbf{DES}    &2014 & ICIMCS & 135  &One      &Indoor           & Microsoft Kinect        & $640\\times 480$ \\\\ \\hline\n         4 & \\textbf{NLPR}     &2014 & ECCV   & 1,000 &Multiple &Indoor/outdoor   & Microsoft Kinect        & $640\\times 480$, $480\\times 640$ \\\\ \\hline\n         5 & \\textbf{LFSD}   &2014 & CVPR   & 100  &One      &Indoor/outdoor   & Lytro Illum camera      & $360\\times 360$ \\\\ \\hline\n         6 & \\textbf{NJUD}   &2014 & ICIP   & 1,985 &$\\sim${One} &Movie/internet/photo & FujiW3 camera+optical flow & \n         $[231\\sim 1213]\\times[274\\sim 828]$  \\\\ \\hline\n         7 & \\textbf{SSD}   &2017 & ICCVW  & 80            &Multiple &Movies & Sunâ€™s optical flow        & $960\\times 1080$ \\\\ \\hline\n         8 & \\textbf{DUT-RGBD}   &2019 & ICCV   & 1,200 &Multiple &Indoor/outdoor & --        & $400\\times 600$ \\\\ \\hline\n         9 & \\textbf{SIP}    &2020 & TNNLS   & 929   &Multiple &Person in the wild & Huawei Mate10        & $992\\times 744$ \\\\ \\hline\n         \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8141, 6218, 6245, 6236, 6244, 8145, 6241, 6243, 6238, 8144, 6237], "cite_extract_rate": 0.2682926829268293, "origin_cites_number": 41, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section merely lists open-source implementations of RGB-D SOD models with years, frameworks, and code links. It provides no synthesis of the underlying techniques, no comparative analysis of their performance or design choices, and no abstraction or generalization of trends in implementation practices. The cited papers are referenced only in the context of their availability as code, not for their conceptual or methodological contributions."}}
{"id": "1678f0ad-2340-45c9-845a-7ee8ea54d978", "title": "RGB-D Datasets", "level": "section", "subsections": [], "parent_id": "cbdb372b-cea7-411a-9f50-8d818e9f2d0d", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "RGB-D Datasets"]], "content": "\\label{sec:Results}\n\\label{sec:dataset}\nWith the rapid development of RGB-D based SOD, various datasets have been constructed over the past several years. Tab~\\ref{tab:03} summarizes nine popular RGB-D datasets, and Fig.~\\ref{fig_03} shows examples of images (including RGB images, depth maps, and annotations) from these datasets. Moreover, we provide the details for each dataset as follows.\n$\\bullet$ \\textbf{STERE} . The authors first collected 1,250 stereoscopic images from Flickr \\footnote{http://www.flickr.com/}, NVIDIA 3D Vision Live \\footnote{http://photos.3dvisionlive.com/}, and Stereoscopic Image Gallery \\footnote{http://www.stereophotography.com/}. The most salient objects in each image were annotated by three users. All annotated images were then sorted based on the overlaping salient regions and the top 1,000 images were selected to construct the final dataset. This is the first collection of stereoscopic images in this field. \n$\\bullet$ \\textbf{GIT}  consists of 80 color and depth images, which were collected using a mobile-manipulator robot in a real-world home environment. Moreover, each image is annotated based on the pixel-level segmentation of the objects.\n$\\bullet$ \\textbf{DES}  consists of 135 indoor RGB-D images, which were taken by Kinect with a resolution of $640\\times{640}$. When collecting this dataset, three users were asked to label the salient object in each image, and then the overlapping areas of the labeled object were regarded as the ground truth.\n$\\bullet$ \\textbf{NLPR}  consists of 1,000 RGB images and their corresponding depth maps, which were obtained by a standard Microsoft Kinect. This dataset includes a series of outdoor and indoor locations, \\eg, offices, supermarkets, campuses, streets, and so on. \n$\\bullet$ \\textbf{LFSD}  includes 100 light fields collected using a Lytro light field camera, and consists of 60 indoor and 40 outdoor scenes. To label this dataset, three individuals were asked to manually segment salient regions, and then the segmented results were deemed ground truth when the overlap of the three results was over $90\\%$. \n$\\bullet$ \\textbf{NJUD}  consists of 1,985 stereo image pairs, and these images were collected from the internet, 3D movies, and photographs that are taken by a Fuji W3 stereo camera. \n$\\bullet$ \\textbf{SSD}  was constructed using three stereo movies and includes indoor and outdoor scenes. This dataset includes 80 samples, and each image has the size of $960\\times{1080}$.\n$\\bullet$ \\textbf{DUT-RGBD}  consists of 800 indoor and 400 outdoor scenes with their corresponding depth images. This dataset includes several challenging factors, \\ie, multiple or transparent objects, complex backgrounds, similar foregrounds and backgrounds, and low-intensity environments.\n$\\bullet$ \\textbf{SIP}  consists of 929 annotated high-resolution images, with multiple salient persons in each image. In this dataset, depth maps were captured using a real smartphone (\\ie, Huawei Mate10). Besides, it is worth noting that this dataset covers diverse scenes, and various challenging factors, and is annotated with pixel-level ground truths.\nNote that a detailed dataset statistics analysis (including center bias, size of objects, background objects, object boundary conditions, and number of salient objects) can be found in . \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Summary of popular light field SOD methods.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:031}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{|p{0.3cm}|p{0.5cm}<{\\centering}|r|p{0.8cm}<{\\centering}|p{3.1cm}|p{9.0cm}|}\n        \\hline\n         No. & Year & Method & Pub.  & Dataset & Description \\\\ \\hline\\hline\n    1 & 2014 & LFS  & CVPR  & LFSD  & The first light-field saliency detection algorithm employs objectness and focusness cues based on the refocusing capability of the light field \\\\ \\hline  \n    2 & 2015 & WSC  & CVPR  & LFSD &  Uses a weighted sparse coding framework to learn a saliency/non-saliency dictionary \\\\ \\hline  \n    3 & 2015 & DILF  & IJCAI  & LFSD & Incorporates depth contrast to complement the disadvantage of color and conducts focusness-based background priors to boost the saliency detection performance \\\\ \\hline  \n    4 & 2016 & RL  & ICASSP  & LFSD & Utilizes the inherent structure information in light field images to improve saliency detection \\\\ \\hline  \n    5 & 2017 & MA  & TOMM  & HFUT, LFSD &  Integrates multiple saliency cues extracted from light field images using a random-search-based weighting strategy \\\\ \\hline  \n    6 & 2017 & BIF  & NPL  & LFSD & Integrates color-based contrast, depth-induced contrast, focusness map of foreground slice, and background weighted depth contrast using a two-stage Bayesian integration framework \\\\ \\hline \n    7 & 2017 & LFS  & TPAMI  & LFSD & An extension of  \\\\ \\hline    \n    8 & 2017 & RLM  & ICIVC  & LFSD & Utilizes the light field relative location measurement for SOD on light field images \\\\ \\hline    \n    9 & 2018 & SGDC  & CVPR  & LFSD & Designs a saliency-guided depth optimization framework for multi-layer light field displays \\\\ \\hline    \n    10 & 2018 & DCA  & FiO  & LFSD & Proposes a graph model depth-induced cellular automata to optimize saliency maps using light field data \\\\ \\hline  \n    11 & 2019 & DLLF  & ICCV  & DUTLF-FS, LFSD & Utilizes a recurrent attention network to fuse each slice from the focal stack to learn the most informative features  \\\\ \\hline  \n    12 & 2019 & DLSD  & IJCAI  & DUTLF-MV & Formulates saliency detection into two subproblems, including 1) light field synthesis from a single view and 2) light-field-driven saliency detection \\\\ \\hline  \n    13 & 2019 & Molf  & NIPS  & UTLF-FS & Uses a memory-oriented decoder for light field SOD \\\\ \\hline \n    14 & 2020 & ERNet  & AAAI  & DUTLF-FS, HFUT, LFSD & Uses an asymmetrical two-stream architecture to overcome computation-intensive and memory-intensive challenges in a high-dimensional light field data \\\\ \\hline \n    15 & 2020 & DCA  & TIP  & LFSD & Presents a saliency detection framework on light fields based on the depth-induced cellular automata (DCA) model. It can enforce spatial consistency to optimize the inaccurate saliency map using the DCA model \\\\ \\hline \n    16 & 2020 & RDFD  & MTAP  & LFSD & Defines a region-based depth feature descriptor extracted from the light field focal stack to facilitate low- and high-level cues for saliency detection \\\\ \\hline \n    17 & 2020 & LFNet  & TIP  & DUTLF-FS, LFSD, HFUT & Utilizes a light field refinement module and a light field integration module to effectively integrate multiple cues (\\ie, focusness, depths and objectness) from light field images \\\\ \\hline \n    18 & 2020 & LFDCN  & TIP  & Lytro Illum, LFSD, HFUT & Uses a deep convolutional network based on the modified DeepLab-v2 model to explore spatial and multi-view properties of light field images for saliency detection \\\\ \\hline \n        \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [6246], "cite_extract_rate": 0.04, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of various RGB-D datasets and their characteristics without synthesizing insights or connecting them to broader themes in the field. It lacks critical evaluation or comparison of the datasets in terms of their strengths, weaknesses, or suitability for specific SOD tasks. The mention of challenging factors is minimal and not abstracted into general principles or patterns."}}
{"id": "98a73f57-0734-4cd6-bb67-b44dfb6a81f1", "title": "Light Field SOD Models", "level": "subsection", "subsections": [], "parent_id": "8e9e0d25-e7ab-4db4-bd54-70e2af83284f", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Saliency Detection on Light Field"], ["subsection", "Light Field SOD Models"]], "content": "\\label{sec:lf_models}\nExisting works for SOD can be grouped into three categories according to the input data type, including RGB SOD, RGB-D SOD, and light field SOD~. We have already reviewed RGB-D based SOD models, in which depth maps provide layout information to improve SOD performance to some extent. However, inaccurate or low-quality depth maps often decrease the performance. To overcome this issue, light field SOD methods have been proposed to make use of rich information captured by the light field. Specifically, light field data contains an all-focus image, a focal stack, and a rough depth map . A summary of related light field SOD works is provided in Tab~\\ref{tab:031}. Further, to provide an in-depth understanding of these models, we also review them in more detail as follows.\n\\textbf{Traditional/Deep Models}. The classic models for light field SOD often use superpixel-level handcrafted features . Early work  showed that the unique refocusing capability of light fields can provide useful focusness, depth, and objectness cues. Thus, several SOD models using light field data were further proposed. For example, Zhang \\etal~ utilized a set of focal slices to compute the background prior, and then combined it with the location prior for SOD. Wang \\etal~ proposed a two-stage Bayesian fusion model to integrate multiple contrasts for boosting SOD performance. Recently, several deep learning-based light field SOD models  have also been developed, obtaining remarkable performance. Besides, in , an attentive recurrent CNN was developed to fuse all focal slices, while the data diversity was increased using adversarial examples to enhance model robustness. Zhang \\etal~ developed a memory-oriented decoder for light field SOD, which fuses multi-level features in a top-down manner using high-level information to guide low-level feature selection. LFNet  employs a new integration module to fuse features from light field data according to their contributions and captures the spatial structure of a scene to improve SOD performance. \n\\textbf{Refinement based Models}. Several refinement strategies have been used to enforce neighboring constraints or reduce the homogeneity of multiple modalities for SOD. For example, in , the saliency dictionary was refined using the estimated saliency map. The MA method  employs a two-stage saliency refinement strategy to produce the final prediction map, which enables adjacent superpixels to obtain similar saliency values. Besides, LFNet  presents an effective refinement module to reduce the homogeneity among different modalities as well refine their dissimilarities", "cites": [6246], "cite_extract_rate": 0.0625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of light field SOD models by categorizing them into traditional/deep and refinement-based approaches. It synthesizes key methods and connects ideas across works, such as the use of focal slices and refinement strategies. However, it lacks deeper critical evaluation of the cited papers and could better articulate overarching principles or frameworks beyond method classification."}}
{"id": "4c35991d-191e-4e55-b87e-94b7d3fceaff", "title": "Light Field Data for SOD", "level": "subsection", "subsections": [], "parent_id": "8e9e0d25-e7ab-4db4-bd54-70e2af83284f", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Saliency Detection on Light Field"], ["subsection", "Light Field Data for SOD"]], "content": "\\label{sec:lf_daataset}\nThere are five representative datasets widely used in existing light field SOD models. We describe the details of each dataset as follows.\n$\\bullet$ \\textbf{LFSD}  \\footnote{{https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/}} consists of 100 light fields of different scenes with a $360\\times{360}$ spatial resolution, captured using a Lytro light field camera. This dataset contains 60 indoor and 40 outdoor scenes, and most scenes consist of only one salient object. Besides, three individuals were asked to manually segment salient regions in each image, and then the ground truth was determined when all three segmentation results had an overlap of over 90\\%. \n$\\bullet$ \\textbf{HFUT}  \\footnote{https://github.com/pencilzhang/HFUT-Lytro-dataset} consists of 255 light fields captured using a Lytro camera. In this dataset, most scenes contain multiple objects that appear within different locations and scales under complex background clutter. \n$\\bullet$ \\textbf{DUTLF-FS}  \\footnote{https://github.com/OIPLab-DUT/ICCV2019\\_Deeplightfield\\_Saliency} consists of 1,465 samples, 1,000 of which are used as the training set, while the remaining 465 images make up the test set. The resolution of each image is $600\\times{400}$. This dataset contains several challenges, including lower contrast between salient objects and cluttered background, multiple disconnected salient objects, and dark or strong light conditions.\n$\\bullet$ \\textbf{DUTLF-MV}  \\footnote{https://github.com/OIPLab-DUT/IJCAI2019-Deep-Light-Field-Driven-Saliency-Detection-from-A-Single-View} consists of 1,580 samples, 1,100 of which are for training and the remaining is for testing. Images were captured by a Lytro Illum camera, and each light field consists of multi-view images and a corresponding ground truth. \n$\\bullet$ \\textbf{Lytro Illum}  \\footnote{https://github.com/pencilzhang/MAC-light-field-saliency-net} consists of 640 light fields and the corresponding per-pixel ground-truth saliency maps. It includes several challenging factors, \\eg, inconsistent illumination conditions, and small salient objects existing in a similar or cluttered background. \n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{Fig5.pdf} \\vspace {-1mm}\n    \\caption{A comprehensive evaluation for 24 representative RGB-D based SOD models, including LHM , ACSD , DESM , GP , LBE , DCMC , SE , CDCP , CDB , DF , PCF , CTMF , CPFP , TANet , AFNet , MMCI , DMRA , D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net . We report the mean values of $S_{\\alpha}$ and {MAE} across the five datasets (\\ie, STERE , NLPR , LFSD , DES , and SIP ) in each model. Note that better models are shown in the upper left corner (\\ie, with a larger $S_{\\alpha}$ and smaller MAE). Here, red diamonds denote deep models and green circles denote traditional models.}  \\label{fig_04}\n\\end{figure*}", "cites": [8141, 6218, 6246, 8144], "cite_extract_rate": 0.13333333333333333, "origin_cites_number": 30, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily provides a factual summary of five light field datasets used in SOD, without critically analyzing their strengths or weaknesses. It makes minimal connections between the cited papers and the datasets, and fails to generalize or identify broader patterns or principles in the field."}}
{"id": "77096881-5df1-48ba-a9fe-e30abc33240c", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "484ad8ca-308e-4a44-872a-fc85979e9069", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Model Evaluation and Analysis"], ["subsection", "Evaluation Metrics"]], "content": "\\label{sec:metrics}\nWe briefly review several popular metrics for SOD evaluation, \\ie, precision-recall (PR), F-measure , mean absolute error (MAE) , structural measure (S-measure) , and enhanced-alignment measure (E-measure) . \n$\\bullet$ {\\textbf{PR}}. Given a saliency map $S$, we can convert it to a binary mask $M$, and then compute the \\emph{precision}\nand \\emph{recall} by comparing $M$ with ground-truth $G$:\n\\begin{equation}\nPrecision=\\frac{|M\\cap{G}|}{|M|},~ Recall=\\frac{|M\\cap{G}|}{|G|}.\n\\end{equation}\nA popular strategy is to partition the saliency map $S$ using a set of thresholds (\\ie, it changes from 0 to 255). For each threshold, we first calculate a pair of recall and precision scores, and then combine them to obtain a PR curve that describes the performance of the model at the different thresholds.\n$\\bullet$ {\\textbf{F-measure} ($F_{\\beta}$)}. To comprehensively consider both precision and recall, the F-measure is proposed by calculating the weighted harmonic mean:\n\\begin{equation}\nF_{\\beta}=\\left(1+\\beta ^2\\right)\\frac{P*R}{\\beta ^{2}P+R},\n\\end{equation}\nwhere $\\beta^2$ is set to 0.3 to emphasize the precision . We use different fixed $[0,255]$ thresholds to  compute the $F$-measure metric. This yields a set of $F$-measure values for which we report the maximal or average $F_{\\beta}$.\n$\\bullet$ {\\textbf{MAE}}. This measures the average pixel-wise absolute error between a predicted saliency map $S$ and a ground truth $G$ for all pixels, which can be defined by\n\\begin{equation}\nMAE=\\frac{1}{W*H}\\sum_{i=1}^{W}\\sum_{i=1}^{H}\\left|S_{i,j}-G_{i,j} \\right|,\n\\end{equation}\nwhere $W$ and $H$ denote the width and height of the map, respectively. MAE values are normalized to [0,1].\n$\\bullet$ {\\textbf{S-measure} ($S_{\\alpha}$)}. To capture the importance of the structural information in an image, $S_{\\alpha}$  is used to assess the structural similarity between the regional perception ($S_r$) and object perception ($S_o$). Thus, $S_{\\alpha}$ can be defined by\n\\begin{equation}\nS_{\\alpha}=\\alpha * S_{o}+\\left(1-\\alpha\\right)*S_{r},  \n\\end{equation}\nwhere $ \\alpha \\in \\left[ 0,1\\right]$ is a trade-off parameter. Here, we set $\\alpha$ = 0.5 as the default setting, as suggested by Fan \\etal~. \n$\\bullet$ {\\textbf{E-measure} ($E_{\\phi}$)}. $E_{\\phi}$  was proposed based on cognitive vision studies to capture image-level statistics and their local pixel matching information. Thus, $E_{\\phi}$ can be defined by\n\\begin{equation}\nE_{\\phi}=\\frac{1}{W*H}\\sum_{i=1}^{W}\\sum_{i=1}^{H}\\phi_{FM}\\left(i,j\\right), \n\\end{equation}\nwhere $ \\phi_{FM} $ denotes the enhanced-alignment matrix .\n\\begin{figure*}[t]\n\\centering\n\\begin{overpic}[width=0.99\\linewidth]{Fig6.pdf}\n\\end{overpic}\\vspace{-0.25cm}\n\\caption{PR curves for 24 RGB-D based models on the STERE , NLPR , LFSD , DES , SIP , GIT , SSD , and NJUD  datasets.} \n    \\label{fig_05}\n\\end{figure*}\n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{Fig7.pdf} \\vspace {-1mm}\n    \\caption{F-measures under different thresholds for 24 RGB-D based models on the STERE , NLPR , LFSD , DES , SIP , GIT , SSD , and NJUD  datasets.}  \\label{fig_06}\n\\end{figure*}", "cites": [6235, 6247, 6248], "cite_extract_rate": 0.23076923076923078, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of commonly used evaluation metrics in SOD, including PR, F-measure, MAE, S-measure, and E-measure, along with their mathematical formulations. However, it lacks deeper synthesis of the cited papers, as it does not explicitly connect the development of these metrics or discuss their evolution in context. There is little critical analysis or abstraction beyond the specific definitions, with only brief mentions of default parameter settings."}}
{"id": "177142bd-d83e-4c1f-ac33-db77c968bd52", "title": "Performance Comparison and Analysis", "level": "subsection", "subsections": ["98fc5dce-0fcc-43ee-9931-dcbb61b8ad7c", "9a99b633-f6fa-43bc-8a6d-41be0427ad05"], "parent_id": "484ad8ca-308e-4a44-872a-fc85979e9069", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Model Evaluation and Analysis"], ["subsection", "Performance Comparison and Analysis"]], "content": "\\label{sec:comparison}\n\\renewcommand\\arraystretch{1.1}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Attribute-based study \\emph{w.r.t.} salient object scales. Comparison results for 24 representative RGB-D based SOD models (9 traditional models and 15 deep learning-based models) are provided in terms of MAE and $S_{\\alpha}$. The three best results are shown in \\rev{red}, \\blu{blue} and \\gre{green} fonts.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:006}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{p{0.12cm}<{\\centering}|p{0.75cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}}\n        \\hline\n     \\multirow{2}{*}{}    \n     & & \\multicolumn{9}{c|}{\\textbf{Traditional models} } & \\multicolumn{15}{c}{\\textbf{Deep learning-based models}}\\\\ \\hline\n     &\\rotatebox{90}{Scale} \n     &\\rotate{LHM }   &\\rotate{ACSD }   &\\rotate{DESM }       &\\rotate{GP }  \n     &\\rotate{LBE }  &\\rotate{DCMC } &\\rotate{SE }      &\\rotate{CDCP } \n     &\\rotate{CDB } &\\rotate{DF }      &\\rotate{PCF }  \n     &\\rotate{CTMF }   &\\rotate{CPFP }&\\rotate{TANet }     &\\rotate{AFNet } \n     &\\rotate{MMCI } &\\rotate{DMRA }   &\\rotate{D$^3$Net }  &\\rotate{SSF }\n     &\\rotate{A2dele } &\\rotate{S$^2$MA }       &\\rotate{ICNet } &\\rotate{JL-DCF } &\\rotate{UC-Net }  \n    \\\\ \\hline\\hline\n    \\multirow{4}{*}{\\rotate{MAE}}\n    & Small \n    &.065 &.149 &.319 &.098 &.177 &.108 &.056 &.128 &.073 &.087 &.042 &.065 &.044 &.041 &.046 &.051 &\\rev{.030} &.033 &\\blu{.031} &\\gre{.032} &.035 &.036 &\\gre{.032} &.034 \\\\\n    & Medium \n    &.178 &.183 &.287 &.180 &.210 &.158 &.150 &.173 &.179 &.152 &.068 &.107 &.055 &.067 &.095 &.079 &.069 &.053 &\\gre{.045} &.054 &.052 &.052 &\\rev{.041} &\\blu{.042} \\\\\n    & Large\n    &.403 &.311 &.310 &.377 &.261 &.305 &.364 &.308 &.385 &.310 &.112 &.183 &.093 &.118 &.213 &.130 &.181 &.102 &.105 &.114 &\\gre{.088} &.104 &\\blu{.085} &\\rev{.072} \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.166 &.184 &.296 &.173 &.206 &.156 &.142 &.171 &.167 &.147 &.065 &.102 &.055 &.065 &.091 &.076 &.067 &.052 &\\gre{.046} &.053 &.051 &.052 &\\rev{.041} &\\blu{.042} \\\\ \\hline \\hline\n    \\multirow{4}{*}{\\rotate{$S_{\\alpha}$}}\n    & Small \n    &.624 &.668 &.517 &.650 &.645 &.700 &.775 &.661 &.666 &.745 &.847 &.789 &.840 &.846 &.792 &.832 &.860 &.879 &.876 &.859 &.877 &\\blu{.882} &\\gre{.881} &\\rev{.883} \\\\\n    & Medium \n    &.543 &.732 &.658 &.598 &.723 &.727 &.676 &.683 &.585 &.730 &.863 &.805 &.877 &.862 &.779 &.859 &.838 &.888 &\\gre{.893} &.865 &\\gre{.893} &.892 &\\rev{.906} &\\blu{.901} \\\\\n    & Large\n    &.386 &.630 &.686 &.450 &.731 &.604 &.479 &.586 &.424 &.597 &.838 &.761 &.855 &.827 &.682 &.830 &.734 &.846 &.837 &.815 &\\blu{.863} &.845 &\\gre{.859} &\\rev{.876} \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.552 &.710 &.626 &.601 &.705 &.712 &.686 &.671 &.593 &.725 &.857 &.798 &.867 &.856 &.776 &.851 &.836 &.883 &.885 &.860 &\\gre{.887} &.886 &\\rev{.897} &\\blu{.895} \\\\ \\hline \\hline\n    \\end{tabular}\n\\end{table*}  \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Attribute-based study \\emph{w.r.t.} background clutter. Comparison results for 24 representative RGB-D based SOD models (9 traditional models and 15 deep learning-based models) are provided in terms of MAE and $S_{\\alpha}$. The three best results are shown in \\rev{red}, \\blu{blue} and \\gre{green} fonts.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:007}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{p{0.12cm}<{\\centering}|p{0.98cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}}\n        \\hline\n     \\multirow{2}{*}{}    \n     & & \\multicolumn{9}{c|}{\\textbf{Traditional models} } & \\multicolumn{15}{c}{\\textbf{Deep learning-based models}}\\\\ \\hline\n     &\\rotatebox{90}{background} \n     &\\rotate{LHM }   &\\rotate{ACSD }   &\\rotate{DESM }       &\\rotate{GP }  \n     &\\rotate{LBE }  &\\rotate{DCMC } &\\rotate{SE }      &\\rotate{CDCP } \n     &\\rotate{CDB } &\\rotate{DF }      &\\rotate{PCF }  \n     &\\rotate{CTMF }   &\\rotate{CPFP }&\\rotate{TANet }     &\\rotate{AFNet } \n     &\\rotate{MMCI } &\\rotate{DMRA }   &\\rotate{D$^3$Net }  &\\rotate{SSF }\n     &\\rotate{A2dele } &\\rotate{S$^2$MA }       &\\rotate{ICNet } &\\rotate{JL-DCF } &\\rotate{UC-Net }  \n    \\\\ \\hline\\hline\n    \\multirow{4}{*}{\\rotate{MAE}}\n    & Simple \n    &.100 &.163 &.219 &.150 &.202 &.056 &.084 &.028 &.136 &.045 &.031 &.053 &.018 &.033 &.031 &.041 &.028 &.017 &\\blu{.012} &\\rev{.010} &.016 &\\gre{.013} &.014 &\\gre{.013}\\\\\n    & Uncertain\n    &.164 &.195 &.294 &.175 &.210 &.140 &.133 &.139 &.159 &.129 &.062 &.081 &.050 &.059 &.075 &.070 &.058 &.045 &\\gre{.043} &\\gre{.043} &.049 &\\blu{.041} &\\rev{.037} &\\rev{.037}\\\\\n    & Complex\n    &.159 &.190 &.349 &.180 &.205 &.190 &.147 &.236 &.143 &.163 &.085 &.110 &.079 &.077 &.108 &.094 &.087 &.071 &\\blu{.065} &\\gre{.070} &.072 &.079 &\\rev{.063} &\\blu{.065} \n    \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.160 &.193 &.295 &.174 &.209 &.140 &.132 &.141 &.157 &.127 &.063 &.082 &.051 &.059 &.076 &.070 &.059 &\\gre{.046} &\\blu{.043} &\\blu{.043} &.049 &\\blu{.043} &\\rev{.038} &\\rev{.038} \\\\\n    \\hline \\hline\n    \\multirow{4}{*}{\\rotate{$S_{\\alpha}$}}\n    & Simple \n    &.781 &.787 &.761 &.694 &.748 &.930 &.856 &.941 &.704 &.944 &.944 &.913 &.958 &.937 &.922 &.933 &.935 &.960 &\\blu{.966} &\\gre{.965} &\\gre{.965} &\\rev{.969} &.961 &.962 \\\\\n    & Uncertain\n    &.572 &.694 &.638 &.606 &.695 &.736 &.723 &.727 &.610 &.774 &.873 &.853 &.882 &.873 &.818 &.868 &.854 &.900 &.894 &.884 &.895 &\\rev{.910} &\\blu{.909} &\\gre{.907}\\\\\n    & Complex\n    &.496 &.627 &.509 &.545 &.616 &.577 &.605 &.487 &.575 &.627 &.782 &.742 &.787 &.790 &.694 &.768 &.751 &\\gre{.822} &.815 &.786 &.813 &.808 &\\blu{.829} &\\rev{.833}  \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.576 &.693 &.633 &.606 &.691 &.732 &.720 &.718 &.612 &.770 &.869 &.847 &.878 &.869 &.813 &.863 &.850 &\\blu{.896} &.891 &.879 &\\gre{.892} &\\rev{.904} &\\rev{.904} &\\rev{.904} \\\\ \\hline \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8141, 6218, 8144], "cite_extract_rate": 0.125, "origin_cites_number": 24, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a detailed comparative performance evaluation of 24 RGB-D SOD models across attributes like object scale and background clutter. However, it lacks synthesis of ideas across the cited papers, critical evaluation of their design or limitations, and abstraction to broader principles or trends in the field. The content is primarily data-driven with limited interpretative analysis."}}
{"id": "98fc5dce-0fcc-43ee-9931-dcbb61b8ad7c", "title": "Overall Evaluation", "level": "subsubsection", "subsections": [], "parent_id": "177142bd-d83e-4c1f-ac33-db77c968bd52", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Model Evaluation and Analysis"], ["subsection", "Performance Comparison and Analysis"], ["subsubsection", "Overall Evaluation"]], "content": "To quantify the performance of different models, we conduct a comprehensive evaluation of 24 representative RGB-D based SOD models, including 1) nine traditional methods: LHM , ACSD , DESM , GP , LBE , DCMC , SE , CDCP , CDB ; and 2) fifteen deep learning-based methods: DF , PCF , CTMF , CPFP , TANet , AFNet , MMCI , DMRA , D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net . We report the mean values of $S_{\\alpha}$ and {MAE} across the five datasets (STERE , NLPR  , LFSD , DES , and SIP ) for each model in Fig.~\\ref{fig_04}. It is worth noting that better models are shown in the upper left corner (\\ie, with a larger $S_{\\alpha}$ and smaller MAE). From Fig.~\\ref{fig_04}, we have following observations:\n\\begin{itemize}\n\\item \\textbf{Traditional vs. Deep Models}. Compared with traditional RGB-D based SOD models, deep learning methods obtain significantly better performance. This confirms the powerful feature learning ability of deep networks. \n\\item \\textbf{Comparison of Deep Models}. Among the deep learning-based models, D{$^3$}Net , JL-DCF , UC-Net , SSF , {ICNet} , and S${^2}$MA  obtain the best performance. \n\\end{itemize}\nMoreover, Fig.~\\ref{fig_05} and Fig.~\\ref{fig_06} show the PR and F-measure curves for the 24 representative RGB-D based SOD models on eight datasets (\\ie, STERE , NLPR , LFSD , DES , SIP , GIT , SSD  , and NJUD ). Note that there are 1000, 300, 100, 135, 929, 80, and 80 test samples for NLPR, LFSD, DES, SIP, GIT, and SSD, respectively. For the NJUD  dataset, there are 485 test images for CPFP , S$^2$MA , ICNet , JL-DCF , and UC-Net , while 498 testing images for all other models. \nTo understand the top six models in depth, we discuss their main advantages for the six models below.\n$\\bullet$ D{$^3$}Net  consists of two key components, \\ie, a three-stream feature learning module and a depth depurator unit. In the three-stream feature learning module, there are three subnetworks, \\ie, RgbNet, RgbdNet, and DepthNet. The RgbNet and DepthNet are used to learn high-level feature representations for RGB and depth images, respectively, while the RgbdNet is used to learn their fused representations. It is worth noting that this three-stream feature learning module can capture modality-specific information as well as the correlation between modalities. Thus, balancing the two aspects is very important for multi-modal learning and it has helped to improve the SOD performance. Besides, the depth depurator unit acts as a gate to explicitly filter out low-quality depth maps, which several existing methods do not consider the effects. Because low-quality depth maps can inhibit the fusion between RGB images and depth maps, thus the depth depurator unit can ensure effective multi-modal fusion to achieve robust SOD performance.\n$\\bullet$ In JL-DCF , there are two key components, \\ie, a joint learning (JL) and a densely-cooperative fusion (DCF). Specifically, the JL module is used to learn robust saliency features, while the DCF module is used for complementary feature discovery. It is worth noting that this method uses a middle-fusion strategy to extract deep hierarchical features from RGB images and depth maps, in which the cross-modal complementarity can be effectively exploited to achieve accurate prediction.\n$\\bullet$ In UC-Net , instead of producing a single saliency prediction, this model produces multiple predictions by modeling the distribution of the feature output space as a generative model conditioned on RGB-D images. Because each person has some specific preferences in labeling a saliency map, it could fail to capture the stochastic characteristic of saliency while only a single saliency map is produced for an image pair using a deterministic learning pipeline. Thus, the strategy in this model can take into account human uncertainty in saliency annotations. Moreover, considering the fact that depth maps could suffer from noise, directly fusing RGB images and depth maps could cause the network to fit to this noise. Therefore, a depth correction network, designed as an auxiliary component, is proposed to refine depth information with a semantic guided loss. Thus, the above key components are all helpful for improving SOD performance. \n$\\bullet$ In SSF , a complementary interaction module (CIM) is developed to explore discriminative cross-modal complementarities and fuse cross-modal features, where a region-wise attention is introduced to supplement rich boundary information for each modality. Besides, a compensation-aware loss is proposed to improve the networkâ€™s confidence for hard samples in unreliable depth maps. Thus, these key components enable the proposed model to effectively explore and establish the complementarity of cross-modal feature representations, while at the same time reducing the negative effects introduced by low-quality depth maps, boosting SOD performance. \n$\\bullet$ In {ICNet} , an information conversion module is proposed to interactively and adaptively explore the correlations between high-level RGB and depth features. Besides, a cross-modal depth-weighted combination block is introduced to enhance the difference between the RGB and depth features in each level, which ensures that the features are treated differently. It is also worth noting that ICNet exploits the complementarity of cross-modal features, as well as explores the continuity of cross-level features, both of which are helpful for achieving accurate predictions. \n$\\bullet$ In S${^2}$MA , a self-mutual attention module (SAM) is proposed to fuse RGB and depth images, integrating self-attention and each otherâ€™s attention to propagate context more accurately. The SAM can provide additional complementary information from multi-modal data to improve SOD performance, overcoming the limitations of the original self-attention, which only uses a single modality. Besides, to reduce the low-quality (\\eg, noise) effects of depth cues, a selection mechanism is proposed to reweight the mutual attention. This mechanism can filter out unreliable information, resulting in more accurate saliency prediction. \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Attribute-based study \\emph{w.r.t.} background objects (\\ie, car, barrier, flower, grass, road, sign, tree, and other). The comparison methods including 24 representative RGB-D based SOD models (9 traditional models and 15 deep learning-based models) evaluated on the SIP dataset  in terms of MAE and $S_{\\alpha}$. The three best results are shown in \\rev{red}, \\blu{blue} and \\gre{green} fonts.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:010}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{p{0.12cm}<{\\centering}|p{0.75cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}}\n        \\hline\n     \\multirow{2}{*}{}    \n     & & \\multicolumn{9}{c|}{\\textbf{Traditional models}} & \\multicolumn{15}{c}{\\textbf{Deep learning-based models}}\\\\ \\hline\n     &\\rotatebox{90}{Categories} \n     &\\rotate{LHM }   &\\rotate{ACSD }   &\\rotate{DESM }       &\\rotate{GP }  \n     &\\rotate{LBE }  &\\rotate{DCMC } &\\rotate{SE }      &\\rotate{CDCP } \n     &\\rotate{CDB } &\\rotate{DF }      &\\rotate{PCF }  \n     &\\rotate{CTMF }   &\\rotate{CPFP }&\\rotate{TANet }     &\\rotate{AFNet } \n     &\\rotate{MMCI } &\\rotate{DMRA }   &\\rotate{D$^3$Net }  &\\rotate{SSF }\n     &\\rotate{A2dele } &\\rotate{S$^2$MA }       &\\rotate{ICNet } &\\rotate{JL-DCF } &\\rotate{UC-Net }  \n    \\\\ \\hline\\hline\n    \\multirow{9}{*}{\\rotate{{MAE}}}\n    & Car\n    &.158 &.163 &.301 &.159 &.201 &.185 &.154 &.202 &.171 &.171 &.085 &.134 &.094 &.084 &.101 &.093 &.069 &.061 &.063 &.078 &\\rev{.055} &.067 &\\gre{.058} &\\blu{.057} \\\\\n    & Barrier\n    &.197 &.177 &.308 &.180 &.201 &.196 &.176 &.251 &.203 &.202 &.073 &.149 &.060 &.078 &.128 &.089 &.093 &.068 &\\gre{.054} &.074 &.057 &.075 &\\rev{.052} &\\blu{.053} \\\\\n    & Flower\n    &.105 &.122 &.306 &.099 &.186 &.158 &.063 &.141 &.101 &.132 &.091 &.075 &.133 &.100 &.090 &.081 &\\blu{.046} &.095 &.107 &.051 &.104 &\\rev{.025} &\\gre{.054} &.075 \\\\\n    & Grass\n    &.164 &.161 &.279 &.155 &.184 &.167 &.138 &.182 &.176 &.167 &.041 &.110 &.035 &.048 &.088 &.059 &.056 &.037 &\\gre{.030} &.046 &.033 &.043 &\\rev{.023} &\\blu{.029} \\\\\n    & Road\n    &.189 &.167 &.281 &.176 &.187 &.181 &.164 &.225 &.189 &.169 &.070 &.140 &.054 &.072 &.125 &.078 &.093 &.059 &\\gre{.049} &.072 &.050 &.065 &\\blu{.045} &\\rev{.044}\\\\\n    & Sign\n    &.107 &.126 &.268 &.110 &.184 &.126 &.079 &.134 &.118 &.096 &.058 &.101 &.063 &.060 &.077 &.083 &.051 &.055 &\\gre{.051} &.054 &\\rev{.048} &.054 &\\blu{.050} &.057 \\\\\n    & Tree\n    &.192 &.193 &.310 &.190 &.241 &.194 &.183 &.230 &.219 &.205 &.083 &.157 &.083 &.091 &.132 &.109 &.106 &.083 &\\blu{.067} &.074 &.092 &.097 &\\rev{.063} &\\gre{.071}\\\\\n    & Other\n    &.246 &.217 &.329 &.224 &.229 &.216 &.229 &.274 &.233 &.233 &.106 &.177 &.111 &.111 &.170 &.124 &.140 &.095 &\\rev{.083} &.099 &.100 &.100 &\\blu{.084} &\\gre{.086} \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.184 &.172 &.298 &.173 &.200 &.186 &.164 &.224 &.192 &.185 &.071 &.139 &.064 &.075 &.118 &.086 &.085 &.063 &\\gre{.053} &.070 &.057 &.069 &\\rev{.049} &\\blu{.051} \\\\ \\hline\\hline\n\\multirow{9}{*}{\\rotate{{$S_{\\alpha}$}}}\n    & Car\n    &.516 &.731 &.590 &.603 &.714 &.671 &.591 &.613 &.546 &.631 &.811 &.726 &.786 &.807 &.736 &.813 &.817 &\\gre{.856} &.845 &.804 &\\rev{.870} &.846 &.855 &\\blu{.859} \\\\\n    & Barrier\n    &.497 &.727 &.609 &.575 &.728 &.672 &.612 &.553 &.552 &.643 &.837 &.698 &.860 &.831 &.708 &.830 &.792 &.855 &.874 &.821 &\\gre{.871} &.848 &\\rev{.876} &\\blu{.875} \\\\\n    & Flower\n    &.477 &.775 &.573 &.673 &.703 &.707 &.772 &.667 &.639 &.750 &.771 &.738 &.714 &.760 &.688 &.785 &.824 &.789 &.768 &\\gre{.845} &.804 &\\rev{.901} &\\blu{.856} &.811 \\\\\n    & Grass\n    &.537 &.756 &.643 &.605 &.760 &.728 &.683 &.672 &.559 &.672 &.908 &.770 &.908 &.899 &.780 &.888 &.876 &.917 &\\gre{.924} &.878 &\\blu{.928} &.910 &\\rev{.939} &\\gre{.924} \\\\\n    & Road\n    &.521 &.739 &.634 &.598 &.751 &.685 &.641 &.595 &.576 &.680 &.851 &.722 &.871 &.848 &.705 &.847 &.807 &.873 &\\gre{.885} &.832 &\\gre{.885} &.868 &\\blu{.889} &\\rev{.892} \\\\\n    & Sign\n    &.578 &.786 &.634 &.628 &.719 &.745 &.761 &.714 &.615 &.757 &.855 &.756 &.833 &.857 &.771 &.818 &.848 &.849 &.849 &.842 &\\rev{.871} &\\blu{.861} &\\gre{.859} &.840 \\\\\n    & Tree\n    &.505 &.699 &.606 &.577 &.661 &.648 &.600 &.588 &.543 &.625 &.802 &.679 &.804 &.778 &.691 &.779 &.748 &.806 &\\blu{.837} &.807 &.800 &.788 &\\rev{.848} &\\gre{.825}\\\\\n    & Other\n    &.460 &.687 &.594 &.532 &.706 &.669 &.563 &.554 &.542 &.600 &.786 &.677 &.774 &.782 &.647 &.790 &.722 &.800 &\\rev{.828} &.785 &.809 &.799 &\\gre{.821} &\\blu{.823} \\bigstrut\\\\\\cline{2-26}\n    & Overall\n    &.511 &.732 &.616 &.588 &.727 &.683 &.628 &.595 &.557 &.653 &.842 &.716 &.850 &.835 &.720 &.833 &.806 &.860 &\\gre{.874} &.828 &.872 &.854 &\\rev{.880} &\\blu{.875} \\\\\\hline\\hline\n    \\end{tabular}\n\\end{table*}", "cites": [8141, 6218, 8144], "cite_extract_rate": 0.10714285714285714, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key components from the cited papers, particularly highlighting innovative design choices in each top-performing model. It goes beyond description by critically analyzing how these components contribute to performance and noting limitations like noise in depth maps. The abstraction is strong, as it identifies broader design principles (e.g., modality fusion, handling uncertainty, noise reduction strategies) that are relevant to the field of RGB-D SOD."}}
{"id": "9a99b633-f6fa-43bc-8a6d-41be0427ad05", "title": "Attribute-based Evaluation", "level": "subsubsection", "subsections": [], "parent_id": "177142bd-d83e-4c1f-ac33-db77c968bd52", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Model Evaluation and Analysis"], ["subsection", "Performance Comparison and Analysis"], ["subsubsection", "Attribute-based Evaluation"]], "content": "To investigate the influence of different factors, such as object scale, background clutter, number of salient objects, indoor or outdoor scene, background objects, and lighting conditions, we carry out diverse attribute-based evaluations on several representative RGB-D based SOD models.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{Fig8.pdf} \n    \\caption{Sample images with different objects scales. The scale ratios are denoted in yellow.}  \\label{fig_0072}\n\\end{figure}\n$\\bullet$ \\textbf{Object Scale}. To characterize the scale of a salient object area, we compute the ratio between the size of the salient area and the whole image. We define three types of object scales: 1) when the ratio is less than 0.1, it is denoted as ``small\"; 2) when the ratio is larger than 0.4, it is denoted as ``large\"; and 3) when the ratio is in the range of $[0.1, 0.4]$, it is denoted as ``medium\". In this evaluation, we build a hybrid dataset with 2,464 images collected from STERE , NLPR  , LFSD , DES , and SIP , where 24\\%, 69.2\\% and 6.8\\% of images have small, medium, and large salient object areas, respectively. The constructed hybrid dataset can be found at \\href{https://github.com/taozh2017/RGBD-SODsurvey}{https://github.com/taozh2017/RGBD-SODsurvey}. Some sample images with different object scales are shown in Fig.~\\ref{fig_0072}. The comparison results of the attribute-based study \\emph{w.r.t.} object scale are shown in Tab.~\\ref{tab:006}. From the results, it can be observed that all comparison methods obtain better performance in detecting small salient objects while they obtain worse performance in detecting large salient objects. Besides, the three most recent models, \\ie, JL-DCF , UC-Net , and S$^2$MA , obtain the best performance. D$^3$Net , SSF , A2dele , and ICNet  also obtain promising performance. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{Fig9.pdf} \n    \\caption{Sample images with three types of background clutter.\n    }  \\label{fig_007}\n\\end{figure}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{Fig10.pdf} \n    \\caption{Sample images with single or multiple salient objects.}  \\label{fig_0071}\n\\end{figure}\n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Fig11.pdf} \\vspace {-0.45cm}\n    \\caption{Attribute-based study \\emph{w.r.t.} number of salient objects (\\ie, single vs. multiple (multi)). The comparison results on 24 representative RGB-D based SOD models (\\ie, LHM , ACSD , DESM , GP , LBE , DCMC , SE , CDCP , CDB , DF , PCF , CTMF , CPFP , TANet , AFNet , MMCI , DMRA , D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net ) are given in terms of MAE (top) and $S_{\\alpha}$ (bottom).}  \\label{fig_08}\n\\end{figure*}\n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Fig12.pdf} \\vspace {-0.45cm}\n    \\caption{Attribute-based study \\emph{w.r.t.} indoor vs. outdoor environments. The comparison results for 24 representative RGB-D based SOD models (\\ie, LHM , ACSD , DESM , GP , LBE , DCMC , SE , CDCP , CDB , DF , PCF , CTMF , CPFP , TANet , AFNet , MMCI , DMRA , D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net ) are provided in terms of MAE (top) and $S_{\\alpha}$ (bottom).}  \\label{fig_09}\n\\end{figure*}\n$\\bullet$ \\textbf{Background Clutter}. It is difficult to directly characterize background clutter. Since classic SOD methods tend to use prior information or color contrast to locate salient objects, they often fail under complex backgrounds. Thus, in this evaluation, we utilize five traditional SOD methods, \\ie, BSCA , CLC , MDC , MIL , and WFD , to first detect salient objects in various images and then group these images into different categories (\\eg, simple or complex background) according to the results. Specifically, we first construct a hybrid dataset with 1,400 images collected from three datasets (STERE , NLPR , and LFSD ). Then, we apply the five models to this dataset and obtain the $S_{\\alpha}$ values for each, which we use to characterize images as follows: 1) If all $S_{\\alpha}$ values are higher than $0.9$, the image is denoted as having a ``simple\" background; 2) If all $S_{\\alpha}$ values are lower than $0.6$, the image is said to have a ``complex\" background; 3) The remaining images are denoted as ``uncertain\". Some example images with the three types of background clutter are shown in Fig.~\\ref{fig_007}. The constructed hybrid dataset can be found at \\href{https://github.com/taozh2017/RGBD-SODsurvey}{https://github.com/taozh2017/RGBD-SODsurvey}. The comparison results of the attribute-based study \\emph{w.r.t.} background clutter are shown in Tab.~\\ref{tab:007}. As can be seen, all models obtain worse SOD performance on images containing complex backgrounds than simple ones. Among the representative models, JL-DCF , UC-Net  and SSF  achieve the top-three best results. Besides, the four most recent models, \\ie, D$^3$Net , S$^2$MA , A2dele , and ICNet , also obtain better performance than the other models. \n\\renewcommand\\arraystretch{1.0}\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Attribute-based study \\emph{w.r.t.} light conditions (sunny vs. low-light). The comparison methods include 24 representative RGB-D based SOD models (9 traditional models and 15 deep learning-based models) evaluated on the SIP dataset  in terms of MAE and $S_{\\alpha}$. The three best results are shown in \\rev{red}, \\blu{blue} and \\gre{green} fonts.}\n    \\scriptsize\n    \\vspace {-2.5mm}\n    \\label{tab:011}\n    \\setlength{\\tabcolsep}{4.4pt}\n    \\begin{tabular}{p{0.12cm}<{\\centering}|p{1.1cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.33cm}<{\\centering}|p{0.31cm}<{\\centering}|p{0.31cm}<{\\centering}|p{0.31cm}<{\\centering}|p{0.31cm}<{\\centering}|p{0.31cm}<{\\centering}}\n        \\hline\n     \\multirow{2}{*}{}    \n     & & \\multicolumn{9}{c|}{\\textbf{Traditional models}} & \\multicolumn{15}{c}{\\textbf{Deep learning-based models}}\\\\ \\hline\n     &\\rotatebox{90}{Conditions} \n     &\\rotate{LHM }   &\\rotate{ACSD }   &\\rotate{DESM }       &\\rotate{GP }  \n     &\\rotate{LBE }  &\\rotate{DCMC } &\\rotate{SE }      &\\rotate{CDCP } \n     &\\rotate{CDB } &\\rotate{DF }      &\\rotate{PCF }  \n     &\\rotate{CTMF }   &\\rotate{CPFP }&\\rotate{TANet }     &\\rotate{AFNet } \n     &\\rotate{MMCI } &\\rotate{DMRA }   &\\rotate{D$^3$Net }  &\\rotate{SSF }\n     &\\rotate{A2dele } &\\rotate{S$^2$MA }       &\\rotate{ICNet } &\\rotate{JL-DCF } &\\rotate{UC-Net }  \n    \\\\ \\hline\\hline\n    \\multirow{4}{*}{\\rotate{{MAE}}}\n    & Sunny \n    &.182 &.171 &.294 &.171 &.200 &.183 &.160 &.218 &.190 &.181 &.069 &.137 &.062 &.075 &.116 &.085 &.083 &.062 &\\gre{.052} &.068 &.057 &.068 &\\rev{.048} &\\blu{.051} \\\\\n    & Low-light\n    &.198 &.178 &.323 &.187 &.201 &.207 &.193 &.268 &.208 &.211 &.078 &.154 &.073 &.076 &.130 &.091 &.103 &.067 &\\gre{.059} &.080 &\\blu{.058} &.081 &\\gre{.059} &\\rev{.055} \\\\\n    & Overall\n    &.184 &.172 &.298 &.173 &.200 &.186 &.164 &.224 &.192 &.185 &.071 &.139 &.064 &.075 &.118 &.086 &.085 &.063 &\\gre{.053} &.070 &.057 &.069 &\\rev{.049} &\\blu{.051}\\\\ \\hline\\hline\n    \\multirow{4}{*}{\\rotate{$S_{\\alpha}$}}\n    & Sunny \n    &.516 &.733 &.622 &.593 &.728 &.690 &.639 &.607 &.560 &.660 &.843 &.718 &.852 &.834 &.723 &.833 &.811 &.861 &\\gre{.875} &.831 &.872 &.856 &\\rev{.882} &\\rev{.876} \\\\\n    & low-light\n    &.481 &.721 &.573 &.554 &.722 &.635 &.556 &.515 &.543 &.610 &.838 &.701 &.838 &.837 &.700 &.832 &.775 &\\gre{.855} &\\blu{.867} &.810 &\\rev{.871} &.839 &\\blu{.867} &\\rev{.871} \\\\\n    & Overall\n    &.511 &.732 &.616 &.588 &.727 &.683 &.628 &.595 &.557 &.653 &.842 &.716 &.850 &.835 &.720 &.833 &.806 &.860 &\\gre{.874} &.828 &.872 &.854 &\\rev{.880} &\\blu{.875} \\\\ \\hline\\hline\n    \\end{tabular}\n\\end{table*} \n$\\bullet$ \\textbf{Single vs. Multiple Objects}. In this evaluation, we construct a hybrid dataset with 1,229 images collected from the NLPR   and SIP  datasets. Some example images  with single or multiple salient objects are shown  in Fig.~\\ref{fig_0071}. The comparison results are shown in Fig.~\\ref{fig_08}. From the results, we can see that it is easier to detect single salient object than multiple ones. \n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=.98\\linewidth]{Fig13.pdf} \\vspace {-0.15cm}\n    \\caption{Visual comparisons for two classical non-deep methods (DCMC  and SE ) and three state-of-the-art CNN-based models (DMRA , D$^3$Net , SSF ).}  \\label{fig_010}\n\\end{figure*}\n\\begin{figure*}[t]\n    \\vspace {-4mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{Fig14.pdf} \\vspace {-0.45cm}\n    \\caption{Visual comparisons for five state-of-the-art CNN-based models (A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net ).}  \\label{fig_011}\n\\end{figure*}\n$\\bullet$ \\textbf{Indoor vs. Outdoor}. We evaluate the performance of different RGB-D based SOD models on indoor and outdoor scenes. In this evaluation, we construct a hybrid dataset collected from the DES , NLPR , and LFSD  datasets. The comparison results are shown in Fig.~\\ref{fig_09}. From the results, it can be seen that most models struggle more to detect salient objects in indoor scene than outdoor scenes. This is possibly because indoor environments often have varying light conditions. \n$\\bullet$ \\textbf{Background Objects}. We evaluate the performance of the RGB-D based SOD models when different background objects are present.  We use the SIP dataset , and split it into nine categories, \\ie, car, barrier, flower, grass, road, sign, tree, and other. The comparison results are shown in Tab.~\\ref{tab:010}. As can be seen, all methods obtain diverse performances under different background objects. Among the 24 representative RGB-D based models, JL-DCF , UC-Net  and SSF  achieve the top-three best results. In addition, the four most recent models, \\ie, D$^3$Net , S$^2$MA , A2dele , and ICNet  obtain better performance than the others. \n$\\bullet$ \\textbf{Lighting Conditions}. The performance of SOD can be affected by different lighting conditions. To determine the performance of different RGB-D based SOD models under different lighting conditions, we conduct an evaluation on the SIP dataset , which we split it into two categories, \\ie, sunny and low-light. The comparison results are shown in Tab.~\\ref{tab:011}. \nAs can be seen, low-light negatively impacts SOD performance. Among comparison models, UC-Net  obtains the best performance under sunny conditions while JL-DCF  achieves the best result under low-light condition. \nIn addition, we report the saliency maps generated for various challenging scenes to visualize the performance of different RGB-D based SOD models. Fig.~\\ref{fig_010} and Fig.~\\ref{fig_011} show some representative examples using two classic non-deep methods (DCMC  and SE ) and eight state-of-the-art CNN-based models (DMRA , D$^3$Net , SSF , A2dele , S$^2$MA , ICNet , JL-DCF , and UC-Net ). The $1^{st}$ row shows a small object, while the $2^{nd}$ row is an example of a large one. The $3^{rd}$ and  $4^{th}$ rows contain complex backgrounds and boundaries, respectively. The $5^{th}$ and  $6^{th}$ rows contain multiple salient objects. In the $7^{th}$ row, there are low-light condition. In the $8^{th}$ row, the depth map is coarse with very inaccurate object boundaries, which could inhibit the SOD performance. From the results in Fig.~\\ref{fig_010} and Fig.~\\ref{fig_011}, it can be observed that deep models perform better than non-deep models on these challenging scenes, confirming the powerful expression ability of deep features over handcrafted ones. In addition, D$^3$Net , S$^2$MA , JL-DCF , and UC-Net  perform better than other deep models.", "cites": [8141, 6218, 8144], "cite_extract_rate": 0.0967741935483871, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of RGB-D SOD models by conducting attribute-based evaluations, including object scale, background clutter, and lighting conditions. It synthesizes results from multiple models (including JL-DCF, UC-Net, and S$^2$MA) into a coherent narrative about performance trends. The analysis is critical in identifying how different factors affect detection accuracy and highlighting top-performing models, but it lacks deeper theoretical or methodological critique of the cited works. It abstracts some patterns (e.g., better performance in small-scale objects), though not to a meta-level of broader principles in saliency detection."}}
{"id": "2c55f922-c823-469a-81ed-8e11d54724f1", "title": "Effects of Imperfect Depth", "level": "subsection", "subsections": [], "parent_id": "a03f5e39-8412-4535-a9a9-12713c3a3cc2", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Challenges and Open Directions"], ["subsection", "Effects of Imperfect Depth"]], "content": "\\textbf{Effects of Low-quality Depth Maps}. Depth maps with affluent spatial information have been proven beneficial in detecting salient objects from cluttered backgrounds, while the depth quality also directly affects the subsequent SOD performance. The quality of depth maps varies tremendously across different scenarios due to the limitations of depth sensors, posing a challenge when trying to reduce the effects of low-quality depth maps. However, most existing methods directly fuse RGB images and original raw data from depth maps, without considering the effects of low-quality depth maps. There are a few notable exceptions. For example, in , a contrast-enhanced network was proposed to learn enhanced depth maps, which have much higher contrasts compared with the original depths. In , a compensation-aware loss was designed to pay more attention to hard samples containing unreliable depth information. Moreover, D{$^3$}Net  uses a depth depurator unit (DDU) to classify depth maps into two classes (\\ie, reasonable and low-quality). The DDU also acts as a gate that can filter out the low-quality depth maps. However, the above methods often employ a two-step strategy to achieve depth enhancement and multi-modal fusion  or an independent gate operation for filtering out poor depths, which could bring a suboptimal problem. There is thus a need to develop an end-to-end framework that can achieve depth enhancement or adaptively weight the depth maps (\\eg, assign low weights to poor depth maps) during multi-modal fusion, which would be more helpful for reducing the effects of low-quality depth maps and boosting SOD performance. \n\\textbf{Incomplete Depth Maps}. In RGB-D datasets, it is inevitable for there to be some low-quality depth maps due to the limitations of the acquisition devices. As previously discussed, several depth enhancement algorithms have been used to improve the quality of depth maps. However, depth maps that suffer from severe noise or blurred edges, are often discarded. In this case, we have complete RGB images but some samples do not have depth maps, which is similar to the incomplete multi-view/modal learning problem . Thus, we call it ``incomplete RGB-D based SOD\". As current models only focus on the SOD task using complete RGB images and depth maps, we believe this could be a new direction for RGB-D SOD. \n\\textbf{Depth Estimation}. Depth estimation provides an effective solution to recover high-quality depths and overcome the effects of low-quality depth maps. Various depth estimation approaches  have been developed, which could be introduced into the RGB-D based SOD task to improve performance.", "cites": [6249, 2324, 8147], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple cited papers to highlight the importance of handling imperfect depth in RGB-D SOD and identifies common strategies like depth enhancement and gate-based filtering. It shows some critical perspective by pointing out limitations such as suboptimal two-step strategies. While it draws connections across the works, the abstraction remains moderate, framing the issue within the broader context of incomplete multi-modal learning but not offering a highly generalized framework."}}
{"id": "aef12a92-85ac-40fd-a688-1dd72060207a", "title": "Effective Fusion Strategies", "level": "subsection", "subsections": [], "parent_id": "a03f5e39-8412-4535-a9a9-12713c3a3cc2", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Challenges and Open Directions"], ["subsection", "Effective Fusion Strategies"]], "content": "\\textbf{Adversarial Learning-based Fusion}. It is important to effectively fuse RGB images and depth maps for RGB-D based SOD. Existing models often employ different fusion strategies (\\eg, early fusion, middle fusion, or late fusion) to exploit the correlations between RGB images and depth maps. Recently, generative adversarial networks (GANs)  have gained widespread attention for the saliency detection task . In common GAN-based SOD models, a generator takes RGB images as inputs and generates the corresponding saliency maps, while a discriminator is adopted to determine whether a given image is synthetic or ground-truth. GAN-based models could easily be extended to RGB-D SOD, which could be helpful for boosting performance due to their superior feature learning ability. Moreover, GANs could also be used to learn common feature representations for RGB images and depth maps , which could help with feature or saliency map fusion and further boost the SOD performance.  \n\\textbf{Attention-induced Fusion}. Attention mechanisms have been widely applied to various deep learning-based tasks , allowing networks to selectively pay attention to a subset of regions for extracting discriminative and powerful features. Besides, co-attention mechanisms have been developed to explore the underlying correlations across multiple modalities, and are widely studied in visual question answering  and video object segmentation . Thus, for the RGB-D based SOD task, we could also develop attention-based fusion algorithms to exploit correlations between RGB images and depth cues to improve the performance.", "cites": [735, 529, 6253, 6250, 6252, 6254, 9135, 38, 6251], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from adversarial learning and attention mechanisms by linking their applications in SOD to broader computer vision tasks. It abstracts these methods into general categories (e.g., adversarial fusion, attention-based fusion) but stops short of proposing a novel framework. Critical evaluation is limited to stating potential benefits without deeper critique of limitations or comparing approaches rigorously."}}
{"id": "d0672bd2-0f17-4789-a0c1-a597d7752a51", "title": "Different Supervision Strategies", "level": "subsection", "subsections": [], "parent_id": "a03f5e39-8412-4535-a9a9-12713c3a3cc2", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Challenges and Open Directions"], ["subsection", "Different Supervision Strategies"]], "content": "Existing RGB-D models often use a fully supervised strategy to learn saliency prediction models. However, annotating pixel-level saliency maps is a tedious and time-consuming procedure. To alleviate this issue, there has been increased interest in weakly and semi-supervised learning, which have been applied to salient object detection . Semi-/weak supervision could also be introduced into RGB-D SOD, by leveraging image-level tags  and pseudo pixel-wise annotations , for improving the detection performance. Besides, several studies  have suggested that models pretrained using self-supervision can effectively be used to achieve better performance. Therefore, we could train saliency prediction models on large amounts of annotated RGB images in a self-supervised manner and then transfer the pretrained models to the RGB-D SOD task.", "cites": [8148, 6255, 6256, 6257], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss supervision strategies in RGB-D SOD, connecting ideas like semi-supervised, weakly supervised, and self-supervised learning. It provides some abstraction by generalizing the benefits of using weak supervision and pretraining. However, the critical analysis is limited to stating challenges and benefits without deep evaluation of the methods or identifying specific limitations."}}
{"id": "3ac00a25-1155-4958-a8e3-afa17ba9bf14", "title": "Model Design for Real-world Scenarios", "level": "subsection", "subsections": [], "parent_id": "a03f5e39-8412-4535-a9a9-12713c3a3cc2", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Challenges and Open Directions"], ["subsection", "Model Design for Real-world Scenarios"]], "content": "Some smartphones can capture depth maps (\\eg, images in the SIP dataset were captured using Huawei Mate 10). Thus it would be feasible to conduct the SOD task in real-world applications, \\eg, on smart devices. However, most existing methods include complicated and deep DNNs to increase the model capacity and achieve better performance, preventing them from being directly applied on real-work platforms. To overcome this, model compression  techniques could be used to learn compact RGB-D based SOD models with promising detection accuracy. Moreover, JL-DCF  utilizes a shared network to locate salient objects using RGB and depth views, which largely reduces the model parameters and makes real-world applications feasible.", "cites": [8141, 6258, 849], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea from two papers (JL-DCF and model compression) to address the challenge of deploying complex SOD models on real-world devices. It shows some critical thinking by pointing out the infeasibility of deep DNNs for mobile deployment and suggests a solution. However, the analysis remains somewhat general and does not deeply compare or contrast multiple methods in the field."}}
{"id": "069a3270-9a2c-4c33-8b8e-2de88ddce637", "title": "Extension to RGB-T SOD", "level": "subsection", "subsections": [], "parent_id": "a03f5e39-8412-4535-a9a9-12713c3a3cc2", "prefix_titles": [["title", "RGB-D Salient Object Detection: A Survey"], ["section", "Challenges and Open Directions"], ["subsection", "Extension to RGB-T SOD"]], "content": "In addition to RGB-D SOD, there are several other methods that fuse different modalities for better detection, such as RGB-T SOD, which integrates RGB and thermal infrared data. Thermal infrared cameras can capture the radiation emitted from any object with a temperature above absolute zero, making thermal infrared images insensitive to illumination conditions . Therefore, thermal images can provide supplementary information to improve SOD performance when salient objects suffer from varying light, reflective light, or shadows. Some RGB-T models  and datasets (VT821 , VT1000  and VT5000 ) have already been proposed over the past few years. Similar to RGB-D SOD, the key aim of RGB-T SOD is to fuse RGB and thermal infrared images and exploit the correlations between the two modalities. Thus, several advanced multi-modal fusion technologies in RGB-D SOD could be extended to the RGB-T SOD task.", "cites": [6261, 6259, 6260], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers by discussing the shared goal of fusing RGB and thermal data for salient object detection and highlighting the usefulness of thermal images in challenging lighting conditions. It shows limited critical analysis, pointing to the existence of datasets but not deeply evaluating their strengths or weaknesses. The abstraction is moderate, as it identifies the general benefit of multi-modal fusion and the potential for extending RGB-D techniques to RGB-T, but does not offer a broader theoretical or methodological framework."}}
