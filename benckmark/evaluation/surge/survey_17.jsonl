{"id": "6d364127-a607-4032-a1cb-63dea6785a78", "title": "Intrinsically-Motivated Exploration", "level": "subsection", "subsections": [], "parent_id": "d04f9c5f-6263-4d25-9556-9bcf054502d0", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Reward-Free Exploration"], ["subsection", "Intrinsically-Motivated Exploration"]], "content": "\\label{sec:intrinsic}\nThe second exploration type in the \\emph{reward-free exploration} category is the \\emph{intrinsically motivated exploration}, which is composed of the methods that utilize a form of intrinsic motivation in the absence of external rewards to promote exploring the unexplored parts of the environment. In contrast to blind exploration, intrinsically-motivated exploration techniques utilize some form of intrinsic information to encourage exploring the state-action spaces. The idea of employing internal incentives in exploratory tasks is borrowed from \\emph{intrinsically-motivated behaviour} in humans, which has been studied and discussed extensively in education and psychology literature . In psychology, the distinction between an extrinsically and an intrinsically motivated behaviour is made based on the types of the stimuli that ``move'' the person to perform a task . While intrinsic motivation leads to an inherent satisfaction of performing a job, extrinsic motivation sets an external regulation, which encourages a person to do a task in order to obtain some separable outcome (\\emph{e.g.} reward or reinforcement). Studies show that internalization of the external regulations, also known as self-regularization or self-determination , helps children attain higher achievements  in terms of learning or completing complex tasks, in contrast to using extrinsic motivations in the form of rewards or reinforcement. Analogs of intrinsic motivation in human can be employed in order to promote exploration in the RL framework. Here, we review some of these studies and discuss different forms of intrinsic motivation that have been used in exploration techniques in the reinforcement learning tasks. \nIn the context of reinforcement learning, curiosity takes various interpretations and forms depending on the types of problems and the defined goals as well as the approaches taken toward understanding and solving the problems. In general, we can define curiosity as a way or desire to explore new situations that may help the agent with pursuing goals in the future. As agents are encountered with deceptive and/or sparse rewards in many RL set-ups, exploratory agents that rely on extrinsic rewards might end up in \\emph{local optima} because of deceptive rewards or get stuck due to zero gradient in the received rewards. Thus, the techniques that intrinsically motivate the agent to explore the environment and do not rely on the extrinsic reinforcement are effective in learning of such tasks.\nMany of the reward-free intrinsically-motivated exploration strategies aim at minimizing the agent's \\emph{uncertainty} or \\emph{error} in its predictions . In order to evaluate the precision of the agent's predictions of the environment behavior, a model of the environment dynamics is required, such that given the current state and the chosen action, the model predicts the next state. Minimization of the resulting error in the model prediction encourages the exploration of the underlying space. There are other reward-free techniques that pursue the maximization of \\emph{space coverage} (new states or state-action pairs visitation), which utilize a form of intrinsic motivation to govern the agent's exploratory behaviour . More \\emph{space coverage} essentially means visiting more unexplored states in a shorter amount of time and in turn, learning more about the environment. In this section, we survey and discuss some of the reward-free intrinsically-motivated exploration approaches that seek either of the above-mentioned goals. Note that the notion of \\emph{intrinsic motivation} in RL tasks has been also used in combination with external rewards, which is not the subject of our discussion in this section and will be elaborated in the ``Bonus/Optimism-Based Exploration'' category (section \\ref{Bonus}). \nThe early use of intrinsic motivation in computational framework dates back to 1976, when  used the notion of ``interestingness'' in mathematics to encourage new concepts and hypotheses.  introduced DIDO, a curiosity-driven learning strategy that can help the agent explore initially unknown domains in an unsupervised set-up using the notion of Shannon's uncertainty function defined as\n\\begin{align}\nsh = -\\sum_{i=1}^n\\left(p_i\\times\\log_2(p_i)\\right),\n\\end{align}\nwhere $p_i$ is an estimate of the probability of the outcome $O_i$, and the summation is taken over all outcomes. Minimization of \\emph{uncertainty} in their proposed formalism leads to a broader searching span, as well as more effective learning of the search space. DIDO, in fact, promotes the idea of using an experience generator that provides experiences, which are novel compared to the previous ones and are related to them at the same time. The obtained experiences help the agent search for a better representation in the space of possible representations while DIDO's representation generator is employed in finding more informative experiences.  performed DIDO in several discrete domains, which showed that their exploratory algorithm enables the agent to select sensible experiences and eventually leads to a good representation of the domain. \nIn the following years, another form of curiosity-driven exploration was introduced  based on the improvement in the reliability of the RL agent's predictions of the world model. In particular,  proposed a model-building control system that could provide an adaptive model of the environmental dynamics. He further proposed the notion of dynamic \\emph{curiosity} and \\emph{boredom}, described as ``the explicit desire to improve the world model'', as a potential means of increasing the knowledge of the animat about the world in the exploration phase. In his work, \\emph{curiosity} aims at minimization of the agent's ignorance and is triggered when the agent comes to the realization that it does not have enough knowledge of something. It provides a source of reinforcement for the agent and is defined as the Euclidean distance between the real and the predicted model network. A failure in the correct prediction of the environment leads to a positive reinforcement that encourages the agent to further explore the corresponding actions. Improvement in the world model predictions with time leads to less reinforcement and thus, discouragement of exploring the corresponding actions, also referred to as \\emph{boredom}. \nWhile the idea of using curiosity was not implemented in , Schmidhuber later utilized the notion of \\emph{adaptive curiosity}  to encourage exploration of the unpredictable parts of the environment. In particular, he proposed a curious model-building control system, where the notion of \\emph{adaptive confidence} was used for modeling the reliability of a predictor's predictions, and \\emph{adaptive curiosity} was utilized to reward the agent for encountering hard but learnable states and thus improve the \\emph{exploration} phase by reducing the extra time spent on the non-useful or well-modelled parts of the environment. The strength of his proposed approach in comparison to its predecessors including his previous study , lies in its ability to work in \\emph{uncertain non-deterministic} environments by adaptively modeling the reliability of a predictor's predictions and learning to predict \\emph{cumulative} error changes in the model network. This goal can be achieved via maximization of the expectation of cumulative changes in prediction reliability.  tested his proposed curiosity-driven algorithm based on Watkin's Q-learning in two-dimensional \\emph{discrete-state} toy environments with over 100 states and compared the results to the ones obtained using random search as the exploratory approach. Utilizing an adaptive curious agent led to a decrease of an order of magnitude in the learning time. \nAnother similar yet different exploration approach was proposed by  around the same time, which suggested using the notion of \\emph{competence map} for guiding exploration via estimating the controller's \\emph{accuracy}. In particular,  introduced a notion of energy \n\\begin{align}\nE = (1-\\Gamma)E_{\\mbox{\\scriptsize explore}} +\\Gamma E_{\\mbox{\\scriptsize exploit}},\n\\end{align}\nwhere gain parameter $0<\\Gamma<1$ controls the exploration-exploitation trade-off and is a function of the change in the exploration energy $E_{\\mbox{\\scriptsize explore}}$ and the exploitation energy $E_{\\mbox{\\scriptsize exploit}}$. A competence network system is trained to estimate the upper bound of the ``model network error''; minimization of the ``expected competence'' leads to the exploration of the world.  employed ``competence map'' in a continuous two-dimensional robot navigation task, which revealed that their suggested exploration method can perform better compared with ``random walk'' and ``purely greedy'' approaches. \nAn exploration approach was later proposed by  as an extension of previous similar studies, such as . Their proposed exploration method, called \\emph{Reinforcement Driven Information Acquisition} (RDIA), is devised for non-deterministic environments and utilizes the notion of \\emph{information gain}, which is used as an intrinsic motivation to govern the agent’s exploratory movements. In particular, the RDIA agent models the environment via estimating the transition probability $p^\\star_{ijk}(t)$ at each time step $t$ as the ratio of the number of times so far that the pair $(s_i, a_j)$ has led to the state $s_k$ over the number of times the agent has experienced $(s_i, a_j)$. The information gain is then defined as the difference between the agent’s current estimation of the transition probability $p^\\star_{ijk}(t)$ and $p^\\star_{ijk}(t+1)$ at time $t+1$. Information gain represents the information that the agent has acquired upon performing the respective action, which consequently leads to an increase in the estimator’s accuracy.  assess their proposed method in simple discrete environments with certain numbers of states and actions using two different information gain measures, namely the entropy difference and the Kullback-Leibler (KL) distance, between the probability distributions, and show that the results obtained by RDIA surpass those of simple random search.\nInformation gain as intrinsic motivation has been used in other exploration strategies such as the studies performed by  and . In particular, the exploration method proposed by  takes a Bayesian approach, where the agent builds an internal model of the environment, and upon taking an action and observing the next state, calculates the KL-divergence of its current internal model from the one it had predicted prior to taking the action. The resulting unweighted sum of the KL-divergences yields the \\emph{missing information} $I_M$, which is utilized as a measure of inaccuracy in the agent’s internal model. The agent subsequently takes actions that maximize the expected (predicted) information gain (PIG), defined as the expected decrease in the missing information $I_M$ between the internal models.  Another similar exploration method  extends the application of PIG  to perform in environments with \\emph{unbounded} discrete state spaces. In particular,  utilize the Chinese Restaurant Process (CRP) to find the probability of revisiting a state or discovering a new one, and use the obtained results to calculate the agent’s internal model. The subsequent steps are similar to those presented and discussed by . Another study by  introduces the Bayesian Model-based Active eXploration (MAX) method, which utilizes the \\emph{novelty of transitions} as a learning signal, and is applicable in discrete and continuous environments. In particular, MAX agent calculates the Jenson-Shannon divergence and the Jensen-R\\'enyi divergence  of the predicted space of distributions from the resulting one in discrete and continuous environments, respectively. Maximization of the resulting novelty measure governs the agent’s exploratory behaviour. The evaluation of MAX performance in several discrete and continuous tasks presents promising results compared with those obtained from MAX counterparts and other baselines.\n\\begin{table}\n    \\centering\n    \\begin{tabular}{p{4.9cm}|p{2.5cm} |p{7cm}}\n        Approach & Intrinsic  & Remarks \\\\\n         & Motivation & \\\\\n        \\hline\n        & None (Blind) & Early use of \\emph{random walk}\n        \\\\\n        & None (Blind) & Early use of \\emph{random walk}\n        \\\\\n\t& None (Blind) & Early use of \\emph{random walk}\n\t\\\\\n\t& None (Blind) & $\\epsilon$-greedy\n\t\\\\\n\t& None (Blind) & max-random ($\\epsilon$-greedy)\n\t\\\\\n\t& None (Blind) & $\\epsilon z$-greedy (Temporally-extended actions)\n\t\\\\\n\t& None (Blind) & Spiral search (For planar environments only)\n\t\\\\\n\t& Uncertainty & Adaptive model of the environment dynamics\n\t\\\\\n\t& Uncertainty & Minimization of Shannon's uncertainty function\n\t\\\\\n\t& Uncertainty & Prediction of cumulative error changes\n\t\\\\\n\t& Uncertainty & Competence map\n\t\\\\\n\t& Uncertainty & Maximization of information gain\n\t\\\\\n\t& Uncertainty & Maximization of expected information gain\n\t\\\\\n\t& Uncertainty & Maximization of expected information gain\n\t\\\\\n\t& Uncertainty & Uses an ensemble of forward dynamics models\n\t\\\\\n\t& Uncertainty & Minimization of predicted error in feature representation\n\t\\\\\n\t& Space coverage & Maximization of entropy of the distribution over the visited states\n\t\\\\\n\t& Space coverage & Generation of correlated trajectories in the state and action spaces\n\t\\\\\n\t& Self-generated goals & Coverage maximization of the space of goals\n\t\\\\\n\t& Self-generated goals & Combines  with DDPG\n\t\\\\\n\t& Space coverage & Maximization of eigenpurposes\n\t\\\\\n\t& Space coverage & Extension of  to stochastic environments\n\t\\\\\n\t& Space coverage & Minimization of cover time\n\t\\\\\n\t& Space coverage & Extension of  to large or continuous state spaces\n\t\\\\\n\t& Space coverage & Encouraging new policies using a distance measure between the policies\n    \\end{tabular}\n    \\caption{Examples of some reward-free exploration approaches.}\n    \\label{tab:reward-free}\n\\end{table}\nAnother curiosity-driven approach is ``Intrinsic Curiosity Module'' (ICM) , where curiosity is defined as ``the error in an agent's ability to predict the consequence of its own actions''. In their set-up, the agent interacts with high-dimensional continuous state spaces (images in this case). The authors show that ICM helps the agent to learn and improve its exploration policy in the presence of sparse extrinsic rewards as well as in the absence of any sort of environmental rewards. Moreover, they show that the curious agent can apply its gained knowledge and skills in new scenarios and still achieve improved results. The main idea behind ICM is that instead of targeting \\emph{learnable} states and rewarding the agent for detecting them , ICM  focuses only on a \\emph{feature representation} that reflects the parts of the environment that either affect the agent or get affected by the agent's choice of actions. Intuitively, by focusing on the influential feature space instead of the state space, ICM is able to avoid the unpredictable or unlearnable parts of the environment. \nNote that in some of the fore-mentioned proposed algorithms , while the external reinforcement is not a necessary component, the external reward, if exists, can be added to the curious reinforcement. Thus, these studies are included in the list of ``Bonus/Optimism-Based Exploration'' category as well (section \\ref{Bonus}). \nSome of the intrinsically-motivated exploration techniques utilize the analogy between the dynamical and physical systems, and thus propose the notion of \\emph{entropy maximization} to encourage exploration of the search space. In this regard, an early utilization of entropy in intelligent adaptation of search control was in the context of \\emph{search effort allocation} problem in genetic search procedures , where entropy was used as a measure of diversity. Around the same time,  defined a notion of entropy for the case of bandits as a measure of the uncertainty regarding the identity of the optimal action. In his proposed exploration algorithm, the agent selects the more informative action, which is the one that on expectation will lead to a larger entropy reduction. In the field of reinforcement learning (which is the main focus of the current survey), there are several studies that utilize notion of entropy in their proposed exploration techniques . In this section, however, we discuss the method proposed by  as it is the only \\emph{reward-free} technique among the studies that utilize the notion of entropy in guiding exploration.  introduce an exploration approach, which targets environments that do not provide the agent with extrinsic reward. In their proposed method, the intrinsic objective is to maximize the entropy of the distribution over the visited states. They introduce an algorithm, which optimizes objectives that are only functions of the state-visitation frequencies. In particular, it generates and optimizes a sequence of intrinsic reward signals, which consequently leads to the entropy maximization of the distribution that the policy induces over the visited states. The reward signals form a concave reward functional $R(d_\\pi)$, which is a function of the entropy of the induced state distribution $d_\\pi$ given policy $\\pi$. The obtained optimal policy is referred to as \\emph{maximum-entropy} (MaxEnt) exploration policy, which is defined as $\\pi^\\star \\in \\argmax_{\\pi} R(d_\\pi)$.\nAnother intrinsically-motivated exploration approach that encourages space coverage is the method of PolyRL , which is designed for tasks with continuous state and action spaces and sparse reward structures. PolyRL is inspired by the statistical models used in the field of polymer physics to explain the behaviour of simplified polymer models. In particular, PolyRL exploration policy selects orientationally correlated actions in the action space and induces persistent trajectories of visited states (\\emph{locally self-avoiding} walks) in the state space using a measure of spread known as the \\emph{radius of gyration squared},\n\\begin{align}\n    U_g^2(\\tau_\\states) &:= \\frac{1}{T_e-1} \\sum_{s \\in \\tau_\\states} d^2(s,\\Bar{\\tau}_\\states).\\label{eq:radius_gyration}\n\\end{align}\nIn equation \\ref{eq:radius_gyration}, $T_e$ denotes the number of exploratory steps taken so far in the current exploratory trajectory, $\\tau_\\states$ is the trajectory of the visited states, and $d(s,\\bar{\\tau}_\\states)$ is a measure of distance between a visited state $s$ and the empirical mean of all visited states $\\Bar{\\tau}_\\states$. At each time step, the exploratory agent computes  $U_g^2(\\tau_\\states)$, and subsequently compares it with the value obtained from the previous step. In addition, it calculates the high-probability confidence bounds on the radius of gyration squared, within which the stiffness of the trajectory is maintained. If the change in $U_g^2(\\tau_\\states)$ is within the confidence interval, the agent continues to explore, otherwise it selects the subsequent action using the target policy.  assess the performance of PolyRL in 2D continuous navigation tasks as well as several high-dimensional sparse MuJoCo tasks and show improved results compared with those obtained from several other exploration techniques. \\\\\n\\textbf{Policy-Search Based Exploration without Extrinsic Reward -}\n\\label{sec:pure-policy-seach}\\emph{Policy-search} methods search in the parameter space $\\theta$ for the appropriate parameterized policy $\\pi_\\theta$. As policy-search methods typically do not learn value functions, the choice of a proper parameter $\\theta$ is essential for ensuring an efficient, stable and robust learning. This calls for an efficient exploration strategy in order to provide the policy evaluation step in the policy search methods with new trajectories and thus new information, which is subsequently used for policy update . The exploration approaches performed in policy search methods use stochastic policies, and they can either employ rewards obtained from the environment to guide the exploratory trajectories or function in a completely reward-free manner. In the current section, we review the policy-search methods that do \\emph{not} incorporate extrinsic rewards in their exploratory decision making. We provide a more thorough introduction to policy-search methods in section \\ref{sec:value-policy-search}, where we discuss the policy-search approaches that utilize extrinsic rewards.\nOne of the approaches to solving problems autonomously is breaking the problem/goal into smaller sub-problems/sub-goals . This idea is inspired from the way children tend to select their objectives such that they are not too easy or too hard for them to handle. These intermediate learned goals facilitate learning more complex goals, which ultimately lead to building up more skills required to achieve bigger goals. Based on this intuition,  propose a curiosity-driven exploration algorithm called ``Intrinsically Motivated Goal Exploration Process'' (IMGEP). The IMGEP approach structure relies on assuming that the agent is capable of choosing goal $p$ from the space of RL problem and is able to calculate the corresponding reward $r$ using the reward function $R\\left(p,c,\\theta,o_\\tau\\right)$ given the parameterized policy $\\pi_\\theta$, context $c$ (which gives the current state of the environment) and the observed outcome $o_\\tau$ in the trajectory $\\tau=\\{ s_{t_0},a_{t_0}, s_{t_1}, a_{t_1},\\dots,s_{t_{\\mbox{\\scriptsize end}}}, a_{t_{\\mbox{\\scriptsize end}}}\\}$. The reward function $R\\left(p,c,\\theta,o_\\tau\\right)$ is thus non-Markovian and can be calculated at any time during or after performing the tasks. Using the computed rewards, the agent samples the \\emph{interesting} goal $p$, which is a self-generated goal that leads to faster learning progress. In the exploration phase, the agent uses the meta policy $\\Pi_\\epsilon\\left(\\theta|p,c\\right)$ to find the parameter $\\theta$ for goal $p$, which is subsequently utilized in a goal-parameterized policy search process. The obtained outcome is then used in computing the intrinsic reward $r$, which in turn provides useful information regarding the interestingness of the samples goal $p$. The goal sampling strategy and the meta-policy are subsequently updated. The performance of IMGEP in the case of a real humanoid robot shows that the IMGEP robot can effectively explore high-dimensional spaces through discovering skills with increasing complexity. \nOne of the exploration methods proposed based on the ``Goal Exploration Processes'' (GEPs)  is the ``Goal Exploration Process- Policy Gradient'' (GEP-PG) , which combines the intrinsically-motivated exploration processes GEPs with the deep reinforcement learning method DDPG in order to improve exploration in continuous state-action spaces and learn the tasks.  perform GEP-PG in the low-dimensional ``Continuous Mountain Car'' and the higher-dimensional ``Half-Cheetah'' tasks. GEP-PG is tested in the fore-mentioned problems with different variants of DDPG. The authors show that specifically in the Half-Cheetah task, the performance, variability and sample efficiency of their proposed method surpasses those of DDPG.\nAnother policy-search based exploration strategy proposed by  utilizes the notion of \\emph{proto-value functions} (PVFs) to discover \\emph{options} that lead the agent towards efficient exploration of the state space. PVFs, first introduced by , are the basis functions used for approximating value functions through incorporating topological properties of the state space. In particular, using the MDP's transition matrix, a diffusion model is generated, whose diagonalized form subsequently gives rise to PVFs. The diffusion model provides the diffusion information flow in the environment. This feature allows the PVFs to provide useful information regarding the geometry of the environment, including the \\emph{bottlenecks}.  define an intrinsic reward function (a.k.a. \\emph{eigenpurpose}) as,\n\\begin{align}\nr^{\\textbf e}_{\\mbox{\\scriptsize in}}\\left(s,s^\\prime\\right) = {\\textbf e}^{\\intercal}\\left(\\phi\\left(s^\\prime\\right)-\\phi\\left(s\\right)\\right),\\label{eq:eigenpurpose}\n\\end{align}\nwhere ${\\textbf e}\\in\\mathbb{R}^{|\\mathcal{S}|}$ is the proto-value function and $\\phi(s)$ is the feature representation of state $s$, which can be replaced by the state $s$ itself in tabular cases. Machado \\emph{et al.} utilize eigenpurpose $r_i^{\\textbf e}$ to discover options (\\emph{a.k.a.} eigenoptions) and their corresponding eigenbehaviors. They subsequently use policy iteration to solve the problem for an optimal policy. \nAlthough the exploration technique introduced by  is applicable to discrete domains only, one of its major advantages is that it provides a dense intrinsic reward function, which facilitates exploration tasks with sparse-extrinsic-reward structures. Moreover, since it is equipped with options, their proposed method can cover a relatively larger span in the state space compared with that of a simple random walk. Finally, the authors show that their method with options is effective for exploration tasks with the goal of maximizing the cumulative rewards. Later work by  proposed an improved version of \\emph{eigenoption discovery}, extending it to stochastic environments with non-tabular states. In particular, using the notion of successor representation , in their proposed method the agent learns the non-linear representation of the states which in turn gives the diffusive information flow (DIF) model, and subsequently, the eigenpurposes and eigenoptions are obtained. \nIn the recent years,  introduced the method of \\emph{covering options}, where options are generated with the goal of minimizing cover time. Their proposed method encourages the agent to visit less-explored regions of the state space by generating options for those parts, without using the information obtained from extrinsic rewards. Their empirical evaluation in discrete sparse-reward domains present reduced learning time in comparison with that of some of their predecessors. The method of \\emph{deep covering options}  extends covering options to large or continuous state spaces while minimizing the agent's expected cover time in the state space. The authors have successfully shown the behaviour of deep covering options in challenging sparse-reward tasks, including Pinball, as well as some MuJoCo and Atari domains.\nIn a study by , the authors apply a diversity-driven method to off- and on-policy DRL algorithms and improve their performances in large state spaces with sparse or deceptive rewards via encouraging the agent to try new policies. In particular, the agent uses a distance measure to evaluate the novelty of $\\pi$ in comparison to the prior ones and subsequently modifies the loss function,\n\\begin{align}\nL_D = L - \\E_{\\pi^{\\prime}\\in\\Pi^{\\prime}}\\left[\\alpha D\\left(\\pi,~\\pi^{\\prime}\\right)\\right], \\label{eq:div}\n\\end{align}\nwhere $L$ denotes the loss function of the deep RL algorithm, $\\alpha$ is a scaling factor, and $D$ is a distance measure between the current policy $\\pi$ and the policy $\\pi^{\\prime}$ sampled from a set of most recent policies $\\Pi^{\\prime}$. Equation \\ref{eq:div}, encourages the agent to try new policies and explore unvisited states without relying on the extrinsic rewards received from the environment. The agent will consequently overcome the problem of getting stuck in local optima due to the presence of deceptive rewards or failing to learn tasks with sparse rewards or large state spaces. The authors apply their proposed exploration method in 2D grid worlds with deceptive or sparse rewards, Atari 2600 as well as MuJoCo, and show that it enhances the performance of DRL algorithms through a more effective exploration strategy.", "cites": [7349, 1377, 8445, 1376, 1379, 1380, 1378, 7348, 1381], "cite_extract_rate": 0.21428571428571427, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple intrinsic motivation approaches in RL (e.g., error minimization, space coverage, adaptive curiosity) and connects them to broader psychological and educational concepts, offering a coherent narrative. It critically addresses limitations such as reliance on external rewards and the risk of local optima, while also identifying how different methods aim to improve exploration efficiency. The discussion abstracts beyond individual papers, highlighting principles like adaptive confidence, self-regularization, and the role of curiosity in driving exploration."}}
{"id": "e090a952-bd98-468e-9438-f10edb7ee45b", "title": "Policy-Search Based Methods", "level": "subsection", "subsections": ["5bfe0a3d-2856-4e21-a8d6-4fdc3a1a34e4", "3ada8b06-df00-49cb-abe0-e8bd0b5bfc9c", "d65fac78-896a-44b3-8b64-a56ec5050e71", "02167b5f-d623-4b12-b82c-cdeccecd7381"], "parent_id": "3300099c-14cd-487a-b43c-e6ac86d0086e", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Randomized Action Selection"], ["subsection", "Policy-Search Based Methods"]], "content": "\\label{sec:value-policy-search}\nAfter having discussed methods that implicitly represent a policy using value function, we turn our attention to policy search methods.\nPolicy search methods explicitly represent a policy, instead of, or in addition to, a value function. In the latter case, these methods are more specifically referred to as actor-critic methods. Most policy search methods learn a stochastic policy. The stochasticity in the policy is usually also the main driver of exploration in such methods.  The different ways in which such perturbations can be applied will be the focus of the next several sections\\footnote{A more general overview of the properties of policy search methods is given in~.}: after this short introduction, Section~\\ref{ssec:perturbed} will introduce the organizational principle for the section, with Sections~\\ref{ssec:actionspace} and~\\ref{ssec:parameterspace} explaining the individual methods in detail. \nMany policy search methods belong to the policy gradient family. These methods aim to update the policy in the direction of the gradient of the expected return $\\nabla J^\\pi$. A basic way to do so is by calculating a finite-difference approximation of the gradient. In this approach, rollouts are performed for one or more perturbations of the original parameter vector, which are then used to estimate the gradient. When the system is non-deterministic, these estimates are extremely noisy, although better estimates can be obtained in simulation when the stochasticity of the environment is controlled by fixing the sequence of pseudo-random numbers~. \nMore sophisticated approaches are based on the log-ratio policy gradient . The log-ratio policy gradient relies on stochastic policies, and exploits the knowledge of the policy's score function (gradient of the log-likelihood). Stochastic policies for continuous actions based on the Gaussian distribution~ are still frequently used~. For discrete action spaces, a Gibbs distributions with a learned energy function~ can be used instead.\nThe initialization of the exploration policy can be freely chosen. In some policy architectures, the amount of exploration is fixed to some constant or decreased according to a set schedule. In other architectures, the amount of exploration is controlled by learned parameters, possibly separate from other  parameters (such as those controlling the `mean action' for any given state).  Policy search methods typically maximize the expected return, and thus probability mass tends to slowly be shifted towards a more greedy policy (usually resulting in a decreasing amount of exploration). These and more advanced strategies will be discussed in more detail in Sec. \\ref{ssec:policydist}. \nThe discussed approaches differ in an important aspect: while in finite-difference methods  the \\emph{parameters} of the policy are perturbed, the method proposed by  selects the \\emph{actions} stochastically. Where perturbations are applied at the level of parameters, they often affect an entire episode (\\emph{episode-based} perturbations). In contrast, classically action-space perturbations are often only applied for a single time step (\\emph{independent} perturbations). \nIn this section, we will focus on research in the area of policy search methods that  introduce new exploration strategies or that explicitly evaluate the effects of different exploration strategies. \nWe will focus on such policy search methods that are trained on a single task and where the policy has its own representation. Policies that are defined only in terms of value function are covered in Section~\\ref{sec:valuebased}. Policies explicitly optimizing over a distribution of tasks are covered with Bayesian and meta-learning approaches in Section~\\ref{sec:prior}. An overview of the methods we will cover in this section is given in Table~\\ref{tab:policysearch}. The table groups the method by type and coherence of perturbation that, like , we consider to be key characteristics of exploration strategies in policy search. The following subsection will give a more detailed explanation of these characteristics.\n\\begin{table}\n    \\centering\n    \\begin{tabular}{c|p{0.2\\textwidth} |p{0.17\\textwidth}|p{0.17\\textwidth}}\n        Approach & Perturbed space & Temporal coherence\\textsuperscript{\\textasteriskcentered} & Remarks \\\\\n        \\hline\n        & Action-space & Independent & -\n        \\\\\n        & Action-space & Independent & -\n        \\\\\n         & Action-space & Independent & - \\\\\n         & Action-space & Correlated & Multi-modal (hierarchy) \\\\\n         & Action-space & Correlated & - \\\\\n         & Action-space & Correlated & - \\\\\n         & Action-space & Correlated & -\\\\\n         & Action-space & Independent & Multi-modal\\\\ \n         & Action-space & Independent & - \\\\\n        \\hline\n         & Parameter-space & Episode-based & - \\\\\n         & Parameter-space & Episode-based & - \\\\\n         & Parameter-space & Episode-based & -\\\\\n        \" & Action-space & Episode-based & - \\\\\n         & Parameter-space & Episode-based & - \\\\\n          & Parameter-space & Episode-based & Correlated \\mbox{parameters}\\\\\n         & Parameter-space & Episode-based & -\\\\\n         & Parameter-space & Episode-based & - \\\\\n         & Parameter-space\\textsuperscript{\\textdagger} & Correlated\\textsuperscript{\\textdagger} & -  \\\\\n         & Parameter-space\\textsuperscript{\\textdagger} & Episode-based\\textsuperscript{\\textdagger} & - \\\\\n         & Parameter-space\\textsuperscript{\\textdagger} & Episode-based\\textsuperscript{\\textdagger} & -\\\\\n         & Parameter-space & Episode-based & Multi-agent \\\\\n    \\end{tabular}\n    \\caption{Different exploration approaches proposed in the context of policy search algorithms. \n    The first section of the table lists methods that mainly perturb the policy in the action space, these methods will be discussed in Sec.~\\ref{ssec:actionspace}. The second section lists methods that mainly perturb the policy in the parameter space, that will be discussed in Sec.~\\ref{ssec:parameterspace}. Within these two broad categories, papers are ordered roughly chronologically, although papers within a similar line of work are kept together. Multiple entries for the same paper refer to different variants. \\\\\n    \\textsuperscript{\\textasteriskcentered} Denotes whether perturbations are applied \\emph{independently} at each timestep, don't change at all throughout an \\emph{episode}, or have an intermediate \\emph{correlation} structure. Details in Sec.~\\ref{ssec:perturbed}.\\\\ \n    \\textsuperscript{\\textdagger} These methods have additional step-based action-space noise for numeric reasons or to ensure a differentiable objective.  }\n    \\label{tab:policysearch}\n\\end{table}", "cites": [1385, 1386, 1382, 1384, 1383], "cite_extract_rate": 0.23809523809523808, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers to differentiate policy search methods based on how exploration is introduced—either through action-space or parameter-space perturbations, with a focus on temporal coherence. It offers some abstraction by organizing methods into categories and identifying patterns in how exploration strategies evolve during training. However, the critical analysis is limited, as the section mostly describes trends and strategies without evaluating trade-offs or limitations of the cited approaches in depth."}}
{"id": "5bfe0a3d-2856-4e21-a8d6-4fdc3a1a34e4", "title": "Perturbed space and coherence", "level": "subsubsection", "subsections": [], "parent_id": "e090a952-bd98-468e-9438-f10edb7ee45b", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Randomized Action Selection"], ["subsection", "Policy-Search Based Methods"], ["subsubsection", "Perturbed space and coherence"]], "content": "\\label{ssec:perturbed}\nIn policy based methods, exploratory behavior is usually obtained by applying random perturbations. One of the main characteristics that differentiate exploration methods is where those perturbations are applied. Within policy gradient techniques, there are two main candidates: either the actions or the parameters are perturbed (although we will discuss some approaches beyond these two shortly). These possibilities reflect two views on the learning problem. On the one hand, the actions that are executed on the system are what actually affects the reward and the next state, no matter which parameter vector generated the actions. From this perspective, it is more straightforward to start with finding good actions, and subsequently find a parametric policy that can generate them. From another perspective, parametrized policies have limited capacity, and the resulting inductive bias might mean that the true optimal policy is excluded from the set of representable policies. We are then looking for parameters with which the overall policy behavior is best across all states. Also, if the structure of the policy is chosen to  reflect prior information about the structure of the solution (e.g. policies linear in hand-picked features or policies with a hierarchical structure), perturbing the policy parameters ensure that explorative behavior follows the same structure. \nThe space in which explorative behaviors are applied is usually closely linked to the coherence of behavior. Coherence here refers to the question of whether (and how closely) perturbations in subsequent time steps depend on one another. Exploration with a low coherence (e.g., perturbations chosen independently at every time step) has the advantage that many different strategies might be tried within a single episode. On the other hand, exploration with a high coherence (e.g., perturbing the policy at the beginning of each episode only) has the advantage that the long-term effect of following a certain policy can be evaluated~. Whereas independent perturbations can result in inefficient random walk behavior,  following a perturbed policy consistently could result in reaching a greater variety of states~. Intermediate strategies between the extremes of completely identical perturbation across an entire episode and completely independent perturbation per time step are also possible~, and can be used to compromise between the advantages of the more extreme strategies. \nMost exploration approaches which perturb the policy in action space have focused on independent perturbations in each time step, as applying the same perturbation at all time steps would not cover the space of possible policies well (see Table~\\ref{tab:policysearch}). In contrast, parameter-space exploration tends to go together with episode-based exploration, because certain parameters might only influence behavior in certain states, so such a perturbation has to be evaluated across multiple states to give a good indication of its merit~. However, exceptions to this pattern exist, especially in the area of exploration strategies of intermediate coherence. In the following paragraphs, papers presenting or analyzing specific exploration strategies will be discussed in more detail. We will start by discussing exploration strategies that apply explorative perturbations in the action space, before turning our attention to strategies that perturb the policy parameters. Finally, we will discuss the issue of what distribution these perturbations are sampled from.", "cites": [7349], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a conceptual analysis of policy-based exploration, distinguishing between action-space and parameter-space perturbations. It synthesizes ideas by connecting the structure of policy perturbations to coherence and long-term behavior, and offers abstraction by framing exploration in terms of general design principles. While it introduces a few key concepts and references the cited paper, it lacks deeper critical evaluation of the cited work's strengths and limitations."}}
{"id": "3ada8b06-df00-49cb-abe0-e8bd0b5bfc9c", "title": "Action-space perturbing strategies ", "level": "subsubsection", "subsections": [], "parent_id": "e090a952-bd98-468e-9438-f10edb7ee45b", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Randomized Action Selection"], ["subsection", "Policy-Search Based Methods"], ["subsubsection", "Action-space perturbing strategies "]], "content": "\\label{ssec:actionspace}\nUsing stochastic policies to generate action-space perturbations has been used at least since the early 80's.   proposed an early actor critic-type algorithm, that perturbed the output of a simple neural network before applying the thresholding activation function. This resulted in Bernouilli-distributed outputs . \nSubsequent work also investigated the use of Gaussian noise in continuous action domains.  introduced specific learning rules for the mean and standard deviation of Gaussian policies, which was introduced to learn policies with continuous actions.   provided a more general algorithm that provides an update rule for a general class of policy functions including stochastic and deterministic operations.  notes that because a Gaussian distribution has separate parameters controlling location and scale, such random units have the potential to control both the degree of exploration as well as where to explore.  These early approaches all perturbed the action chosen at each time step independently.\nWhile independent perturbation is still a popular method, attention has also turned to strategies that attempt to correlate behavior in subsequent time steps. An interesting early example can be found in . That paper describes a hierarchical policy, where an upper-level policy sets a sub-goal which a lower-level policy then tries to achieve. Exploration on the upper-level by itself causes some consistency in the exploration behavior, as a perturbation in the goal-picking strategy will consistently perturb the system's behavior until the subgoal is reached. Additionally, the lower-level learner itself applies a low-pass filter on the action perturbations. As a result, similar perturbations will typically be applied on subsequent time steps. The authors applied this algorithm to learn a stand-up behavior for a real robot.\\footnote{Other work has specifically evaluated the potential of lower-level sub-policies to decrease the diffusion time during the exploration process in the context of pure exploration . This paper is explained in more detail in Sec. \\ref{sec:pure-policy-seach}.\n}\n studied hierachical methods in more detail, among others focusing on their exploration behavior. In their experiments, they investigate two simple exploration heuristics that share certain properties with hierarchical policies. The first heuristic, Explore \\& Exploit, randomly sets 'goals' for a separately trained explore policies analogous to the high-level actions in a goal-conditioned hierarchical method. Goals stay active for one or multiple time steps. Their second method, Switching Ensembles, trains several separate networks that individually attempt to optimize rewards. During training, the active policy is periodically switched, and when the policies are different, this switching leads again to exploration behavior that is coherent over several time steps. \\citeauthor{nachum2019does} find that both methods benefit from temporal coherence, and their results suggest that setting goals in a meaningful space might additionally benefit exploration. \nSimilar to the approach by ,  investigated performing explorative perturbations on physical robots. As the authors note, applying perturbations independently at each time step (e.g., independent draws from a stochastic policy) causes jerkiness in the trajectories, which damages the robot. As an alternative, the paper proposes to apply an auto correlated noise signal. This signal is generated in a slightly different way than the previously discussed approach, as it is generated by summing up independent perturbations from the last $M$ time steps. The authors explicitly evaluated the suggested strategy on various continuous robot control problems. Their experiments suggest that the proposed strategy leads to equivalent asymptotic performance (although sometimes a slower learning speed), while causing less stress to the robot's joints by reducing the jerkiness of trajectories.\nCorrelating the perturbations over several time steps, however, complicates the calculation of log-ratio policy gradients, as the policy is no longer Markov (as the selected action is no longer independent of earlier events given the state).  instead apply auto-correlated noise for their deep deterministic policy gradient (DDPG). Since DDPG is an off-policy algorithm, the generating distribution of the behavior policy does not need to be known, simplifying the use of various kinds of auto-correlated noise. They proposed generating this noise using an Ornstein-Uhlenbeck process, which generates noise with the same properties as that used by . This paper did not focus on real-robot experiments, thus motor strain was not a major concern. They did, however, determine that auto-correlated noise does help learn in (simulated) `physical environments that have momentum', in other words, environments where a sequence of similar actions need to be performed to cause appreciable movement in high-inertia objects.\nMore recently, attention seems to have swung back to independent action perturbations, with recent work attempting to make the distribution from which actions are drawn more expressive, or the resulting explorative actions more informative. While classically, simple parametric distributions have been used as stochastic policies, these  typically cannot represent multi-modal policies.  point out that it is useful to maintain probability mass on \\emph{all} good policies during the learning process, even if this results in a multi-modal distribution. In particular, a seemingly slightly-suboptimal mode might in a later stage of learning be discovered to actually be optimal, which would be hard to uncover if this mode was discarded earlier in the learning process. The authors define the exploration policy as maximizing an objective composed of a reward term and an entropy term. The solution to this maximization problem is an energy-based policy. As one cannot generally sample from such distributions, an approximating neural-network policy is fit to it instead. The authors show that with certain (initially) multi-modal reward distributions the method outperforms exploration using single-modal exploration policies. They also show empirically that a multi-modal policy learned on an initial task can provide a useful bias for exploring more refined tasks later.\nAlthough maintaining a high entropy can be a useful strategy to obtain more informative data, it might be even more effective to directly maximize the amount of improvement to a target policy caused by data gathered using the exploratory behavior policy. This was the approach proposed by , who study the optimization of the behavior policy in off-policy reinforcement learning methods, where the exploration policy can be fundamentally different from the target policy. The authors' insight is that good exploration policies might indeed be quite different from good target policies, and thus might not be centered on the current target policy, but instead have a separate parametrization. While the target policy is adapted in an off-policy manner in the direction of maximum reward, the behavior policy is separately updated by an on-policy algorithm towards greater improvements to the target policy. This is achieved by using an estimate of the improvement of the target policy as the reward of a `meta-MDP'. Experiments show that learned variance, and even more so a learned mean function, results in faster learning and better average rewards compared to conventional exploration strategies centered on the target policy. The authors attribute this performance gain to more diversity in the exploration samples leading to more global exploration.", "cites": [1385, 7349, 1383], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, linking action-space perturbation strategies across time, from independent to correlated methods, and highlighting connections to policy diversity and robot control. It provides critical insights by discussing limitations (e.g., jerkiness in robot trajectories, difficulties with non-Markov policies) and evaluating trade-offs in different strategies. The abstraction is strong, as it identifies broader principles such as the importance of temporal coherence, multi-modal exploration, and meta-policy optimization."}}
{"id": "d65fac78-896a-44b3-8b64-a56ec5050e71", "title": "Parameter-space perturbing strategies ", "level": "subsubsection", "subsections": [], "parent_id": "e090a952-bd98-468e-9438-f10edb7ee45b", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Randomized Action Selection"], ["subsection", "Policy-Search Based Methods"], ["subsubsection", "Parameter-space perturbing strategies "]], "content": "\\label{ssec:parameterspace}\nInstead of using action-space perturbation, one might directly perturb the parameters of the policy. Especially where the parametrization of the policy can be restricted due to prior knowledge about the problem, it might be advantageous to do so. For example, if we know or assume that the optimal action will be directly proportional to the deviation from a set point, it is not informative to perturb the policy at the set point, because the policy will choose an action of 0 at the set-point regardless of the parameter value. This information is implicitly taken into account if parameters, rather than actions, are perturbed. \nPerturbing parameters rather than actions also ensure that any explorative action can also, in fact, be reproduced by some policy in hypothesis space . Referring back to the previous example, a non-zero action perturbation at the set-point, for example, would not be reproducible by any of the considered controllers.\nPerturbing parameters also works very well together with temporally coherent exploration. A parameter vector might simply be perturbed only at the beginning of an episode, and then kept constant for the rest of the episode. Contrast this to action-perturbing schemes, where keeping the action perturbation constant for a whole episode in general would not yield behavior that covers the state-action space well.\nThe most straightforward way to find out what parameter perturbations improve a policy is to treat the entire interaction between the policy parameters and environment as a black-box system and calculate a finite-difference estimate of the policy gradient, keeping the policy perturbation fixed during the episode. An example of this approach is given by . For each policy roll-out, each policy parameter randomly chosen to be either left as is or to be perturbed by a adding a small positive or negative constant. After obtaining a set of such roll-outs, the policy gradient is then estimated using a finite-difference calculation. This method was demonstrated to able to optimize walking behaviors on four-legged robots better than earlier hand-tuned or learned gaits. \nA potential problem with this approach is that stochasticity (from the policy or environmental transitions) can make gradient estimates extremely high-variance. In simulation, where stochasticity can be controlled, this can be addressed by using common random numbers, as was proposed by  in their PEGASUS algorithm to learn policies for gridworld problems and a bicycle simulator.  \nRatio-likelihood policy gradient estimators exploit the knowledge of the parametric form of the policy to calculate more informed estimates of the policy gradient.  proposed a stochastic policy gradient with parameter-based exploration by positing a parameterized distribution $\\pi(\\theta|\\rho)$ over the parameters $\\theta$ of a (deterministic) low-level policy, and learning the hyper-parameters $\\rho$. Their experiments showed that the resulting parameter-perturbing, episode-based exploration strategy outperformed conventional action-perturbing strategies on several simulated dynamical systems tasks, including robotic locomotion and manipulation tasks.  extended the idea of parameter-based exploration to several other policy-search algorithms, including a new method called state-dependent exploration. In that approach, perturbations are defined in the action-space, but are generated based on an `exploration function' that is a deterministic function of a randomly generated vector and the current state. The authors show that state-dependent exploration is equal to parameter-based exploration in the special case of linear policies, and argue that in other cases it combines some of the advantages of parameter-based and action-based exploration. The resulting data was then used to perform REINFORCE updates. As these updates do not fully account for the dependency between actions, they might thus have an increased variance. \n proposed `per-basis' exploration, which is a variant parameter-based exploration scheme where the perturbation is only applied to the parameter corresponding to the basis function with the highest activation and kept constant as long as that basis function had the highest activation.  noted that they emperically observed this trick improved the learning speed.\nThe effect of step-based (independent) versus correlated or episode-based exploration was further studied by . \nThey investigated connections between CMA-ES (Covariance matrix adaptation evolutionary strategies) from the stochastic search literature  and PI$^2$, a reinforcement learning algorithm with roots in the control community .  found that episode-based exploration indeed outperformed per time-step exploration on a simulated reaching task. Furthermore, it also outperformed per-basis exploration proposed by . \n applied the idea of episode-level log-ratio policy gradients (as used in earlier work by e.g. \\citealt{sehnke2010parameter}) to complex neural network policies. They proposed the use of virtual batch normalization to increase the methods sensitivity to small initial difference. The authors connect this method to approaches from the evolutionary strategies community (e.g. \\citealt{koutnik2010evolving}).  found their approach to compare favorably to Trust Region Policy Optimization (TRPO, \\citealt{schulman2015trust}), an action-space perturbing method, on simulated robotic tasks. Furthermore, the approach performed competitively with the Asynchronous Advantage Actor Critic (A3C, \\citealt{mnih2016asynchronous}), while training an order of magnitude faster.\n combined similar ideas from the evolutionary strategies community with directed exploration strategies. This combination results in two new hybrid approaches, where evolutionary strategies are used to maximize a scalarization of the original reward-maximization objective with a term encoding novelty and diversity. A heuristic strategy for adapting the scalarization constant is proposed. The proposed approach is evaluated on a simulated locomotion task and a benchmark of Atari games, where it outperforms a regular evolutionary strategy approach and performs competitively with the deep RL approach by \\citeauthor{fortunato2018noisy} (\\citeyear{fortunato2018noisy}; described below). In this experiment, the evolutionary strategies were given more frames, but still ran faster due to better parallelizability.\n\\Citet{hoof2017generalized} apply auto-correlated noise in parameter space. This noise is  distributed similarly to that proposed by  and , but applied to the parameters rather than the actions. As a result, intermediate trade-offs between independent and episode-based perturbations are obtained. The latent parameter vector violates the independence assumption of step-based log-ratio policy gradients methods, which is resolved by explicitly using the joint log-probability of the entire sequence of actions. Expressing this log-probability in closed form requires the use of a restricted policy class (e.g., linear policies). Note that the auto-correlated action-space noise used by  was applied in an off-policy setting, avoiding this problem. Auto-correlated parameter-space noise was compared to several baselines, including action-space perturbations as well as episode-based and independent parameter-space perturbations. On various simulated and real continuous control tasks, intermediate trade-offs between independent and episode-based perturbation led to faster learning and a way to control state space coverage and motor strain.\nTwo methods   independently proposed strategies to use parameter-based perturbations for reinforcement learning approaches based on deep neural networks. Both of these works consider both value-networks and policy-based approaches. Here, we will discuss the policy-based variants.  Both of the approaches  also build on the principle of episode-based perturbation of the parameter space, but better exploit the temporal structure of roll-outs than previous studies  that largely ignored it. \nBy making the perturbations fixed over an entire episode and using the reparametrization trick, these methods allow the use of a wide range of policies, but possibly increase variance. Subsequent actions are now conditioned on a shared sample from the noise distribution, and their conditional independence means the trajectory likelihood can be factored as usual. As a result, this methods are applicable to non-linear neural network policies.  used a pre-determined amount of noise that was decreased over time according to a pre-set schedule. On the other hand,  learned the magnitude of the parameter noise together with the policy mean\nApplying this principle in an off-policy setting is relatively easy: since any behavior policy could be used to generate data, this could easily be the current deterministic target policy with noise added to the parameters.  modified the Deep Deterministic Policy Gradient (DDPG, \\citealt{lillicrap2016continuous}) algorithm in this manner. With on-policy algorithms, this is more challenging. On-policy algorithms with per time-step updates tend to also require stochastic action selection in each time step. For the on-policy Asynchronous Advantage Actor Critic (A3C, \\citealt{mnih2016asynchronous}) in ; and for Trust Region Policy Optimization (TRPO, \\citealt{schulman2015trust}) in ; this problem was resolved by using a combination of parameter perturbations and stochastic action selection.\nThe NoisyNet-A3C method by  compared favorably to the baseline A3C variant on a majority of 57 Atari games.\nThe parameter-exploring variants of DDPG and TRPO proposed by \n compared favorably to baselines with (correlated or uncorrelated) action-space noise on several simulated robot environment. \n\\footnote{, whose exploration method is discussed in Sec. \\ref{sec:pure}, verify the performance difference between action- and parameter space noise for DDPG, and compare their methods to the exploration methods by  and . Like these methods, their proposal benefits from the flexibility of DDPG as off-policy method to work with data from any behavior policy.}\nIncoherent behavior causes particular problems in multi-agent learning, as pointed out by . In cooperative decentralized execution scenarios, uncoordinated exploration between the agent can lead to state visitation frequencies for which the factorized q-function approximation is catastrophically bad. Such failure traps the agents in suboptimal behavior.  remedy the situation by making exploration at train time coherent across both time and the individual agents, by conditioning on a common latent variable generated by a high-level policy\\footnote{Note that although the perturbed parameters are of a value network, this hybrid value- and policy based approach was covered here as the novelty in the exploration strategy stems from the high-level parametrized policy.}. A separate variational network is used to estimate a mutual information term which avoids collapse of the high-level policy on constant low-level behavior. The proposed approach is compared on both a toy task as well as challenging scenarios from the StarCraft Multi-Agent Challenge .", "cites": [7348, 1387, 1386, 1382, 1384], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and connects multiple works on parameter-space perturbation in reinforcement learning, illustrating how these methods relate to both policy search and evolutionary strategies. It provides critical evaluation by highlighting limitations such as high-variance gradients and contrasting the effectiveness of different exploration schemes. Furthermore, it abstracts key principles like the trade-off between step-based and episode-based exploration and the interplay between policy parametrization and exploration efficiency."}}
{"id": "02167b5f-d623-4b12-b82c-cdeccecd7381", "title": "The distribution of perturbations", "level": "subsubsection", "subsections": [], "parent_id": "e090a952-bd98-468e-9438-f10edb7ee45b", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Randomized Action Selection"], ["subsection", "Policy-Search Based Methods"], ["subsubsection", "The distribution of perturbations"]], "content": "\\label{ssec:policydist}\nSeparate from the issue of how perturbations are applied, is the issue of what distribution these perturbations are sampled from. Often, these are Gaussian distributions centered on the policy mean, leaving the choice of standard deviation open. Other parametric policies may have different parameters controlling the amount of exploration.\nSometimes, such parameters are treated as additional hyperparameters  or governed by a specific heuristic .\nMore commonly, they can also be adapted like the other parameters during learning. The most straightforward way is to adapt the parameters controlling this standard deviation using the same policy gradient . Without additional regularization, policy gradient methods will tend to reduce the uncertainty, leading to a loss of exploration that is hard to control and might result in premature convergence to a suboptimal solution .   \nTo address this problem, several approaches of them have been proposed. Many of them involve regularization using the entropy of the policy or the relative entropy from a reference policy. These entropies can either be constrained or added as regularization term to the optimization objective. A unified view on regularized MDPs is presented by .\nAs an example,  studied   natural policy gradients through the lens of limiting the divergence between successive policies. They found the natural gradient can be derived from a bound on the approximate Kullback-Leibler divergence between trajectory distributions. \nDynamic policy programming  uses a similar formulation but instead uses the relative entropy as penalty term. \n provides a more exact method, focused on limiting the equivalent expected Kullback-Leibler divergence between policies. They  connects this update to the idea of trust region optimization, and provides several steps to make scale this type of network to deep reinforcement learning architectures with tens of thousands of parameters. \nThe `relative entropy policy search' method proposed by  bounds the KL divergence in the joint state-action distribution to avoid a loss of exploration during training. Their bound on the joint KL is a stricter condition than a bound on the expected KL divergence of state-action conditionals which has theoretically attractive properties . However, the method is more complex and seems harder to scale to deep architectures .  propose a method building on relative entropy policy search, that combines regularization terms on the joint- and expected KL. A large benefit is that the resulting algorithm can be faithfully implemented in deep RL frameworks, and thus does need further approximations of the policy. \nAn alternative used early on was to add the derivative of the policy entropy to the policy updates.  found this strategy to improve exploration open on a toy example and several optimization problems. This strategy has also proved fruitful in practice in deep learning approaches:   applied this strategy and informally observed it lead to improved exploration by discouraging premature convergence. Such an entropy term can be seen as a special case of the expected relative entropy objective, with the reference policy being the maximum entropy distribution.  studied such regularized objectives in detail, and conclude that a entropy penalty based on samples from the previous policy distribution distribution can lead to optimization problems.\nThe entropy regularization methods in the previous paragraph only took the instantaneous policy entropy into account.  instead propose two methods that explicitly encourages policies that reaching high-entropy states in the future. They note their method improves exploration by acquiring diverse behaviors.", "cites": [1388, 7350, 1389, 7351, 1391, 8446, 1390], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes various methods of perturbation distribution in policy search, linking Gaussian policies, entropy regularization, and trust region optimization. It provides critical analysis by discussing limitations such as premature convergence and scalability issues. The content abstracts key principles like the role of entropy and KL divergence in maintaining exploration, contributing to a deeper understanding of the broader concept of regularized policy updates."}}
{"id": "42a18849-98e7-4e91-8280-7559f0c25cbf", "title": "Optimism-based methods", "level": "subsection", "subsections": ["2164b2b8-809d-447e-85a9-802747bc5789", "1059c12f-a243-4ede-aee7-a6e9976591c8"], "parent_id": "1462d246-b21a-4fa7-9f79-e4bff1b8b853", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Optimism-based methods"]], "content": "Optimistic exploration is a category of exploration methods that adopt the philosophy of Optimism in the Face of Uncertainty (OFU) principle, which was first introduced as an ad-hoc technique by . From an optimistic perspective, a state is considered a good state if it induces a higher level of uncertainty in the state-value estimate and greater return potential. Optimistic exploration methods are typically realized by implicitly utilizing an \\textit{exploration bonus} either in the form of \\textit{optimistic initialization}  or \\textit{Upper Confidence Bounds} (UCBs). In the optimistic initialization approach, the key assumption is that the unvisited state-action pairs yield the best outcome, whereas in the UCB-based methods, the unvisited state-action pairs are assumed to collect the outcome proportional to the largest statistically possible reward. In this section, we only focus on the methods that do not employ count-based approaches to implement the OFU principle as we address the count-based methods in a separate section.\n\\begin{table} \n\\centering\n\\begin{tabular}{p{6cm}|p{4.5cm}|p{4.5cm}}\n    Reference & Approach & Performance measure \\\\\n    \\hline\n    &\\textbf{Tabular Methods}& \\\\\n    \\hline\n     & optimistic initialization & PAC bound \\\\\n     & UCB-based & PAC bound\\\\\n     & optimistic initialization & PAC bound/Empirical-Grid and Empirical-Toy MDP\\\\\n    \\hline\n    &\\textbf{Function Approximation Methods}& \\\\\n    \\hline\n     & optimistic initialization & Empirical-Grid world/Mountain Car\\\\\n     & optimistic initialization & PAC bound/Empirical \\\\\n     & UCB action selection & Regret bound \\\\\n     & UCB action selection & Empirical-MuJoCo and Puddle World/ PAC Bound\\\\\n     & optimistic initialization & Empirical-MuJoCo\\\\\n     & UCB action selection &  Regret Bound/Empirical-Atari\\\\\n     & UCB action selection & Empirical-MuJoCo\n\\end{tabular}\n\\caption{Exploration methods that implement an optimism-based bonus mechanism.}\n\\label{tab:optimism-based}\n\\end{table}", "cites": [1394, 8447, 8441, 1393, 1392], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of optimism-based exploration methods, listing approaches like optimistic initialization and UCB-based strategies. It includes a table of references and methods but lacks meaningful synthesis or comparison of the cited works. There is minimal critical evaluation or abstraction into broader principles, focusing instead on factual presentation."}}
{"id": "1059c12f-a243-4ede-aee7-a6e9976591c8", "title": "Optimism-based methods: function approximation", "level": "subsubsection", "subsections": [], "parent_id": "42a18849-98e7-4e91-8280-7559f0c25cbf", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Optimism-based methods"], ["subsubsection", "Optimism-based methods: function approximation"]], "content": "After the successful application of OFU to RL with finite state-action MDPs, which we addressed in Section \\ref{sec:optim-tab}, some recent approaches extended this idea to MDPs with large or infinite state-action spaces . This section provides a comprehensive overview of the methods that employ OFU in the  MDPs with large or infinite state-action spaces.\nThe study presented by  is a model-based exploration-exploitation trade-off algorithm for continuous state spaces, termed Fitted R-max. The proposed algorithm is a combination of R-max  with fitted value iteration . The algorithm first updates environment models at each time-step and then applies the value iteration step to solve their proposed Bellman optimality equation. In discrete setting, the proposed method simply implements the optimistic value function proposed by   and control the exploration-exploitation trade-off through a visitation count threshold. In continuous setting, the authors propose a new counting method based on the sum of the unnormalized kernel values based in the estimated environment dynamics. The performance of the Fitted R-max algorithm is experimentally tested in the two environments Mountain Car  and Puddle World . \nAnother line of work that implements the OFU principle in continuous state space environments is , where the authors combine their proposed Multi-resolution Exploration (MRE) algorithm with fitted Q-iteration . Their proposed model-based method is built upon  and introduces a method that measures the uncertainty associated with the visited states through building regression trees, termed as \\textit{knownness-tree}. Knownness-tree is used to model the environment transition dynamics and optimistically update its model at each time step. Theoretically,  show, under some smoothness assumption on transition dynamics, the near-optimal performance of the proposed algorithm, and assess the performance of MRE against $\\epsilon$-greedy algorithm empirically in continuous Mountain Car environment . \n propose a model-free computationally efficient exploration strategy based upon computing Upper-Confidence Least-Squares (UCLS), which are UCBs for least-squares temporal difference learning (LSTD). Since LSTD maintains the agent's past interactions efficiently, the computed upper confidence bounds induce context-dependent variance, which encourages the exploration of states with higher variance. This study provides the first theoretical results that obtain UCBs for policy evaluation using function approximation. Empirically, UCLS algorithm shows outperformance over DGPQ , UCBootstrap , and RLSVI  in Sparse Mountain Car, Puddle World and RiverSwim environments.\n provide an optimistic actor-critic algorithm to resolve two phenomena: \\textit{pessimistic under-exploration}, that is deviating the algorithm from sampling actions that result in improvement on the critic estimates, and \\textit{directionally uninformed action sampling}, which is uniform sampling of actions that are lying in two opposite sides of mean in Gaussian policies. They state that these two phenomena prevent state-of-the-art actor-critic-based algorithms such as SAC  from performing efficient exploration. To tackle these challenges, they calculate approximate upper confidence bounds on the value function estimates to encourage directed exploration and lower confidence bound to prevent overestimation of the value function estimates. They benchmark their proposed algorithm (OAC) in high-dimensional MuJoCo tasks, and the plotted results demonstrate marginal improvement against the SAC algorithm.\nTo avoid the pessimistic initialization phenomenon, commonly used in deep network initialization schemes,  propose an optimistic initialization algorithm, termed Optimistic Pessimistically Initialised Q-Learning (OPIQ) that decouples optimistic initialization of Q function from network initialization.  propose a simple count-based bonus augmented with the Q-value estimates. In the tabular setting, their proposed algorithm is based on . In the Deep RL setting, they adopt commonly employed methods of calculating pseudo counts such as  to compute the additive bonus term. OPIQ is evaluated in the three domains toy randomized Markov chain, Maze and Montezuma's Revenge against the naive extension of UCB-H  to the deep RL and some variations of DQN augmented with some state-of-the-art pseudo-count estimate methods.\nWhen both the environment dynamics and task objective are unknown to the RL agent,  propose a model-based exploration algorithm, termed Deep Optimistic Value Exploration (DOVE), to encourage deep exploration through adopting optimistic value function. Throughout each episode, DOVE learns a transition function and reward function using supervised learning. The initial conditions are applied to the learning policy generated by perturbing locally observed states fetched from the replay memory. The local perturbation performed throughout each episode is employed to ensure information propagation. Empirically,  benchmark DOVE in some high-dimensional continuous control MujuCo tasks.", "cites": [1401, 1392, 1389, 1399, 7352, 1397, 9110, 1398, 1395, 8447, 1393, 1396, 1400], "cite_extract_rate": 0.5, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section demonstrates strong synthesis by grouping works under the OFU framework and explaining their adaptations to function approximation settings. It offers some critical evaluation by identifying limitations (e.g., pessimistic under-exploration, uninformed action sampling) and comparing algorithm performance. While it provides useful generalizations (e.g., context-dependent variance and pseudo-counts in deep RL), the abstraction remains somewhat limited to the specific context of OFU and function approximation, without reaching a meta-level synthesis of exploration principles."}}
{"id": "78fe4cb8-b093-4c65-8292-a9d60e17bf17", "title": "Count-based bonus", "level": "subsection", "subsections": ["7e563231-5cc2-482e-98a1-436b2b7e8e13", "d6a4ec69-cae8-4393-973e-b228f0430431"], "parent_id": "1462d246-b21a-4fa7-9f79-e4bff1b8b853", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Count-based bonus"]], "content": "One way to model the intrinsic reward is to measure how surprising a state-action pair is. An intuitive approach to measuring surprise is to count how frequently a particular state-action pair is visited.  In the count-based setting, the notion of bonus is defined as a function of state-action pair visitation count. In table \\ref{tab:visitation-count}, we provide a list of exploration methods that explicitly employ a form of visitation count to control the exploration-exploitation trade-off.  \n\\begin{table} \n\\centering\n\\begin{tabular}{p{6cm}|p{5cm}|p{5cm}}\n    Reference & Bonus Type & Performance measure \\\\\n    \\hline\n    &\\textbf{Tabular Methods}& \\\\\n    \\hline\n     &  count-based & Empirical-Grid world \\\\\n      &  count-based threshold/optimistic initialization & Empirical-Grid world \\\\\n     &  count-based &  Empirical-Toy MDP\\\\\n     & count-based  & Empirical-Grid world\\\\\n     & count-based/optimistic initialization  &  Empirical-Grid world/AGV-scheduling\\\\\n     &  count-based/optimistic initialization  & PAC bound \\\\\n     & distance-based count & PAC bound\\\\\n     & UCB-based & PAC bound\\\\\n     & UCB-based & PAC bound\\\\\n     & UCB-based & PAC/Regret bound\\\\\n     & UCB-based & Regret bound\\\\\n     & count-based  & PAC bound\\\\\n     & optimistic initialization / distance-based bonus  & PAC bound/Empirical \\\\\n     & count-based threshold &  PAC bound\\\\\n     & count-based/UCB-based  & PAC bound \\\\\n     & count-based/UCB-based  & PAC bound\\\\\n    \\hline\n    &\\textbf{Function Approximation Methods}& \\\\\n    \\hline\n     & & \\\\\n     & density-based  & Empirical \\\\\n     & density-based  & Empirical\\\\\n     & density-based  & Empirical\\\\\n     & count-based  & Empirical\\\\\n     &count-based  & Empirical\\\\\n     & optimistic initialization/ count-based  &  Empirical\n\\end{tabular}\n\\caption{Count-based methods.}\n\\label{tab:visitation-count}\n\\end{table}", "cites": [1404, 1392, 1399, 1403, 1395, 8448, 1402, 1400], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily lists exploration methods that use visitation counts, with minimal synthesis or explanation of how these methods connect or differ. It lacks critical analysis of the cited works and does not abstract broader principles or trends, focusing instead on empirical and PAC/Regret bound categorizations without deeper insights."}}
{"id": "7e563231-5cc2-482e-98a1-436b2b7e8e13", "title": "Count-based bonus: tabular", "level": "subsubsection", "subsections": [], "parent_id": "78fe4cb8-b093-4c65-8292-a9d60e17bf17", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Count-based bonus"], ["subsubsection", "Count-based bonus: tabular"]], "content": "\\label{sec:count-based-tabular}\nIn the tabular setting, counting visited states or state-action pairs is a trivial problem and the bonus term is typically used in one of the following forms, \n\\begin{align}\\label{eq:bonus-forms-tabular}\n   \\mathcal{B}(\\textbf{s},\\textbf{a})~\\text{or}~\\mathcal{B}(\\textbf{s}) \\propto  \n\\begin{cases}\n\\sqrt{\\frac{\\ln{n}}{N(\\textbf{s},\\textbf{a})}}~\\text{or}~\\sqrt{\\frac{\\ln{n}}{N(\\textbf{s})}},\\\\    \n\\frac{1}{\\sqrt{N(\\textbf{s},\\textbf{a})}}~\\text{or}~\\frac{1}{\\sqrt{N(\\textbf{s})}},\\\\\n\\frac{1}{N(\\textbf{s},\\textbf{a})}~\\text{or}~\\frac{1}{N(\\textbf{s})}.\n\\end{cases}\n\\end{align}\nwhere $n$ denotes the total number of time-steps taken by the RL agent.\nAn intuitive interpretation of equation \\eqref{eq:bonus-forms-tabular} is that the bonus for visiting a state-action pair $(s,a)$ is highest when $(s,a)$ is novel, and decays each time the pair $(s,a)$ is revisited. The main limitation of such definitions of bonus is that they are mainly applicable in tabular settings, where the set of state-action pairs is countable and finite. Although bonus-based methods employed in the tabular settings are not necessarily suitable for large state-and-action space settings, they still provide useful intuitions. The first study that employed the notion of bonus in the context of exploration algorithms was , where a new RL architecture Dyna-Q+ was proposed. Dyna-Q+, which is a combination of Dyna architecture  and Watkins Q-learning  uses an additional explicit count-based exploration bonus assigned to state-action pair $(s,a)$. The bonus term used in Dyna-Q+ is proportional to the square root of the number of time steps that have elapsed after the last trial of action $a$ at state $s$. This exploration bonus is added to the update rule designed to update Q-value. The main advantage of using such bonuses is to increase the chance of visiting the state-action pairs that have not been frequently visited.  tests his proposed model in the two RL environments Blocking and Shortcut tasks, designed as a small $2D$ maze environment against two other variations of Dyna-Q that do not use exploration bonus to encourage exploration.  shows that in both experiments, Dyna-Q+ outperforms other variations of Dyna-Q in the setting, where the performance is measured with respect to the collected reward. To address the two issues, namely high computational expense raised by , and instability of the bonus raised by  in large state and action spaces,  proposed the \\textit{prioritized sweeping} algorithm that uses a preset threshold parameter $T_{board}$ to determine whether the state-action pair is worth exploring more or not. Prior to reaching the visitation threshold, the bonus parameter is set to the max return value in the discounted reward setting $\\Rmax/(1-\\gamma)$. Once the visitation count exceeds the optimistic threshold, the algorithm uses the non-optimistic true discounted return. Apart from the tabular nature of this approach, its main bottleneck is that the key hyperparameter $T_{board}$ is prefixed and set manually. The proposed algorithm's performance is experimentally tested against Dyna-Q in two deterministic and stochastic grid world environments.\nAn early study that adopts the OFU principle in a model-based setting for exploration is . The proposed \\textit{H-learning} algorithm is designed in the context of the \\textit{average-reward RL} setting, where the RL agent's goal at each time-step $t$ is to optimize the average expected reward. At each time-step $t$, an empirical model of the environment is computed, and consequently, a set of greedy actions accessible from the current state with respect to the Bellman equation for average-reward RL is reported, as well as the expected long-term advantage function $h(s)$. The long-term advantage function $h(s)$ reflects the long-term impact of start state $s$ on the obtained expected average reward. Eventually, the final expected average reward associated with actions in the set of greedy actions at the current state $s$ is calculated with respect to $h(s)$, a temperature parameter $\\alpha$ and expected average reward computed at time $t-1$. Experimentally, the proposed H-learning algorithm is compared with four other exploration methods, random exploration, counter-based exploration, Boltzmann exploration, and recency-based exploration in a two-dimensional grid world with discrete state and action spaces, termed Delivery domain. The non-tabular version of the H-learning algorithm is proposed based on local linear regression as the function approximator and Bayesian network representation of the action space. The extended H-learning method called \\textit{LBH-learning} is tested in three AGV-scheduling tasks  and compared to six different H-learning baselines.\nThe \\textit{Explicit Explore or Exploit} algorithm (known as $E^3$)  adopts a model-based approach that initiates the exploration phase by dividing the set of states into two categories of known and unknown states. A state is considered to be known if the number of state visitations passes a certain threshold such that the learned dynamics are sufficiently close to the true one. If the current state is unknown, the algorithm calls the procedure of \\textit{balanced wandering}, in which the algorithm chooses the least frequent action at the unknown state and assigns the max reward to the unknown state. When the algorithm is not engaged in the balanced wandering phase, it performs two offline optimal policy computations sub-routines. Later,  proposed Metric-$E^3$ algorithm as a generalization of $E^3 algorithm$. Metric-$E^3$ provides the time complexity bound on finding a near-optimal policy that depends on the covering numbers of the state space rather than the size of the state space as presented by . This difference is mainly due to the difference in their definition of a \"known\" state.  \nIn the context of undiscounted RL,  use the notion of upper confidence bounds to manage exploration-exploitation trade-off. In their study, count-based confidence bound proportional to $\\sqrt{\\frac{1}{N(s,a)}}$ is updated at each step and, together with empirical estimates of reward and transition functions, help the agent to control the exploration-exploitation trade-off. The regret analysis performed by  shows logarithmic performance in the number of time steps taken by the algorithm based on the optimal policy. \n provide an explicit notion of bonus, called \\textit{Bayesian Exploration Bonus}, to manage exploration-exploitation trade-off. Their proposed algorithm focuses on the Bayes-adaptive RL setting with a tabular representation of state-action space. The bonus term is proportional to $\\frac{1}{1+N(s,a)}$, where $N(s,a)$ is calculated based on the number of visitation counts implied by the prior.  provide a template for count-based bonus terms in the form of a theorem stating that any algorithm that adopts an exploration bonus of the form $\\frac{1}{(N(s,a))^p}$ with $p \\leq 1/2$ is not a PAC-MDP. In their proposed algorithm, called BEB, the action-selection is performed with respect to the mean of the current learned belief over transition model, with an additional Bayesian bonus. In the main theorem of this work, the authors show that their proposed algorithm, while allowing a higher rate of exploration, provides a near-optimal sample complexity bound, which is polynomial in $|\\states|$, $|\\actions|$, and time horizon $T$, where the optimality is defined in the Bayesian sense.\nIn the continuous state space setting,  introduce C-PACE as a PAC-optimal exploration algorithm for continuous state MDPs. C-PACE adopts the OFU principle in the estimation of the Bellman equation. At each time step, from the k-nearest neighbours, the action that maximizes the optimistic Q value function is selected. The optimistic Q value function is defined based on the knowledge of k-neighbouring state-action pairs (the bonus term) and the immediate reward obtained upon transiting to any neighbouring pairs. C-PACE assumes the existence of a Lipschitz continuous distance metric over the set of state-action pairs. The main result of this paper provides a PAC bound that shows the near-optimal C-PACE performance with respect to the covering number of the state-action space. Finally, the authors evaluate the performance of C-PACE in a simulated HIV treatment environment.\n propose a confidence-based exploration algorithm called PAC-EXPLORE in a model-based setting, which is operationally very similar to the $E^3$ algorithm , with the difference in the confidence bounds used to compute policies that are practically more efficient. The PAC-EXPLORE algorithm takes a state-action pair visitation threshold and divides the space of state-action pairs into two clusters of known and unknown pairs. If the state falls into the set of known states, the algorithm applies the same technique as in  to estimate confidence bounds on transition probability. Authors in this work show that by sharing the experience of concurrently running agents on top of , one can achieve linear improvement on the algorithm's sample complexity.\nDelayed Q-learning  is one of the first papers that study model-free PAC optimal algorithm. At each time-step $t$, the agent keeps track of three values for each visited state-action pairs $(s,a)$, the value function $Q_t(s,a)$, the Boolean flag $\\operatorname{LEARN}_t(s,a)$ that indicates whether or not the change has occurred to the Q estimate for $(s,a)$, and a visitation counter $N(s,a)$. The exploration-exploitation trade-off is managed based on a visitation count threshold and the value $\\operatorname{LEARN}_t(s,a)$. When the visitation count for $(s,a)$ is larger than a pre-set threshold and the $\\operatorname{LEARN}_t(s,a)$ is true, the $Q_t(s,a)$ estimate is updated. At the initial phase, the Boolean flag $\\operatorname{LEARN}(s,a)$ is set to TRUE for all state-action pairs and $N(s,a)$ is set to zero.  under certain assumptions prove that their proposed algorithm is PAC-MDP in the tabular setting.\n provide two types of upper confidence-based bonuses for Q-learning in the episodic tabular MDP setting: 1) Hoeffding-style bonus, 2) Bernstein-style bonus. By employing the Hoeffding-style bonus, the authors show $\\mathcal{O}(\\sqrt{T})$ regret dependency with respect to the total number of time-step $T$. They also show $\\sqrt{H}$ improvement by using Bernstein-style bonus over the Hoeffding-style bonus exploration algorithm, where $T$ denotes the time horizon.\n introduce another method that addresses the sample efficiency of model-free algorithms by adopting UCB-exploration bonus in Q-learning.  Their proposed UCB-based algorithm maintains two types of visitation counts, $N_t(s,a)$ that denotes the number of times the pair $(s,a)$ has been visited up to time-step $t$, and $\\tau(s,a,k)$ that records the number of time steps that state-action pair $(s,a)$ has occurred for the $k$-th time. If $(s,a)$ has not been visited $k$ times, then $\\tau(s,a,k) = \\infty$. At each time-step, a bonus term proportional to $\\sqrt{\\frac{|\\states||\\actions|\\ln(N_t(s,a))}{N_t(s,a)}}$ is added to the discounted value estimate, and the action-value function gets updated accordingly. To assess the PAC efficiency of the proposed algorithm, the authors first propose a learning instance illustrating $\\Omega(1/\\epsilon^3)$ lower bound incurred by Delayed Q-learning, which leaves a quadratic gap in $1/\\epsilon$ from the best known lower bound in the class of UCB-based exploration algorithms.", "cites": [1400, 1402], "cite_extract_rate": 0.11764705882352941, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple tabular exploration methods and their associated bonuses, connecting them through shared principles like optimistic exploration and visitation counts. It provides some critical analysis by highlighting limitations (e.g., high computational cost, instability in large spaces, manual hyperparameter tuning). However, it could offer deeper abstraction by more explicitly identifying overarching theoretical or design patterns in count-based exploration across works."}}
{"id": "d6a4ec69-cae8-4393-973e-b228f0430431", "title": "Count-based: function approximation", "level": "subsubsection", "subsections": [], "parent_id": "78fe4cb8-b093-4c65-8292-a9d60e17bf17", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Count-based bonus"], ["subsubsection", "Count-based: function approximation"]], "content": "Despite the near-optimal performance guarantees often achieved in the tabular setting, these methods are mostly not suitable for environments with large or infinite state spaces. This section summarizes exploration methods that adopt a notion of visitation count to design exploration algorithms for environments with large state and action spaces. \n revisit the problem of extending count-based exploration to non-tabular setting and propose a density model that hinges upon ideas from the intrinsic motivation literature (refer to section \\ref{sec:pure}) and propose an algorithm that measures state novelty for any choice of action given an arbitrary density model. The key contribution of their study is drawing a connection, called \\textit{pseudo-count}, between intrinsic motivation and count-based exploration. Pseudo-count quantity is derived from an arbitrary density model over the state space. The density model proposed in  models a marginal distribution in which states are independently distributed. For any given choice of density model $ \\rho $, the paper draws a connection between two unknowns: 1) pseudo-count function and 2) pseudo-count total.  The paper also introduces a connection between the conditional probability assigned to state $s$ using $ \\rho $ after observing its new occurrence conditioning based on its prior observations, pseudo-count function and pseudo-count total.  The notion of information gain as a popular measure of novelty and curiosity is then shown to be related to pseudo-count, which leads to the main theorem in the paper that suggests using pseudo-count bonus leads to an exploratory behaviour as good as when the information gain bonus is used. Under two major assumptions: 1) the given density model asymptotically behaves similarly to the limiting empirical distribution, and 2) the learning rate at which $\\rho$ changes with respect to the true state distribution $\\mu$ is positive in the asymptotic sense, the limit of ratios of pseudo-counts to empirical counts is finite and exists for all states.  test their proposed method in comparison with two state-of-the-art RL algorithms, Double DQN  and A3C (Asynchronous Advantage Actor-Critic)  on some of the Atari 2600 games. \nIn a subsequent work,  answer two questions regarding the modeling assumptions raised in : 1) what is the impact of the quality of density model on exploration? 2) To what extent do Monte-Carlo updates influence exploration? To address the first question,  adopt an advanced neural density model PixelCNN , and discuss the challenges involved in this approach in terms of model choice, model training and model use. PixelCNN is a convolutional neural network that models a probability distribution over pixels conditioned on the previous pixels. The paper provides a list of properties that the density model requires and subsequently suggests a suitable notion of pseudo-count for DQN agents that leads to state-of-the-art results in difficult Atari games like Montezuma's Revenge.\nFollowing the pseudo-count technique proposed by  and ,  propose a new density model to measure the similarity between states and, consequently, a generalized visitation count method. Even though  construct the density model over raw state visitations, the method proposed by  relies on the feature map used in value function approximation to construct the density model. The bonus-based exploration algorithm $\\phi$\\textit{-Exploration Bonus} proposed by  augments the extrinsic reward with the bonus term proportional to the inverse of the square root of the pseudo-count calculated based on the proposed feature-based density model. Empirically, $\\phi$\\textit{-Exploration Bonus} algorithm is evaluated against the $\\epsilon$-greedy, A3C , Double DQN , Double DQN with pseudo-count , TRPO , Gorila , and Dueling Network  baselines in five different games from the Arcade Learning Environment (ALE).\n introduce another study that uses count-based bonuses to conduct exploration in high-dimensional domains using the notions of curiosity and novelty. Effective exploration methods that are based on a notion of visitation novelty typically require either a tabular representation of states and actions or a generative model over state and actions, which can be difficult to train in high-dimensional and continuous settings.  propose an approach to approximate state visitation densities using a discriminative model (exemplar model) over the complex model of states using deep neural networks, where the classifier assigns reward bonuses if the recently visited state is novel. The authors of this work show that discriminative modeling is equivalent to implicit density estimation. They argue that learning a discriminative model using standard convolutional classifier networks in the case of rich sensory inputs like images is typically easier than learning the generative model of the environment. Their proposed model is inspired by the concept of Generative Adversarial Networks  and employs the intuition that novel states are typically more easily distinguished from all other visited states. The main idea is to maintain a density estimator using exemplar models based on a discriminatively trained classifier instead of maintaining explicit counts. To train the proposed discriminator, a cross-entropy loss is employed.  evaluate their proposed method in sparse-reward continuous high-dimensional control tasks in MuJoCo  and Vizdoom . They compare the algorithm's performance with the two state-of-the-art baseline by  (discussed in Section \\ref{sec:pred-bounus-fa}) and .\nAs an extension of count-based exploration to high-dimensional and continuous deep RL benchmarks,  use a hashing mechanism to map novel states and visited states to hash codes and subsequently count the state visitations using the corresponding hash table. In the simple domains, authors propose a static hashing approach, in which the state space is discretized using a hash function such as SimHash , and subsequently the bonus term is set to be proportional to the inverse of the square root of state count with respect to the hash code. In environments with complex structures, the authors adopt the Learned Hashing mechanism that implements an autoencoder to learn the appropriate hash codes. Like the static hash mechanism, the Learned Hashing mechanism also employs a bonus term proportional to the inverse of the square root of count on the hash codes. This approach outperforms the method presented by   in some  rllab benchmark tasks, as well as the vanilla DQN agent in some Atari 2600 games. \nInspired by the results from  and , authors of  show that the inverse of $l_1$ norm of successor representation  can be interpreted as an exploration bonus in both tabular and function approximation setting. Successor representation  can be interpreted as an implicit estimator of the transition dynamics of the environment. In the tabular setting, they augment the Sarsa  update rule with the inverse of the norm of the successor representation of the visited states. Their proposed algorithm is empirically compared with traditional Sarsa in traditional PAC-MDP domains SixArms and RiverSwim . In the function approximation case, the bonus used is similar to the one utilized in tabular setting and is the inverse of $l_1$ norm of the parameterized successor feature vector.  evaluate their proposed algorithm empirically in the Arcade Learning Environments  with sparse reward structure, including Freeway, Gravitar, Montezuma's Revenge, Private Eye, Solaris, and Venture.", "cites": [507, 1409, 1404, 7349, 8448, 1390, 1408, 8449, 1407, 1399, 8450, 1391, 1397, 1405, 1403, 1406], "cite_extract_rate": 0.6956521739130435, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of count-based exploration methods in non-tabular RL by connecting pseudo-counts, density models, and intrinsic motivation. It critically evaluates the challenges of model choice and training, and highlights the trade-offs between generative and discriminative modeling. The abstraction is strong as it identifies broader patterns in how novelty can be approximated in high-dimensional spaces using different techniques like hashing and successor representations."}}
{"id": "3963943a-74ff-4071-80fb-56e2ebd6d40a", "title": "Prediction error-based bonus", "level": "subsection", "subsections": ["d399aa1a-f5c1-41c7-8110-5e0239d3e031", "23a3e396-710b-4292-ba31-0793ae0a122a"], "parent_id": "1462d246-b21a-4fa7-9f79-e4bff1b8b853", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Prediction error-based bonus"]], "content": "In this category of exploration methods, the bonus term is computed based on the change in the agent's knowledge about the environment dynamics. The agent's knowledge about the environment is often measured through a prediction model of environment dynamics. This section focuses on the exploration techniques that use Prediction Error (PE) as an exploration bonus. PE is a term used to measure the difference between the true environment model parameters and their estimates that are used to predict transition dynamics. The methods that fall into this category use the discrepancy between the induced prediction from the learned model of environment dynamics and state-action representation models, and the real observation to assess the novelty of the visited states. The states that lead to more considerable discrepancy are considered more informative than those with a smaller discrepancy. The first two studies that employ PE as an exploration bonus to encourage curiosity are  and , which are explained in detail in Section \\ref{sec:pure} (due to the fact that they can also function in environments that do not provide extrinsic rewards).\nFormally, let $H_t$ be the history of observations until time-step $t$, $a_t$ denote the action taken at time $t$, and $M_\\phi$ be the predictive model of transition parameterized by the feature function $\\phi$. Then, the prediction error at time $t$ is proportional to, \n\\begin{align}\n    e(H_{t-1},a_t,s_{t}) \\propto \\norm{s_{t} - M_\\phi(H_{t-1},a_t)}_p,\n\\end{align}\nwhere $\\norm{.}_p$ denotes the p-norm of a given vector.\n\\begin{table}\n\\centering\n\\begin{tabular}{p{5cm}|p{5cm}|p{4cm}}\n    Reference & Approach & Performance measure \\\\\n    \\hline\n    &\\textbf{Tabular Methods}& \\\\\n    \\hline\n     & Confidence-based  & Empirical-Grid World \\\\\n     & Information gain-based  & Convergence/Empirical\\\\\n     & Confidence-based & PAC bound \\\\\n     & Confidence-based/Entropy-based & Empirical-Grid\\\\\n     & Confidence-based &  Empirical \\\\\n     & Confidence-based &  PAC bound/Empirical\\\\\n     & density-based & PAC bound/Empirical\\\\\n    \\hline\n    &\\textbf{Function Approximation Methods}& \\\\\n    \\hline\n     & Confidence based & Convergence/Empirical\\\\\n     & PE-based bonus & Empirical\\\\\n     & Information gain & Empirical\\\\\n     & PE-based bonus & Empirical\\\\\n     & PE-based bonus & Empirical\\\\\n     & PE-based bonus & Empirical\\\\ \n     & density-based & Empirical\\\\\n     & Information gain-based & Empirical\\\\\n\\end{tabular}\n\\caption{Prediction Error-based methods}\n\\label{tab:prediction-error}\n\\end{table}", "cites": [8449, 1376, 1410, 8445, 7353, 1411], "cite_extract_rate": 0.375, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of prediction error-based exploration bonuses and references several relevant papers, but it lacks meaningful synthesis of their ideas into a unified framework. There is minimal critical evaluation of the cited works, and the table of methods is presented without deeper analysis or interpretation. The content remains largely at the level of summarizing approaches without abstracting broader principles or trends."}}
{"id": "23a3e396-710b-4292-ba31-0793ae0a122a", "title": "Prediction error-based bonus: Function Approximation", "level": "subsubsection", "subsections": [], "parent_id": "3963943a-74ff-4071-80fb-56e2ebd6d40a", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Bonus-Based/Optimism-Based Exploration"], ["subsection", "Prediction error-based bonus"], ["subsubsection", "Prediction error-based bonus: Function Approximation"]], "content": "\\label{sec:pred-bounus-fa}\nIn this section, we focus on the exploration techniques that use PE as a measure of uncertainty to design exploration bonuses in domains with large or infinite state spaces, where methods that focus on tabular settings fail to generalize. \n propose a method of exploration that hinges upon a model of system dynamics trained using past experiences and observations.  A state is considered novel and accordingly receives an exploration bonus based on its disagreement with the environment's learned model. Formally, given the state encoding function $\\sigma$, the prediction error with respect to $\\sigma$ at time $t$ is denoted by $e_t(\\sigma)$ and thus the bonus term is proportional to ${e_t(\\sigma)}/{t}$. The authors benchmark their proposed algorithm on Atari tasks and show that it is an efficient method of assigning exploration bonuses for large and complex RL domains. The predictive model introduced by  is a simple two-layer neural network, and the prediction error is measured with respect to the $l_2$ Euclidean norm. They evaluate their proposed method in 14 Arcade Learning Environments (ALE) against Boltzmann, DQN and Thompson sampling methods. \nIn another diversity-driven method,  augment a diversity-based bonus with the loss function and encourage the agent to explore diverse behaviours during the training phase. The modified loss function is computed by subtracting the current policy's expected deviation or distance from a set of recently adopted policies. They use a clipping threshold in the case of observing extraordinary deviation in the computed empirical expectation. The authors evaluate the performanc of their proposed method in Mujoco and Atari environments against vanilla DDPG and the Parameter-Noise exploration method .\n conducted a large set of experiments on curiosity-driven learning algorithms that work with intrinsic reward mechanisms across 54 different environments. Interestingly, the results presented show the impact of feature learning on better generalizability while using prediction error as a deriving force for exploration. Through the conducted experiments, they also demonstrate the limitations of prediction-based bonus mechanisms.   \nSome studies measure the observed state's novelty based on the amount of PE the observed state induces on the network that is trained using the agent's past experience. For example,  introduce a simple notion of exploration bonus, which is based on the PE induced by the features of observed states and the prediction of the randomly initialized network when the environment is stochastic.  count four factors as the primary sources of error in prediction, 1) the amount of training data, 2) stochasticity of environment, 3) model misspecification, and 4) learning dynamics. The uncertainty factor considered in their study is based upon the uncertainty quantification method proposed initially by .  assess their proposed exploration method in the difficult Atari game Montezuma's Revenge and outperforms the state-of-the-art baselines. \nA line of research uses information gain (IG) as an exploration bonus (For a more detailed explanation regarding information gain, refer to section \\ref{sec:intrinsic}). For instance,  propose a curiosity-driven strategy, which uses information gain as a driving force to encourage exploration of actions that lead to states that cause a larger change in the agent’s internal model of environment dynamics. The state and action spaces are considered to be continuous. The paper proposes a variational approach to approximate the true posterior, and therefore, the information gain is measured using the KL-divergence between the agent’s internal belief over environment dynamics at different time steps. The main challenge in their proposed model is the computation of variational lower-bound. The way  compute variational lower-bound, requires the calculation of the posterior probability, which is generally considered to be computationally intractable. The computed variational lower-bound is used to measure the agent’s curiosity.  assess their proposed algorithm in continuous Mujoco domains, and compare its performance with TRPO, ERWR and REINFORCE. \nAnother study that uses IG as an exploration bonus is introduced by . The authors apply the information bottleneck (IB) principle  to design an exploration bonus to handle the exploration-exploitation trade-off, particularly in distractive environments. The bonus term in Curiosity-Bottleneck (CB) objective (inspired by IB principle) appears in the form of mutual information, measured by KL-divergence between the latent representation of the environment and the input observation. To inspect the performance of the proposed CB method,  perform experiments on three environments: 1) Novelty detection on MNIST and Fashion-MNIST, 2) Treasure Hunt in a grid-world environment, and 3) Atari's Gravitar, Montezuma’s Revenge, and Solaris  games. The CB performance is compared with the work of .", "cites": [8449, 9111, 1376, 1410, 7353, 1411, 1412], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers that use prediction error and information gain for exploration, connecting methods like RND, VIME, and CB under a coherent theme of uncertainty estimation in function approximation. It also provides critical analysis by highlighting limitations of prediction-based bonuses and computational challenges in variational approaches. The abstraction is strong, as it generalizes these methods into broader categories and identifies common principles such as novelty-driven and diversity-driven exploration."}}
{"id": "fdc29b7c-2327-4e2c-98ae-5e181e7438c1", "title": "Probability Matching", "level": "section", "subsections": ["95acb24d-dd09-4d3d-91ce-282d0b382e85"], "parent_id": "8f48f526-7a68-424e-9375-8e2fdaf1b297", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Probability Matching"]], "content": "\\label{sec:thompson}\nAn entire body of algorithms for efficient exploration is inspired by the Probability Matching approach, also known as Thompson Sampling .\nProbability matching (or Thompson Sampling)  is a heuristic for balancing the  exploration-exploitation dilemma in the Multi-Arm Bandit setting . In this setting, the agent maintains a posterior distribution over its beliefs regarding the optimal action, but instead of selecting the action with the highest expected return according to the belief posterior, the agent selects the action randomly according to the probability with which it deems that action to be optimal. This approach uses the variance of the posterior to induce randomization and incentivizes the exploration of uncertain states and actions. As more experience is gathered, the variance of the posterior will decrease and concentrate on the true value. Thompson sampling is provably efficient for the bandit setting .\nWe use the setting from  to give an example of the Thompson Sampling algorithm for Bernoulli bandit setting, i.e. when the agent gets a binary reward (0 or 1) for selecting an arm $i$, and the probability of success is $\\mu_i$. The algorithm maintains Bayesian priors on the Bernoulli means $\\mu_i$. The algorithm initially assumes arm $i$ to have a uniform prior on $\\mu_i$ ($\\texttt{Beta}(1,1)$). At time $t$, having observed $S_i(t)$ successes and $F_i(t)$ failures plays of the arm $i$, the algorithm corresponding updates distribution on $\\mu_i$ as $ \\texttt{Beta}(S_i(t) + 1, F_i(t) + 1)$. The algorithm then samples the model of the means from these posterior distributions of the $\\mu_i$'s and plays an arm according to the probability of its mean being the largest.", "cites": [9090], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of the Probability Matching approach and Thompson Sampling in the Multi-Arm Bandit setting. However, it lacks synthesis of broader literature, does not engage in critical evaluation of the method or its limitations, and offers only minimal abstraction beyond specific algorithmic mechanics."}}
{"id": "95acb24d-dd09-4d3d-91ce-282d0b382e85", "title": "Posterior Sampling for Reinforcement Learning (PSRL)", "level": "paragraph", "subsections": ["ae393174-0672-4d61-ad9e-a1da105460c4"], "parent_id": "fdc29b7c-2327-4e2c-98ae-5e181e7438c1", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Probability Matching"], ["paragraph", "Posterior Sampling for Reinforcement Learning (PSRL)"]], "content": "\\textit{Bayesian dynamic programming} was first introduced in  and is more recently known as \\textit{posterior sampling for reinforcement learning} (PSRL) . PSRL can be thought of as an extension of the Thompson Sampling algorithm to the RL setting with finite state and action spaces.  \nCompared to Thompson Sampling, where a model is re-sampled at every time-step\\footnote{In the bandit setting the length of an episode is 1 time-step.}, PSRL samples a single model for an episode and follows this policy for the duration of the episode.\nIn PSRL, the agent starts with a prior belief over the model of the MDP \nand then proceeds to update its full posterior distribution over models with the newly observed samples. For each episode, a model hypothesis is then sampled from this distribution, and traditional planning techniques are used to solve the MDP and obtain the optimal value function. For the current episode, the agent follows the greedy policy with respect to the optimal value function. They evaluate their approach on a 6-state chain MDP with 3 actions and a random MDP with 10 state and 5 actions. They show that PSRL outperforms UCRL2 by a large margin in both the above domains.  \nAlthough, both categories of the methods maintain a distribution over the rewards and transition dynamics obtained using a Bayesian modeling approach, PSRL based methods employ the posterior sampling exploration algorithm that requires solving for an optimal policy for a single MDP in each iteration. As such, PSRL is more computationally efficient compared to typical Bayesian-Adaptive algorithms that find optimal exploration strategy via either dynamic programming or tree look-ahead in the Bayesian belief state space over a set of a prior distribution over MDPs.\n\\textit{Best of Sampled Set} (BOSS)  drives exploration by sampling multiple models from the posterior and combining them to select actions optimistically.   The proposed algorithm resembles RMAX  in the sense that it samples multiple models from the posterior only when the number of transitions from a state-action pair exceeds a certain threshold. The sampled models are then merged into an optimistic MDP which is solved to select the best action. They show that this approach leads to sufficient exploration to guarantee finite-sample performance guarantees.  They compare their approach against BEETLE and RMAX and show superior results on the 5-state chain problem and 6x6 grid-world.\n propose an algorithm based on posterior sampling that achieves near-optimal worst-case regret bounds when the underlying MDP is communicating with (unknown) finite diameter. The diameter $D$ is defined as an upper bound on the time it takes to move from any state $s$ to any other $s'$ using an appropriate policy, for each pair of $s, s'$. The algorithm combines the optimism in the face of uncertainty principle (Section~\\ref{Bonus}) with the posterior sampling heuristic. The algorithm proceeds in epochs, where, at the beginning of each epoch the algorithm generates $\\psi = \\Tilde{O}(S)$ sample transition probability vectors from a posterior distribution for every state and action. It then proceeds to solve the extended MDP with $\\psi A$ actions and $S$ states formed using these samples. The optimal policy found from the extended MDP is then used for the entire epoch. This algorithm can be viewed as a combination of methodologies from BOSS and PSRL algorithms described above. The main contribution of this work is providing tighter regret bounds, and as such, they do not provide any experimental results for their algorithm.", "cites": [7354], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear synthesis of PSRL and related methods such as BOSS, RMAX, and BEETLE, effectively connecting their ideas and showing how they build upon or differ from one another. It offers a critical comparison between PSRL and other Bayesian-adaptive approaches by highlighting computational efficiency and exploration strategies. While it identifies some broader patterns (e.g., the use of posterior sampling and optimism), it stops short of deeper meta-level insights or novel frameworks."}}
{"id": "ae393174-0672-4d61-ad9e-a1da105460c4", "title": "Randomized Value Functions", "level": "paragraph", "subsections": [], "parent_id": "95acb24d-dd09-4d3d-91ce-282d0b382e85", "prefix_titles": [["title", "A Survey of Exploration Methods in Reinforcement Learning"], ["section", "Probability Matching"], ["paragraph", "Posterior Sampling for Reinforcement Learning (PSRL)"], ["paragraph", "Randomized Value Functions"]], "content": "The PSRL approach is limited to the finite state and action setting, where learning the model and planning might be tractable. For the rest of the section, we will look at the approaches that aren't based on modeling the MDP transition and rewards explicitly, but instead focus on estimating distributions over the value functions directly. The underlying assumption of these approaches is that approximating the posterior distribution for the value function is more statistically and computationally efficient than learning the MDP.  proposed a family of methods called \\textit{Randomized Value Functions} (RVFs) in order to improve the scalability of PSRL. At an abstract level, RVFs can be interpreted as a model-free version of PSRL. These methods directly model a distribution over the value functions instead of over MDPs. The agent then works by sampling a randomized value function at the beginning of each episode and following that for the rest of the episode.  Exploring with dithering strategies (Sec.~\\ref{sec:Random},~\\ref{sec:stochastic},~ \\ref{sec:value-policy-search}), is inefficient as the agent may oscillate back and forth, it might not be able to discover temporally extended interesting behaviours. On the other hand, exploring with Randomized Value Functions, the agent is committed to a randomized but internally consistent strategy for the entire length of the episode. The switch to value function modelling also facilitates the use of function approximation. \nIn order to scale posterior sampling approach to large MDPs with \\textit{linear} function approximation,   introduce \\textit{Randomized Least Square Value Iteration (RLSVI)} that involves using Bayesian linear regression for learning the value function. \nThe goal is to extend PSRL to value function learning, that would involve maintaining a belief distribution over candidates for the optimal value function. Before each episode, the agent would then sample a value function from its posterior distribution and then apply the associated greedy policy throughout the episode. \nLeast Square Value Iteration (LSVI)  performs a linear regression for the Bellman error at each timestep - similar to Fitted Q-Iteration  . As the value function learned from LSVI has no notion of uncertainty, algorithms based on just LSVI have to rely on other exploration strategies, like blind exploration (Sec.~\\ref{sec:Random}). \nRLSVI also performs linear regression for one-step Bellman error but it incorporates a Gaussian uncertainty estimate for the resultant value function. This is equivalent to replacing the linear regression step of LSVI with a Bayesian linear regression as if the one-step Bellman error was sampled from a Gaussian distribution. Even though this is not the correct Bayesian distribution,  show that it is still useful for approximating the uncertainty. As the RLSVI updates the value function based on a random sample from this distribution, the resultant value function is also a random sample from the approximate posterior. RLSVI is a provably efficient algorithm for exploration in large MDPs with linear value function approximators .\nThe authors compare their approach with the dithering based strategies in a didactic chain environment where RLVSI is able to scale up to 100 state length chain. They also show better learning performance of RLVSI compared LSPI and LSVI on learning to play Tetris task, and a recommendation engine task, both of which have exponential state space but they have access to the appropriate basis functions for the task.\nOne of the problems with this approach is that the distributions over the value functions can be as complex to represent as distributions over transition model, and exact Bayesian inference might not be computationally tractable. RLSVI does not explicitly maintain and update belief distributions, as a coherent Bayesian method would, but still serves as a computationally tractable method for sampling value functions. \n\\begin{table}\n    \\centering\n    \\begin{tabularx}{\\columnwidth}{p{0.25\\textwidth}|p{0.4\\textwidth}|p{0.35\\textwidth}}\n        Approach & Posterior Sampling & Theoretical properties \\\\\n        \\hline\n         & Sample 1 MDP model per episode  & Bounded Expected regret \\newline $\\Tilde{O}(HS\\sqrt{AT})$, \\newline Bounded Bayesian regret \\newline $\\Tilde{O}(H\\sqrt{SAT})$\n        \\\\\n         & Sample $K$ MDP models per step & PAC-MDP\n        \\\\\n         & Sample 1 value function per episode & Bounded Expected regret \\newline $\\Tilde{O}(\\sqrt{H^3SAT})$\n        \\\\\n         & Sample $\\Tilde{O}(S)$ transition probability vectors per epoch &  Bounded Worst-case regret \\newline $\\Tilde{O}(D\\sqrt{SAT})$\n        \\\\\n         & Sample 1 head Q-network from the bootstrap ensemble per episode & - \\\\ \n         & Sample weights for last layer of Q-network per episode & - \\\\\n         & Sample noise variables for the normalizing flow per episode & - \\\\\n         & Sample weights for the Q-network based on the successor features for every episode & - \\\\\n    \\end{tabularx}\n    \\caption{Overview of the main techniques covered in Section~\\ref{sec:thompson}. For the theoretical properties column, $S$ and $A$ denote the cardinalities of the state and action spaces, $T$ denotes time elapsed, $H$ denotes the episode duration, and $D$ denotes the diameter.\n    }\n    \\label{tab:thompson}\n\\end{table}\n propose bootstrapped Q-learning, an RVF based approach, as an extension of RLSVI to nonlinear function approximators. Bootstrapped-DQN consists of a simple non-parametric bootstrap\\footnote{Bootstrap uses the empirical distribution of a sampled dataset as an estimate of the population statistic.} with random initialization to generate approximate posterior samples over Q-values. This technique helps in the scenario where exact Bayesian inference is intractable, such as in deep networks. Bootstrapped-DQN consists of a network with $K$ bootstrapped estimates of the Q-function, trained in parallel. This means that each $Q_1, \\dots, Q_K$ provide a temporally extended (and consistent) estimate of the value uncertainty. At the start of each episode, the agent samples one head, which it follows for the duration of the episode. Bootstrapped-DQN is a non-parametric approach to uncertainty estimation. The authors also show that, when used with deep neural networks, the bootstrap can produce reasonable estimates of uncertainty. They compare their method against DQN  on the didactic chain MDP with 100 states, and on the Atari domain on the Arcade Learning Environment , where they show that Bootstrapped-DQN is able to learn faster and also improves the final score in most of the games.\nMany other exploration methods, such as  can be interpreted as combining the concept of RVF with neural network function approximation. It allows these methods to scale to high-dimensional problems, such as Atari domain , that otherwise might be too computationally extensive for PSRL. However, the approximations introduced in these works come with trade-offs that are not present in the original PSRL work.  Specifically, because a value function is defined with respect to a particular policy, constructing posterior over the value functions requires selection of a reference policy or distribution over policies.  \nHowever, in practice, the above methods do not enforce any explicit structure or dependencies.  propose Successor Uncertainties (SU), a scalable and computationally cheaper (compared to Bootstrapped DQN) model-free exploration algorithm that retains the key elements of the PSRL.  SU models the posterior over rewards and state transitions directly and derives the posterior over the value functions analytically, thereby ensuring the posterior over the values estimates matches the posterior sampling policy. Empirically, SU performs much better on hard tabular didactic chain problem where the algorithm scales up to chains of length 200 states. On the Atari domain, SU outperforms the closest RVF algorithm - Bootstrapped-DQN on 36 of 49 games.", "cites": [1401, 1415, 1416, 7354, 1413, 1407, 1414, 9091], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative on Randomized Value Functions as a scalable alternative to PSRL. It critically discusses limitations such as the complexity of posterior distributions and the inexact Bayesian nature of RLSVI. The section also abstracts the core idea of using uncertainty in value functions to guide exploration, while contrasting it with dithering and other model-based approaches."}}
