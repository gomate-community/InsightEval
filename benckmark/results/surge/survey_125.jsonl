{"id": "1978fd93-ec47-4f73-b68a-5ad7017384f0", "title": "Introduction", "level": "section", "subsections": ["925a6e17-12dc-4a97-bc86-dbb3a6df52b1"], "parent_id": "84c962b5-92c2-4d52-a6ce-d3060c601e98", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Introduction"]], "content": "Transformers~ are a formidable force in the modern deep learning stack. Transformers are pervasive and have made tremendous impact in many fields such as language understanding~ and image processing~. As such, it is only natural that a wealth of research has been dedicated to making fundamental improvements to the model over the past few years~. This immense interest has also spurred research into more efficient variants of the model~.\nThere has been such a surge of Transformer model variants proposed recently, that researchers and practitioners alike may find it challenging to keep pace with the rate of innovation. As of this writing and this manuscript's first draft (circa August 2020), there have been nearly a dozen new efficiency-focused models proposed in just the past 6 months. Thus, a survey of the existing literature is both beneficial for the community and quite timely.\nThe self-attention mechanism is a key defining characteristic of Transformer models. The mechanism can be viewed as a graph-like inductive bias that connects all tokens in a sequence with a relevance-based pooling operation. A well-known concern with self-attention is the quadratic time and memory complexity, which can hinder model scalability in many settings. There has been an overwhelming influx of model variants proposed recently that address this problem. We hereinafter name this class of models \\textit{``efficient Transformers''}. \nThe \\emph{efficiency} of a model can be interpreted in a variety of ways. It might refer to the memory footprint of the model, which is of importance when the memory of accelerators on which the model is running is limited. Efficiency might also refer to computational costs, e.g. the number of FLOPs, both during training and inference. In particular, for on-device applications, models often must operate within a highly constrained computational budget. Throughout this survey, we refer to the efficiency of Transformers both in terms of memory and computation. We are especially interested in how such models perform when they are applied to large inputs.  \nEfficient self-attention models are crucial in applications that model long sequences. For example, documents, images, and videos are all often composed of a relatively large number of pixels or tokens. Efficiency in processing long sequences is therefore paramount for widespread adoption of Transformers.\nThis survey sets out to provide a comprehensive overview of the recent advances made in this class of models. We are primarily interested in modeling advances and architectural innovations that improve the general efficiency of Transformers, including but not limited to tackling the quadratic complexity issue of the self-attention mechanism or reducing the computation costs by means such as pooling and/or sparsity. We also briefly discuss general improvements and other efficiency improvements such as parameter sharing.\nWe propose a taxonomy of efficient Transformer models, characterizing them by their technical innovation and primary use case. Specifically, we review Transformer models that have applications in both language and vision domains, attempting to consolidate the literature across the spectrum. We also provide a detailed walk-through of many of these models and draw connections between them.", "cites": [679, 1465, 1473, 7360, 798, 38, 7333, 7298, 1488, 7, 4730, 1453, 8384, 1472, 4731, 9, 1461], "cite_extract_rate": 0.8947368421052632, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a wide range of cited works to establish a coherent narrative around the importance and challenges of efficient Transformers. It abstracts the common theme of addressing quadratic complexity and introduces a taxonomy for organizing the literature. While it touches on limitations (e.g., quadratic complexity), the critical analysis is moderate and not deeply evaluative of specific approaches."}}
{"id": "46766dd5-31e6-4f8f-8d8d-17f539d3c014", "title": "Author notes on the updated version (March 2022)", "level": "paragraph", "subsections": [], "parent_id": "925a6e17-12dc-4a97-bc86-dbb3a6df52b1", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Introduction"], ["paragraph", "Author notes on the updated version (December 2021)"], ["paragraph", "Author notes on the updated version (March 2022)"]], "content": "We wanted to post the update to arxiv in Jan but forgot about it. We lightly revised it again in Mar by adding newer SOTA sparse models such as ST-MoE-32B .\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/transformer_arch.pdf}\n        \\vspace{-2em}\n    \\label{fig:transformer_arch}\n\\caption{Architecture of the standard Transformer~}\n\\label{stats}\n\\end{figure}", "cites": [38], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis, critical analysis, or abstraction. It merely mentions the addition of newer models without elaborating on their significance, comparing them to existing ones, or situating them within a broader framework. The only cited paper is mentioned in passing, with no deeper integration or discussion."}}
{"id": "d93522a9-7156-4a25-9283-15966d4a97a9", "title": "Background on Transformers", "level": "section", "subsections": ["2401a3ad-badb-490a-aa57-1f203fac9801", "a8d1ee9c-6011-42fd-8ebc-c270b514e543", "3e948159-1fd5-4d90-bdf1-7c0775042d3d", "bf77b2c0-220f-4f15-ad9a-4b0683e44d0f", "7b7010c5-4356-460b-b201-085f8da41a41", "0c503c99-673c-4fd8-b89d-b8e2ac45d86a"], "parent_id": "84c962b5-92c2-4d52-a6ce-d3060c601e98", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Background on Transformers"]], "content": "This section provides an overview of the well-established Transformer architecture~. Transformers are multi-layered architectures formed by stacking Transformer blocks on top of one another. \nTransformer blocks are characterized by a multi-head self-attention mechanism, a position-wise feed-forward network, layer normalization~ modules and residual connectors. The input to the Transformer model is often a tensor of shape $\\reals^B \\times \\reals^N$, where $B$ is the batch size, $N$ the sequence length.\nThe input first passes through an embedding layer that converts each one-hot token representation into a $d_{model}$ dimensional embedding, i.e., $\\reals^B \\times \\reals^N \\times \\reals^{d_{model}}$. The new tensor is then additively composed with positional encodings and passed through a multi-headed self-attention module. Positional encodings can take the form of a sinusoidal input (as per~) or be trainable embeddings.\nThe inputs and output of the multi-headed self-attention module are connected by residual connectors and a layer normalization layer. The output of the multi-headed self-attention module is then passed to a two-layered feed-forward network which has its inputs/outputs similarly connected in a residual fashion with layer normalization. The sub-layer residual connectors with layer norm is expressed as:\n\\begin{align*}\nX = \\text{LayerNorm}(F_S(X)) + X    \n\\end{align*}\nwhere $F_S$ is the sub-layer module which is either the multi-headed self-attention or the position-wise feed-forward layers.", "cites": [57, 38], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual overview of the Transformer architecture and its components, drawing on the foundational paper (Attention Is All You Need) and the Layer Normalization paper. However, it primarily describes these components without synthesizing them into a broader narrative or offering critical analysis of their strengths and limitations. There is minimal abstraction or insight beyond the concrete structure of the model."}}
{"id": "bf77b2c0-220f-4f15-ad9a-4b0683e44d0f", "title": "On the compute cost of Transformers", "level": "subsection", "subsections": [], "parent_id": "d93522a9-7156-4a25-9283-15966d4a97a9", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Background on Transformers"], ["subsection", "On the compute cost of Transformers"]], "content": "The computation costs of Transformers is derived from multiple factors. Firstly, the memory and computational complexity required to compute the attention matrix is quadratic in the input sequence length, i.e., $N \\times N$. In particular, the $QK^\\top$ matrix multiplication operation alone consumes $N^2$ time and memory. This restricts the overall utility of self-attentive models in applications which demand the processing of long sequences. Memory restrictions are tend to be applicable more to training (due to gradient updates) and are generally of lesser impact on inference (no gradient updates). The quadratic cost of self-attention impacts speed$\\footnote{We would like to emphasize that complexity does not always translate to real world throughput or latency. A model of linear complexity can be slower than a model with quadratic complexity in practice.}$ in both training and inference. The compute costs of the self-attention mechanism contributes partially to the overall compute cost of the Transformer. A non-trivial amount of compute still stems from the two layer feed-forward layers at every Transformer block (approximately half the compute time and/or FLOPs). The complexity of the FFN is linear with respect to sequence length but is generally still costly. Hence, a large portion of recent work have explored sparsity~ as a means to scale up the FFN without incurring compute costs. Efficient attention and efficient models are generally orthogonal - although some efficient attention methods explicitly aim to reduce the sequence length~ and as a result also save computation costs in both aspects. Efficiency and computational costs is generally a complicated affair and we would suggest readers peruse~ for more details on trade-offs, intricacies etc.", "cites": [1488, 707, 4732, 8454], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers, such as the quadratic cost of attention and the use of sparsity in FFNs (as in Switch Transformers and The Efficiency Misnomer). It provides a coherent narrative by linking the computational costs of different components. However, it lacks deeper critical evaluation of the approaches or their trade-offs, and while it begins to generalize around efficiency patterns, it stops short of offering a meta-level framework or novel insight."}}
{"id": "7b7010c5-4356-460b-b201-085f8da41a41", "title": "Transformer Mode", "level": "subsection", "subsections": [], "parent_id": "d93522a9-7156-4a25-9283-15966d4a97a9", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Background on Transformers"], ["subsection", "Transformer Mode"]], "content": "It is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) \\emph{encoder-only} (e.g., for classification), (2) \\emph{decoder-only} (e.g., for language modeling), and (3) \\emph{encoder-decoder} (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder.\nThis influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the self-attention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs.\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT~, this is a \\textit{[CLS]} token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5~. Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation~. A decoder-only model needs to be causal and a upper triangular mask needs to be applied to prevent tokens from peeping into the future. We refer interested readers to~ for more detailed descriptions of the various Transformer modes.", "cites": [679, 7, 9], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of the different Transformer modes (encoder-only, decoder-only, encoder-decoder) and briefly references how specific models like BERT and T5 utilize these. It integrates some information from the cited papers, particularly in describing encoder-only and encoder-decoder applications, but lacks deeper synthesis, critical evaluation, or abstraction into broader principles."}}
{"id": "0c503c99-673c-4fd8-b89d-b8e2ac45d86a", "title": "Applications", "level": "subsection", "subsections": [], "parent_id": "d93522a9-7156-4a25-9283-15966d4a97a9", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Background on Transformers"], ["subsection", "Applications"]], "content": "Transformers have a wide range of applications ranging from language to vision, speech and reinforcement learning. It was initially introduced within the context of sequence to sequence machine translation in NLP. Following which, most of the applications of Transformers have been within the context of language - given the concurrent advance of pretrained models such as BERT~. Many early improvements to this line of efficient transformers is therefore focused on language processing applications~. For historical reasons, this survey paper leans slightly towards language. However, it is also worth noting that a substantial amount of papers considered in our survey also considers multimodal applications whereby a sequence processor is required. For example  considers generative modeling task on images or other modalities such as proteins. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/Venn_Transformers_new.pdf}\n    \\caption{Taxonomy of Efficient Transformer Architectures.}\n    \\label{fig:taxonomy}\n\\end{figure}", "cites": [1453, 8384, 7298, 793, 1473, 7], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes the breadth of Transformer applications, focusing primarily on language and mentioning a few others like vision and protein modeling. It integrates minimal information from the cited papers, offering only superficial connections. There is no critical evaluation or abstraction into broader principles or trends."}}
{"id": "331ef788-f428-4379-af9c-107d866ac486", "title": "A Survey of Efficient Transformer Models", "level": "section", "subsections": ["dce52792-e50d-4180-8ffb-06888cef96b1", "1068ae2e-644b-469f-bc75-6922be1837a6"], "parent_id": "84c962b5-92c2-4d52-a6ce-d3060c601e98", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"]], "content": "In this section, we provide a high-level overview of efficient Transformer models. We begin by presenting a characterization of the different models. Table~\\ref{tab:summarytable} lists the efficient Transformers released to date while Figure \\ref{fig:taxonomy} presents a graphical overview of several key efficient Transformer models. \n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c|l}\n    \\hline\n       Model / Paper  &  Complexity & Decode & Class\\\\\n       \\hline\n       Memory Compressed~ & $\\mathcal{O}(N_c^2)$& $\\checkmark$ &  FP+M \\\\\n        Image Transformer~ & $\\mathcal{O}(N.m)$& $\\checkmark$ & FP \\\\\n        Set Transformer~ & $\\mathcal{O}(kN)$& $\\text{\\xmark}$  & M\\\\ \n         Transformer-XL~ & $\\mathcal{O}(N^2)$ & $\\checkmark$ & RC \\\\ \n        Sparse Transformer   ~ & $\\mathcal{O}(N \\sqrt{N})$ &  $\\checkmark$& FP\\\\\n        Reformer~ & $\\mathcal{O}(N \\log N)$ & $\\checkmark$ & LP\\\\\n        Routing Transformer~ & $\\mathcal{O}(N\\sqrt{N)}$ & $\\checkmark$ & LP \\\\\n         Axial Transformer~ & $\\mathcal{O}(N \\sqrt{N})$ & $\\checkmark$ & FP \\\\\n         Compressive Transformer~ & $\\mathcal{O}(N^2)$ & $\\checkmark$& RC \\\\\n        Sinkhorn Transformer~ & $\\mathcal{O}(B^2)$ & $\\checkmark$  & LP \\\\\n        Longformer~ & $\\mathcal{O}(n(k+m))$ & $\\checkmark$ & FP+M\\\\ \n        ETC~ & $\\mathcal{O}(N_g^2 + N N_g)$ &  $\\text{\\xmark}$ & FP+M\\\\ \n        Synthesizer~ & $\\mathcal{O}(N^2)$ & $\\checkmark$ & LR+LP\\\\\n          Performer~ & $\\mathcal{O}(N)$ & $\\checkmark$ & KR\\\\\n        Funnel Transformer~ & $\\mathcal{O}(N^2)$ & $\\checkmark$ & FP+DS\\\\\n           Linformer~  & $\\mathcal{O}(N)$ & $\\text{\\xmark}$ & LR \\\\\n        Linear Transformers ~ & $\\mathcal{O}(N)$ & $\\checkmark$ & KR \\\\\n        Big Bird~ &$\\mathcal{O}(N)$ &$\\text{\\xmark}$ & FP+M\\\\\n        Random Feature Attention~ & $\\mathcal{O}(N)$ & $\\checkmark$ & KR\\\\\n        Long Short Transformers~ &$\\mathcal{O}(kN)$ & $\\checkmark$ & FP + LR \\\\ \n        Poolingformer~ & $\\mathcal{O}(N)$ & $\\text{\\xmark}$   & FP+M \\\\\n        Nystr\\\"{o}mformer~ & $\\mathcal{O}(kN)$ &  $\\text{\\xmark}$ & M+DS\\\\\n          Perceiver~ &$\\mathcal{O}(kN)$ &$\\checkmark$ & M+DS \\\\ \n        Clusterformer~ & $\\mathcal{O}(N\\log N)$ & $\\text{\\xmark}$ & LP \\\\\n        Luna~ &  $\\mathcal{O}(kN)$ &$\\text{\\checkmark}$ & M\\\\\n        TokenLearner~ & $\\mathcal{O}(k^2)$ & $\\text{\\xmark}$ &DS \\\\\n         \\hline\n        Adaptive Sparse Transformer~ & $\\mathcal{O}(N^2)$ & $\\text{\\checkmark}$ & Sparse \\\\\n        Product Key Memory~ & $\\mathcal{O}(N^2)$ & $\\checkmark$& Sparse\\\\\n        Switch Transformer~ & $\\mathcal{O}(N^2)$& $\\checkmark$ & Sparse\\\\\n        ST-MoE~ & $\\mathcal{O}(N^2)$& $\\checkmark$ & Sparse\\\\\n        GShard~ & $\\mathcal{O}(N^2)$ & $\\checkmark$ & Sparse \\\\\n        Scaling Transformers~ & $\\mathcal{O}(N^2)$ &$\\checkmark$ & Sparse\\\\\n        GLaM~  & $\\mathcal{O}(N^2)$ &$\\checkmark$ & Sparse\\\\ \n        \\hline\n    \\end{tabular}\n    \\caption{Summary of Efficient Transformer Models. Models in the first section are mainly efficient attention methods. Models in the subsequent lower section generally refer to sparse models. Class abbreviations include: FP = Fixed Patterns or Combinations of Fixed Patterns, M = Memory, LP = Learnable Pattern, LR = Low-Rank, KR = Kernel RC = Recurrence, and DS = Downsampling. Furthermore, $N$ generally refers to the sequence length and $B$ is the local window (or block) size. $N_g$ and $N_c$ denote global model memory length and convolutionally-compressed sequence lengths respectively.}\n    \\label{tab:summarytable}\n\\end{table}", "cites": [1484, 4735, 7840, 707, 1473, 1494, 798, 7366, 1133, 1503, 1182, 1482, 7333, 1490, 7298, 793, 1488, 4733, 1453, 4731, 7370, 4734, 8454, 1499], "cite_extract_rate": 0.7741935483870968, "origin_cites_number": 31, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, summarizing a list of efficient Transformer models with their complexity, decoding capability, and classification. There is minimal synthesis of ideas or cross-model connections, and no critical evaluation or comparison of their approaches. The abstraction is limited to a classification system, but broader insights or principles are not explored."}}
{"id": "dce52792-e50d-4180-8ffb-06888cef96b1", "title": "A Taxonomy of Efficient Transformers", "level": "subsection", "subsections": [], "parent_id": "331ef788-f428-4379-af9c-107d866ac486", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "A Taxonomy of Efficient Transformers"]], "content": "This section outlines a general taxonomy of efficient Transformer models, characterized by their core techniques and primary use case. While, the primary goal of most of these models is to improve the memory complexity if the self-attention mechanism, we also include methods that improve the general efficiency of the Transformer architecture.\n\\begin{itemize}\n\\item \\textbf{Fixed Patterns (FP)} - The earliest modifications to self-attention simply sparsifies the attention matrix by limiting the field of view to fixed, predefined patterns such as local windows and block patterns of fixed strides.\n\\begin{itemize}\n    \\item \\textbf{Blockwise Patterns} The simplest example of this technique in practice is the blockwise (or chunking) paradigm which considers blocks of local receptive fields by chunking input sequences into fixed blocks. Examples of models that do this include Blockwise~ and/or Local Attention~. Chunking input sequences into blocks reduces the complexity from $N^2$ to $B^2$ (block size) with $B<<N$, significantly reducing the cost. These blockwise or chunking methods serve as a basis for many more complex models. \n    \\item \\textbf{Strided Patterns} Another approach is to consider strided attention patterns, i.e., only attending at fixed intervals. Models such as Sparse Transformer~ and/or Longformer~ employ strided or ``dilated'' windows.\n    \\item \\textbf{Compressed Patterns} - Another line of attack here is to use some pooling operator to down-sample the sequence length to be a form of fixed pattern. For instance, Compressed Attention~ uses strided convolution to effectively reduce the sequence length.\n\\end{itemize}\n\\item \\textbf{Combination of Patterns (CP)} - The key idea of combined\\footnote{We note that this is also often referred to as factorization approaches, e.g., in~. We decide to refer to this class of models as combination approaches because (1) it is a better fit to what these models are actually doing and (2) to avoid confusion with matrix factorization or low-rank approaches. } approaches is to improve coverage by combining two or more distinct access patterns. For example, the Sparse Transformer~ combines strided and local attention by assigning half of its heads to each pattern. Similarly, Axial Transformer~ applies a sequence of self-attention computations given a high dimensional tensor as input, each along a single axis of the input tensor. In essence, the combination of patterns reduces memory complexity in the same way that fixed patterns does. The difference, however, is that the aggregation and combinaton of multiple patterns improves the overall coverage of the self-attention mechanism.\n\\item \\textbf{Learnable Patterns (LP)} - An extension to fixed, pre-determined pattern are \\emph{learnable} ones. Unsurprisingly, models using learnable patterns aim to learn the access pattern in a data-driven fashion. A key characteristic of learning patterns is to determine a notion of token relevance and then assign tokens to buckets or clusters~. Notably, Reformer~ introduces a hash-based similarity measure to efficiently cluster tokens into chunks. In a simlar vein, the Routing Transformer~ employs online $k$-means clustering on the tokens. Meanwhile, the Sinkhorn Sorting Network~ exposes the sparsity in attention weights by learning to to sort blocks of the input sequence. In all these models, the similarity function is trained end-to-end jointly with the rest of the network. The key idea of learnable patterns is still to exploit fixed patterns (chunked patterns). However, this class of methods learns to sort/cluster the input tokens - enabling a more optimal global view of the sequence while maintaining the efficiency benefits of fixed patterns approaches.\n\\item \\textbf{Neural Memory} - Another prominent method is to leverage a learnable side memory module that can access multiple tokens at once. A common form is \\emph{global} neural\\footnote{We use the term neural here to refer to a representation-like memory that is often manifested in the model.} memory which is able to access the entire sequence. The global tokens act as a form of model memory that learns to gather from input sequence tokens. This was first introduced in Set Transformers~ as the \\textit{inducing points} method. These parameters are often interpreted as ``memory'' and are used as a form of \\textit{temporary} context for future processing. This can be thought of as a form of parameter attention~. Global memory tokens are also used in ETC~ and Longformer~. With a limited amount of neural memory (or inducing points), we are able to perform a preliminary \\textit{pooling}-like operation of the input sequence to compress the input sequence - a neat trick to have at one's disposal when designing efficient self-attention modules.\n\\item \\textbf{Low-Rank Methods} - Another emerging technique is to improve efficiency by leveraging low-rank approximations of the self-attention matrix. The key idea is to assume low-rank structure in the $N\\times N$ matrix. The Linformer~ is a classic example of this technique, as it projects the length dimension of keys and values to a lower-dimensional representation ($N \\rightarrow k$). It is easy to see that the low-rank method ameliorates the memory complexity problem of self-attention because the $N \\times N$ matrix is now decomposed to $N \\times k$.\n\\item \\textbf{Kernels} - Another recently popular method to improve the efficiency of Transformers is to view the attention mechanism through kernelization. The usage of kernels~ enable clever mathematical re-writing of the self-attention mechanism to avoid explicitly computing the $N \\times N$ matrix. Since kernels are a form of approximation of the attention matrix, they can be also viewed as a type of low-rank approach~. Examples of recent work in this area include Performers, Linear Transformers and Random Feature Attention (RFA,~)\n\\item \\textbf{Recurrence} - A natural extension to the blockwise method is to connect these blocks via recurrence. Transformer-XL~ proposed a segment-level recurrence mechanism that connects multiple segments and blocks. These models can, in some sense, be viewed as \\textit{fixed pattern} models. However, we decided to create its own category due to its deviation from other block / local approaches.\n\\item \\textbf{Downsampling} - Another popular method of reducing computation cost is to reduce the resolution of the sequence, hence reducing computation costs by a commensurate factor. Examples of this class of models include Perceiver~, Funnel Transformers~, Swin Transformer~, and Charformer~ models. Notably, there might also be some form of overlap of this class of models with models that leverage \\textit{memory} tokens as models such as Set Transformer can also be viewed as a form of downsampling, albeit within the attention mechanism. The recent Nystr\\\"{o}mformer~, on the surface, may seem like a low-rank or kernal-based approach. However, it is actually a downsampling approach where the \\textit{`landmarks`} are simply strided based pooling - in similar spirit to Set Transformer, Funnel Transformer or Perceiever. \n\\item \\textbf{Sparse Models and Conditional Computation} - While not targeted specifically at the attention modules, sparse models \\textit{sparsely} activate a subset of the parameters which generally improves the parameter to FLOPs ratio. Examples of this class of model includes Switch Transformers~, ST-MoE , GShard~, Product-Key Memory Layers~. Within the scope of our studied models, sparse models typically operate on an adaptive basis in which the sparsity is typically learned (via mixture-of-experts like mechanism). Within this context, we can also consider sparsification of attention weights to fall under this paradigm. For this reason, we believe there is a close connection to fixed or learned patterns in attention. However, we believe that the emergence of an entire research direction~ based on sparse efficient should warrant a new category of efficient Transformers.\n\\end{itemize}\nWe note that these buckets are a broad characterization of the different efficient Transformer models. In reality, there is no sharp boundary between the buckets as models may be comprised of multiple technical innovations. For example, the $k$-means clustering in Routing Transformer~ can also be interpreted as a form of global model memory approach, since one can view the centroids as parameterized model memory. In Reformer, however, clustering is used to learn the sparsity pattern of the attention weights. Additionally, pooling~ can be also interpreted as a form of model memory mechanism. We also note that the recent xformer models (circa December 2021) have started adopting some form of two-staged attention mechanism. Many times, these attention mechanisms explicitly combine one or more flavours of the above, e.g., local windows and then memory in Poolingformer~, or Long Short Transformers~ that utilize low rank attention with fixed windows (e.g., a combination of local attention with Linformer-like inductive bias).", "cites": [1484, 4735, 7840, 7053, 707, 1473, 4737, 4736, 1494, 798, 1133, 1182, 1482, 7333, 1490, 7298, 793, 1488, 1451, 4531, 1501, 1453, 1472, 4734, 7370, 8454], "cite_extract_rate": 0.8125, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a well-structured analytical overview by organizing efficient Transformer models into distinct categories based on core techniques. It synthesizes multiple cited works into a coherent taxonomy, connecting related ideas (e.g., kernels and low-rank methods). While it identifies general advantages and mechanisms, it could offer deeper critical evaluation of trade-offs or limitations in specific approaches."}}
{"id": "5c93d5a4-2735-4123-9126-b5f6deff845f", "title": "Structure of this section", "level": "paragraph", "subsections": [], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["paragraph", "Structure of this section"]], "content": "We begin by discussing local and fixed patterns models such as the Memory Compressed Transformer~ and Image Transformer~. We then discuss the Set Transformers~, an early approach for utilizing global model memory. Following which, we move on to models that utilize combinations of patterns such as Sparse Transformers~, CCNet~, and Axial Transformers~. Next, we discuss Longformer~ and ETC~, as examples of memory-based Sparse Transformer  approaches. Our detailed walkthrough then moves on to models that incorporate learnable patterns (LP) such as Routing Transformers~, Reformer~ and Sinkhorn Transformers~. After which, we introduce Linformer~ and Synthesizers~, models that can be considered low-rank factorization approaches. We then discuss models based on kernel approaches such as Performer~ and Linear Transformers~. Following which, we discuss the models that are based on segment-based recurrence such as Transformer-XL~ and Compressive Transformers~. Finally, we discuss the family of Sparse models which primarily leverage Mixture-of-Experts (MoE) type architectures and conditional computation to achieve computational efficiency. The logical flow of this section is aimed to be loosely chronological instead of categorically organized (with the exception of certain buckets like recurrence or sparsity that are more orthogonal approaches). We believe this is pedagogically helpful.", "cites": [1453, 7333, 2576, 1484, 7298, 7370, 793, 1494, 1473, 1133, 7366, 798], "cite_extract_rate": 0.8, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various efficient Transformer models, organizing them loosely chronologically and by pattern types. It synthesizes the general categorization of these models and connects some of them under shared themes (e.g., sparse attention, low-rank factorization). However, it lacks deeper critical analysis or comparison of the cited works, and abstraction is limited to surface-level groupings rather than identifying broader, meta-level principles or trends."}}
{"id": "f3229814-d986-4490-8337-ac4d6c1f1b7a", "title": "Memory Compressed Transformer", "level": "subsubsection", "subsections": ["aba67fd0-2d45-4210-a34b-b5f6e014b7db", "f2d18cf0-4188-497c-8c6c-9625a9dfddab", "361091ab-d721-46c1-92fc-143efc47614e"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Memory Compressed Transformer"]], "content": "Memory Compressed Transformer~ is one of the early attempts at modifying Transformers to better handle longer sequences. The modification introduced by Memory Compressed Transformers is in two folds: localizing the attention span and using memory compressed attention.", "cites": [1133], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Memory Compressed Transformer, mentioning its two key modifications without elaborating on their implications or connecting them to broader themes in efficient Transformer research. It cites one paper but does not synthesize ideas across multiple sources, nor does it offer a critical evaluation or abstract general principles from the cited work."}}
{"id": "aba67fd0-2d45-4210-a34b-b5f6e014b7db", "title": "Local Attention Span", "level": "paragraph", "subsections": [], "parent_id": "f3229814-d986-4490-8337-ac4d6c1f1b7a", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Memory Compressed Transformer"], ["paragraph", "Local Attention Span"]], "content": "A straightforward solution for dealing with long sequences in Transformers is to limit the attention span to a local neighborhood.  proposed dividing the input sequence into blocks of similar length so that self-attention can be computed within each block independently. This keeps the cost of attention per block constant, thus the number of activations scales linearly with the input length.", "cites": [1133], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the concept of local attention span and references one paper, but lacks synthesis of multiple sources or deeper analysis. It does not critically evaluate the approach, compare it with alternatives, or abstract it into broader design principles. The narrative is limited to a factual summary of the method."}}
{"id": "fb32d632-0000-48b5-890e-5814cf6c032e", "title": "Localized Attention Span", "level": "paragraph", "subsections": [], "parent_id": "02a47794-db5c-4bb8-8d87-cf564d18fe45", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Image Transformer"], ["paragraph", "Localized Attention Span"]], "content": "Limiting the receptive field to a local neighborhood~ addresses the issues with the computational and memory costs of running global self-attention on large inputs, but changing the neighborhood per query position would prohibit packing the computations of the self-attention into two matrix multiplications. To avoid that, Image Transformer proposes partitioning the inputs into ``query blocks’’ and their associated ``memory blocks``, where for all queries from a single query block, the model attends to the same memory block.\nThere are two different schemes for choosing query blocks and their associated memory block neighborhoods: \\emph{1-dimensional local attention} and \\emph{2-dimensional local attention}. Here we briefly explain these schemes in the decoder case. \n\\begin{figure}\n     \\centering\n     \\begin{subfigure}[b]{0.4\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/image_transformer_1.pdf}\n         \\caption{1-dimensional local attention}\n         \\label{fig:image_transformer-1d}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.4\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/image_transformer_2.pdf}\n         \\caption{2-dimensional local attention}\n         \\label{fig:image_transformer-2d}\n     \\end{subfigure}\n     \\caption{Attention span in Image Transformer on a two-dimensional input.}\n    \\label{fig:image_transformer}\n\\end{figure}\nFor the 1-dimensional local attention, the image is flattened in the raster order\\footnote{Given a 2D image as a grid of pixels, the horizontally left-to-right scanning of pixels, line-by-line, creates a raster order.} and partitioned into non-overlapping query blocks $Q$ of length $l_q$, and for each query block, a memory block $M$ is built from the same pixels in the $Q$ as well as a fixed number of pixels, $l_m$, generated before the query pixel.  In 2-dimensional local attention, pixels are generated in raster order. \nFor the 2-dimensional local attention, the image is partitioned into multiple non-overlapping rectangular query blocks of length $l_q = w_q \\times h_q$. The memory block extends the query block to the top, left $h_m$ and  $w_m$ pixels and to the right $w_m$ pixels, so $l_m = (w_q \\times q_h) + 2 \\times (h_m  + w_m)$.\nThe query pixel can attend to all other pixels. In the 2-dimensional local attention, pixels in the image are generated one query block after another. Generated blocks are in raster order, as well as generated pixels inside every block.", "cites": [4738], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the technical implementation of localized attention in the Image Transformer, focusing on the structure of query and memory blocks under 1D and 2D attention schemes. It integrates some information from the cited paper but lacks deeper synthesis, critical evaluation, or abstraction to broader trends or principles in efficient Transformer design."}}
{"id": "3d5a3287-d154-4aea-a184-9211b8d98e15", "title": "Set Transformer", "level": "subsubsection", "subsections": ["5c61d8dc-eca9-4585-9657-44c54041507e", "65e9033a-faf1-4149-891c-da5618de18f2"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Set Transformer"]], "content": "The Set Transformer~ adapts the Transformer model for \\emph{set-input} problems - that is, problems wherein the input is a set of features and the output is some function of this set (and is thereby invariant to the permutation, or ordering, of the input features). The Set Transformer leverages attention to capture interactions between elements of the input set. Furthermore, it applies the idea of \\emph{inducing points} from the sparse Gaussian process literature to reduce the complexity of attention from quadratic to linear in the size of the input set.\nProblems involving sets of objects often have a \\emph{permutation invariance} property: the target value for the set is the same regardless of the order of the objects in the set.  proved that all permutation-invariant functions can be represented by the following functional form:\n\\begin{align*}\n\\text{network}\\left(\\{x_1,\\dots, x_N\\}\\right) = \\rho\\left(\\text{pool}\\left(\\{\\phi(x_1),\\dots,\\phi(x_N)\\}\\right)\\right),\n\\end{align*}\nwhere the pooling function $\\text{pool}$ is a simple summation and $\\phi$ and $\\rho$ are continuous functions. This form can be interpreted as the composition of an \\emph{encoder} $\\phi$ and \\emph{decoder} $\\rho\\left(\\text{pool}(\\cdot)\\right)$.\nWhile this form is a universal approximator in the space of permutation-invariant functions, it is unclear how well such models fit tasks in practice. The Set Transformer proposes a solution that can be viewed as an encoder and pooled decoder, but where, unlike the form given above, the encoder and decoder can attend to input elements individually and the pooling function is parameterized.", "cites": [1523], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the Set Transformer by connecting it to the theoretical foundation provided by the 'Deep Sets' paper, highlighting its adaptation for set-input problems. It abstracts the core idea of permutation invariance and presents a generalized functional form, offering broader conceptual insight. The section also includes a critical evaluation of the limitations of the Deep Sets framework in practical settings, which adds depth to the analysis."}}
{"id": "0f8216da-de45-4a85-9828-916ee5003ac5", "title": "Sparse Transformer", "level": "subsubsection", "subsections": ["7b1e7bdf-617b-430b-85d0-de896e47cc4e", "9f70d90b-93e2-48df-8b69-bcf3a1c84221", "37cc7ca5-f236-4184-ac59-caa54007c88a", "eab15392-b995-40cc-93d9-9b7a9b365514"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Sparse Transformer"]], "content": "The Sparse Transformer~ presents a simple initial attempt to reduce the quadratic complexity of the standard self-attention mechanism. The key idea is to reduce the dense attention matrix to a sparse version by only computing attention on a sparse number of $q_i,k_j$ pairs. Sparse Transformer employs fixed attention patterns which are defined by strides and local neighborhoods. Computation is \\textit{factorized}, wherein local and stride patterns are split amongst the heads.", "cites": [793], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of the Sparse Transformer, focusing on its method of reducing complexity through sparse attention patterns. It mentions key concepts from the cited paper but lacks deeper synthesis, critical evaluation, or abstraction into broader patterns or principles."}}
{"id": "9f70d90b-93e2-48df-8b69-bcf3a1c84221", "title": "Strided Attention Heads", "level": "paragraph", "subsections": [], "parent_id": "0f8216da-de45-4a85-9828-916ee5003ac5", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Sparse Transformer"], ["paragraph", "Strided Attention Heads"]], "content": "The other half of the heads are dedicated to fixed strided patterns. Concretely,\n\\begin{align*}\n    \\hat{A}_{ij} = \n    \\begin{cases}\n    Q_{i}(K)_{j}^\\top),& \\text{if } (i-j) \\mod N =0 \\\\\n    0              & \\text{otherwise}\n\\end{cases}\n\\end{align*}\nThe final result of the factorized sparse attention is visualized in Figure~\\ref{fig:sparse_transformer}. We refer interested to~ for some additional theoretical analysis about the expressiveness of the Sparse attention mechanism.", "cites": [4739], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the strided attention mechanism in the Sparse Transformer but lacks synthesis of ideas from the cited paper, critical evaluation of its strengths or limitations, and abstraction to broader principles. It primarily presents a formula and a reference without integrating or analyzing the content insightfully."}}
{"id": "6e85534f-068e-4a9c-ab6e-7292792a2be8", "title": "Axial Transformer", "level": "subsubsection", "subsections": ["e6f4cc3d-ce12-4ce7-a0a2-ec8435de2cb1"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Axial Transformer"]], "content": "Axial Transformer~ uses factorization in a simple yet effective setup for the self-attention mechanism to process large inputs that are organized as multidimensional tensors. Instead of applying attention to the flattened version of the input, Axial Transformer simply applies multiple attentions, each along a single axis of the input tensor. Each attention, in fact, mixes information along a particular axis, while keeping information along other axes independent.  Since the length of any single axis is typically much smaller than the total number of elements, Axial Transformer significantly saves computation and memory. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.4\\linewidth]{figs/axial_transforme.pdf}\n    \\caption{Attention span in Axial Transformer on a two-dimensional input.}\n    \\label{fig:axial_transforme}\n\\end{figure}\nAxial Transformer offers an encoder-decoder architecture. For the decoding, to be able to implement the causal mask, Axial Transformer combines axial attentions with shift operations. For instance, for a model on 2-dimensional tensors, pixels are generated in raster order and to do that, first, the model encodes all pixels through an unmasked row and unmasked column attention. Then, for each row, the model applies an unmasked row and masked column attention to integrate the previously sampled rows. Finally, the model shifts the encoded representation up to make sure the conditioning information satisfies causality, and runs a masked row-attention to sample a new row in the image.\nAn advantage of Axial Transformer over similar methods like Sparse Transformer is that while it provides the global receptive field, it is straightforward to implement and does not require a custom kernel for an efficient implementation.", "cites": [4740, 1484], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of the Axial Transformer, including its architecture and implementation details. While it integrates information from the cited papers, the synthesis is limited to basic explanation of the method's structure. There is minimal critical analysis or comparison with other approaches, and abstraction is modest, focusing on the mechanics rather than broader design principles or trends in efficient Transformers."}}
{"id": "735b2410-fd41-456d-8b2c-a547b050834b", "title": "Longformer", "level": "subsubsection", "subsections": ["7d2388a1-a977-4b23-ad67-c0357a865382", "0b8f611d-6391-4f1b-9ee6-a5f7705bf9d3"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Longformer"]], "content": "Longformer~ is a variant of Sparse Transformer. \nIts key distinction compared to Sparse Transformer is ``Dilated Sliding Windows'', which can enable better long-range coverage without sacrificing sparsity. This is achieved by increasing the receptive fields by having gaps in the attention patterns. The Longformer also gradually increases the receptive field as the model goes deeper, dedicating lower levels for modeling local patterns and upper levels for modeling global patterns.", "cites": [7298], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of Longformer, focusing on its key feature of 'Dilated Sliding Windows' and how it improves long-range attention. However, it lacks meaningful synthesis with other cited works (only one paper is cited), does not critically analyze its strengths or weaknesses, and offers minimal abstraction beyond the specific model. It reads more like a summary than an insightful analysis."}}
{"id": "fedcf63b-a401-4b9d-9ca0-3aa532a5dc1a", "title": "BigBird", "level": "subsubsection", "subsections": ["630e500c-c499-48b6-bbdc-81f42298b08d", "0f0083e5-d21c-4dbd-80f4-c0e1df48e911", "9e377e46-7851-48e4-922d-a4af6ee6fa5b", "f607bc9e-b56c-4faf-95e5-5f05510d0b07", "59396049-db11-4345-a29a-399e64369535"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "BigBird"]], "content": "The BigBird model~ is another Transformer for modeling longer sequences and is primarily built on top of ETC~. The Big Bird model is comprised of several key components, namely (1) global tokens, (2) random attention (queries attend to random keys), and (3) fixed patterns (local sliding windows).", "cites": [1499], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the BigBird model and its components without effectively integrating ideas from other cited works. It lacks critical analysis or comparison with other efficient Transformer models and offers minimal abstraction beyond the specific model being discussed."}}
{"id": "f4887ab8-1a5a-4c38-a880-3fcd20b37b2e", "title": "Routing Transformer", "level": "subsubsection", "subsections": ["1e4a5e25-b500-4ec9-9ea1-7afad7fdb8ed", "65c92855-5cde-4bd1-a99a-6f5f97b107d6", "78d22117-a762-41fe-9dda-353afb116d29"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Routing Transformer"]], "content": "The Routing Transformer~ is a content-based sparse attention mechanism. It proposes a clustering-based attention mechanism that learns the attention sparsity in a data driven fashion. The first step is to project $Q$ and $K$ into a routing matrix $R$ of dimensions $n \\times d$.\n\\begin{align}\nR = QW_R + KW_R     \n\\end{align}\nwhere $W_R$ is a $d \\times d$ orthonormal projection matrix.", "cites": [1453], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the Routing Transformer, focusing on a technical detail (the routing matrix equation) from the cited paper without broader context or connections to other works. It lacks critical evaluation and abstraction, offering no comparative analysis or generalization to larger themes in efficient Transformers."}}
{"id": "1e4a5e25-b500-4ec9-9ea1-7afad7fdb8ed", "title": "$k$-means Clustering", "level": "paragraph", "subsections": [], "parent_id": "f4887ab8-1a5a-4c38-a880-3fcd20b37b2e", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Routing Transformer"], ["paragraph", "$k$-means Clustering"]], "content": "The $R$ matrix undergoes $k$-means clustering with a series of parameterized cluster centroids $u_1, u_2 \\cdots c_k$. The $k$-means in Routing Transformer is trained in an online fashion. To ensure a similar number of tokens in each cluster, the model initializes $\\sqrt{n}$ clusters, computes each token's distance against the cluster centroid, and takes an equal top-$k$ for each centroid. Since the cluster centroids are trainable parameters, this is also reminiscent of the \\emph{all-attention} layer proposed by~.", "cites": [7053], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the $k$-means clustering mechanism in the Routing Transformer and only makes a passing reference to a related concept in a cited paper. It lacks deeper synthesis of ideas, critical evaluation, and abstraction to broader principles or trends in efficient Transformer research."}}
{"id": "00c367aa-0d44-4c9b-aa7c-c00fe6726789", "title": "Sinkhorn Transformers", "level": "subsubsection", "subsections": ["5dec351a-2c52-44ff-94ae-393492ba90b0", "929b46cd-0569-444b-a0d3-83247d432041", "5403c490-0e45-4bdb-9b9a-54b777bd2a94"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Sinkhorn Transformers"]], "content": "This section introduces the Sparse Sinkhorn Transformer~. The Sinkhorn Transformer belongs to the family of \\textit{learned patterns}. This model is a chunked/blocked model that learns sparse patterns by re-sorting the input key and values in a block-wise fashion and then applying local block-based attention. \n\\begin{align*}\n    A_{ij} = \n    \\begin{cases}\n    (Q_{i}\\psi_S(K)_{j}^\\top),& \\text{if} \\lfloor{{j}/{N}}\\rfloor = \\lfloor{i/{N}}\\rfloor\\\\\n    0              & \\text{otherwise}\n\\end{cases}\n\\end{align*}\nwhere $\\psi_S$ applies a sorting operator on the sequence length dimension.", "cites": [1473], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Sinkhorn Transformer and includes the mathematical formulation from the cited paper. However, it lacks synthesis beyond direct reproduction of content, does not critically analyze the method or its limitations, and offers no abstraction or broader context linking it to other efficient Transformer approaches."}}
{"id": "929b46cd-0569-444b-a0d3-83247d432041", "title": "Sinkhorn Sorting", "level": "paragraph", "subsections": [], "parent_id": "00c367aa-0d44-4c9b-aa7c-c00fe6726789", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Sinkhorn Transformers"], ["paragraph", "Sinkhorn Sorting"]], "content": "$\\phi$ is the Sinkhorn balancing operator~ which converts the $n_B \\times n_B$ matrix into a soft permutation matrix. Specifically, a series of row- and column-wise normalizations are applied on the matrix output of $F_S\\text{BlockSum}(X)$. For the sake of brevity, we do not delve into details of this operation. Further details can be found at~.", "cites": [1473, 8834], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the Sinkhorn balancing operator but primarily serves as a factual description without deep synthesis or abstraction. It mentions the method in a general sense and refers the reader to external sources for details, showing minimal critical evaluation or comparison with related approaches."}}
{"id": "af6a261e-54bd-47b2-bc1e-362d6141ff6e", "title": "Linformer", "level": "subsubsection", "subsections": ["c49f2e0b-42ca-40f6-8fb6-e52017ec9adb", "8e69f67d-d374-4a43-8756-988c01988c0a"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Linformer"]], "content": "Linformer~ is an efficient Transformer based on the idea of low-rank self-attention.", "cites": [7333], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section is extremely brief and only mentions Linformer with a single sentence, lacking synthesis, critical analysis, or abstraction. It fails to integrate the cited paper's ideas, evaluate its contributions, or place it within a broader context of efficient Transformers."}}
{"id": "c49f2e0b-42ca-40f6-8fb6-e52017ec9adb", "title": "Low-Rank Projections on Length Dimensions", "level": "paragraph", "subsections": [], "parent_id": "af6a261e-54bd-47b2-bc1e-362d6141ff6e", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Linformer"], ["paragraph", "Low-Rank Projections on Length Dimensions"]], "content": "Linformer projects the $N \\times d$ dimensional keys and values to $k \\times d$ dimensions using additional projection layers. Note that this is a reduction on the length dimension instead of the key and value dimensions. This can \nGiven the newly projected keys ($K'$) and values ($V'$), the $QK'$ matrix is now $(N \\times k)$ dimensions instead of $(N \\times N)$. The attention matrix $\\text{Softmax}(QK')$ multiplies with $V' \\in \\mathbb{R}^{k \\times d}$ to result in an output tensor of dimensions $N \\times d$. To some extent, Linformer is reminiscent of depth-wise convolutions~. A projection on the length dimension causes mixing of sequence information (dimension-wise) in a single transformation. Hence, it is non-trivial to maintain causal masking and/or prevent mixing of past and future information when computing attention scores. The formulation of Linformer (for each attention head) can be expressed as:\n\\begin{align}\nSoftmax(\\frac{1}{\\sqrt{d_k}}XW^{Q}_{i}(E_i X W_i^K)) \\cdot F_iXW_i^V    \n\\end{align}\nwhere $W^{Q,K,V}$ are the default linear transformation of $X$ into queries (as per vanilla Transformer) and $E_{i}, F_i$ are additional $k \\times N$ projection of the key and values into $k \\times d$ tensors.", "cites": [4741], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of Linformer's approach and connects it to the concept of depth-wise convolutions, which demonstrates moderate synthesis and abstraction. While it identifies some non-trivial challenges (e.g., maintaining causal masking), it does not extensively compare Linformer with other efficient Transformers or critically evaluate its trade-offs and limitations. The integration of concepts and the analytical framing of the projection method contribute to a medium insight level."}}
{"id": "c86fa3dc-2cb0-454b-b346-23e3e03e17ae", "title": "Performer", "level": "subsubsection", "subsections": ["d73836ea-6083-4eef-9ce6-4bb374a06c4b", "e8ae810f-af02-44aa-8944-35d696f51a07", "caea67e9-e356-4384-bfb8-f76e36779e95"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Performer"]], "content": "The Performer~ model is characterized by its Generalized Attention mechanism and its usage of random Kernels.", "cites": [8384, 1494], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the Performer model by mentioning its key features such as Generalized Attention and random Kernels. It cites two papers but does not integrate or synthesize information from them in a meaningful way, nor does it offer critical evaluation or comparison with other models. The level of abstraction is limited to surface-level generalization without identifying broader patterns or principles in efficient Transformer design."}}
{"id": "e8ae810f-af02-44aa-8944-35d696f51a07", "title": "Fast Attention via Orthogonal Random Features (FAVOR)", "level": "paragraph", "subsections": [], "parent_id": "c86fa3dc-2cb0-454b-b346-23e3e03e17ae", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Performer"], ["paragraph", "Fast Attention via Orthogonal Random Features (FAVOR)"]], "content": "The above computation is still quadratic in complexity. Hence, the Performer leverages approximation tricks to avoid storing and computing the $N \\times N$ attention matrix. It leverages \\textit{orthogonal random features} (ORF) for doing so. The final attention output $Y$ of the Performer is described as follows:\n\\begin{align}\nY = \\hat{D}^{-1}(Q'((K')^\\top V))\n\\end{align}\nwhere $\\hat{D}=\\text{diag}(Q'((K')^\\top1_N))$, $Q'=D_Q\\phi(Q^\\top)^\\top$, and $K'=D_K\\phi(K^\\top)^\\top$. Note that $D_Q=g(Q_i^\\top),D_K=h(K_i^\\top)$. The function $\\phi(x)$ is defined as:\n\\begin{align}\n\\phi(X)= \\frac{c}{\\sqrt{M}}f(Wx +b)^\\top\n\\end{align}\nwhere $c > 0$ is a constant, $W \\in \\mathbb{R}^{M \\times d}$ is a random feature matrix and $M$ is the dimensionality of this matrix that controls the number of random features. We are able to see that we do not explicitly compute $A=QK^\\top$ and hence avoid paying the $N^2$ cost. For rigorous theoretical analysis and further details, we refer interested readers to~.", "cites": [1494], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Performer model and its FAVOR+ mechanism, but it does not effectively integrate or synthesize insights from multiple papers. There is minimal critical analysis or identification of broader patterns or limitations. The presentation is largely factual and lacks deeper analytical or comparative discussion."}}
{"id": "4587bece-9f38-4735-9cf0-5c393b64224d", "title": "Linear Transformer", "level": "subsubsection", "subsections": [], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Linear Transformer"]], "content": "The Linear Transformer~ improves the complexity of self-attention from quadratic to linear by using a kernel-based formulation of self-attention and the associative property of matrix products. Furthermore, it reduces attention with causal masking (which is used in auto-regressive decoding) to a linear-time, constant memory recurrent neural network (RNN). The model has been shown to improve inference speeds up to \\emph{three orders of magnitude} without much loss in predictive performance. Linear Transformers are similar to Performers with the exception of the kernel function and therefore also suffer from the same drawbacks (unable to be parallelized across the time dimension during training in an autoregressive teacher forced setting). \nThe method rests on the simple but powerful observation that the accumulated value $V_i'$ for the query $Q_i$ in position $i$ can be written as:\n\\begin{align*}\nV_i' &= \\frac{\\sum_{j=1}^p \\text{sim}(Q_i, K_j) V_j}{\\sum_{j=1}^p \\text{sim}(Q_i, K_j)}.\n\\end{align*}\nHere, $p = N$ in full, unmasked attention and $p = i$ in the case of causal masking. Now, in usual softmax attention, $\\text{sim}(q, k) = \\exp\\left(\\frac{q^T k}{\\sqrt{d}}\\right)$. Linear Transformer, however, expresses the similarity as a kernel function. That is, $\\text{sim}(q, k) := \\phi(q)^T \\phi(k)$, where $\\phi$ is a, possibly high-dimensional, feature map. With this choice,\nwe can rewrite $V_i'$ as:\n\\begin{align*}\nV_i' &= \\frac{\\phi(Q_i)^T S_p}{\\phi(Q_i)^T Z_p},\\\\\nS_p &:= \\sum_{j=1}^p \\phi(K_j) V_j^T,\\\\\nZ_p &:= \\sum_{j=1}^p \\phi(K_j).\n\\end{align*}\nFor unmasked attention, since $p = N$ we only need to compute $S_N$ and $Z_N$ once and we reuse them for the computation at every position $0 \\leq i \\leq N$. For causal attention, the $S_i$'s and $Z_i$'s can be viewed as states of an RNN that are updated by the following recurrence relations:\n\\begin{align*}\nS_i &= S_{i-1} + \\phi(K_i)V_i^T,\\\\\nZ_i &= Z_{i-1} + \\phi(K_i)\n\\end{align*}\nwith initial condition $S_0 = Z_0 = 0$.\nIf the dimension of the key, query, and values are all $d$ and the cost to compute $\\phi$ is $\\mathcal{O}(c)$, then the overall run-time complexity of Linear Transformer is $\\mathcal{O}{(N c d)}$. The authors choose\n\\begin{align*}\n\\phi(x) = \\text{elu}(x) + 1,\n\\end{align*}\nwhere $\\text{elu}(\\cdot)$ denotes the exponential linear unit~. With this choice of feature map, $c = d$ and the end-to-end complexity of the model is $\\mathcal{O}(N d^2)$.", "cites": [798, 4742], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear synthesis of the Linear Transformer by integrating its key mathematical formulation from the cited paper with broader concepts like kernel-based attention and RNN-like recurrence. It also offers critical insight by highlighting the trade-off between linear complexity and lack of parallelization during training. The abstraction level is strong, as it generalizes the model’s mechanism and positions it within the context of similar approaches like Performers."}}
{"id": "76930262-894e-4e4c-855b-52de1d76e24c", "title": "Synthesizers", "level": "subsubsection", "subsections": ["fdaeb482-2e71-4c41-b491-baf36a77a316", "6ae3e20a-147d-40e2-a3fb-62409e04a2a5", "3bb7cddd-5c54-4035-8463-d7656227ca82", "0229e228-e7a1-4b7e-8609-3ff42cc61031"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Synthesizers"]], "content": "Synthesizer models~ are an attempt to study and investigate the true importance of conditioning within the self-attention mechanism and are also the first attempts at unconditional token-mixing. In~, the authors study a synthetic self-attention module in which attention weights are approximated instead of being computed by pairwise dot products. Synthesizers are only implicitly related to efficient Transformers and can be considered more as a MLP-Mixer~. However, the factorized variants can be considered a low-rank efficient Transformer model.", "cites": [7366, 7660], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by situating synthesizers within the broader context of self-attention and token mixing, and by contrasting them with MLP-Mixers. It synthesizes the key idea from Paper 1 that dot product self-attention is not strictly necessary and introduces the factorized variant as a low-rank approximation. However, it lacks deeper critical evaluation of the papers' limitations or trade-offs and offers limited abstraction beyond the specific models discussed."}}
{"id": "6ae3e20a-147d-40e2-a3fb-62409e04a2a5", "title": "Random Synthesizers", "level": "paragraph", "subsections": [], "parent_id": "76930262-894e-4e4c-855b-52de1d76e24c", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Synthesizers"], ["paragraph", "Random Synthesizers"]], "content": "Another variant of the Synthesizer model uses random matrices for $A$. In this case, the output can be expressed by:\n\\begin{align}\nY = \\text{Softmax}(R)G(X).    \n\\end{align}\nwhere $R \\in \\mathbb{R}^{N \\times N}$ is a trainable and/or non-trainable matrix. In~, the authors show that Random Synthesizers achieve competitive performance.", "cites": [7366], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces Random Synthesizers and cites one paper, but it lacks synthesis of broader ideas, critical evaluation of the approach, or abstraction to higher-level principles. It primarily describes a method and mentions competitive performance without deeper analysis or contextualization."}}
{"id": "397e594a-88ee-4d92-94b6-456d9e680abe", "title": "Transformer-XL", "level": "subsubsection", "subsections": ["88c6813f-47fa-4dc3-a9e3-657d8d1e35c7", "823ca029-539c-4f78-8287-5e12bd14fed9"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Transformer-XL"]], "content": "The Transformer-XL model~ relies on segment-based recurrence. Segment-based recurrence can be considered an orthogonal approach to the other techniques discussed since it does not explicitly sparsify the dense self-attention matrix. Instead, it connects adjacent blocks with a recurrent mechanism.", "cites": [7370], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal summary of the Transformer-XL model, focusing on its use of segment-based recurrence. It lacks synthesis with other works, critical evaluation of the model's strengths or limitations, and abstraction to broader principles or patterns in efficient Transformer research."}}
{"id": "823ca029-539c-4f78-8287-5e12bd14fed9", "title": "Relative Positional Encodings", "level": "paragraph", "subsections": [], "parent_id": "397e594a-88ee-4d92-94b6-456d9e680abe", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Transformer-XL"], ["paragraph", "Relative Positional Encodings"]], "content": "Transformer-XL introduces novel relative position encodings. In this scheme, absolute positional encodings are not added to the content embeddings. Instead, they are only considered while computing attention weights where they can be replaced with relative position encodings. Since the relative position encodings are not directly relevant to the efficiency of the model, we refer interested readers to~ for more details.", "cites": [7370], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic factual description of Transformer-XL's relative positional encodings without integrating this concept with other methods or critically analyzing its strengths or weaknesses. It fails to offer a broader perspective or comparative insights, and simply mentions that the encodings are not directly relevant to efficiency, deferring to the original paper for details."}}
{"id": "d74b6b7e-1acf-4525-be7f-2f0b21a45203", "title": "Sparse Models", "level": "subsubsection", "subsections": ["d2e5ca8f-69fa-4690-8ec1-61d368c85d69"], "parent_id": "1068ae2e-644b-469f-bc75-6922be1837a6", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "A Survey of Efficient Transformer Models"], ["subsection", "Detailed Walk-through of Efficient Transformer Models"], ["subsubsection", "Sparse Models"]], "content": "In this section we describe the family of Sparse models. Sparse models typically achieve a high parameter to FLOP ratio by sparsely activating a subset of parameters or activations. It is good to note that while most of the works within the scope of this survey deals with efficient attention, the scope of sparse models goes beyond the attention module and is generally applied more frequently to the feed forward layers~. In this section, we discuss the prime variant for Sparse models, i.e., the Mixture-of-Experts based Sparse models which includes models such as GShard~, Switch Transformer~ and GLaM~.", "cites": [707, 8454], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of sparse models and introduces the Mixture-of-Experts approach using examples like Switch Transformer, GShard, and GLaM. However, it lacks deep synthesis of the cited works, does not compare or critically evaluate them, and offers minimal abstraction beyond the specific models. The narrative remains largely descriptive and does not elevate the discussion to a higher conceptual or analytical level."}}
{"id": "37fcf857-20c6-44d7-a0e9-f78ff08cb42d", "title": "On Evaluation", "level": "subsection", "subsections": [], "parent_id": "568b3cb1-3180-48cc-9b2a-86346db55bc0", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Discussion"], ["subsection", "On Evaluation"]], "content": "While the field is bustling with new Transformer models, there is not an easy way to compare these models side by side. Many research papers select their own benchmarks to showcase the abilities of the proposed model. This is also coupled with different hyperparameter settings like model sizes and configurations which can make it difficult to correctly attribute the reason for the performance gains.\nMoreover, some papers conflate this with pretraining~ which makes it even harder to distinguish the relative performance of these different models. It is still a mystery to which fundamental efficient Transformer block one should consider using.\nOn one hand, there are multiple models that focus on generative modeling, showcasing the ability of the proposed Transformer unit on auto-regressive modeling of sequences. To this end, Sparse Transformers~, Adaptive Transformers~, Routing Transformers~ and Reformers~ are mainly focused on generative modeling tasks. These benchmarks typically involve language modeling and/or pixel-wise image generation on datasets such as wikitext~, and/or ImageNet~ / CIFAR~. Models that use segment based recurrence such as Transformer-XL and Compressive Transformers are also focused on long-range language modeling tasks such as PG-19. \nOn one hand, a collection of models is mainly focused on encoding-only tasks such as question answering, reading comprehension and or selections from the GLUE benchmark. For example, the ETC model~ only runs experiments on question answering benchmarks such as NaturalQuestions~ or TriviaQA~. On the other hand, the Linformer~ focuses on subsets of the GLUE~ benchmark. This split is very natural and intuitive, since models like ETC and Linformer cannot be used in an auto-regressive fashion. This exacerbates the challenges associated with comparing these encoder-only models with the other models.\nThere are models that focus on a balance of both. Longformer~ tries to balance this by running benchmarks on both generative modeling and encoder-only tasks. The Sinkhorn Transformer~ compares on both generative modeling tasks as well as encoding only tasks. \nAdditionally, it is also worth noting that, although Seq2Seq machine translation (MT) was one of the problems that popularized Transformer models, not many of these efficient Transformer models are evaluated on MT tasks. This is likely because sequence lengths in MT are not long enough to warrant the usage of these models. \nWhile generative modeling, GLUE tasks and/or question answering appear to be the common evaluation benchmarks adopted by many of these tasks, there are several niche benchmarks that a small isolated number of papers choose to evaluate on. For starters, the Performer model~ evaluates on masked language modeling on proteins, deviating from serious head-on comparisons with other efficient Transformer models. The Linear Transformer~ also evaluates on speech recognition, which is a rare benchmark amongst this group of papers. \nThere have been recent attempts to unify evaluation on Efficient Transformers, namely Long Range Arena, i.e., LRA, ~ that benchmarked 10 different xformer variants on long range modeling tasks. It is good to note that LRA was designed for evaluating Transformers in encoder-only mode and do not consider generative (or autoregressive tasks) that require causal masking.", "cites": [1453, 7333, 7298, 4731, 793, 451, 1473, 1494, 798, 7, 461, 4743], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple efficient Transformer papers by categorizing them based on their evaluation focus, such as generative modeling, encoder-only tasks, and hybrid approaches. It critically discusses the lack of standardized benchmarks and how varying hyperparameters and pretraining practices complicate model comparison. The section also abstracts these observations into a broader narrative about the evaluation challenges in the field and highlights the significance of the Long Range Arena benchmark as a unifying effort."}}
{"id": "112ca8cf-1604-4b9f-a8d3-e45e9926df02", "title": "On Model Design Trends", "level": "subsection", "subsections": [], "parent_id": "568b3cb1-3180-48cc-9b2a-86346db55bc0", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Discussion"], ["subsection", "On Model Design Trends"]], "content": "When matching our broad categorization against the timeline of the introduction of these models, we are able to see the trend that the community is taking towards designing efficient Transformer models. Early work in this area has primarilyy been focused on more intuitive and simple approaches such as \\textit{fixed patterns}. To this end, most early work in this area is based on block/local patterns such as Image Transformer~, Compressed Attention~, Blockwise Transformer~ or the local windows in Sparse Transformer~. \nThe paradigm of factorizing various fixed patterns was first introduced in~ and CCNet~. Around this same time, we start to observe early traces of \\textit{model-memory}-based approaches from both the inducing point methods in the Set Transformer~ or global nodes in the Star Transformer~ model.\nWe observe the next wave of models comes in the form of learnable sparsity patterns. Reformer~ and Routing Transformers~ are very similar in the sense that they are models that learn to cluster/bucket tokens before performing attention. The key difference is the means to the end whereby Reformer uses a hashing function while the Routing Transformer uses online $k$-means for cluster assignment. In parallel, Sinkhorn Transformers~ are also based on the idea of sorting, albeit at the block level. These three models largely follow a similar paradigm of re-arranging sequences for efficient computation of attention scores.\nNext, we then observe several extensions that are largely built off the Sparse Transformer paradigm. The ETC~ and Longformer~ models are very similar ideas that are fundamentally Sparse Transformer extensions. These models incorporate the notion of a global model memory, which is reminiscent of the Set Transformer's inducing point method or the global model memory of the Star Transformer. Modifications to strides, such as using dilated windows was also proposed in the Longformer work.\nThe most recent wave of models we've been seeing is models that are based on low-rank approximation or kernel methods, e.g., models such as Low-Rank Transformer~, Linformer~, Performer~ and/or Linear Transformers~. Although due to the state of evaluation and the high parallelism of research, it is quite unclear if this low-rank or kernel paradigm is actually better than the learnable pattern (LP) or model memory based efficient Transformer models.  \nMore recently, there have been more models that propose a two-pronged or two-step attention mechanism combining models from different techniques. The Long Short Transformer~ is a dynamic form of Linformer combined with Fixed Pattern attention mechanisms. On the other hand, models like Poolingformer also explicitly construct a two-level attention mechanism with techniques reminiscent of memory-based approaches and local attention. Scatter Brain is a new work~ attempts to unify sparse (fixed pattern) attention with low-rank attention. Two stage attention mechanisms are also proposed by Luna~\nOn the side, it is important to note that the recurrent based models (Transformer-XL and Compressive Transformers) seem to operate orthogonally and are not as directly comparable to the other models. We also observe that Sparse models~ that are not only applicable to attention modules, are also recently emerging and becoming more popular and have demonstrated considerable success in the recent months~.", "cites": [2576, 4735, 707, 1473, 1494, 798, 1503, 1133, 7333, 4744, 7298, 793, 7371, 4531, 1453, 8454], "cite_extract_rate": 0.7619047619047619, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of efficient Transformer models, organizing them into clear design trends and connecting related ideas across multiple papers. It includes critical observations such as the unclear superiority of low-rank methods over others and the orthogonal nature of recurrent-based models. The abstraction is strong, identifying broader design paradigms like fixed patterns, learnable sparsity, low-rank approximation, and two-step attention mechanisms."}}
{"id": "8dd33c6a-c511-4b8f-a7b9-6a64b3094a2f", "title": "Brief Discussion on Orthogonal Efficiency Efforts", "level": "subsection", "subsections": [], "parent_id": "568b3cb1-3180-48cc-9b2a-86346db55bc0", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Discussion"], ["subsection", "Brief Discussion on Orthogonal Efficiency Efforts"]], "content": "While this paper is mainly focused on (1) the computational and memory complexity of the self-attention module and (2) sparsity and adaptive computation, we briefly summarize several orthogonal efforts that may also contribute to model efficiency, scalability, and overall usability of Transformer models. \n\\begin{itemize}\n\\item \\textbf{Weight Sharing} - Sharing parameters of the Transformer models would help in reducing overall model size. The Universal Transformers~ tie attention and transition weights across layers. Similarly, Albert~ does the same parameter sharing across layers. On the other hand, the Quaternion Transformer~ proposes a weight sharing scheme inspired by Hamilton products that locally shares the components in the linear transformation layers.\n\\item \\textbf{Quantization / Mixed Precision} - Learning mixed precision models has the potential to improve memory costs. Q-BERT~ is a model that quantizes Transformer models to ultra-low precision. Meanwhile mixed precision training~ is a highly popular technique to reduced the memory costs of training Transformers.~ applies Quantization Aware training to Transformer models. \n\\item \\textbf{Inference-time Efficiency and Network Pruning} - Multiple research directions explore improving the Transformer efficiency at inference time. One prime example is network model. An example is to prune attention heads during inference~. This has shown to have minimal degradation of performance on downstream tasks. On the other hand,  proposes a ``block'' pruning approach which can make a Transformer 2.4x faster with little loss in predictive performance on language tasks. Another line of work involved fast exit during inference which allows us to exit compute if the model is confident of its predictions~. \n\\item \\textbf{Knowledge Distillation} - \nKnowledge distillation (KD)~ has been a useful technique for transfering the knowledge learned from a larger teacher model to a smaller student model. The smaller model can then be efficiently deployed into production. There have been many attempts to distill large Transformer models. For example, DistilBERT~, task-specific distillation~ and TinyBERT~.\n\\item \\textbf{Neural Architecture Search (NAS)} -\nSearching for more efficient Transformer architectures is also a common strategy.  proposed Neural Architecture Transformer (NAT), using NAS to search for more compact and efficient Transformers by removing redundant operations.  proposed HAT (Hardware-aware Transformers), a method that leverages NAS and uses hardware efficiency feedback as a reward signal.\n\\item \\textbf{Task Adapters} - This line of research has been primarily focused on the problem of fine-tuning large Transformer on $T$ tasks and aiming to reuse parameters across a variety of tasks. The key idea is that task adapters~ enable reuse of parameters across tasks and reuse the need of serving $T$ models in production - resulting in overall parameter savings. A modest number of models have been proposed, such as PALS~, MAD-X~ and HyperGrid~.\n\\item \\textbf{Alternative Architectures} - A considerable amount of effort have gone into designing Transformer alternatives. Amongst the many alternatives considered, a prominent line of emerging research belongs to the family of MLP Mixers~. Different mixing operations have been proposed, such as the G-MLP~, FNet~. Synthesizers~, although commonly referred to as an efficient attention method, is also an early manifestation of the mixer line of work, as the random matrices similarly act as an unconditioned mixing operation. A recent promising line of work, based on Structured State Spaces~ also demonstrated very promising results on long range modeling. Lastly, convolutional models are generally more efficient than Transformers since convolutional kernels operate on a fixed, small local neighborhood around the input token.  shows that, when pre-trained, these more efficient convolutional models can sometimes match the predictive performance of Transformer ones.\n\\end{itemize}", "cites": [4743, 4752, 4745, 7567, 1150, 4749, 4514, 4485, 4751, 4750, 856, 7366, 681, 4519, 7660, 8835, 8361, 4747, 4746, 2488, 4748], "cite_extract_rate": 0.8076923076923077, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various efficiency-related techniques and briefly references associated papers, but lacks deep synthesis or integration of ideas. It does not provide a comparative or critical analysis of the approaches, nor does it abstract to broader principles or trends. The narrative remains largely descriptive."}}
{"id": "91d98cd0-2945-44ee-bd10-bca2c14e83d0", "title": "A Retrospective on the Past Year and Future Research Directions", "level": "subsection", "subsections": [], "parent_id": "568b3cb1-3180-48cc-9b2a-86346db55bc0", "prefix_titles": [["title", "Efficient Transformers: A Survey"], ["section", "Discussion"], ["subsection", "A Retrospective on the Past Year and Future Research Directions"]], "content": "With our timely V2 update of this survey (updated December 2021), we present retrospective thoughts about how the field has evolved over the past year or so. From the time of last update, it is undeniable that more xformer variants have emerged to offer more efficient alternatives for vanilla Transformers.\nNotably, examples of these include  Nystr\\\"{o}mformer~, Perceiver~, RFA~, Luna~ and Long Short Transformer~. There were also other notable models that sprung up around the time when this manuscript was published and narrowly missed the inclusion in the first edition (e.g., Funnel Transformer~). Amongst all the new xformer variants, it is good to note that most do not stray away from the fundamental concepts presented in the first version. Our taxonomy and categorization was more or less broad enough to capture many of these models as they use fundamental ideas that are already present in existing work and therefore can be categorized appropriately. \nMany works can be thought of explicit combinations of existing techniques (two-staged or combination of two method classes) or improvements over existing methods (dynamic formulation of Linformer's low rank projection or better kernels for Linear Transformers). Even though many existing \\textit{`memory'} models utilize a form of downsampling to achieve a speed and efficiency gain, we erected a new categoriation of \\textit{`downsampling'} to better reflect this new emerging trend~.\nOver the past year, it is evident that a lot of research investment have been poured into making quadratic attention scalable, in terms of complexity, or sometimes memory.  \nAt this juncture, it is good to ponder about real tangible need for linear-time attention. Many applications even in language and vision are still dominated by vanilla Transformers with quadratic attention and \\emph{none of these xformer variants have caught on as the defacto standard}. There might be multiple explanations from multiple angles for this phenomena. Firstly, linear attention (e.g., Performer) models struggle to be competitive on common benchmarks, as noted from multiple sources~. \nIt is good to note that, apart from toy setups or specific domains and problems, they have never been battle-tested against common paradigms like pretrain-and-finetuning only up till recently. Meanwhile, local attention models based on fixed and/or learned patterns such as Sparse Transformers~, Longformer~, ETC~ or BigBird~ have seen more reasonable usage, especially within the areas of long context question answering. \nHowever, the high intrinsic implementation complexity of methods such as in ETC~ (substantially increases code complexity by having so many different directions of attention), Swin Transformer~ or Longformer~ (requiring custom CUDA kernels and thus making it prohibitive on hardware such as TPUs) might be reasons why these models have yet to found themselves serving as a good, simple-to-use drop-in Transformer replacement.\nAs noted by~, for applications that require to flex on sequence length and memory needs time to time, it might be suffice to \\textit{`just sequentially process it'} even if that might not be inherently as satisfying as finding a theoretical approximate. In parallel,~ suggests that local attention, when done right, can be a really tough baseline to beat. \nA notable fact about the barrage of efficient attention models is the overloading of the term efficient. It is commonly misunderstood that efficient attention models always imply that the Transformer is fast. The truth is that many of these efficient attention models, owing to their innovation constraints, may make the model much slower. Moreover, many linear attention models do not observe any speed or memory gain at all if the sequence length is short. Many of them have extraordinarily painful requirements to achieve causal masking (or TPU packing)~ and often have to substantially trade-off throughput for linear complexity. On the other hand, some models cannot be packed or causally masked at all. More notes and discussions about this efficiency misnomer can be found in this paper~ which we encourage readers to also peruse.\nThis update also extends the original scope of efficient attention based xformer models to sparse models even if they did not necessarily target the attention modules. We believe that sparse models were a necessarily addition to the scope to this paper given its recent signs of promise . A special note was made to recognize the work done in alternative architectures in the past year (in the section on orthogonal directions). Mixer type architectures  have garnered some interest in computer vision but seem to not perform well on language . Meanwhile, alternative models based on Structured State Spaces such as S4  have solved the hardest Path-X task in the Long Range Arena benchmark . It should be exciting to see how a model such as S4 would perform at scale, and under pretrained conditions.\nAs the year comes to a close and as we reflect back on the amazing advances made by the community, we begin to ponder about the future of \\textit{efficient transfomers} and what the ideal transformer model should look like. We think that the ideal xformer should hopefully take care of the quadratic memory problem, while retaining universality (e.g., do well on most tasks and not only on long range tasks). The ideal xformer should also not trade-off speed for memory and should not sacrifice the ability to be TPU-packed and/or causal masked. It should ideally be simple and not make use of rigid hard-coding or over-excessive engineering, i.e., it should be elegant and scale well. Ideally, efficiency would be baked right into the next generation of Transformers instead of always having a side variant that one could use for long context tasks. While we cannot explicitly point at any of the xformer variants as the definitive one that have solved the efficiency problem in Transformers, we are optimistic that, given about the pace of advance, \\textit{the} true xformer will emerge eventually. It is then a question of whether that new xformer will still be a Transformer.", "cites": [4753, 4735, 7840, 707, 4743, 4737, 1503, 1182, 7660, 4732, 7333, 8835, 7298, 7841, 793, 1488, 1501, 8384, 1499], "cite_extract_rate": 0.7307692307692307, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes recent xformer variants and connects them to the broader landscape of attention mechanisms and efficiency trade-offs. It critically evaluates the practical adoption and limitations of linear and sparse attention models, highlighting benchmark performance, implementation complexity, and the misnomer of 'efficiency.' The section abstracts beyond individual models to discuss overarching trends, such as the need for universality, simplicity, and scalability in ideal transformer designs."}}
