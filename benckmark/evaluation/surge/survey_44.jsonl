{"id": "259e3016-29dd-4d94-91c7-f265b734bb7d", "title": "Introduction", "level": "section", "subsections": ["32ca0d97-ce1e-43c2-a6a2-8b459454e5e4", "13feb38e-f168-4100-aaac-a9303bf823b5"], "parent_id": "06b41427-1045-4fd8-b025-95b34ab2f969", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Introduction"]], "content": "An event is a real-world occurrence that takes place in a specific location and time that relates to a particular topic. Events can range from large-scale (e.g., civil unrest events and earthquakes), to medium-scale (e.g., system failures and crime incidents), to small-scale (e.g., authentication events and individual actions) occurrences ~. Event analytics are important in domains as different as healthcare, business, cybersphere, politics, and entertainment, influencing almost every corner of our lives~. The analysis of events has thus been attracting huge attention over the last few decades and can be categorized in terms of their timeliness for various research directions, such as event summarization, detection, and prediction. Unlike retrospective analyses such as event summarization and detection~, event prediction focuses on anticipating events in the future and is the focus of this survey. Accurate anticipation of future events enables one to maximize the benefits and minimize the losses associated with some event in the future, bringing huge benefits for both society as a whole and individual members of society in key domains such as disease prevention~, disaster management~, business intelligence~, and economics stability~.\n\\chapquote{``Prediction is very difficult, especially if it's about the future.'' \\ \\ \\         -- Niels Bohr, 1970}{}{}\nEvent prediction has traditionally been prohibitively challenging across different domains, due to the lack or incompleteness of our knowledge regarding the true causes and mechanisms driving event occurrences in most domains. With the advent of the big data era, however, we now enjoy unprecedented opportunities that open up many alternative approaches for dealing with event prediction problems, sidestepping the need to develop a complete understanding of the underlying mechanisms of event occurrence. Based on large amounts of data on historical events and their potential precursors, event prediction methods typically strive to apply predictive mapping to build on these observations to predict future events, utilizing predictive analysis techniques from domains such as machine learning, data mining, pattern recognition, statistics, and other computational models~. Event prediction is currently experiencing extremely rapid growth, thanks to advances in sensing techniques (physical sensors and social sensors), prediction techniques (Artificial Intelligence, especially Machine Learning), and high performance computing hardware~.\nEvent prediction in big data is a difficult problem that requires the invention and integration of related techniques to address the serious challenges caused by its unique characteristics, including: \\textbf{1) Heterogeneous multi-output predictions.} Event prediction methods usually need to predict multiple facets of events including their time, location, topic, intensity, and duration, each of which may utilize a different data structure~. This creates unique challenges, including how to jointly predict these heterogeneous yet correlated facets of outputs. Due to the rich information in the outputs, label preparation is usually a highly labor-intensive task performed by human annotators, with automatic methods introducing numerous errors in items such as event coding. So, how can we improve the label quality as well as the model robustness under corrupted labels? The multi-faceted nature of events make event prediction a multi-objective problem, which raises the question of how to properly unify the prediction performance on different facets. It is also challenging to verify whether a predicted event ``matches'' a real event, given that the various facets are seldom, if ever, 100\\% accurately predicted. So, how can we set up the criteria needed to discriminate between a correct prediction (``true positive'') and a wrong one (``false positive'')? \\textbf{2) Complex dependencies among the prediction outputs.} Beyond conventional isolated tasks in machine learning and predictive analysis, in event prediction the predicted events can correlate to and influence each other~. For example, an ongoing traffic incident event could cause congestion on the current road segment in the first 5 minutes but then lead to congestion on other contiguous road segments 10 minutes later. Global climate data might indicate a drought in one location, which could then cause famine in the area and lead to a mass exodus of refugees moving to another location. So, how should we consider the correlations among future events? \\textbf{3) Real-time stream of prediction tasks.} Event prediction usually requires continuous monitoring of the observed input data in order to trigger timely alerts of future potential events~. However, during this process the trained prediction model gradually becomes outdated, as real world events continually change dynamically, concepts are fluid and distribution drifts are inevitable. For example, in September 2008 21\\% of the United States population were social media users, including 2\\% of those over 65. However, by May 2018, 72\\% of the United States population were social media users, including 40\\% of those over~. Not only the data distribution, but also the number of features and input data sources can also vary in real time. Hence, it is imperative to periodically upgrade the models, which raises further questions concerning how to train models based on non-stationary distributions, while balancing the cost (such as computation cost and data annotation cost) and timeliness?\nIn addition, event prediction involves many other common yet open challenges, such as imbalanced data (for example data that lacks positive labels in rare event prediction)~, data corruption in inputs~, the uncertainty of predictions~, longer-term predictions (including how to trade-off prediction accuracy and lead time)~, trade-offs between precision and recall~, and how to deal with high-dimensionality~ and sparse data involving many unrelated features~. Event prediction problems provide unique testbeds for jointly handling such challenges.\nIn recent years, a considerable amount of research has been devoted to event prediction technique development and applications, in order to address the aforementioned challenges~. Recently, there has been a surge of research that both proposes and applies new approaches in numerous domains, though event prediction techniques are generally still in their infancy. Most existing event prediction methods have been designed for a specific application domains, but their approaches are usually general enough to handle problems in other application domains. Unfortunately, it is difficult to cross-reference these techniques across different application domains serving totally different communities. Moreover, the quality of event prediction results require sophisticated and specially-designed evaluation strategies due to the subject matter's unique characteristics, for example its multi-objective nature (e.g., accuracy, resolution, efficiency, and lead time) and heterogeneous prediction results (e.g., heterogeneity and multi-output). As yet, however, we lack a systematic standardization and comprehensive summarization approaches with which to evaluate the various event prediction methodologies that have been proposed. This absence of a systematic summary and taxonomy of existing techniques and applications in event prediction causes major problems for those working in the field who lacks clear information on the existing bottlenecks, traps, open problems, and potentially fruitful future research directions. \nTo overcome these hurdles and facilitate the development of better event prediction methodologies and applications, this survey paper aims to provide a comprehensive and systematic review of the current state of the art for event prediction in the big data era. The paper's major contributions include:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{A systematic categorization and summarization of existing techniques.} Existing event prediction methods are categorized according to their event aspects (time, location, and semantics), problem formulation, and corresponding techniques to create the taxonomy of a generic framework. Relationships, advantages, and disadvantages among different subcategories are discussed, along with details of the techniques under each subcategory. The proposed taxonomy is designed to help domain experts locate the most useful techniques for their targeted problem settings.\n    \\item \\textbf{A comprehensive categorization and summarization of major application domains.} The first taxonomy of event prediction application domains is provided. The practical significance and problem formulation are elucidated for each application domain or subdomain, enabling it to be easily mapped to the proposed technique taxonomy. This will help data scientists and model developers to search for additional application domains and datasets that they can use to evaluate their newly proposed methods, and at the same time expand their advanced techniques to encompass new application domains.\n    \\item \\textbf{Standardized evaluation metrics and procedures.} Due to the nontrivial structure of event prediction outputs, which can contain multiple fields such as time, location, intensity, duration, and topic, this paper proposes a set of standard metrics with which to standardize existing ways to pair predicted events with true events. Then additional metrics are introduced and standardized to evaluate the overall accuracy and quality of the predictions to assess how close the predicted events are to the real ones.\n    \\item \\textbf{An insightful discussion of the current status of research in this area and future trends.} Based on the comprehensive and systematic survey and investigation of existing event prediction techniques and applications presented here, an overall picture and the shape of the current research frontiers are outlined. The paper concludes by presenting fresh insights into the bottleneck, traps, and open problems, as well as a discussion of possible future directions.\n\\end{itemize}", "cites": [340, 339, 341, 8342], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes key characteristics of event prediction from multiple cited papers, such as handling sparsity (Paper 1), capturing uncertainty in asynchronous events (Paper 2), forecasting civil unrest using open indicators (Paper 3), and data-driven prediction of extreme events (Paper 4). It provides a coherent narrative around challenges like multi-output predictions and dynamic data. However, the critical analysis is limited, as it primarily outlines problems rather than evaluating the strengths or weaknesses of the cited works. The section offers some abstraction by framing event prediction as a multi-objective problem with unique dependencies, but it does not fully generalize to a meta-level framework."}}
{"id": "32ca0d97-ce1e-43c2-a6a2-8b459454e5e4", "title": "Related Surveys", "level": "subsection", "subsections": [], "parent_id": "259e3016-29dd-4d94-91c7-f265b734bb7d", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Introduction"], ["subsection", "Related Surveys"]], "content": "This section briefly outlines previous surveys in various domains that have some relevance to  event prediction in big data in three categories, namely: 1. event detection, 2. predictive analytics, and 3. domain-specific event prediction. \nEvent detection has been an extensively explored domain with over many years. Its main purpose is to detect historical or ongoing events rather than to predict as yet unseen events in the future~. Event detection typically focuses on pattern recognition~, anomaly detection~, and clustering~, which are very different from those in event prediction. There have been several surveys of research in this domain  in the last decade . For example, Deng et al.~ and Atefeh and Khreich~ provided  overviews of event extraction techniques in social media, while Michelioudakis et al.~ presented a survey of event recognition with uncerntainty. Alevizos et al.~ provided a comprehensive literature review of event recognition methods using probabilistic methods.\nPredictive analysis covers the prediction of target variables given a set of dependent variables. These target variables are typically homogeneous scalar or vector data for describing items  such  as economic indices, housing prices, or sentiments. The target variables may not necessarily be values in the future. Larose~ provides a good tutorial and survey for this domain. Predictive analysis can be broken down into subdomains such as structured prediction~, spatial prediction~, and sequence prediction~, enabling users to handle different types of structure for the target variable. F{\\\"u}l{\\\"o}p et al.~ provided a survey and categorization of applications that utilize predictive analytics techniques to perform event processing and detection, while Jiang~ focused on spatial prediction methods that predict the indices that have spatial dependency. Baklr et al.~ summarized the literature on predicting structural data such as geometric objects and networks, and Arias et al.~ Phillips et al.~, and Yu and Kak~ all proposed the techniques for predictive analysis using social data.\nAs event prediction methods are typically motivated by specific application domains, there are a number of surveys event predictions for domains such as flood events~, social unrest~, wind power ramp forecasting~, tornado events~, temporal events without location information~, online failures~, and business failures~. However, in spite of its promise and its rapid growth in recent years, the domain of event prediction in big data still suffers from the lack of a comprehensive and systematic literature survey covering all its various aspects, including relevant techniques, applications, evaluations, and open problems.", "cites": [344, 343, 166, 342], "cite_extract_rate": 0.17391304347826086, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.7, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information by grouping related surveys into three categories and connecting their relevance to event prediction in big data. It provides some abstraction by identifying common themes and limitations across the domains. However, the critical analysis is somewhat limited, with only brief mentions of shortcomings without deeper evaluation or comparison of the cited works."}}
{"id": "4328a3f1-5620-4dc1-a83c-161ef2a0d369", "title": "Problem Formulation", "level": "subsection", "subsections": [], "parent_id": "a1fdb3a3-fa2f-4b15-9a7d-81755feafd07", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Problem Formulation and Performance Evaluations"], ["subsection", "Problem Formulation"]], "content": "\\label{sec:problem_formulation}\nAn event refers to a real-world occurrence that happens at some specific time and location with specific semantic topic~. We can use $y=(t,l,s)$ to denote an event where its time $t\\in\\mathcal T$, its location $l\\in\\mathcal{L}$, and its semantic meaning $s\\in\\mathcal{S}$. Here, $\\mathcal T$, $\\mathcal L$, and $\\mathcal S$ represent the time domain, location domain, and semantic domain, respectively. Notice that these domains need to have very general meanings that cover a wide range of types of entities. For example, the location $\\mathcal L$ can include any features that can be used to locate the place of an event in terms of a point or a neighborhood in either Euclidean space (e.g., coordinate and geospatial region) or non-Euclidean space (e.g., a vertex or subgraph in a network). Similarly, the semantic domain $\\mathcal S$ can contain any type of semantic features that are useful when elaborating the semantics of an event's various aspects, including its actors, objects, actions, magnitude, textual descriptions, and other profiling information. For example, \\emph{(``11am, June 1, 2019'', ``Hermosillo, Sonora, Mexico'', ``Student Protests'')} and \\emph{(``June 1, 2010'', “Berlin, Germany'', ``Red Cross helps pandemics control'')} denote the time, location, and semantics, for two events respectively.\nAn event prediction system requires inputs that could indicate future events, called event indicators, and these could contain both critical information on events that precede the future event, known as precursors, as well as irrelevant information~. Event indicator data can be denoted as $X\\subseteq\\mathcal{T}\\times\\mathcal{L}\\times\\mathcal{F}$, where $\\mathcal{F}$ is the domain of the features other than location and time. If we denote the current time as $t_{\\mathrm{now}}$ and define the past time and future time as $\\mathcal{T}^{-}\\equiv\\{t|t\\le t_{\\mathrm{now}},t\\in\\mathcal{T}\\}$ and $\\mathcal{T}^{+}\\equiv\\{t|t> t_{\\mathrm{now}},t\\in\\mathcal{T}\\}$, respectively, the event prediction problem can now be formulated as follows:\n\\begin{definition}[Event Prediction] Given the event indicator data $X\\subseteq\\mathcal{T}^{-}\\times\\mathcal{L}\\times\\mathcal{F}$ and historical event data $Y_0\\subseteq\\mathcal{T}^{-}\\times\\mathcal{L}\\times\\mathcal{S}$, event prediction is a process that outputs a set of predicted future events $\\hat Y\\subseteq\\mathcal{T}^{+}\\times\\mathcal{L}\\times\\mathcal{S}$, such that for each predicted future event $\\hat y=(t,l,s)\\in \\hat Y$ where $t>t_{\\mathrm{now}}$.\n\\end{definition}\nNot every event prediction method necessarily focuses on predicting all three domains of time, location, and semantics simultaneously, but may instead predict any part of them. For example, when predicting a clinical event such as the recurrence of disease in a patient, the event location might not always be meaningful~, but when predicting outbreaks of seasonal flu, the semantic meaning is already known and the focus is the location and time~ and when predicting political events, sometimes the location, time, and semantics (e.g., event type, participant population type, and event scale) are all necessary~. Moreover, due to the intrinsic nature of time, location, and semantic data, the prediction techniques and evaluation metrics of them are necessarily different, as described in the following.", "cites": [341], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section formulates the event prediction problem in a structured, abstract manner and introduces the key components such as event indicators, time domains, and semantic features. It incorporates a cited paper (EMBERS) to provide a concrete example, though it does not deeply synthesize insights from multiple sources. Critical analysis is limited, as it primarily describes the problem and example systems without evaluating their strengths or limitations. The abstraction is moderate, as it generalizes the problem formulation and acknowledges varying prediction focuses (time, location, semantics) across different domains."}}
{"id": "4bedfae7-0c35-44d7-8abd-d94a4f78bca8", "title": "Matching predicted events and real events.", "level": "subsubsection", "subsections": [], "parent_id": "4dd48b70-fd8e-40e4-94b7-33662c5dd512", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Problem Formulation and Performance Evaluations"], ["subsection", "Event Prediction Evaluation"], ["subsubsection", "Matching predicted events and real events."]], "content": "\\label{sec:matching}\nThe following two types of matching are typically used: \n\\begin{itemize}\n\\item\\textbf{Prefixed matching:} The predicted events will be matched with the corresponding ground-true real events if they share some key attributes. For example, for event prediction at a particular time and location point, we can evaluate the prediction against the ground truth for that time and location. This type of matching is most common when each of the prediction results can be uniquely distinguished along the predefined attributes (for example, location and time) that have a limited number of possible values, so that one-on-one matching between the predicted and real events are easily achieved~. For example, to evaluate the quality of a predicted event on June 1, 2019 in San Francisco, USA, the true event occurrence on that date in San Francisco can be used for the evaluation.\n\\item\\textbf{Optimized matching:} In situations where one-on-one matching is not easily achieved for any event attribute, then the set of predicted events might need to assess the quality of the match achieved with the set of real events, via an optimized matching strategy~. For example, consider two predictions, \\textbf{Prediction 1:} (``9am, June 4, 2019'', ``Nogales, Sonora, Mexico'', ``Worker Strike''), and \\textbf{Prediction 2}: (``11am, June 1, 2019'', ``Hermosillo, Sonora, Mexico'', ``Student Protests''). The two ground truth events that these can usefully be compared with are \\textbf{Real Event 1}: (``9am, June 1, 2019'', ``Hermosillo, Sonora, Mexico'', ``Teacher Protests''), and \\textbf{Real Event 2}: (``June 4, 2019'', ``Navojoa, Sonora, Mexico'', ``General-population Protest''). None of the predictions are an exact match for any of the attributes of the real events, so we will need to find a ``best'' matching among them, which in this case is between \\textbf{Prediction 1} and \\textbf{Real Event 2} and \\textbf{Prediction 2} and \\textbf{Real Event 1}. This type of matching allows some degree of inaccuracy in the matching process by quantifying the distance between the predicted and real events among all the attribute dimensions. The distance metrics are typically either Euclidean distance~ or some other distance metric~. Some researchers have hired referees to manually check the similarity of semantic meanings~, but another way is to use event coding to code the events into an event type taxonomy and then consider a match to have been achieved if the event type matches~.\nBased on the distance between each pair of predicted and real events, the optimal matching will be the one that results in the smallest average distance~. However, suppose there are $m$ predicted events and $n$ real events, then there can be as many as $2^{m\\cdot n}$ possible ways of matching, making it prohibitively difficult to find the optimal solution. Moreover, there could be different rules for matching. For example, the ``multiple-to-multiple'' rule shown in Figure \\ref{fig:event_matching}(a) allows one predicted (real) event to match multiple real (predicted) events~, while ``Bipartite matching'' only allows one-to-one matching between predicted and real events (Figure \\ref{fig:event_matching}(b)). ``Non-crossing matching'' requires that the real events matched by the predicted events follow the same chronological order (Figure \\ref{fig:event_matching}(c)). In order to utilize any of these types of matching, researchers have suggested using event matching optimization to learn the optimal set of ``(real event, predicted event)'' pairs~.\n\\end{itemize}\n\\begin{figure}[htb]\n  \\centering\n    \\includegraphics[width=\\textwidth]{images/matching_events.jpg}\\vspace{-0.3cm}\n\\caption{Generic framework for hierarchical RNN-based event forecasting.\\vspace{-0.3cm}}\n\\label{fig:event_matching}\n\\end{figure}", "cites": [8343, 341], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from the cited EMBERS papers to explain two distinct matching strategies for event prediction evaluation—prefixed and optimized matching. It abstracts the problem into a general framework, discussing distance metrics and matching rules like bipartite and non-crossing. While it identifies challenges such as computational complexity and different matching rules, it stops short of deeply critiquing the effectiveness or limitations of the approaches in the literature."}}
{"id": "d3502027-6bbb-4d5b-bdf0-90e57f4a8b2a", "title": "Metrics of Effectiveness", "level": "subsubsection", "subsections": [], "parent_id": "4dd48b70-fd8e-40e4-94b7-33662c5dd512", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Problem Formulation and Performance Evaluations"], ["subsection", "Event Prediction Evaluation"], ["subsubsection", "Metrics of Effectiveness"]], "content": "\\label{sec:acc_err}\nThe effectiveness of the event predictions is evaluated in terms of two indicators: 1) Goodness of Matching, which evaluates performance metrics such as the number and percentage of matched events~, and 2) Quality of Matched Predictions, which evaluates how close the predicted event is to the real event for each pair of matched events~.\n\\begin{itemize}\n\\item\\textbf{Goodness of Matching.} A \\emph{true positive} means a real event has been successfully matched by a predicted event; if a real event has not been matched by any predicted event, then it is called a \\emph{false negative} and a \\emph{false positive} means a predicted event has failed to match any real event, which is referred to as a \\emph{false alarm}. Assume the total number of predictions is $N$, the number of real events is $\\hat N$, the number of true positives is $N_{TP}$, the number of false negatives is $N_{FN}$ and the number of false positives is $N_{FP}$. Then, the following key evaluation metrics can be calculated: Prediction=$N_{TP}/(N_{TP}+N_{FP})$, Recall=$N_{TP}/(N_{TP}+N_{FN})$, F-measure = $2\\cdot\\mathrm{Precision}\\cdot\\mathrm{Recall}/(\\mathrm{Precision}+\\mathrm{Recall})$. Other measurements such as the area under the ROC curves are also commonly used~. This approach can be extended to include other items such as multi-class precision/recall, and Precision/Recall at Top $K$~.\n\\item\\textbf{Quality of Matched Predictions.}\nIf a predicted event matches a real one, it is common to go on to evaluate how close they are. This reflects the quality of the matched predictions, in terms of different aspects of the events. Event time is typically a numerical values and hence can be easily measured in terms of metrics such as mean squared error, root mean squared error, and mean absolute error~. This is also the case for location in Euclidean space, which can be measured in terms of the Euclidean distance between the predicted point (or region) and the real point (or region). Some researchers consider the administrative unit resolution. For example, a predicted location (``New York City'', ``New York State'', ``USA'') has a distance of 2 from the real location (``Los Angeles'', ``California'', ``USA'')~. Others prefer to measure multi-resolution location prediction quality as follows: $(1/3)(l_{country}+l_{country}\\cdot l_{state}+l_{country}\\cdot l_{state}\\cdot l_{city})$, where $l_{city}$, $l_{state}$, and $l_{country}$ can only be either $0$ (i.e., no match to the truth) or $1$ (i.e., completely matches the truth)~. For a location in non-Euclidean space such as a network~, the quality can be measured in terms of the shortest path length between the predicted node (or subgraph) and the real node (or subgraph), or by the F-measure between the detected subsets of nodes against the real ones, which is similar to the approach for evaluating community detection~. For event topics, in addition to conventional ways of evaluating continuous values such as population size, ordinal values such as event scale, and categorical values such as event type, actors, and actions, as well as more complex semantic values such as texts, can be evaluated using Natural Language Process measurements such as edit distance, BLEU score, Top-$K$ precision, and ROUGE~.\n\\end{itemize}", "cites": [341], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear analytical framework by categorizing evaluation metrics into two overarching dimensions—Goodness of Matching and Quality of Matched Predictions—and elaborates on how different aspects (time, location, topics) are assessed. It integrates ideas from the cited work (e.g., EMBERS) implicitly while generalizing to broader principles in event prediction evaluation. While it lacks explicit comparisons or critiques of the cited approaches, the level of abstraction and synthesis is strong, leading to a high insight level."}}
{"id": "53f5b3d4-464b-4905-b5fb-aea3535cd2c7", "title": "Occurrence Prediction", "level": "subsubsection", "subsections": [], "parent_id": "38bc1c87-73d2-4a36-9803-aac9d2369721", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Time Prediction"], ["subsubsection", "Occurrence Prediction"]], "content": "Occurrence prediction is arguably the most extensive, classical, and generally simplest type of event time prediction task~. It focuses on identifying whether there will be event occurrence (positive class) or not (negative class) in a future time period~. This problem is usually formulated as a binary classification problem, although a handful of other methods instead leverage anomaly detection or regression-based techniques.\n\\noindent\\textbf{1. Binary classification.} Binary classification methods have been extensively explored for event occurrence prediction. The goal here is essentially to estimate and compare the values of $f(y=``Yes''|X)$ and $f(y=``No''|X)$, where the former denotes the score or likelihood of event occurrence given observation $X$ while the latter corresponds to no event occurrence. If the value of the former is larger than the latter, then a future event occurrence is predicted, but if not, there is no event predicted. To implement $f$, the methods typically used rely on discriminative models, where dedicated feature engineering is leveraged to manually extract potential event precursor features to feed into the models. Over the years, researchers have leveraged various binary classification techniques ranging from the simplest threshold-based methods~, to more sophisticated methods such as logistic regression~, Support Vector Machines~, (Convolutional) Neural Networks~, and decision trees~. In addition to discrminative models, generative models~ have also been used to embed human knowledge for classifying event occurrences using Bayesian decision techniques. Specifically, instead of assuming that the input features are independent, prior knowledge can also be directly leveraged to establish Bayesian networks among the observed features and variables based on graphical models such as (semi-)hidden Markov models~ and autoregresive logit models~. The joint probabilities $p(y=``Yes'',X)$ of $p(y=``No'',X)$ can thus be estimated using graphical models, and then utilized to estimate $f(y=``Yes''|X)=p(y=``Yes''|X)$ and $f(y=``No''|X)=p(y=``No''|X)$ using Bayesian rules~.\n\\noindent\\textbf{2. Anomaly detection.} Alternatively, anomaly detection can also be utilized to learn a ``prototype'' of normal samples (typical values corresponding to the situation of no event occurrence), and then identify if any newly-arriving sample is close to or distant from the normal samples, with distant ones being identified as future event occurrences. Such methods are typically utilized to handle ``rare event'' occurrences, especially when the training data is highly imbalanced with little to no data for ``positive'' samples. Anomaly detection techniques such as one-classification~ and hypotheses testing~ are often utilized here.\n\\noindent\\textbf{3. Regression.} In addition to simply predicting the occurrence or not, some researchers have sought to extend the binary prediction problem to deal with ordinal and numerical prediction problems, including event count prediction based on (auto)regression~, event size prediction using linear regression~, and event scale prediction using ordinal regression~.", "cites": [8344], "cite_extract_rate": 0.043478260869565216, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of occurrence prediction methods (binary classification, anomaly detection, regression) and briefly mentions techniques used in the cited paper. However, it lacks in-depth synthesis of ideas across sources, critical evaluation of the methods, and abstraction to broader principles. It remains largely descriptive without offering a cohesive analysis or meta-level insights."}}
{"id": "25fc629e-0d7b-4dbd-8fa3-37efada755ed", "title": "Continuous-time Prediction.", "level": "subsubsection", "subsections": [], "parent_id": "38bc1c87-73d2-4a36-9803-aac9d2369721", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Time Prediction"], ["subsubsection", "Continuous-time Prediction."]], "content": "Discrete-time prediction methods, although usually simple to establish, also suffer from several issues. First, their time-resolution is limited to the discretization granularity; increasing this granularity significantly increases the computations al resources required, which means the resolution cannot be arbitrarily high. Moreover, this trade-off is itself a hyperparameter that is sensitive to the prediction accuracy, rendering it difficult and time-consuming to tune during training. To address these issues, a number of techniques work around it by directly predicting the continuous-valued event time~, usually by leveraging one of three techniques.\n\\noindent\\textbf{1. Simple Regression. }The simplest methods directly formalize continuous-event-time prediction as a regression problem~, where the output is the numerical-value future event time~ and/or their duration~. Common regressors such as linear regression and recurrent neural networks have been utilized  for this. Despite their apparent simplicity, this is not straightforward as simple regression typically assumes Gaussian distribution~, which does not usually reflect the true distribution of event times. For example, the future event time needs to be left-bounded (i.e., larger than the current time), as well asbeing typically non-symmetric and usually periodic, with recurrent events having multiple peaks in the  probability density function along the time  dimension.\n\\noindent\\textbf{2. Point Processes.} As they allow more flexibility in fitting true event time distributions, point process methods~ are widely leveraged and have demonstrated their effectiveness for continuous time event prediction tasks. They require a conditional intensity function, defined as follows:\n\\begin{align}\n\\label{eq:intensity}\n    \\lambda(t|X)=\\mathbb{E}[N(t,t+\\mathrm{d}t)/{\\mathrm{d}t}|X]=g(t|X)/(1-G(t|X))\n\\end{align}\nwhere $g(t|X)$ is the conditional density function of the event occurrence probability at time $t$ given an observation $X$, and whose corresponding cumulative distribution function, $G(t|X))$, $N(t,t+\\mathrm{d}t)$, denotes the count of events during the time period between $t$ and $t+\\mathrm{d}t$, where $\\mathrm{d}t$ is an infinitely-small time period.\nHence, by leveraging the relation between density and accumulative functions and then rearranging Equation \\eqref{eq:intensity}, the following conditional density function is obtained:\n\\begin{align}\n    g(t|X)=\\lambda(t|X)\\cdot\\mathrm{exp}\\cdot\\left(-\\int_{t_0}^t\\lambda(u|X)\\mathrm{d}u\\right)\n\\end{align}\n\\indent Once the above model has been trained using a technique such as maximal likelihood~, the time of the next event in the future is predicted as:\n\\begin{align}\n    \\hat t=\\int_{t_0}^{\\infty}t\\cdot g(t|X)\\mathrm{d}t\n\\end{align}\nAlthough existing methods typically share the same workflow as that shown above, they vary in the way they define the conditional intensity function $\\lambda(t|X)$. Traditional models typically utilize prescribed distributions such as the Poisson distribution~, Gamma distribution~, Hawks~, Weibull process~, and other distributions~. For example, Damaschke et al.~ utilized a Weibull distribution to model volcano eruption events, while Ertekin et al.~ instead proposed the use of a non-homogeneous Poisson process to fit the conditional intensity function for power system failure events. However, in many other situations where there is no information regarding appropriate prescribed distributions, researchers must start by leveraging nonparametric approaches to learn sophisticated distributions from the data using expressive models such as neural networks. For example, Simma and Jordan~ utilized of RNN to learn a highly nonlinear function of $\\lambda(t|X)$.\n\\noindent\\textbf{3. Survival Analysis}. Survival analysis~ is related to point processes in that it also defines an event intensity or hazard function, but in this case based on survival probability considerations, as follows:\n\\begin{align}\n\\label{eq:survival_analysis}\n    H(t|X)=\\left(\\xi(t-\\mathrm{d}t|X)-\\xi(t|X)\\right)/\\xi(t|X),\\ \\mathrm{where }\\ \\xi(t|X)=p(\\hat t>t)\n\\end{align}\nwhere $H(t|X)$ is the so-called Hazard function denoting the hazard of event occurrence between time $(t-\\mathrm{d}t)$ for a $t$ for a given observation $X$. Either $H(t|X)$ or $\\xi(t|X)$ could be utilized for predicting the time of future events. For example, the event occurrence time can be estimated when $\\xi(t|X)$ is lower than a specific value. Also, one can obtain $\\xi(t|X)=\\mathrm{exp\\left(-\\int_0^t H(u|X)\\mathrm{d}u\\right)}$ according to Equation \\eqref{eq:survival_analysis}~. Here $H(t|X)$ can adopt any one of several prescribed models, such as the well-known Cox hazard model~. To learn the model directly from the data, some researchers have recommended enhancing it using deep neural networks~. Vahedian et al.~ suggest learning the survival probability $\\xi(t|X)$ and then applying the function $H(\\cdot|X)$ to indicate an event at time $t$ if $H(t|X)$ is larger than a predefined threshold value. A classifier can also be utilized.\nInstead of using the raw sequence data, the conditional intensity function can also be projected onto additional continuous-time latent state layers that eventually map to the observations~. These latent states can then be extracted using techniques such as hidden semi-Markov models~, which ensure the elicitation of the continuous time patterns.", "cites": [9104, 345, 346, 9105], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by connecting the core concepts of regression, point processes, and survival analysis in the context of continuous-time event prediction. It critically discusses the limitations of simple regression and the need for more flexible models like point processes and survival analysis. The abstraction level is strong, as it identifies general modeling strategies and mathematical formulations that underpin the approaches, such as conditional intensity and hazard functions, while also pointing to the broader use of deep learning for nonparametric modeling."}}
{"id": "7eb012cf-4f90-44e9-af3d-10d78151a2f6", "title": "Raster-based Location Prediction", "level": "subsubsection", "subsections": [], "parent_id": "16b7bc6c-2456-4e9d-850f-7cd33640592e", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Location Prediction"], ["subsubsection", "Raster-based Location Prediction"]], "content": "There are three types of techniques used for raster-based event location prediction, namely spatial clustering, spatial embedding, and spatial convolution.\n\\noindent\\textbf{1. Spatial clustering}. In raster-based representations, each location unit is usually a regular grid cell with the same size and shape. However, regions with similar spatial characteristics typically have irregular shapes and sizes, which could be approximated as composite representations of a number of grids~. The purpose of spatial clustering here is to group the contiguous regions who collectively exhibit significant patterns. The methods are typically agglomerative style. They typically start from the original finest-grained spatial raster units and proceed by merging the spatial neighborhood of a specific unit in each iteration. But different research works define different criteria for instantiating the merging operation. For example, Wang and Ding~ merge neighborhoods if the unified region after merging can maintain the spatially frequent patterns. Xiong et al.~ chose an alternative approach by merging spatial neighbor locations into the current locations sequentially until the merged region possesses event data that is sufficiently statistically significant. These methods usually run in a greedy style to ensure their time complexity remains smaller than quadratic. After the spatial clustering  is completed, each spatial cluster will be input into the classifier to determine whether or not there is an event corresponding to it.\n\\noindent\\textbf{2. Spatial interpolation}. Unlike spatial clustering-based methods, spatial interpolation based methods maintain the original fine granularity of the event location information. The estimation of event occurrence probability can be further interpolated for locations with no historical events and hence achieve spatial smoothness. This can be accomplished using commonly-used methods such as kernel density estimation~ and spatial Kriging~. Kernel density estimation is a popular way to model the geo-statistics in numerous types of events such as crimes~ and terrorism~:\n\\begin{align}\n    k(s)=1/(n\\cdot \\gamma)\\sum\\nolimits_{i=1}^n K\\left((s-s_i)/\\gamma\\right)\n\\end{align}\nwhere $k(s)$ denotes the kernel estimation for the location point $s$, $n$ is the number of historical event locations, each $s_i$ is a historical event location, $\\gamma$ is a tunable bandwidth parameter, and $K(\\cdot)$ is a kernel function such as Gaussian kernel~. \nMore recently, Ristea et al.~ further extended KDE-based techniques by leveraging Localized KDE and then applying spatial interpolation techniques to estimate spatial feature values for the cells in the grid. Since each cell is an area rather than a point, the center of each cell is usually leveraged as the representative of this cell. Finally, a classifier will take this as its input to predict the event occurrence for each grid~.\n\\noindent\\textbf{3. Spatial convolution}. In the last few years, Convolutional Neural Networks (CNNs) have demonstrated significant success in learning and representing sophisticated spatial patterns from image and spatial data~. A CNN contains multiple convolutional layers that  extract the hierarchical spatial semantics of images. In each convolutional layer, a convolution operation is executed by scanning a feature map with a filter, which results in another smaller feature map with a higher level semantic. Since raster-based spatial data and images share a similar mathematical form, it is natural to leverage CNNs to process it. \nExisting methods~ in this category typically formulate a spatial map as input to predict another spatial map that denotes future event hotspots. Such a formulation is analogous to the ``image translation'' problem popular in recent years in the computer vision domain~. Specifically, researchers typically leverage an encoder-decoder architecture, where the input images (or spatial map) are processed by multiple convolutional layers into a higher-level representation, which is then decoded back into an output image with the same size, through a reverse convolutional operations process known as transposed convolution~.\n\\noindent\\textbf{4. Trajectory destination prediction.} This type of method typically focuses on population-based events whose patterns can be interpreted as the collective behaviors of individuals, such as ``gathering events'' and ``dispersal events''. These methods share a unified procedure that typically consists of two steps: 1) predict future locations based on the observed trajectories of individuals, and 2) detect the occurrence of the ``future'' events based on the future spatial patterns obtained in Step 1. The specific methodologies for each step are as follows:\n\\begin{itemize}\n    \\item \\textbf{Step 1: } Here, the aim is to predict each location an  individual will visit in the future, given a historical sequence of locations visited. This can be formulated as a sequence prediction problem. For example, Wang and Gerber~ sought to predict the probability of the next time point $t+1$'s location $s_{t+1}$ based on all the preceding time points: $p(s_{t+1}|s_{\\le t})=p(s_{t+1}|s_{t},s_{t-1},\\cdots,s_{0})$, based on various strategies including a historical volume-based prior model, Markov models, and multi-class classification models. Vahedian et al.~ adopted Bayesian theory $p(s_{t+1}|s_{\\le t})=p(s_{\\le t}|s_{t+1})\\cdot p(s_{t+1})/p(s_{\\le t})$ which requires the conditional probability $p(s_{\\le t}|s_{t+1})$ to be stored. However, in many situations, there is huge number of possible trajectories for each destination. For example, with a $128\\times 64$ grid, one needs to store $(128\\times 64)^3\\approx 5.5\\times 10^{11}$ options. To improve the memory efficiency, this can be limited to a consideration of just the source and current locations, leveraging a quad-tree style architecture to store the historical information. To achieve more efficient storage and speed up $p(s_{\\le t}|s_{t+1})$ queries, Vahedian et al.~ further extended the quad-tree into a new technique called VIGO, which removes duplicate destination locations in different leaves.\n    \\item \\textbf{Step 2: } The aim in this step is to forecast future event locations based on the future visiting patterns predicted in Step 1. The most basic strategy here is to consider each grid cell independently. For example, Wang and Gerber~ adopted supervised learning strategies to build predictive mapping between the visiting patterns and the event occurrence. A more sophisticated approach is to consider the spatial outbreaks composited by multiple grids. Scalable algorithms have also been proposed to identify regions containing statistically significant hotspots~, such as spatial scan statistics~. Khezerlou et al.~ proposed a greedy-based heuristic tailored for the grid-based data formulation, which extends the original ``seed'' grid containing statistically-large future event densities to four directions until the extended region is no longer a statistically-significant outbreak.\n\\end{itemize}", "cites": [347, 166, 59], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to explain raster-based event location prediction techniques, clearly grouping them into spatial clustering, interpolation, and convolution. It connects their approaches and integrates them into a structured overview. While it provides some critical insights, such as the memory challenges in trajectory-based prediction, it does not deeply evaluate or contrast the trade-offs of each method. The abstraction is moderate, as it identifies general patterns and methodological categories but stops short of offering a higher-level theoretical framework."}}
{"id": "3692ca18-9165-42fb-b67f-304b8a112583", "title": "Point-based Prediction", "level": "subsubsection", "subsections": [], "parent_id": "16b7bc6c-2456-4e9d-850f-7cd33640592e", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Location Prediction"], ["subsubsection", "Point-based Prediction"]], "content": ". Unlike the raster-based formulation, which covers the prediction of a contiguous spatial region, point-based prediction focuses specifically on locations of interest, which can be distributed sparsely in a Euclidean (e.g., spatial region) or non-Euclidean space (e.g., graph topology). These methods can be categorized into supervised and unsupervised approaches.\n\\noindent\\emph{1. Supervised approaches.}\nIn supervised methods, each location will be classified as either ``positive'' or ``negative'' with regard to a future event occurrence. The simplest setting is based on the independent and identically distributed (i.i.d.) assumption among the locations, where each location is predicted by a classifier independently using their respective input features. However, given that different locations usually have strong spatial heterogeneity and dependency, further research has been proposed to tackle them based on different locations' predictors and outputs, resulting in two research directions: 1) Spatial multi-task learning, and 2) Spatial auto-regressive methods.\n\\begin{itemize}\n\\item\\textbf{Spatial multi-task learning.} Multi-task learning is a popular learning strategy that can jointly learn the models for different tasks such that the learned model can not only share their knowledge but also preserve some exclusive characteristics of the individual tasks~. This notion coincides very well with spatial event prediction tasks, where combining the outputs of models from different locations needs to consider both their spatial dependency and heterogeneity. Zhao et al.~ proposed a spatial multi-task learning framework as follows:\n\\begin{align}\n    \\min_{W}\\sum\\nolimits_i^m\\mathcal{L}(Y_i, f(W_i,X_i))+\\alpha\\cdot\\mathcal{R}(\\{W_i\\}_i^k,M),\\ \\  \\ s.t.,\\ \\mathcal{C}(\\{W_i\\}_i^m,D)\\in\\mathbb{C}\n\\end{align}\nwhere $m$ is the total number of locations (i.e., tasks), $W_i$ and $Y_i$ are the model parameters and true labels (event occurrence for all time points), respectively, of task $i$. $\\mathcal{L}(\\cdot)$ is the empirical loss, $f(W_i,X_i)$ is the predictor for task $i$, and $\\mathcal{R}(\\cdot)$ is the spatial regularization term based on the spatial dependency information $M\\in\\mathbb{R}^{m\\times m}$, where $M_{i,j}$ records the spatial dependency between location $i$ and $j$. $\\mathcal{C}(\\cdot)$ represents the spatial constraints imposed over the corresponding models to enforce them to remain within the valid space $\\mathbb{C}$. Over recent years, there have been multiple studies proposing different strategies for $\\mathcal{R}(\\cdot)$ and $\\mathcal{C}(\\cdot)$. For example, Zhao et al.~ assumed that all the locations would be evenly correlated and enforced their similar sparsity patterns for feature selection, while Gao et al.~ further extended this to differentiate the strength of the correlation between different locations' tasks according to the spatial distance between them. This research has been further extended this approach to tree-structured multitask learning to handle the hierarchical relationship among locations at different administrative levels (e.g., cities, states, and countries)~ in a model that also considers the logical constraints over the predictions from different locations who have hierachical relationships. Instead of evenly similar, Zhao, et al.~ further estimated spatial dependency $D$ utilizing inverse distance using Gaussian kernels, while Ning et al.~ proposed estimating the spatial dependency $D$ based on the event co-occurrence frequency between each pair of locations.\n\\item\\textbf{Spatial auto-regressive methods}. Spatial auto-regressive models have been extensively explored in domains such as geography and econometrics, where they are applied to perform predictions where the i.i.d. assumption is violated due to the strong dependencies among neighbor locations. Its generic framework is as follows:\n\\begin{align}\n    \\hat Y_{t+1}=\\rho M \\hat Y_{t+1}+X_t\\cdot W+\\varepsilon\n\\end{align}\nwhere $X_t\\in\\mathbb{R}^{m\\times D}$ and $\\hat Y_{t+1}\\in\\mathbb{R}^{m\\times m}$ are the observations at time $t$ and event predictions at time $t+1$ over all the $m$ locations, and $M\\in\\mathbb{R}^{m\\times m}$ is the spatial dependency matrix with zero-valued diagonals. This means the prediction of each location $\\hat Y_{t+1,i}\\in \\hat Y_{t+1}$ is jointly determined by its input $X_{t,i}$ and neighbors $\\{j|M_{i,j}\\ne 0\\}$  and $\\rho$ is a positive value to balance these two factors. Since event occurrence requires discrete predictions, simple threshold-based strategies can be used to discretize $\\hat Y_i$ into $\\hat Y_i'=\\{0,1\\}$~. Moreover, due to the complexity of event prediction tasks and the large number of locations, sometimes it is difficult to define the whole $M$ manually. Zhao et al.~ proposed jointly learning the prediction model and spatial dependency from the data using graphical LASSO techniques. Yi et al.~ took a different approach,  leveraging conditional random fields to instantiate the spatial autoregression, where the spatial dependency is measured by Gaussian kernel-based metrics. Yi et al.~ then went on to propose leveraging the neural network model to learn the locations' dependency.\n\\end{itemize}\n\\noindent\\emph{2. Unsupervised approaches}. Without supervision from labels, unsupervised-based methods must first identify potential precursors and determinant features in different locations. They can then detect anomalies that are characterized by specific feature selection and location combinatorial patterns (e.g., spatial outbreaks and connected subgraphs) as the future event indicators~. The generic formulation is as follows:\n\\begin{align}\n\\label{eq:scan}\n    (F,R)=\\arg\\max\\nolimits_{F,R}\\  q(F,R)\\ \\ \\  s.t.,\\ \\mathrm{supp}(F_i)\\in\\mathbb{M}(\\mathbb{G},\\beta),\\ \\mathrm{supp}(R_i)\\in\\mathbb{C}\n\\end{align}\nwhere $q(\\cdot)$ denotes scan statistics which score the significance of each candidate pattern, represented by both a candidate location combinatorial pattern $R$ and feature selection pattern $F$. Specifically, $F\\in\\{0,1\\}^{D'\\times n}$ denotes the feature selection results (where ``1'' means selected; ``0'', otherwise) and $R\\in\\{0,1\\}^{m\\times n}$ denotes the $m$ involved locations for the $n$ events. $\\mathbb{M}(\\mathbb{G},\\beta)$ and $\\mathbb{C}$ are the set of all the feasible solutions of $F$ and $R$, respectively. $q(\\cdot)$ can be instantiated by scan statistics such as Kulldorff's scan statistics~ and the Berk-Jones statistic~, which can be applied to detect and forecast events such as epidemic outbreaks and civil unrest events~. Depending on whether the embedding space is an Euclidean region (e.g., a geographical region) or a non-Euclidean region (e.g., a network topology), the pattern constraint $\\mathcal{C}$ can be either constrained to predefined geometric shapes such as a circle, rectangle, or an irregular shape or subgraphs such as connected, cliques, and k-cliques. The problem in Equation \\eqref{eq:scan} is nonconvex and sometimes even discrete, and hence difficult to solve. A generic way is to optimize $F$ using sparse feature selection; there is a useful survey provided in~ and $R$  can be defined  using the two-step graph-structured matching method detailed in. More recently, new techniques have been developed that are capable of jointly learning both feature and location selection~.", "cites": [9106, 348, 341], "cite_extract_rate": 0.2, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple approaches to point-based location prediction by categorizing them into supervised and unsupervised methods and connecting them with spatial dependency modeling. It provides a critical discussion by highlighting the limitations of the i.i.d. assumption and the difficulty in defining spatial dependency matrices manually. The abstraction is strong, as the section formulates general mathematical frameworks and identifies broader patterns like the use of spatial regularization and joint learning of dependencies."}}
{"id": "f9095edf-4e9e-4cc4-8180-071c171e0ec4", "title": "Causality-based prediction", "level": "subsubsection", "subsections": [], "parent_id": "0b117367-592e-4a5b-aa08-4c81aba9a952", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Semantic Prediction"], ["subsubsection", "Causality-based prediction"]], "content": "This type of research leverages the causality inferred among the historical events to achieve future event predictions. The  data here typically shares a generic framework consisting of the following procedures: 1) event representation, 2) event graph construction, and 3) future event inference.\n\\noindent\\textbf{Step 1: Event semantic representation. }This approach typically begins by extracting the events from the target texts using natural language processing techniques such as sanitization, tokenization, POS tag analysis, and name entity recognition. Several types of objects can be extracted to represent the events: i) Noun Phrase-based~, where the noun-phrase corresponds to each event (for example, ``2008 Sichuan Earthquake''); ii) Verbs and Nouns~, where an event is represented as a set of noun-verb pairs extracted from news headlines (for example, ``<capture, people>'', ``<escape, prison>'', or ``<send, prison>''); and iii) Tuple-based~, where each event is represented by a tuple consisting of objects (such as actors, instruments, or receptors), a relationship (or property), and time. An RDF-based format has also been leveraged in some works~.\n\\noindent\\textbf{Step 2: Event causality inference. } The goal here is to infer the cause-effect pairs among historical events. Due to its combinatorial nature, narrowing down the number of candidate pairs is crucial. Existing works usually begin by clustering the events into event chains, each of which consist of a sequence of time-ordered events under the relevant semantics, typically the same topics, actors, and/or objects~. The causal relations among the event pairs can then be inferred in various ways. The simplest approach is just to consider the likelihood that $y$ occurs after $x$ has occurred throughout the training data. Other methods utilize NLP techniques to identify causal mentions such as causal connectives, prepositions, and verbs~. Some formulate causal-effect relationship identification as a classification task where the inputs are the cause and effect candidate events, often incorporating contextual information including related background knowledge from web texts. Here, the classifier is built on a multi-column CNN that outputs either ``1'' or ``0'' to indicate whether the candidate has an effect or not~. In many situations, the cause-effect rules learned directly using the above methods can be too specific and sparse, with low generalizability, so a typical next step is to generalize the learned rules. For example, ``Earthquake hits China'' $\\rightarrow$ ``Red Cross help sent to Beijing'' is a specific rule that can be generalized to ``Earthquake hits [A country]'' $\\rightarrow$ ``Red Cross help sent to [The capital of this country]''. To achieve this, some external ontology or a knowledge base is typically needed in order to establish the underlying relationships among items or provide necessary information on their properties, such as Wikipedia (\\url{https://www.wikipedia.org/}), YAGO~, WordNet~, or ConceptNet~. Based on these resources, the similarity between two cause-effect pairs $(c_i,\\varepsilon_i)$ and $(c_j,\\varepsilon_j)$ can be computed by jointly considering the respective similarity of the putative cause and effect: $\\sigma((c_i,\\varepsilon_i),(c_j,\\varepsilon_j))=(\\sigma(c_i,c_j)+\\sigma(\\varepsilon_i,\\varepsilon_j))/2$. An appropriate algorithm can then be utilized to apply hierarchical agglomerative clustering to group them and hence generate a data structure that can efficiently manage the task of storing and querying them to identify any cause-effect pairs. For example,~ leverage an abstraction tree, where each leaf is an original specific cause-effect pair and each intermediate node is the centroid of a cluster. Instead of using hierarchical clustering,~ directly uses the word ontology to simultaneously generalize cause and effect (e.g., the noun ``violet'' is generalized to ``purple'', the verb ``kill'' is generalized to ``murder-42.1\\footnote{the form of verb class in VerbNet~.}'') and then leverage a hierarchical causal network to organize the generalized rules.\n\\noindent\\textbf{Step 3: Future event Inference. } Given an arbitrary query event, two steps are needed to infer the future events caused by it based on the causality of events learned above. First, we need to retrieve similar events that match the query event from historical event pool. This requires the similarity between the query event and all the historical events to be calculated. To achieve this, Lei et al.~ utilized context information, including event time, location, and other environmental and descriptive information. For methods requiring event generalization, the first step is to traverse the abstraction tree starting from the root that corresponds to the most general event rule. The search frontier then moves across the tree if the child node is more similar, culminating in the nodes which are the least general but still similar to the new event being retrieved~. Similarly,~ proposed another tree structure referred to as a ``circular binary search tree'' to manage the event occurrence pattern. We can now apply the learned predicate rules starting from the retrieved event to obtain the prediction results. Since each cause event can lead to multiple events, a convenient way to determine the final prediction is to calculate the support~, or conditional probability~ of the rules. Radinsky et al.~ took a different approach, instead ranking the potential future events by their similarity defined by the length of their minimal generalization path. For example, the minimal\ngeneralization path for ``London'' and ``Paris'' is ``London''$\\xrightarrow{\\text{capital-of}}$``Great Brain''$\\xrightarrow{\\text{in-continent}}$``Europe''$\\xleftarrow{\\text{in-continent}}$``France''$\\xleftarrow{\\text{capital-of}}$``Paris''. Alternatively, Zhao et al.~ proposed embedding the event causality network into a continuous vector space and then applying an energy function designed to rank potential events, where true cause-effect pairs are assumed to have low energies.", "cites": [349], "cite_extract_rate": 0.058823529411764705, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing and integrating multiple causality-based prediction methods into a coherent three-step framework. It provides critical insights by highlighting limitations such as low generalizability and proposing solutions like rule abstraction using ontologies. The abstraction level is notable as it identifies patterns in event representation, causality inference, and future event retrieval, moving beyond specific works to outline overarching principles."}}
{"id": "3b662200-27f1-431a-8228-57bea3941428", "title": "Semantic Sequence", "level": "subsubsection", "subsections": [], "parent_id": "0b117367-592e-4a5b-aa08-4c81aba9a952", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Semantic Prediction"], ["subsubsection", "Semantic Sequence"]], "content": "These methods share a very straightforward problem formulation. Given a temporal sequence for a historical event chain, the goal is to predict the semantics of the next event using sequence prediction~. The existing methods can be classified into four major categories: 1) classical sequence prediction; 2) recurrent neural networks; 3) Markov chains; and 4) time series predictions.\n\\noindent\\textbf{Sequence classification-based methods}. These methods formulate event semantic prediction as a multi-class classification problem, where a finite number of candidate events are ranked and the top-ranked event is treated as the future event semantic. The objective is $\\hat C=\\arg\\max_{C_i} u(s_{T+1}=C_i|s_1,\\cdots,s_T)$, where $s_{T+1}$ denotes the event semantic in time slot $T+1$ and $\\hat C$ is the optimal semantic among all the semantic candidates $C_i$ ($i=1,\\cdots$). Multi-class classification problems can be split into events with different topics/semantic meaning. Three types of sequence classification methods have been utilized for this purpose, namely feature-based methods, prototype-based methods, and model-based methods such as Markov models. \n\\begin{itemize}\n    \\item \\textbf{Feature-based.} One of the simplest methods is to ignore the temporal relationships among the events in the chain, by either aggregating the inputs or the outputs. Tama and Comuzzi~ formulated historical event sequences with multiple attributes for event prediction, testing multiple conventional classifiers. Another type of approach based on this notion utilizes compositional based-methods~ that typically leverage the assumption of independency among the historical input events to simplify the original problem $u(s_{T+1}|s_1,s_2,\\cdots,s_T)=u(s_{T+1}|s_{\\le T})$ into $v(u(s_{T+1}|s_1),u(s_{T+1}|s_2),\\cdots,u(s_{T+1}|s_T))$ where $v(\\cdot)$ is simply an aggregation function that represents a summation operation over all the components. Each component function $u(s_{T+1}|s_i)$ can then be calculated by estimating how likely it is that event semantic $s_{T+1}$ and $s_i$ ($i\\le T$) co-occur in the same event chain. Granroth-Wilding and Clark~ investigated various models ranging from straightforward similarity scoring functions through bigram models and word embedding combined with similarity scoring functions to newly developed composition neural networks that jointly learn the representation of $s_{T+1}$ and $s_i$ and then calculate their coherence. Some other researchers have gone further to consider the dependency among the historical events. For example, Letham et al.~ proposed to optimizing the correct ordering among the candidate events, based on the following equation:\n\\begin{align}\\small\\nonumber\n    \\sum\\nolimits_{i\\in\\mathcal I,\\ j\\in\\mathcal J} \\mathbbm{1}_{[u(s_{T+1}=C_i|s_{\\le T})>u(s_{T+1}=C_j|s_{\\le T})]}\\implies \\sum\\nolimits_{i\\in\\mathcal I,\\ j\\in\\mathcal J} e^{u(s_{T+1}=C_i|s_{\\le T})-u(s_{T+1}=C_j|s_{\\le T})}+\\rho\\|W\\|_2^2\n\\end{align}\\normalsize\nwhere the semantic candidate in the set $\\mathcal I$ should be ranked strictly to be lower than those in $\\mathcal J$, with the goal being to penalize the ``incorrect ordering''. Here, $\\mathbbm{1}_{[\\cdot]}$ is an indicator function which is discrete such that $\\mathbbm{1}_{[b\\ge a]}\\le e^{b-a}$ and can thus be utilized as the upper-bound for minimization, as can be seen in the right-hand-side of the above equation. $W$ is the set of parameters of the function $u(\\cdot)$. This can now be relaxed to an exponential-based approximation for effective optimization using gradient-based algorithms~. Other methods focus on first transferring the sequential data into sequence embeddings that can encode the latent sequential context. For example, Fronza et al.~ apply random indexing to represent the words in  terms of their its vector representations by embedding the information from neighboring words into each word before utilizing conventional classifiers such as Support Vector Machines (SVM) to identify the future events.\n\\item\\textbf{Model-based.} Markov-based models have also been leveraged to characterize temporal patterns~. \nThese typically use $E_i$ to denote each event under a specific type and $\\mathcal{E}$ denotes the set of event types. The goal here is to predict the event type of the next event to occur in the future. In~, the event types are modeled using the Markov model so given the current event type, the next event type can be inferred simply by looking up the state with the highest probability in the transition matrix. A tool called Wayeb~ has been developed based on this method. Laxman et al.~ developed a more complicated model, based on a mixture of Hidden Markov models and introducing new assumptions and the concept of episodes composed of a subsequence of event types. They assumed different event episodes should have different transition patterns so started by discovering the frequent episodes for events, each of which they modeled by a specific  hidden Markov model over various event types. This made it possible to establish the generative process for each future event type $s$ based on the mixture of the above episode Markov models. When predicting, the likelihood of a current observed event sequence over each possible generative process, $p(X|\\Lambda_Y)$ is evaluated, after which a future event type can be considered as either being larger than some threshold (as in~) or the largest among all the different $Y$ values (in~). \n\\item\\textbf{Prototype-based.} Adhikari et al.~ took a different approach, utilizing a prototype-based strategy that first clusters the event sequences into different clusters in terms of their temporal patterns. When a new event sequence is observed, its closest cluster's centroid will then be leveraged as a ``reference event sequence'' whose sub-sequential events will be referred to when predicting future events for this new event sequence.\n\\end{itemize}\n\\noindent\\textbf{Recurrent neural network (RNN)-based methods}. Approaches in this category can be classified into two types: 1. Attribute-based models; and 2. Descriptive-based models. The attribute-based models, ingest feature representation of events as input, while the descriptive-based models typically ingest unstructured information such as texts to directly predict future events.\n\\begin{itemize}\n\\item\\textbf{Attributed-based Methods. }Here, each event $y=(t,l,s)$ at time $t$ is recast and represented as $e_t=(e_{t,1},e_{t,2},\\cdots,e_{t,k})$, where $e_{t,i}$ is the $i$-th feature of the event at time $t$. The feature here can include location and other information such as event topic and semantics. Each sequence $e=(e_1,\\cdots,e_t)$ is then input into the standard RNN architecture for predicting next event $e_{t+1}$ in the sequence at time point $t+1$~. Various types of RNN components and architecture have been utilized  for this purpose~, but a vanilla RNN~ for sequence-based event prediction can be written in the following form:\n\\begin{align}\\nonumber\n   \\indent  h_i=\\mathrm{tahn}(a_{t}),\\ a_{i}=b+W\\cdot h_{i-1}+U\\cdot e_i,\\ o_i=c+V\\cdot h_i,\\ \\psi(i+1)=\\mathrm{softmax}(o_i),\\ i\\le t\n\\end{align}\nwhere $h_i$, $o_i$, and $a_i$ are the latent state, output, and activation for the $i$-th event, respectively, and $W$, $U$, and $V$ are the model parameters for fitting the corresponding mappings. The prediction $e_{t+1}:=\\psi(t+1)$ can then be calculated in a feedforward way from the first event and the model training can be done by back-propagating the error from the layer of $\\psi(t)$. Existing work typically utilizes the variants of vanilla RNN to handle the gradient vanishing problem, especially when the event chain is not short. The most commonly used methods for event prediction are LSTM and GRU~~. For example, the architecture and equation for LSTM are as follows:\n\\begin{align}\n    &a_i=\\sigma(W_j\\cdot[h_{i-1},e_i]+b_j),\\ \\tilde C_i=\\mathrm{tanh}(W_C\\cdot[h_{i-1},h_i]+b_C),\\ C_i=\\zeta_i C_{i-1}+a_i\\tilde C_i,\\\\\\nonumber\n    &\\ \\ \\ \\ \\zeta_i=\\sigma(W_\\zeta\\cdot[h_{i-1},e_i]+b_\\zeta),\\ o_i=\\sigma(W_o[h_{i-1}],e_i)+b_o),\\ h_i=o_i*\\mathrm{tahn}(C_i)\n\\end{align}\nwhere the additional components $C_{i-1}$ and $\\zeta_i$ are introduced to keep tracking the previous ``history'' and gating the information for forgetting in order to handle longer sequences. For example, some researchers opt to leverage a simple type LSTM architecture to extend the RNN-based sequential event prediction~, while others leverage variants of LSTM, such as bi-directional LSTM instead~and yet others prefer to leverage gated-recurrent units (GRU)~.\nMoving beyond considering just the chain relationships among events, Li et al.~ generalized this into graph-structured relationships to better incorporate the event contextual information via the Narrative Event Evolutionary Graph (NEEG). An NEEG is a knowledge graph where each node is an event and each edge denotes the association between a pair of events, enabling the NEEG to be represented by  a weighted adjacency matrix $A$. The basic architecture can be denoted by the following, as detailed in the paper~:\n\\begin{align}\n    &a_i=A^{\\mathrm{T}}h_{i-1}+b,\\ z_i=\\sigma(W_z a_i+U_z h_{i-1}),\\ r_i=\\sigma(W_r a_i+U_r h_{i-1}),\\\\\\nonumber\n    &\\ \\ \\ c_i=\\mathrm{tanh}(W a_i+U(r_i  h_{i-1})),\\ h_i=(1-z_i)h_{i-1}+z_i c_i\n\\end{align}\nHere, the current activation $a_i$ is not only dependent on the previous time point but also influenced by its neighbor nodes in NEEG.\n\\item\\textbf{Descriptive-based Methods. }Attribute-based methods require extra effort during pre-processing in order to convert the unstructured raw data into feature vectors, a process which is not only computationally labor intensive but also not always feasible. Therefore, multiple architectures have been proposed to directly process the raw (textual) event descriptions to enable them to be used to predict future event semantics or descriptions. These models share a similar generic framework~, which begins by encoding each sequence of words into event representations, utilizing an RNN architecture, as shown in Figure~\\ref{figure:descriptive_RNN_framework}. The sequence of events must then be characterized by another higher-level RNN to predict future events. Under this framework, some works begin by decoding the predicted future candidate events into event embedding, after which they are compared with each other and the one with the largest confidence score is selected as the predicted event. These methods are usually constrained by the known list of event types, but sometimes we are interested in open set predictions where the predicted event type can be a new appearance of a type that has not previously been seen in the training set. To achieve this, other methods focus on directly generating future events' descriptions that characterize event semantics that may or may not have appeared before by designing an additional sequence decoder that decodes the latent representation of future events into word sequences. More recent research has enhanced the utility and interpretability of the relationship between words and relevant events, and all the previous events for the relevant future event, by adding a hierarchical attention mechanisms. For example, Yu et al.~ and Su and Jiang~ both proposed word-level attention and event-level attention, while Hu~ leveraged word-level attention in the event encoder as well as in the event decoder. \n\\begin{figure}[htb]\n  \\centering\n    \\includegraphics[width=0.8\\textwidth]{images/descriptive_RNN.png}\\vspace{-0.3cm}\n\\caption{Generic framework for hierarchical RNN-based event forecasting.\\vspace{-0.3cm}}\n\\label{figure:descriptive_RNN_framework}\n\\end{figure}\n\\end{itemize}", "cites": [352, 351, 166, 353, 350], "cite_extract_rate": 0.19230769230769232, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple semantic sequence prediction techniques across different papers, integrating them into a structured classification of methods (feature-based, model-based, prototype-based, and RNN-based). It abstracts the problem formulation and highlights broader patterns, such as the use of Markov models for temporal dependencies and RNN variants for handling long sequences. While it offers some critical points, such as limitations of independence assumptions or the use of exponential approximations for optimization, deeper comparative analysis of strengths and weaknesses is somewhat limited."}}
{"id": "66955a23-5c9e-4abd-8cc9-cb935e343800", "title": "Time and Semantics", "level": "subsubsection", "subsections": [], "parent_id": "a899718d-d44b-4906-ba2a-558ce46075dd", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Multi-faceted Prediction"], ["subsubsection", "Time and Semantics"]], "content": ". For joint time and semantic prediction, there are three popular types of methods, discussed in turn below\n\\noindent\\textbf{Temporal association rule}. A temporal association rule can be developed from the vanilla association rule $LHS\\rightarrow RHS$ by embedding additional temporal information into either $LHS$, $RHS$  or both, thus redefining the meaning of co-occurrence and association with temporal constraints. For example, Vilalta and Ma~ defined $LHS$ as a tuple $(E_L,\\tau)$, where $\\tau$ is the time window before the target in $RHS$ predefined by the user. Only the events occurring within a time window before the event in $RHS$ will satisfy the $LHS$. Similar techniques have also been leveraged by other researchers~. However, $\\tau$ is difficult to define beforehand and it is preferable to be flexible to suit different target events. To handle this challenge, Yang et al.~ proposed a way to automatically identify information on a continuous time interval from the data. Here, each transaction is composed of not only items but also continuous time duration information. $LHS$ is a set of items (e.g., previous events) while $RHS$ is a tuple $(E_R,[t_1,t_2])$ consisting of a future event semantic representation and its time interval of occurrence. To automatically learn the time interval in $RHS$,~ proposed the use of two different methods . The first is called the \\emph{confidence-interval-based} method, which leverages a statistical distribution (e.g., Gaussian and student-t~) to fit all the observed occurrence times of events in $RHS$, and then treats the statistical confidence interval as the time interval. The second method is known as \\emph{minimal temporal region selection}, which aims to find the temporal region with the smallest interval and covers all historical occurrences of the event in $RHS$.\n\\noindent\\textbf{Time expression extraction}. In contrast to the above statistical-based methods, another way to achieve event time and semantics joint prediction comes from the pattern recognition domain, aiming to directly discover time expressions that mention the (planned) future events. As this type of technique can simultaneously identify time, semantics, and other information such as locations, it is widely used and will be discussed in more details later as part of the discussion of ``Planned future event detection methods'' in Section \\ref{sec:time_location_semantics}.\n\\noindent\\textbf{Time series forecasting-based methods. }The methods based on time series forecasting can be separated into direct methods and indirect methods. \\emph{Direct methods} typically formulate the event semantic prediction problem as a multi-variate time series forecasting problem, where each variable corresponds to an event type $C_i\\ (i=1,\\cdots)$ and hence the predicted event type at future time $\\hat t$ is calculated as $\\hat s_{\\hat t}=\\arg\\max_{C_i} f(s_{\\hat t}=C_i|X)$. For example, in~, a longitudinal support vector regressor is utilized to predict multi-attribute events, where $n$ support vector regressors, each of which corresponds to an attribute, is built to achieve the goal of predicting the next time point's attribute value. Weiss and Page~ took a different approach, leveraging multiple point process models to predict multiple event types. To further estimate the confidence of their predictions, Bilo{\\v{s}} et al.~ first leveraged RNN to learn the historical event representation and then input the result into a Gaussian process model to predict future event types. To better capture the joint dynamics across the multiple variables in the time series, Brandt et al.~ extended this to Bayesian vector autoregression. Utilizing \\emph{indirect-style} methods, they focused on learning a mapping from the observed event semantics down to low-dimensional latent-topic space using tensor decomposition-based techniques. Similarly, Matsubara et al.~ proposed a 3-way topic analysis of the original observed event tensor $Y_0\\in\\mathbb{R}^{D_o\\times D_a\\times D_c}$ consisting of three factors, namely actors, objects, and time. They then went on to decompose this tensor into latent variables via three corresponding low-rank matrices $P_{o}\\in \\mathbb{R}^{D_k\\times D_o}$, $P_{a}\\in \\mathbb{R}^{D_k\\times D_a}$, and $P_{c}\\in \\mathbb{R}^{D_k\\times D_c}$ respectively, as shown in Figure~\\ref{figure:tensor_decomposition}. Here $D_k$ is the number of latent topics. For the prediction, the time matrices $P_{c}$ are predicted into the future $\\hat P_{c}$ via multi-variate time series forecasting, after which a future event tensor are estimated by recovering a ``future event tensor'' $\\hat Y$ by the multiplication among the predicted time matrix $\\hat P_{c}$ as well as the known actor matrix $P_{a}$ and object matrix $P_{o}$.\n\\begin{figure}[htb]\n  \\centering\n    \\includegraphics[width=0.8\\textwidth]{images/time_series_forecasting.png}\\vspace{-0.3cm}\n\\caption{Tensor Decomposition and Forecasting for Complex Time-stamped events.\\vspace{-0.3cm}}\n\\label{figure:tensor_decomposition}\n\\end{figure}", "cites": [339], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple methods (temporal association rules, time expression extraction, and time series forecasting) into a coherent framework for joint time and semantic prediction. It provides abstraction by categorizing techniques and explaining how they model temporal and semantic dependencies. Some critical analysis is present, particularly in addressing limitations like the need to predefine time windows in temporal association rules, though deeper comparative critique is limited."}}
{"id": "d4d71435-dadd-4cc3-805f-65911ba23fc9", "title": "Time and Location Prediction", "level": "subsubsection", "subsections": [], "parent_id": "a899718d-d44b-4906-ba2a-558ce46075dd", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Multi-faceted Prediction"], ["subsubsection", "Time and Location Prediction"]], "content": "This category of methods focuses on jointly predicting the location and time of future events. These methods can be classified into two subtypes: 1) raster-based: which focus on predictions for individual time slots and location regions, and 2) point-based: which predicts continuous time and location points.\n\\noindent\\textbf{Raster-based}. These methods usually formulate data into temporal sequences consisting of spatial snapshots. Over the last few years, various techniques have been proposed to characterize the spatial and temporal information for event prediction.\nThe simplest way to consider spatial information is to directly treat location information as one of the input features, and then feed it into predictive models, such as linear regression~, LSTM~ and Gaussian processes~. During model training, Zhao and Tang~ leveraged the spatiotemporal dependency to regularize their model parameters.  \n\\begin{figure}[htb]\n  \\centering\n    \\includegraphics[width=0.9\\textwidth]{images/RNN_CNN.png}\\vspace{-0.3cm}\n\\caption{Generic framework for spatiotemporal event prediction using CNN+RNN-based deep learning framework.\\vspace{-0.3cm}}\n\\label{figure:cnn_rnn}\n\\end{figure}\nMost of the methods in this domain aim to jointly consider the spatial and temporal dependency for predictions~. At present, the most popular framework is the CNN+RNN architecture, which implements sequence-to-sequence learning problems such as the one illustrated in Figure \\ref{figure:cnn_rnn}. Here, the multi-attributed spatial information for each time point can be organized as a series of multi-channel images, which can be encoded using convolution-based operations. For example, Huang et al.~ proposed the addition of convolutional layers to process the input into vector representations. Other researchers have leveraged variational autoencoders~ and CNN autoencoders~ to learn the low-dimensional embedding of the raw spatial input data. This allows the learned representation of the input to be input into the temporal sequence learning architecture. Different recurring units have been investigated, including  RNN,  LSTM, convLSTM, and stacked-convLSTM~. The resulting representation of the input sequence is then sent to the output sequence as input. Here, another recurrent architecture is established. The output of the unit for each time point will be input into a spatial decoder component which can be implemented using transposed convolutional layers~, transposed convLSTM~, or a spatial decoder in a variational autoencoder~. A conditional random field is another popular technique often used to model the spatial dependency~.\n\\noindent\\textbf{Point-based}. The spatiotemporal point process is an important technique for spatiotemporal event prediction as it models the rate of event occurrence in terms of both spatial and time points. It is defined as:\n\\begin{align}\n\\label{eq:stpp}\n    \\lambda(t,l|X)=\\lim\\nolimits_{|\\mathrm{d}t|\\rightarrow 0,|\\mathrm{d}l|\\rightarrow 0}\\mathbb{E}[N(\\mathrm{d}t\\times \\mathrm{d}l)|X]/(|\\mathrm{d}t||\\mathrm{d}l|)\n\\end{align}\nVarious models have been proposed to instantiate the model of the framework illustrated in Equation \\eqref{eq:stpp}. For example, Liu and Brown et al.~ began by assuming there to be a conditional independence among spatial and temporal factors and hence achieved the following decomposition:\n\\begin{align}\n    \\lambda(t,l|X)=\\lambda(t,l|L,T,F)=\\lambda_1(l|L,T,F,t)\\cdot\\lambda_2(t|T)\n\\end{align}\nwhere $X,L,T,$ and $F$ denotes the whole input indicator data as well as its different facets, including location, time, and other semantic features, respectively. Then the term $\\lambda_1(\\cdot)$ can be modeled based on the Markov spatial point process while $\\lambda_2(\\cdot)$ can be characterized using temporal autoregressive models. To handle situations where explicit assumptions for model distributions are difficult, several methods have been proposed to involve the deep architecture during the point process. Most recently, Okawa et al.~ have proposed the following:\n\\begin{align}\n    \\lambda(t,l|X)=\\int g_\\theta\\left(t',l',\\mathcal{F}(t',l')\\right)\\cdot\\mathcal{K}((t,l),(t',l'))\\ \\mathrm{d}t'\\mathrm{d}l'\n\\end{align}\nwhere $\\mathcal K(\\cdot,\\cdot)$ is a kernel function such as a Gaussian kernel~ that measures the similarity in time and location dimensions. $\\mathcal{F}(t',l')\\subseteq F$ denotes the feature values (e.g., event semantics) for the data at location $l'$ and time $t'$. $g_\\theta(\\cdot)$ can be a deep neural network that is parameterized by $\\theta$ and returns an nonnegative scalar. The model selection of $g_\\theta(\\cdot)$ depends on the specific data types. For example, these authors constructed an image attention network by combining a CNN with the spatial attention model proposed by Lu et al.~.", "cites": [356, 355, 166, 354], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to present a coherent classification of time and location prediction methods (raster-based and point-based). It abstracts key patterns such as the use of CNN+RNN frameworks and spatiotemporal point processes, and includes some critical insights like the challenges of distributional assumptions and the need for deep learning in modeling complex dependencies."}}
{"id": "682b3b7c-cca4-4ecf-8300-2e045311a819", "title": "Time, location, and Semantics", "level": "subsubsection", "subsections": [], "parent_id": "a899718d-d44b-4906-ba2a-558ce46075dd", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Event Prediction Techniques"], ["subsection", "Multi-faceted Prediction"], ["subsubsection", "Time, location, and Semantics"]], "content": "\\label{sec:time_location_semantics}In this section, we introduce the strategies that jointly predict the time, location, and semantics of future events, which can be grouped into either system-based or model-based strategies.\n\\noindent\\textbf{System-based}. The first type of the system-based methods considered here is the model-fusion system. The most intuitive approach is to leverage and integrate the aforementioned techniques for time, location, and semantics prediction into an event prediction system. For example, a system named EMBERS~ is an online warning system for future events that can jointly predict the time, location, and semantics including the type and population of future events. This system also provides information on the confidence of the predictions obtained. Using an ensemble of predictive models for time~, location, and semantic prediction, this system achieves a significant performance boost in terms of both precision and recall. The trick here is to first prioritize the precision of each individual prediction model by suppressing their recall. Then, due to the diversity and complementary nature of the different models, the fusion of the predictions from different models will eventually result in a high recall. A Bayesian fusion-based strategy has also been investigated~. Another system named \\emph{Carbon}~ also leverages a similar strategy.\nThe second type of model involves crowd-sourced systems that implement fusion strategies to generate the event predictions made by the human predictors. For example, in order to handle the heterogeneity and diversity of the human predictors' skill sets and background knowledge under limited human resources, Rostami et al.~ proposed a recommender system for matching event forecasting tasks to human predictors with suitable skills in order to maximize the accuracy of their fused predictions. Li et al.~ took a different approach, designing a prediction market system that operates like a futures market, integrating information from different human predictors to forecast future events. In this system, the predictors can decide whether to buy or sell the ``tokens'' (using virtual dollars, for example) for each specific prediction they have made according to their confidence in it. They typically make careful decisions as they will obtain corresponding awards (for correct predictions) or penalties (for erroneous predictions).\n\\noindent\\textbf{Planned future event detection methods}. These methods focus on detecting the planned future events, usually from various media such sources as social media and news and typically relying on NLP techniques and linguistic principles. Existing methods typically follow a workflow similar to the one shown in Figure \\ref{fig:planned_event}, consisting of four main steps: 1) \\emph{Content filtering.} Methods for content filtering are typically leveraged to retain only the texts that are relevant to the topic of interest. Existing works utilize either supervised methods (e.g., textual classifiers~ or unsupervised methods (e.g., querying techniques~); 2) \\emph{Time expression identification} is then utilized to identify future reference expressions and determine the \\emph{time to event}. These methods either leverage existing tools such as the Rosetta text analyzer~ or propose dedicated strategies based on linguistic rules~; 3) \\emph{Future reference sentence extraction} is the core of planned event detection, and is implemented either by designing regular expression-based rules~ or by textual classification~; and 4) \\emph{Location identification}. The expression of locations is typically highly heterogeneous and noisy. Existing works have relied heavily on geocoding techniques that can resolve the event location accurately. In order to infer the event locations, various types of locations are considered by different researchers, such as article locations~, authors' profile locations~, locations mentioned in the articles~, and authors' neighbors' locations~. Multiple locations have been selected using a geometric median~ or fused using logical rules such as probabilistic soft logic~.\n\\begin{figure}[htb]\n  \\centering\n    \\includegraphics[width=0.9\\textwidth]{images/plannedprotest.png}\\vspace{-0.3cm}\n\\caption{Generic framework for planned event detection.\\vspace{-0.3cm}}\n\\label{fig:planned_event}\n\\end{figure}\n\\noindent\\textbf{Tensor-based methods. }Some methods formulate the data into tensor-form, with dimensions including location, time, and semantics. Tensor decomposition is then applied to approximate the original tensors as the product of multiple low-rank matrices, each of which is a mapping from latent topics to each dimension. Finally, the tensor is extrapolated towards future time periods by various strategies. For example, Mirtaheri~ extrapolated the time dimension-matrix only, which they then multiplied with the other dimensions' matrices to recover the estimated extrapolated tensor into the future. Zhou et al.~ took a different approach, choosing instead to add  ``empty values'' for the entries in future time to the original tensor, and then use tensor completion techniques to infer the missing values corresponding to future events.", "cites": [341], "cite_extract_rate": 0.058823529411764705, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches for multi-faceted event prediction, grouping them into system-based, planned future event detection, and tensor-based methods. It integrates ideas across EMBERS, Carbon, and other systems to present a coherent structure. While it offers some abstraction by identifying common workflows (e.g., content filtering, time expression identification), it lacks deeper critical evaluation of the methods' limitations or comparative strengths, which limits its overall insight level."}}
{"id": "033686c5-99a0-450d-bd05-1ef9b270c0ab", "title": "Video- and Audio- based", "level": "subsubsection", "subsections": [], "parent_id": "7a5f3a91-6ad2-44d7-8a93-523b51bd39e8", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Media"], ["subsubsection", "Video- and Audio- based"]], "content": ". While event detection has been extensively researched for video data~ and audio mining~, event prediction is more challenging and has been attracting increasing attention in recent years. The goal here is usually to predict the future status of the objects in the video, such as the next action of soccer players~ or basketball players~, or the movement of vehicles~.", "cites": [9105, 9107], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, merely mentioning their relevance without integrating their core ideas into a cohesive discussion. It lacks critical analysis, does not compare or evaluate the approaches, and offers no broader abstraction or generalization. The content is primarily descriptive, summarizing the goal of video- and audio-based event prediction and citing two papers without elaborating on their contributions or limitations."}}
{"id": "69bc2115-ef52-4f9b-8ce4-250aed770386", "title": "Group transportation pattern", "level": "subsubsection", "subsections": [], "parent_id": "e8db337d-4767-468d-ba4a-7973cc074286", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Transportation"], ["subsubsection", "Group transportation pattern"]], "content": ". Here, researchers typically focus on transportation events such as congestion~, large gatherings~, and dispersal events~. The goal is thus to forecast the future time period~ and location~ of such events. Data from traffic meters, GPS, and mobile devices are usually used to sense real-time human mobility patterns. Transportation and geographical theories are usually considered to determine the spatial and temporal dependencies for predicting these events.", "cites": [357, 346], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions types of transportation events and data sources used but fails to meaningfully synthesize the cited papers. It does not compare the approaches or evaluate their strengths and limitations, offering minimal critical analysis. The discussion remains at a general level without identifying broader patterns or principles in the field."}}
{"id": "06704e24-ba02-4500-92f1-669db72147b3", "title": "Individual transportation behavior", "level": "subsubsection", "subsections": [], "parent_id": "e8db337d-4767-468d-ba4a-7973cc074286", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Transportation"], ["subsubsection", "Individual transportation behavior"]], "content": "Another research thread focuses on individual-level prediction, such as predicting an individual's next location~ or the likelihood or time duration of car accidents~. Sequential and trajectory analyses are usually used to process trajectory and traffic flow data.", "cites": [358, 355], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions two papers but does not synthesize or connect their ideas to form a coherent narrative. It lacks critical evaluation or comparison of the methods, and there is no abstraction to broader patterns or principles in individual transportation behavior prediction."}}
{"id": "52c88885-b2eb-427a-8d0d-f0da918827bb", "title": "Engineering Systems", "level": "subsection", "subsections": [], "parent_id": "dbd1ff7b-a51a-4b40-b28d-3ec77cefa901", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Engineering Systems"]], "content": "Different types of engineering systems have begun to routinely apply event forecasting methods, including: 1) civil engineering, 2) electrical engineering, 3) energy engineering, and 4) other engineering domains. Despite the variety of systems in these widely different domains, the goal is essentially to predict future abnormal or failure events in order to support the system's sustainability and robustness. Both the location and time of future events are key factors for these predictions. The input features usually consist of sensing data relevant to specific engineering systems.\n\\begin{itemize}\n\\item\\textbf{Civil engineering.} This covers various a wide range of problems in diverse urban systems, such as smart building fault adverse event prediction~, emergency management equipment failure prediction~, manhole event prediction~, and other events~.\n\\item\\textbf{Electrical engineering}. This includes teleservice systems failures~ and unexpected events in wire electrical discharge machining operations~. \n\\item\\textbf{Energy engineering}. Event prediction is also a hot topic in energy engineering, as such systems usually require strong robustness to handle the disturbance from the nature environments. Active research domains here include wind power ramp prediction~, solar power ramp prediction~, and adverse events in low carbon energy production~.\n\\item\\textbf{Other engineering domains}. There is also active research on event prediction in other domains, such as irrigation event prediction  in agricultural engineering~ and mechanical fault prediction in mechanical engineering~.\n\\end{itemize}", "cites": [359], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of event prediction applications in various engineering systems, but it lacks synthesis of ideas between the cited papers. It does not evaluate or compare the methods or approaches used in these domains, nor does it identify overarching principles or trends. The content remains largely at the level of listing applications without deeper analytical engagement."}}
{"id": "ee12ae57-3245-4bbb-bf6b-f7c4ce0ef0bc", "title": "Cyber Systems", "level": "subsection", "subsections": [], "parent_id": "dbd1ff7b-a51a-4b40-b28d-3ec77cefa901", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Cyber Systems"]], "content": "Here, the prediction models proposed generally focus on either network-level events or device-level events. For both types, the general goal is essentially to predict the likelihood of future system failure or attacks based on various indicators of system vulnerability. So far these two categories have essentially differed only in their inputs: the former relies on network features, including system specifications, web access logs and search queries, mismanagement symptoms, spam, phishing, and scamming activity, although some researchers are investigating the use of social media text streams to identify semantics indicating future potential attack targets of DDoS~. For device-level events, the features of interest are usually the binary file appearance logs of machines~. Some work has been done on micro-architectural attacks~ by observing and proactively analyzing the observations on speculative branches, out-of-order executions and shared last level caches~.", "cites": [360, 361], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic overview of event prediction in cyber systems and mentions the two main categories—network-level and device-level events. It integrates the cited papers by referencing their focus areas but lacks deeper synthesis or comparison. The critical evaluation is minimal, and abstraction to broader principles or frameworks is limited, making the section primarily descriptive."}}
{"id": "3174d31c-d0af-4f0f-a000-e9b1742e4db3", "title": "Offline events", "level": "subsubsection", "subsections": [], "parent_id": "17123e85-3824-4679-954f-0b1a517bede9", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Political events"], ["subsubsection", "Offline events"]], "content": "This includes civil unrest~, conflicts~, violence~, and riots~. This type of research usually targets the future events' geo-location, time, and topics by leveraging the social sensors that indicate public opinions and intentions. Utilization of social media has become a popular approach for these endeavors, as social media is a source of vital information during the event development stage~. Specifically, many aspects are clearly visible in social media, including complaints from the public (e.g., toward the government), discussions about their intentions regarding specific political events and targets, as well as advertisements for the planned events. Due to the richness of this information, further information on future events such as the type of event~, the anticipated participants population~, and the event scale~ can also be discovered in advance.", "cites": [341], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of offline political event prediction, mentioning civil unrest and social media as data sources. It cites one paper (EMBERS) but does so without integrating it into a broader framework or comparing it with other approaches. There is minimal critical analysis or abstraction, as the section primarily lists general characteristics of the problem space."}}
{"id": "5d333d17-9065-40be-980f-43fa5d914180", "title": "Geophysics-related", "level": "subsubsection", "subsections": [], "parent_id": "7cdc8da3-a431-4a2a-9c18-89a3e9aac1e7", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Natural disasters"], ["subsubsection", "Geophysics-related"]], "content": "\\textbf{Earthquakes.} Predictions here typically focus on whether there will be an earthquake with a magnitude larger than a specified threshold in a certain area  during a future period of time. To achieve this, the original sensor data is usually proccessed using geophysical models such as Gutenberg–Richter’s inverse law, the distribution of characteristic earthquake magnitudes, and seismic quiescence~. The processed data are then input into machine learning models that treat them as input features for predicting the output, which can be either binary values of event occurrence or time-to-event values. Some studies are devoted to identifying the time of future earthquakes and their precursors, based on an ensemble of regressors and feature selection techniques~, while others focus on aftershock prediction and the consequences of the earthquake, such as fire prevention~. It worth noting that social media data has also been used for such tasks, as this often supports early detection of the first-wave earthquake, which can then be used to predict the afterstocks or earthquakes in other locations~.\n\\textbf{Fire events}. Research in this category can be grouped into urban fires and wildfires. This type of research often focuses on the time at which a fire will affect a specific location, such as a building. The goal here is to predict the risk of future fire events. To achieve this, both the external environment and the intrinsic properties of the location of interests are important. Therefore, both static input data (e.g., natural conditions and demographics) and time-varying data (e.g., weather, climate, and crowd flow) are usually involved. Shin and Kim~ focus on building fire risk prediction, where the input is the building's profile. Others have studied wildfires, where weather data and satellite data are important inputs. This type of research focuses primarily on predicting both the time and location of future fires~.\nOther researchers have focused on rarer events such as volcanic eruptions. For example, some leverage chemical prior knowledge to build a Bayesian network for prediction ~, while others adopt point processes to predict the hazard of future events~.", "cites": [8345], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of geophysics-related event prediction, particularly earthquakes and fires, with minimal synthesis of the cited works. It briefly connects methods like geophysical modeling and machine learning but does not offer a deeper integration or novel framework. There is little critical evaluation or abstraction beyond the specific examples, and the discussion remains largely descriptive in nature."}}
{"id": "3396d058-409f-4260-9ee6-4b9d6f66ea05", "title": "Atmospheric science-related", "level": "subsubsection", "subsections": [], "parent_id": "7cdc8da3-a431-4a2a-9c18-89a3e9aac1e7", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Natural disasters"], ["subsubsection", "Atmospheric science-related"]], "content": "\\textbf{Flood events}. Floods may be caused by many different reasons, including atmospheric (e.g., snow and rain), hydrological (e.g., ice melting, wind-generated waves, and river flow), and geophysical (e.g., terrain) conditions. This makes the forecasting of floods highly complicated task that requires multiple diverse predictors~. Flood event prediction has a long history, with the latest research focusing especially on computational and simulation models based on domain knowledge. This usually involves using ensemble prediction systems as inputs for hydrological and/or hydraulic models to produce river discharge predictions. For a detailed survey on flood computational models please refer to~. However, it is prohibitively difficult to comprehensively consider and model all the factors correctly while avoiding all the accumulated errors from upstream predictions (e.g., precipitation prediction). Another direction, based on data-driven models such as statistical and machine learning models for flood prediction, is deemed promising and is expected to be complementary to existing computational models. These newly developed machine learning models are often based solely on historical data, requiring no knowledge of the underlying physical processes. Representative models are SVM, random forests, and neural networks and their variants and hybrids. A detailed recent survey is provided in~.\n\\textbf{Tornado Forecasting}. Tornadoes usually develop within thunderstorms and hence most tornado warning systems are based on the prediction of thunderstorms. For a comprehensive survey, please refer to~. Machine learning models, when applied to tornado forecasting tasks, usually suffer from high-dimensionality issues, which are very common in meteorological data. Some methods have leveraged dimensional reduction strategies to preprocess the data~ before prediction. Research on other atmosphere-related events such as droughts and ozone events has also been conducted~.", "cites": [362], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of flood and tornado prediction approaches, with minimal synthesis of ideas beyond categorization (e.g., computational vs. data-driven models). It cites a relevant paper on machine learning for flood prediction but does not deeply integrate or contrast its findings with others. There is little critical evaluation of the cited works or abstraction to broader principles in event prediction."}}
{"id": "149a91d1-04db-4048-b148-d80a122d1ca5", "title": "Astrophysics-related", "level": "subsubsection", "subsections": [], "parent_id": "7cdc8da3-a431-4a2a-9c18-89a3e9aac1e7", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Natural disasters"], ["subsubsection", "Astrophysics-related"]], "content": "There is a large body of prediction research focusing on events outside the Earth, especially those affecting the star closest to us, the sun. Methods have been proposed to predict various solar events that could  impact life on Earth, including solar flares~, solar eruptions~, and high energy particle storms~. The goal here is typically to use satellite imagery data of the sun to predict the time and location of future solar events and the activity strength~.", "cites": [9087, 8346], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief and factual overview of astrophysics-related event prediction, particularly focusing on solar events, but it lacks meaningful synthesis between the cited papers. It does not critically evaluate the methods or identify limitations, nor does it generalize to broader patterns or principles within the field. The narrative remains descriptive and does not offer deeper analytical or comparative insights."}}
{"id": "3964c722-171e-46d4-9dab-9839e9a229fe", "title": "Organized and serial crimes", "level": "subsubsection", "subsections": [], "parent_id": "3657bacf-32f8-4c67-b6fb-98f1705ab73d", "prefix_titles": [["title", "Event Prediction in the Big Data Era: A Systematic Survey"], ["section", "Applications of Event Predictions"], ["subsection", "Crime"], ["subsubsection", "Organized and serial crimes"]], "content": "Unlike the above research on regional crime risks, some recent studies strive to predict the next incidents of criminal individuals or groups. This is because different offenders may demonstrate different behavioral patterns, such as targeting specific regions (e.g., wealthy neighborhoods),  victims (e.g., women), for specific benefits (e,g, money). The goal here is thus is to predict the next crime site and/or time, based on the historical crime event sequence of the targeted criminal individual or group. Models such as point processes~ or Bayesian networks~ are usually used to address such problems.", "cites": [358], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of research on predicting serial and organized crimes, citing one relevant paper and mentioning common modeling approaches like point processes and Bayesian networks. It lacks deeper synthesis across multiple sources, critical evaluation of the cited work's limitations or assumptions, and abstraction to broader theoretical or methodological patterns in event prediction."}}
