{"id": "030fa328-8718-455f-8c1d-2c718f620e70", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "58c1de30-7774-42e5-a989-9c0459f4e585", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Introduction"]], "content": "As the amount of digital data continuously grows, users demand technologies that offer quick access to such data. In fact, users rely on systems that support information search interactions such as Siri\\footnote{https://www.apple.com/es/siri/}, Google Assistant\\footnote{https://assistant.google.com/}, Amazon Alexa\\footnote{https://www.amazon.com} or Microsoft XiaoIce~, etc. These technologies, called Dialogue Systems (DS), allow the user to converse with a computer system using natural language. Dialogue Systems are applied to a variety of tasks, e.g.:\n\\begin{itemize}\n\\item Virtual Assistants aid users in everyday tasks, such as scheduling appointments. They usually operate on predefined actions which can be triggered by voice command. \n\\item Information-seeking systems provide users with information about a question (e.g. the most suitable hotel in town). These questions also include factual questions as well as more complex questions. \n\\item E-learning dialogue systems train students for various situations. For instance, they train the interaction with medical patients or train military personnel in questioning a witness.\n\\end{itemize}\nOne crucial step in the development of DS is evaluation. That is, to measure how well the DS is performing. However, evaluating a dialogue system can prove to be problematic because there are two important factors to be considered. Firstly, the definition of what constitutes a high-quality dialogue is not always clear and often depends on the application. Even if a definition is assumed, it is not always clear how to measure it. For instance, if we assume that a high-quality dialogue system is defined by its ability to respond with an appropriate utterance, it is not clear how to measure appropriateness or what appropriateness means for a particular system. Moreover, one might ask the users if the responses were appropriate, but as we will discuss below, user feedback might not always be reliable for a variety of reasons.\nThe second factor is that the evaluation of dialogue systems is very cost- and time-intensive. This is especially true when the evaluation is carried out by a user study, which requires careful preparation,  the need for inviting and compensating users for their participation. \nOver the past decades, many different evaluation methods have been proposed. The evaluation methods are closely tied to the characteristics of the dialogue system which they are aimed at evaluating. Thus, quality is defined in the context of the function which dialogue system is meant to fulfil. For instance, a system designed to answer questions will be evaluated on the basis of correctness, which is not necessarily a suitable metric for evaluating a conversational agent.\nMost methods are aimed at automating the evaluation, or at least automating certain aspects of the evaluation. The goal of an evaluation method is to obtain automated and repeatable evaluation procedures that allow efficient comparisons in the quality of different dialogue strategies. \nThis survey is structured as follows; in the next section we give a general overview over the different classes of dialogue systems and their characteristics. We then introduce the evaluation task in greater detail, with an emphasis on the goals of an evaluation and the requirements on an evaluation metric. In Sections \\ref{sec:eval_task}, \\ref{sec:eval_non_task}, and \\ref{sec:eva_qa_dialogue}, we introduce each dialogue system class (i.e. task-oriented systems, conversational agents and question answering dialogue systems). Thereafter, we give an overview of the characteristics, dialogue behaviour, and concepts behind the implementation methods of the various dialogue systems. Finally, we present the evaluation methods and the ideas behind them. Here, we set an emphasis the relationship between these methods and the dialogue system classes, including which aspects of the evaluation are automated. In Section \\ref{sec:datasets}, we give a short overview of the relevant datasets and evaluation campaigns in the domain of dialogue systems. In Section \\ref{sec:discussion}, we discuss the issues and challenges in devising automated evaluation methods and discuss the level of automation achieved.", "cites": [7451], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The introduction section provides a descriptive overview of dialogue systems and their applications, with a brief mention of Microsoft XiaoIce as an example of a social chatbot. However, it does not deeply synthesize or integrate the cited paper into a broader narrative, nor does it critically analyze the work or its limitations. The section begins to abstract by highlighting general challenges in evaluation, but the insights remain at a basic level."}}
{"id": "3c36dea1-7979-445d-b697-0c188d032fdd", "title": "Dialogue Systems", "level": "subsection", "subsections": [], "parent_id": "e5e866a1-6351-4674-8666-e4056a478c2d", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "A General Overview"], ["subsection", "Dialogue Systems"]], "content": "Dialogue Systems (DS) usually structure dialogues in \\emph{turns}, each turn is defined by one or more \\emph{utterances} from one speaker. Two consecutive turns between two different speakers is called an \\emph{exchange}. Multiple exchanges constitute a \\emph{dialogue}. Another different, but related view is to interpret each turn or each utterance as an action (more on this later). \nThe main component of a dialogue system is the dialogue manager that defines the content of the next utterance and thus the behaviour of the dialogue system. There are many different approaches to design a dialogue manager, which are partly dictated by the application of the dialogue system. However, there are three broad classes of dialogue systems that we encounter in the literature: task-oriented systems, conversational agents and interactive question answering systems\\footnote{In recent literature, the distinction is made only between the first two classes of dialogue systems . However, interactive question answering systems cannot be completely placed in either of the two categories. }. \nWe identified the following characteristic features that help differentiate between the three different classes: whether the system is developed to solve a task, whether the dialogue follows a structure, whether the domain is restricted or open, whether the dialogue spans over multiple turns, whether the dialogues are long or rather efficient, who takes the initiative, and what interface is used (text, speech, multi-modal). Table \\ref{tbl:ds_char} depicts the characteristics for each of the dialogue system classes. In this table, we can see the following main features for each class:\n    \\begin{itemize}\n    \\item Task-oriented systems are developed to help the user solve a specific task as efficiently as possible. The dialogues are characterized by following a clearly defined structure that is derived from the domain. The dialogues follow mixed initiative; both the user and the system can take the lead. Usually, the systems found in the literature are built for speech input and output. However, task-oriented systems in the domain of assisting users are built on multi-modal input and output. \n    \\item Conversational agents display a more unstructured conversation, as their purpose is to have open-domain dialogues with no specific task to solve. Most of these systems are built to emulate social interactions, and thus longer dialogues are desired.\n    \\item Question Answering (QA) systems are built for the specific task of answering questions. The dialogues are not defined by a structure as with task-oriented systems, however, they mostly follow the question and answer style pattern. QA systems may be built for a specific domain, but may be also tilted towards more open domain questions. Usually, the domain is dictated by the underlying data, e.g. knowledge bases or text snippets from forums. Traditional QA systems work on a singe-turn interaction, however, there are systems that allow multiple turns to cover follow-up questions. The initiative is mostly done by the user, who asks questions. \n    \\end{itemize}\n\\begin{table}[h!]\n\\centering\n\\begin{tabular}{l||lll}\n                & Task-oriented DS    & Conversational Agents & Interactive QA         \\\\ \\hline \\hline\nTask            & Yes - clear defined & No                    & Yes - answer questions \\\\\nDial. Structure & Highly structured     & Not structured        & No                     \\\\\nDomain          & Restricted          & Mostly open domain    & Mixed                  \\\\\nTurns           & Multi               & Multi                 & Single/Multi           \\\\\nLength          & Short               & Long                  & -                      \\\\\nInitiative      & Mixed/ system init  & mixed/user init       & user init              \\\\\nInterface       & multi-modal         & multi-modal           & mostly text           \n\\end{tabular}\n\\caption{Characterizations of the different dialogue system types. }\n\\label{tbl:ds_char}\n\\end{table}", "cites": [1149], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of dialogue systems by introducing key terminology and categorizing them into three classes with distinguishing features. It integrates information from the cited survey (Paper 1149) but primarily restates it without deeper synthesis or novel framework building. While it begins to generalize characteristics across system types, it lacks critical evaluation or identification of broader trends and principles."}}
{"id": "fb671f8b-02d8-43f8-a113-0a4dd30051bf", "title": "Human Evaluation.", "level": "paragraph", "subsections": ["41d66d9a-31db-424e-b139-de8780ad1050"], "parent_id": "21b7ce87-9068-4481-b345-7e879858248c", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "A General Overview"], ["subsection", "Evaluation"], ["paragraph", "Human Evaluation."]], "content": "There are various approaches to a human evaluation. The test subjects can take on two main roles: interacting with the system or rating a dialogue or utterance, or both. In the following, we differentiate among different types of user populations. Among each of the populations, the subjects can take on any of the two roles. \n\\begin{itemize}\n\\item Lab experiments: Before crowdsourcing was popular, dialogue systems were evaluated in a lab environment. Users were invited to participate in the lab where they interacted with a dialogue system and subsequently filled a questionnaire. For instance,  recruited 36 subjects, which were given instructions and presented with various scenarios. The subjects were asked to solve a task using a spoken dialogue system. Furthermore, a supervisor was present to guide the users. The lab environment is very controlled, which is not necessarily comparable to the real world .\n\\item In-field experiments: Here, the evaluation is performed by collecting feedback from real users of the dialogue systems~. For instance, for the Spoken Dialogue Challenge , the systems were developed to provide bus schedule information in Pittsburgh. The evaluation was performed by redirecting the evening calls to the dialogue systems and getting the user feedback at the end of the conversation. The Alexa Prize \\footnote{\\url{https://developer.amazon.com/alexaprize}} also followed the same strategy, i.e. it let real users interact with operational systems and gathered user feedback over a span of several months.\n\\item Crowdsourcing: Recently, human evaluation has shifted from a lab environment to using crowdsourcing platforms such as Amazon Mechanical Turk (AMT). These platforms provide large amounts of recruited users.  evaluate the validity of using crowdsourcing for evaluating dialogue systems, and their experiments suggest that using enough crowdsourced users, the quality of the evaluation is comparable to the lab conditions. Current research relies on crowdsourcing for human evaluation . \nEspecially conversational dialogue systems are evaluated via crowdsourcing, where there are two main evaluation procedures: crowdworkers either talk to the system and rate the interaction or they are presented with a context from the test set and a response by the system, which they need to rate. In both settings, the crowdworkers are aksed to rate the system based on quality, fluency or appropriateness. Recently,  introduced Sensibleness and Specificity Average (SSA), where humans rate the sensibleness and specificity of a response. These capture two aspects of human behaviour: making sense and being specific. A dialogue system can be sensible by responding with vague answers (e.g. ``I don't know\"), whereas it is only specific if it takes the context into account.\n\\end{itemize}\nHuman based evaluation is difficult to set up and to carry out. Much care has to be taken in setting up the experiments; the users need to be properly instructed and the tasks need to be prepared so that the experiment reflects real-world conditions as closely as possible. Furthermore, one needs to take into account the high variability of user behaviour, which is present especially in crowdsourced environments.", "cites": [1547], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of human evaluation methods in dialogue systems, mentioning lab, in-field, and crowdsourced settings and citing relevant examples. It integrates a few ideas (e.g., the use of SSA from one paper) but lacks deeper synthesis across multiple sources. There is minimal critical analysis or identification of broader patterns, focusing instead on summarizing methods and their applications."}}
{"id": "f46ef7d5-79de-4e9d-962a-dd97da034d5d", "title": "Datasets for Task-Oriented Dialogue Systems", "level": "subsection", "subsections": [], "parent_id": "b7d9cf74-5108-48cc-abe2-a5f162020e3c", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Evaluation Datasets and Challenges"], ["subsection", "Datasets for Task-Oriented Dialogue Systems"]], "content": "\\begin{table}[t]\n\\centering\n\\begin{tabular}{llll}\n\\hline\nName & Topics & \\# dialogues   &  Reference\\\\ \\hline\nDSTC1 & Bus schedules & 15,000  &  \\\\\nDSTC2 & Restaurants & 3,000 & \\\\\nDSTC3 & Tourist information & 2,265 &  \\\\\nDSTC4 \\& DSTC5 & Tourist information & 35 & \\\\\nDSTC6 & Restaurant reservation &-  & \\\\\nDSTC7 (Flex Data) & Student guiding & 500 &  \\\\\nDSTC8 (MetaLWOz)& 47 domains & 37,884  & \\\\\nDSTC8 (Schema-Guided)& 20 domains&22,825&\\\\\nMultiWOZ & Tourist information & 10,438  & \\\\\nTaskmaster-1 & 6 domains & 13,215  &  \\\\\nMultiDoGo & 6 domains & 86,698  &  \\\\\n\\hline\n\\end{tabular}\n\\caption{Datasets for task-oriented dialogue systems.}\n\\label{tab:task}\n\\end{table}\nDatasets are usually designed to evaluate specific dialogue components, and very few public datasets are able to evaluate an entire task-oriented dialogue system (e.g. Section \\ref{sec:eval_task}). The evaluation of these kinds of systems is highly system-specific, and it is therefore difficult to reuse the dataset with other systems. Their evaluation also requires considerable human effort, as  the involvement of individual users or external evaluators is usually needed. For example, in , which is a Partially observable Markov decision process -based dialogue system mentioned in Section \\ref{subsec:slot} for the restaurants domain, the evaluation of policies is done by crowd-sourcers via the Amazon Mechanical Turk service. Mechanical Turk users were asked first to find some specific restaurants, and after each dialogue was finished, they had to fill in a feedback form to indicate if the dialogue had been successful or not. Similarly, for the end-to-end dialogue system by  (cf. Section \\ref{subsec:ent-to-end}), also for the restaurants domain, human evaluation was conducted by users recruited via Amazon Mechanical Turk. Each evaluator had to follow a given task and to rate the system's performance. More specifically, they had to grade the subjective success rate, the perceived comprehension ability and naturalness of the responses. \nMost of the task-oriented datasets are designed to evaluate components of dialogue systems. For example, several datasets have been released through different editions of the Dialog State Tracking Challenge\\footnote{\\url{https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/}}, focused on the development and evaluation of the dialogue state tracker component. However, even if these datasets were designed to test state tracking,  used them to build and evaluate a whole dialogue system, re-adjusting the dataset by ignoring the state annotation and reusing only the transcripts of dialogues. The Schema Guided Dialogue (SGD) dataset released for the 8th edition of DSTC was designed to test not only state tracking, but also intent prediction, slot filling and language generation for large-scale virtual assistants. SGD consists of almost 23K annotated multi-domain (bank, media, calendar, travel, weather, ...), task-oriented dialogues between a human and a virtual assistant.\nThe MultiWOZ (Multi-Domain Wizard-of-Oz) dataset represented a significant breakthrough in the scarcity of dialogues as it contains around 10K dialogues, which is at least one order of magnitude larger than any structured corpus available before  . It is annotated with dialogue belief states and dialogue actions, so it can be used for the development of individual components of a dialogue system. But its considerable size makes it very appropriate for the training of end-to-end based dialogue systems. The main topic of the dialogues is tourism, containing seven domains, such as attractions, hospitals, police, hotels, restaurants, taxis and trains. Each dialogue can contain more than one of these domains.\nSimilar in size and content to MultiWOZ is Taskmaster-1 task-based dialogue dataset . It includes around 13K dialogues in six domains: ordering pizza, setting auto repair appointments, arranging taxi services, ordering movie tickets, ordering coffee drinks and making restaurant reservations. What makes it different from the previous one is that more than a half of the dialogues are created following a self-dialogue methodology, in which a crowd-worker writes the full dialogue themselves. The authors claim that these self-dialogues have richer and more diverse language than, for example, MultiWOZ, as it is not restricted to a small knowledge base.\nThe largest human-generated and multi-domain dialogue dataset that is available to the public is MultiDoGo , which comprises over 81K dialogues. These dialogues were created following the Wizard-of-Oz approach between a crowd-worker and a trained annotator. These participants were guided to introduce specific biases like intent or slot change, multi-intent, multiple slot values, slot overfilling and slot deletion in conversations. Additionally, over 54K of the total amount of the dialogues are annotated at the turn level for intent classes and slot labels. Dialogues are from six different domains: airline, fast food, finance, insurance, media and software support.\nWe will conclude this section by discussing two related tools, rather than a dialogue dataset. The first tool, called PyDial\\footnote{\\url{http://www.camdial.org/pydial/}}, partially addresses the shortage of evaluation datasets for task-oriented systems. This is because it offers the opportunity for developing a dialogue management environment, based on reinforcement-learning for benchmarking purposes . Thus, it makes it possible to evaluate and compare different task-oriented dialogue systems in the same conditions. This toolkit not only provides domain-independent implementations of different modules in a dialogue system, but also simulates users (see Section \\ref{subsec:simulation}). It uses two metrics for the evaluation: (1) the average success rate and (2) the average reward for each evaluated policy model of reinforcement-learning algorithms. The success rate is defined as the percentage of dialogues that are completed successfully. Thus, it is closely related to the task-completion metric used by the PARADISE framework (see Section \\ref{subsec:paradise}).\nAnother dialogue annotation tool is called LIDA . The authors argue that the quality of a dataset has a significant effect on the quality of a dialogue system, hence, a good dialogue annotation tool is essential to create the best annotated dialogue dataset. LIDA is the first annotation tool that handles the entire dialogue annotation pipeline from turn and dialogue segmentation through to labelling structured conversation data. Moreover, it also includes an interface for inter-annotator disagreements resolution.", "cites": [7453, 7331, 7452, 1548], "cite_extract_rate": 0.26666666666666666, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of datasets for task-oriented dialogue systems, listing their domains, sizes, and key features. It integrates some information from cited papers (e.g., MultiWOZ, Taskmaster-1, LIDA) but lacks deeper synthesis or a unifying framework. The critical analysis is minimal, with no evaluation of strengths or limitations of the datasets or tools. Some general patterns are mentioned, such as the use of crowd-sourcing and multi-domain coverage, but the abstraction remains at a basic level."}}
{"id": "dafadc03-6ef1-4b8c-86c5-e93b9441f4fb", "title": "Datasets for Conversational Dialogue Systems", "level": "subsection", "subsections": [], "parent_id": "b7d9cf74-5108-48cc-abe2-a5f162020e3c", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Evaluation Datasets and Challenges"], ["subsection", "Datasets for Conversational Dialogue Systems"]], "content": "\\label{subsec:conv}\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{llll}\n\\hline\nName & Topics & \\# dialogues      & Reference\\\\ \\hline\nSwitchboard & Casual topics & 2,400 &  \\\\\nBritish National Corpus & Casual topics & 854 &  \\\\\nSubTle Corpus & Movie subtitles & 3,35M &  \\\\\nReddit Domestic Abuse Corpus & Abuse help & 21,133 & \\\\\nTwitter Corpus & Unrestricted & 1,3M &  \\\\\nTwitter Triple Corpus & Unrestricted & 4,322 &  \\\\\nUbuntu Dialogue Corpus & Ubuntu problems  & 930K &  \\\\\nbAbI & Restaurant reservation & 3,000 &  \\\\\nOpenSubtitles & Movie subtitles & 36M &  \\\\\nCornellMovie & Movie dialogues & 220K &  \\\\\n\\hline\n\\end{tabular}\n\\caption{Datasets for conversational dialogue systems.}\n\\label{tab:conv}\n\\end{table}\nRegarding the evaluation of Conversational dialogue systems presented in Section \\ref{sec:eval_non_task}, datasets derived from conversations on micro-blogging or social media websites (e.g. Twitter or Reddit) are good candidates, as they contain general-purpose or non-task-oriented conversations that are orders of magnitude larger than other  dialogue datasets used before. For instance, Switchboard  (telephone conversations on pre-specified topics), British National Corpus  (British dialogues many contexts, from formal business or government meetings to radio shows and phone-ins) and SubTle Corpus  (aligned interaction-response pairs from movie subtitles) are three datasets released earlier that have 2,400, 854 and 3.35M dialogues and 3M, 10M and 20M words, respectively. These sizes are relatively small if we compare to the huge Reddit Corpus\\footnote{\\url{https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/}} which contains over 1.7 billion comments\\footnote{As far as we know, this dataset has not been used in any research work. Researchers have used smaller and more curated versions of the Reddit dataset like Reddit Domestic Abuse Corpus , which contains 21,133 dialogues.}, or the Twitter Corpus described below.\nBecause of the limit on the number of characters permitted in each message on Twitter, the utterances are quite short, very colloquial and chat-like. Moreover, as the conversations happen almost in real-time, the conversations of this micro-blogging website are very similar to spoken dialogues between humans. There are two publicly available large corpora extracted from Twitter. The former one is the Twitter Corpus presented in , which contains roughly 1.3 million conversations and 125M words drawn from Twitter. The latter is a collection of 4,232 three-step (context-message-response) conversational snippets extracted from Twitter logs\\footnote{\\url{https://www.microsoft.com/en-us/download/details.aspx?id=52375}}. This is labeled by crowdsourced annotators, who measure the quality of a response in a given context .\nAlternatively,  hypothesized that chat-room style messaging is more closely correlated to human-to-human dialogues than micro-blogging websites like Twitter, or forum-based sites such as Reddit. Thus, they presented the above-mentioned Ubuntu Dialogue Corpus. This large-scale corpus targets a specific domain. Thus, it could accordingly be used as a task-oriented dataset for the research and evaluation of dialogue state trackers. However, it also has the unstructured nature of interactions from microblog services that makes it appropriate for the evaluation of non-task-oriented dialogue systems. \nThese two large datasets are adequate for the three subtypes of non-task-oriented dialogue systems: unsupervised, trained and utterance selection metrics. Notice that, additionally, some human judgments could be needed in some cases, such as in  for the ADEM system (see Section \\ref{sec:general_metrics}). Here, they use human judgments collected via Amazon Mechanical Turk in addition to the evaluation using the Twitter dataset. \nApart from the afore-mentioned two datasets, the five datasets generated recently for bAbI tasks  are appropriate for evaluation using the next utterance classification method (see Section \\ref{subsec:utterance-selection}). These tasks were designed for testing end-to-end dialogue systems in the restaurant domain, but they check whether the systems can predict the appropriate utterances among a fixed set of candidates, and are not useful for systems that generate the utterance directly. The ibAbI dataset mentioned in the next section has been created based on bAbI to cover several representative multi-turn QA tasks.\nAnother interesting resource is the ParlAI framework\\footnote{\\url{http://parl.ai/}} for dialogue research, as it contains many popular datasets available all in one place with the goal of sharing, training and evaluating dialogue models across many tasks . Some of the dialogue datasets that are included have been already mentioned (bAbI Dialog tasks and the Ubuntu Dialog Corpus) but it also contains conversations mined from OpenSubtitles\\footnote{\\url{http://opus.lingfil.uu.se/OpenSubtitles.php}} and Cornell Movie\\footnote{\\url{https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html}}.", "cites": [7454, 7455, 7456, 8419], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of datasets for conversational dialogue systems, listing their characteristics and sources. It makes basic connections between the datasets and their suitability for different evaluation subtypes. However, there is limited critical evaluation or abstraction of broader trends or principles in the field."}}
{"id": "523a0303-f590-4bb9-8ddb-25f2a555296a", "title": "Datasets for Question Answering Dialogue Systems", "level": "subsection", "subsections": [], "parent_id": "b7d9cf74-5108-48cc-abe2-a5f162020e3c", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Evaluation Datasets and Challenges"], ["subsection", "Datasets for Question Answering Dialogue Systems"]], "content": "\\begin{table}[t]\n\\centering\n\\begin{tabular}{llll}\n\\hline\nName & Topics & \\# dialogues   &  Reference\\\\ \\hline\nUbuntu Dialogue Corpus & Ubuntu problems & 930K &  \\\\\nMSDialog & Microsoft products & 35K &  \\\\\nibAbI & Restaurant reservation & - &  \\\\\nCoQA &7 domains & 8,399 &  \\\\\nQuAC & People & 13,594 &  \\\\\nDoQA & Cooking & 1,637 &  \\\\\n\\hline\n\\end{tabular}\n\\caption{Datasets for question answering dialogue systems.}\n\\label{tab:qa}\n\\end{table}\nWith respect to QA dialogue systems, two datasets have been created based on human interactions from technical chats or forums. The first one is the Ubuntu Dialogue Corpus, containing almost one million multi-turn dialogues extracted from the Ubuntu chat logs, which was used to receive technical support for various Ubuntu-related problems . Similarly, MSDialog contains dialogues from a forum dedicated to Microsoft products. MSDialog also contains the user intent of each interaction .\nibAbI represents another approach for creating multi-turn QA datasets . ibAbI interactivity adds to the bAbI dataset that was previously presented (see Section \\ref{subsec:conv}) by adding sentences and ambiguous questions with the corresponding disambiguation question, which should be asked by an automatic system. The authors evaluate their system regarding the successful tasks. However, it is unclear how to evaluate a system if it produces a modified version of the disambiguation question.\nRecently, several datasets that are very relevant for the context of QA dialogue systems have been released. The CoQA (Conversational Question Answering) dataset contains 8K dialogues and 127K conversation turns . The answers from CoQA are free-form text with their corresponding evidence highlighted in the passage. It is a multi-domain dataset, as the passages are selected from several sources, covering seven different domains: children's stories, literature, middle and high school English exams, news, articles from Wikipedia, science and discussions from Reddit. QuAC (Question Answering in Context) consists of 14K information-seeking QA dialogues (100K total QA pairs) over sections from Wikipedia articles about people . What makes it different from other datasets so far is that some of the questions are unanswerable and that context is needed in order to answer some of the questions. Another similar dataset that has unanswerable questions and its questions are context-dependent is DoQA, a dataset for accessing domain-specific Frequently Asked Question sites via conversational QA . It contains 1,637 information-seeking dialogues on the cooking domain (7,329 questions in total). An analysis carried out by the authors showed that in this dataset there are less factoid questions than in the others, as DoQA focuses on open-ended questions about specific topics. Amazon Mechanical Turk was used to collect the dialogues for the three datasets.", "cites": [1098, 1145, 8419, 7457], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of datasets for QA dialogue systems, listing their properties and sources. While it makes basic connections (e.g., noting the use of Amazon Mechanical Turk across multiple datasets), it lacks deeper synthesis, critical evaluation of limitations, and abstraction to broader trends or principles. It remains largely descriptive without offering analytical insights."}}
{"id": "f3a9645e-5633-43f6-9761-385d1a81dbc3", "title": "Automation.", "level": "paragraph", "subsections": ["290f0a6e-8287-45eb-944a-d3979702fa9f"], "parent_id": "8a5853da-4970-44e3-bafd-bb0ee409d69c", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Challenges and Future Trends"], ["paragraph", "Automation."]], "content": "The evaluation methods covered in this survey all achieve a certain degree of automation. However, the automation is achieved with significant engineering effort, or by loss of correlation to human judgements.\nWord-overlap metrics (see Section \\ref{sec:general_metrics}), which are borrowed from the machine translation and summarization community, are fully automated. However, they do not correlate with human judgements on the turn level. On the other hand, BLEU becomes more competitive when applied on the corpus-level or system-level . More recent metrics such as $\\Delta$BLEU and ADEM (see Section \\ref{sec:general_metrics}) have significantly higher correlations to human judgements while requiring a significant amount of human annotated data as well as thorough engineering.  \nTask-oriented dialogue systems can be evaluated semi-automatically or even fully automatically. These systems benefit from having a well-defined task, where success can be measured. Thus, user satisfaction modelling (see Section \\ref{sce:user_satisfaction_modelling}) as well as user simulations (see Section \\ref{subsec:simulation}) exploit this to automate their evaluation. However, both approaches need a significant amount of engineering and human annotation: user satisfaction modelling usually requires prior annotation effort, which is followed by fitting a model that predicts the judgements. In addition to this effort, the process has to be potentially repeated for each new domain or new functionality that the dialogue system incorporates. Although in some cases the model fitted on the data for one dialogue system can be reused to predict another dialogue system, this is not always possible. \nOn the other hand, user simulations require two steps: gathering data to develop a first version of the simulation, and then building the actual user simulation. The first step is only required for user simulations that are based on training corpora (e.g. the neural user simulation). A significant drawback is that the user simulation is only capable of simulating the behaviour which is represented in the corpus or the rules. This means that it cannot cover unseen behaviour well. Furthermore, the user simulation can hardly be used to train or evaluate dialogue systems for other tasks or domains. \nAutomation is thus achieved to a certain degree, but with significant drawbacks. Hence, finding ways to facilitate the automation of evaluation methods is clearly an open challenge.", "cites": [7458, 7454], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section insightfully discusses the trade-offs between automation and human correlation in evaluation methods, integrating concepts from two cited papers to highlight broader trends. It critically assesses the limitations of both user satisfaction modelling and user simulations in terms of generalizability and data requirements. While it provides a coherent analysis, it stops short of fully abstracting an overarching framework for evaluating automation in dialogue systems."}}
{"id": "290f0a6e-8287-45eb-944a-d3979702fa9f", "title": "High Quality Dialogues.", "level": "paragraph", "subsections": ["3ff28339-d0a2-4633-a8f9-3b3b5b9fe537"], "parent_id": "f3a9645e-5633-43f6-9761-385d1a81dbc3", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Challenges and Future Trends"], ["paragraph", "Automation."], ["paragraph", "High Quality Dialogues."]], "content": "One major objective for a dialogue system is to deliver high quality interactions with its users. However, it is often not clear how ``high quality\" is defined in this context or how to measure it. For task oriented dialogue systems, the mostly used definition of quality is often measured by means of task success and number of dialogue turns (e.g. a reward of 20 for task-success minus the number of turns needed to achieve the goal). However, this definition is not applicable to conversational dialogue systems and it might ignore other aspects of the interaction (e.g. frustration of the user). Thus, the current trend is to let humans judge the \\emph{appropriateness} of the system utterances. However, the notion of appropriateness is highly subjective and entails several finer-grained concepts (e.g. ability to maintain the topic, the coherence of the utterance, the grammatical correctness of the utterance itself, etc.). Currently, appropriateness is modelled by means of latent representations (e.g. ADEM), which are derived again from annotated data. \nOther aspects of quality concern the purpose of the dialogue system in conjunction with the functionality of the system. For instance,  define the purpose of their conversational dialogue system to build an emotional bond between the dialogue system and the user. This goal differs significantly from the task of training a medical student in the interaction with patients. Both systems need to be evaluated with respect to their particular goal. The ability to build an emotional bond can be evaluated by means of the interaction length (longer interactions are an indicator of a higher user engagement), whereas training (or e-learning) systems are usually evaluated regarding their ability of selecting an appropriate utterance for the given context.\nThe target audience plays an important role as well. Since quality is mainly a subjective measure, different user groups prefer different types of interactions. For instance, depending on the level of domain knowledge, novice users prefer instructions that use less specialized wording, whereas domain experts might prefer a more specialized vocabulary. \nThe notion of quality is thus dependent on a large amount of factors. The evaluation needs to be adapted to take aspects such as the dialogue system's purpose, the target audience, and the dialogue system implementation itself into account.", "cites": [7451], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by discussing how the concept of 'high quality' in dialogue systems varies depending on the system's purpose, audience, and implementation. It integrates the cited paper on XiaoIce to illustrate the importance of emotional engagement in social chatbots, while contrasting it with other use cases like e-learning. The analysis points out the subjectivity of quality and the limitations of current methods, though it could benefit from deeper comparative or critical insights across multiple works."}}
{"id": "3ff28339-d0a2-4633-a8f9-3b3b5b9fe537", "title": "Lifelong Learning.", "level": "paragraph", "subsections": [], "parent_id": "290f0a6e-8287-45eb-944a-d3979702fa9f", "prefix_titles": [["title", "Survey on Evaluation Methods for Dialogue Systems"], ["section", "Challenges and Future Trends"], ["paragraph", "Automation."], ["paragraph", "High Quality Dialogues."], ["paragraph", "Lifelong Learning."]], "content": "The notion of lifelong learning for machine learning systems has gained traction recently. The main concept of lifelong learning is that a deployed machine learning system continues to improve by interaction with its environment . Lifelong learning for dialogue systems is motivated by the fact that it is not possible to encounter all possible situations during training, thus, a component that allows the dialogue system to retrain itself and adapt its strategy during deployment seems the most logical solution. \nThe evaluation step is critical in order to achieve lifelong learning. Since the dialogue system relies on the ability to automatically find critical dialogue states where it needs assistance, a module is needed which is able to evaluate the ongoing dialogue. One step in this direction is done by , who present a solution that relies on a satisfaction module that is able of to classify the current dialogue state as either satisfactory or not. If this module finds an unsatisfactory dialogue state, a feedback module asks the user for feedback. The feedback data is then used to improve the dialogue system. \nThe aspect of lifelong learning brings a large variety of novel challenges. Firstly, the lifelong learning system requires a module that self-monitors its behaviour and notices when a dialogue is going wrong. For this, the module needs to rely on evaluation methods that work automatically, or at least semi-automatically. The second challenge lies in the evaluation of the lifelong learning system itself. The self-monitoring module as well as the adaptive behaviour need to be evaluated. This brings a new dimension of complexity into the evaluation procedure.", "cites": [7459], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the concept of lifelong learning in dialogue systems and integrates it with the cited work on self-feeding chatbots. It highlights the importance of evaluation in enabling this learning process, though it does so without comparing multiple approaches or deeply critiquing the cited method. The abstraction is moderate, as it identifies general challenges like self-monitoring and adaptive evaluation, but stops short of offering a comprehensive meta-framework."}}
