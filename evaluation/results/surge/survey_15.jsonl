{"id": "1836b30b-bf51-4647-b539-22378b36efc3", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "8b8ae080-427e-4f7b-a004-922aff6933a4", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Introduction"]], "content": "Surveillance videos have been increasingly present in various establishments in order to monitor human activity and prevent crime from happening. It goes without saying that there needs to be someone behind watching the videos and signaling an alert whenever something different from normal is happening. However, these events do not happen very often and that most of the time, the person monitoring these videos would see nothing out of the ordinary . These unusual events can be thought of as \\textit{anomalies} which can be defined as patterns that do not conform to what is considered normal. The task of finding these nonconforming patterns is called \\textit{anomaly detection} . Because of this, researchers have been trying to create a robust anomaly detection algorithms that can automate the process of monitoring and detection of unusual events in surveillance videos. An example of a simple anomaly case can be seen in Fig. \\ref{pic_anomly_simple} where the normal regions are denoted by $N$ and anomalies are those denoted by $O$. As seen in the figure, anomalies tend to clearly lie outside what is normal. However, these anomalies can, in fact, be close to normality which is illustrated by $O_2$\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[scale=0.3]{anomaly_example.png}\n  \\caption{Simple Anomaly Case by Chandola et. al. \\citeyear{chandola_2019_anomaly_detection}}\n  \\label{pic_anomly_simple}\n\\end{figure}\nAnomaly detection is a challenging task due to number of reasons: first, the definition of an anomaly may vary from one context to another . Second, the different possibilities of what constitute an anomaly might be are boundless . Third, anomalous data points, especially with real-world data, tend to lie closely to what might be defined as normal . Lastly, extracting robust features from the data even if anomalies seldom appear . The mentioned list does not entirely capture all of the possible reasons which make the problem hard but these main points are what researchers have been considering for the past years when proposing new solutions to the problem. \nAround a decade ago, most of the researchers have focused on trajectory-based anomaly detection . The main idea is if the objects of interest are not following the learned normal trajectories, the video will be tagged as an anomaly. However, one major drawback of this approach is occlusion since the approach heavily relies on continuously monitoring the objects of interest . Due to these drawbacks, there was an emphasis on using low-level features for feature extraction instead . These approaches based on low-level features rely on the use of appearance, motion, and texture features . Various representations have been used in order to represent these aspects of the video such as in the approach of  where they used social force maps to model motion of the crowds. Similarly, pixel-motion properties were used by  to model behavior. Meanwhile,  made use of optical flows which are then used as inputs to the mixture of probabilistic principal component analysis (MPPCA) model, thus, creating a more compact feature representation. However, features based on motion are not enough which is why there were proposed approaches that make use of both. An example is the approach of  where their approach makes use of mixture of dynamic textures (MDTs) that utilize temporal normalcy and discriminant saliency detectors to model spatial normalcy. Likewise,  used support vector data description for spatial features and optical flow for motion features. In contrast,  used spatially localized histogram of optical flows and uniform local gradient pattern-based optical flows. Most of these techniques and methods, specifically on these \"traditional\" approaches, have been discussed in great detail in the works of .\nDespite the proven success of these traditional approaches on benchmark datasets, they are still ineffective when used in a different domain. Furthermore, they are unable to adapt to anomalies that they have never seen before . For these reasons, recent works have mostly explored the use of Deep Neural Networks for the task of anomaly detection. These neural networks automatically learn useful and discriminant features on their own which removes the hassle of creating handcrafting features . This also makes it more adaptive when used on different domains. Deep learning was proven to be effective for a variety of computer vision tasks such as feature extraction in images , image classification , object detection , video analysis , face detection , visual question answering  and many other tasks.\nAs mentioned previously, there are existing works that have discussed various anomaly detection methods for videos . However, due to the recent traction in the use of deep learning techniques on this field, the goal of this paper is to provide a closer look into these deep learning techniques. This entails providing organization as to how the approaches are related to one another, the rationale as to why these methods have been proposed, and summarizing the conclusions which they have presented in a clear manner. In addition, it would also be necessary to discuss datasets and evaluation metrics which have mostly been used by these approaches. It would also be insightful to determine how these datasets and metrics would scale well when dealing with real-world anomaly detection. Different researchers have created different environmental setups  making some of them incomparable. Thus, the performances of the approaches discussed will not be included to avoid confusion and misinterpretation.\nThe paper is organized as follows: the first section serves as an introduction to the survey. Second, deep learning anomaly detection techniques will be discussed in detail. Third, the mostly used datasets will be tackled. Fourth, the commonly used evaluation metrics will be presented. Fifth, a section for discussion is allocated to synthesize all of the approaches and datasets mentioned. Lastly, the concluding remarks coupled with recommendations as to what directions this area of research could possibly go.", "cites": [6980, 871, 4109, 4108, 4110], "cite_extract_rate": 0.17857142857142858, "origin_cites_number": 28, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of traditional and deep learning-based approaches to video anomaly detection, mentioning several methods and citing relevant papers. However, it lacks synthesis by not clearly connecting the cited papers to broader themes or trends. Critical analysis is limited, as the section mainly states that traditional approaches are ineffective in new domains without evaluating specific shortcomings of the cited works. Abstraction is minimal, as it does not move beyond specific techniques to identify overarching principles or conceptual frameworks."}}
{"id": "b30ed58d-7830-46da-8d93-2d05362712c9", "title": "Deep Learning in Anomaly Detection for Videos", "level": "section", "subsections": ["2f1f833e-00cc-4d50-a9ad-15acf75b7b3d", "ea82b5e5-bb86-44b7-9d0e-7660075cb3dc", "73e251c7-9616-4a66-87e4-7b8e6266aa71", "81774b96-e764-47cf-8717-22146791a3cf"], "parent_id": "8b8ae080-427e-4f7b-a004-922aff6933a4", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Deep Learning in Anomaly Detection for Videos"]], "content": "Deep learning techniques mostly focus on creating new architectures or crafting components that can be suitable for a specific problem. Since deep learning methods have been successful in a number of varied use cases , most of these networks or architectures might be similar to each other. An example of which would be with  where they used Convolutional Neural Networks for image classification. However, almost the same network is also used for face recognition . Because of this, the presented categories below would group these approaches specifically with respect to their final objectives instead of network architecture or learning strategy. Examples of these include using reconstruction error or providing an anomaly score. In line with this, there are four (4) identified categories namely: using reconstruction error or reconstruction-based methods, framing the problem as a classification problem, predicting future frames, and computing for an anomaly score. A quick summary of all these techniques are provided in Table \\ref{deep_learning_techniques}.", "cites": [871, 4108], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief, high-level overview of deep learning in video anomaly detection and mentions the categorization of approaches. However, it does not effectively synthesize or integrate the cited papers into the discussion, nor does it critically evaluate or compare them. The abstraction is minimal, with only vague references to broader patterns without elaboration."}}
{"id": "2f1f833e-00cc-4d50-a9ad-15acf75b7b3d", "title": "Using Reconstruction Error", "level": "subsection", "subsections": [], "parent_id": "b30ed58d-7830-46da-8d93-2d05362712c9", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Deep Learning in Anomaly Detection for Videos"], ["subsection", "Using Reconstruction Error"]], "content": "\\label{subsection_reconstruction}\nReconstruction error has already been used in various traditional anomaly detection techniques . The basic assumption of using reconstruction error is that the reconstruction error for normal samples would be lower since they are closer to the training data. On the other hand, the reconstruction error is assumed or expected to be higher for samples which are not normal . \nMore formally, let $x$ be a video segment or video frame and let $g$ be a neural network that reconstructs $x$. The reconstruction error can be defined as a function $f$ such that is computes for error between $x$ (the original input), and $g(x)$ which is the reconstruction Eqn \\ref{eq_reconstruction}. This concept has been extended recently by making use of deep learning techniques to reconstruct various scenes.\n\\begin{equation} \\label{eq_reconstruction}\n    e = f(x, g(x))\n\\end{equation}\nDifferent from usual feedforward networks, one type of neural network that is able to reconstruct input data is called an autoencoder. The autoencoder is a neural network that has the capability to encode an input into a more compact representation while retaining important and discriminative features. It also has the ability to decode this particular encoding back to its original form . A visual schematic of an autoencoder is shown in Fig. \\ref{pic_autoencoder} where the diagram illustrates a simple architecture of an encoder where the left-hand side is the input to the autoencoder $X$, the middle portion is the encoded representation (sometimes called the latent vector or code) of $X$, and the right-hand side is the decoded encoding called $X'$.\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[scale=0.4]{Autoencoder_schema.png}\n  \\caption{Autoencoder Diagram by Michaela  via Wikimedia Commons}\n  \\label{pic_autoencoder}\n\\end{figure}\nMost approaches whose goal of using reconstruction error as a means to identify anomalies base their method on autoencoders. One such method is introduced by  where they posited that in comparison to sparse coding, the objective function of an autoencoder is more efficient. They have also said that it is able to preserve spatio-temporal information while encoding dynamics. Their approach made use of combining 2D convolutions to autoencoders wherein the 2D convolutions take as input specific raw video segments. Conventionally, inputs to a Convolutional Neural Network is a 2D image having the third channel as the color channel . However, in their approach, the third dimension is instead composed of stacked grayscale frames, allowing the model to encode both spatial and some temporal information for reconstruction. \nSimilarly, the work of  also framed the problem as a reconstruction problem. The approach makes use of a convolutional long short-term memory wherein the Long Short-Term Memory (LSTM) Network is a type of neural network that is capable of learning long-term dependencies of the data . Despite not being explicitly an autoencoder, their approach also makes use of an encoder-decoder sturture. Given an input sequence of video frames, the convolutional long short-term memory extracts relevant features along the spatial and temporal dimension in such a way that the last time step is used as the encoding. The decoder unravels the encoding and then reconstructs the frames which can then be used to compute the reconstruction error for anomaly detection.\nThe proposed approach of  closely resembles that of . The main difference is that the low-level features such as optical flow and edges are used as inputs alongside the raw frames. In addition, they have also presented how these features affect the convolutional autoencoder with regard to detecting anomalies.\nAnother method was proposed by  where they have used two different autoencoders for the task: one is a regular autoencoder and the other is a sparse autoencoder. A sparse autoencoder is an autoencoder but has an additional sparsity penalty. This penalty encourages fewer neurons to activate. This constraint allows the network to learn relevant information without reducing the number of nodes in the hidden layers. Their approach involves two steps, the first step is to compute the sparsity value from cubic patches of the videos, if it is below a specific sparsity threshold, another set of patches are extracted around that patch for reconstruction. \nAccording to , the approach of  which makes use of temporal cuboids by stacking frames in the third dimension, does not necessarily retain the temporal information. Based on their work, a reason for this is that 2D convolutions operate on the frames spatially. Putting this in the perspective of the approach of , the third channel is represented along each of the channels of the first feature map which rarely preserves temporal information. To solve this,  proposed the use of 3D convolutions as a means to retain temporal information during the convolution process. Since it is data intensive, they have also applied data augmentation to increase their samples.\nAs claimed in the work of  , one weakness of the approach of   is that spatial and temporal aspects of the inputs are encoded separately by the convolutions and the long short-term memory. This implies a broken relationship between the two during the encoding process. Furthermore, it was also stated by  that the approach proposed by  was not able to make use of existing pre-trained networks. These networks have shown remarkably improved performances once it has been applied to other domains. Hence, their proposed method makes use of a feature learning subnetwork that combines motion and appearance features into an image. Afterwards, it is then used as an input to a pretrained network for feature extraction. Moreover, they have proposed a novel subnetwork called sparse coding to network (SC2Net) to compute for the sparsity loss and reconstruction loss from the extracted features.\nAmong all of the approaches,  have posited that most of the works on reconstruction generally assume that the anomalous instances will have a high reconstruction error. Based on these works, this assumption does not necessarily hold true mainly because there might be instances where an autoencoder is able to generalize well. This poses a problem since it might accurately reconstruct anomalous instances as well. To mitigate this problem, they have introduced a new autoencoder which has the capability to store encodings into memory. The main difference from previous approaches is that instead of directly feeding the encoding to the decoder, the encoding is treated as a query. This query is expected to return closest normal patterns in memory which is instead used for decoding. In the event that an anomaly is to be reconstructed, it would have a high reconstruction error because the memory only has normal memory items.", "cites": [6980, 8732], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes several approaches using reconstruction error and connects them by highlighting common assumptions and methodological variations, such as the use of 2D vs. 3D convolutions and the inclusion of memory mechanisms. It provides a critical analysis of the limitations of certain methods (e.g., inability to preserve temporal information or generalize well to anomalies). However, the abstraction level is moderate, as it focuses more on specific techniques rather than distilling high-level principles or trends that unify the field."}}
{"id": "ea82b5e5-bb86-44b7-9d0e-7660075cb3dc", "title": "Using Future Frame Prediction", "level": "subsection", "subsections": [], "parent_id": "b30ed58d-7830-46da-8d93-2d05362712c9", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Deep Learning in Anomaly Detection for Videos"], ["subsection", "Using Future Frame Prediction"]], "content": "A different perspective on the problem was presented by . They support the claim of   stating that autoencoders might also accurately reconstruct anomalous frames. Since anomalies can be viewed as events that do not conform with certain expectations,  suggested a frame prediction approach might be a more natural way to view the problem. Mathematically speaking, given $x_t$ which is the video segment or frame $x$ at time $t$, future frame prediction can be expressed as a function $h$ predicting the next segment as shown in Eqn \\ref{eq_future}.\n\\begin{equation}\\label{eq_future}\n    x_{t+1} = h(x_t)\n\\end{equation}\nIn deep learning, there is a specific type of neural network is used for generating new data with the same statistics as the training data. This network which is called generative adversarial network (GAN) . This architecture has two main (2) parts. The first one is a generator whose job is to mimic the original data distribution. Meanwhile, the second network is called a discriminator that gives a probability of whether or not the input is coming from the generator. \nThe approach of  made use of a generator-discriminator structure, likened to that of a generative adversarial network. They used the U-Net architecture  for future frame prediction as the generator because of its exemplary performance in image-to-image translation. While the discriminator at the end of the network determines whether or not the predicted frame is anomalous. \nSome works on reconstruction also have the capability for predicting future frames such as in the work of . Their approach has the ability to encode both spatial and temporal aspects of the video by allowing the autoencoder to learn it from a sequence of video segments (discussed in more detail in Section \\ref{subsection_reconstruction}). It is because of this exact same reason that it can also predict future and past frames given a center frame. Based on their methodology, by padding the center frame with zero values, their model can extrapolate the near future and near past of the center frame. \nMoreover, some of the previous works actually leverage future frame prediction in the process of reconstructing the current frame. An example of this is the work of   where their network learns the future frames along with the task of reconstruction in a different branch of the network. Similarly,  also has a separate branch in parallel that learns how to predict the future. Despite their similarities, they both have big differences as to how future frame prediction is used.  makes use of future frame prediction to identify interest points within the video. On the contrary, in the approach of  , the future frame is actually included in the computation of the loss to guide the network to extract temporal features. In addition, it is also included in the reconstruction score which combines the prediction loss and the reconstruction loss.", "cites": [825, 8732, 6980], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple cited works to discuss future frame prediction as a method for video anomaly detection, showing a reasonable level of synthesis by connecting the use of GANs, U-Net, and autoencoders across different studies. It includes some critical analysis by pointing out differences in how future frame prediction is utilized (e.g., for interest point identification vs. loss computation). However, the abstraction remains limited, as it does not clearly generalize these methods into broader principles or a meta-framework for understanding the field."}}
{"id": "73e251c7-9616-4a66-87e4-7b8e6266aa71", "title": "Using Classifiers", "level": "subsection", "subsections": [], "parent_id": "b30ed58d-7830-46da-8d93-2d05362712c9", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Deep Learning in Anomaly Detection for Videos"], ["subsection", "Using Classifiers"]], "content": "Despite the sophisticated methods that rely mainly on reconstruction loss and future frame prediction, there are also still a handful of approaches that cast the problem as a classification problem. The classification problem can be viewed as a function $j$ that takes as its input a frame or video segment $x$ whose output $y$ is a class or category as seen in Eqn \\ref{eq_classification}.\n\\begin{equation}\\label{eq_classification}\n    y = j(x), y \\in \\mathbb{R}\n\\end{equation}\nBecause of imbalanced datasets, these methods focus mostly on how to create compact, efficient, and robust features. The approach of  tries to solve this problem by proposing a competitive cascade of deep neural networks. The cascade is composed of two stages where the first stage is a small stack of autoencoders which hierarchically models the normality of the video patches. The other one is a Convolutional Neural Network which takes as input video patches that the autoencoders could not handle and would need further probing. The classifier used for the approach is a Gaussian Classifier.\n\\begin{table}[]\n\\small\n\\centering\n\\hspace*{-0.5cm}\n\\caption{Summary of Methods and Contributions}\n\\label{deep_learning_techniques}\n\\hspace*{-0.3cm}\n\\begin{tabular}{|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{000000} \n\\multicolumn{1}{|c|}{\\cellcolor[HTML]{000000}{\\color[HTML]{FFFFFF} \\textbf{Year}}} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{000000}{\\color[HTML]{FFFFFF} \\textbf{Author}}} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{000000}{\\color[HTML]{FFFFFF} \\textbf{Type}}} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{000000}{\\color[HTML]{FFFFFF} \\textbf{Main Contribution}}} \\\\ \\hline\n2016 & Medel et. al & \\begin{tabular}[c]{@{}l@{}}Reconstruction\\\\ \\& Future Frame\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Convolutional Long \\\\ Short-Term Memory\\end{tabular} \\\\ \\hline\n2016 & Hasan et. al. & Reconstruction & \\begin{tabular}[c]{@{}l@{}}Fully 2D Convolutional\\\\ Autoencoder\\end{tabular} \\\\ \\hline\n2016 & Sabokrou et. al. & Reconstruction & \\begin{tabular}[c]{@{}l@{}}Sparse Autoencoder +\\\\ Autoencoder\\end{tabular} \\\\ \\hline\n2016 & Hu et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Deep Neural Network +\\\\ Slow Feature Analysis\\end{tabular} \\\\ \\hline\n2017 & Narasimhan et. al. & Classification & \\begin{tabular}[c]{@{}l@{}}Sparse Denoising \\\\ Autoencoders\\end{tabular} \\\\ \\hline\n2017 & Sabokrou et. al. & Classification & \\begin{tabular}[c]{@{}l@{}}Cascade of Deep \\\\ Convolutional Neural \\\\ Networks + \\\\ Autoencoders\\end{tabular} \\\\ \\hline\n2017 & Zhao et. al. & \\begin{tabular}[c]{@{}l@{}}Reconstruction\\\\ \\& Future Frame\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Spatiotemporal\\\\ Autoencoder\\end{tabular} \\\\ \\hline\n2018 & Sabokrou et. al. & Classification & Deep-Anomaly \\\\ \\hline\n2018 & Sultani et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Multiple-Instance \\\\ Learning\\end{tabular} \\\\ \\hline\n2018 & Ribeiro et. al. & Reconstruction & \\begin{tabular}[c]{@{}l@{}}Low-level Features + \\\\ 2D Convolutional \\\\ Autoencoder\\end{tabular} \\\\ \\hline\n2018 & Liu et. al. & Future Frame & \\begin{tabular}[c]{@{}l@{}}Future Frame using \\\\ U-Net\\end{tabular} \\\\ \\hline\n2019 & Landi et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Localization before\\\\ Feature Extraction\\end{tabular} \\\\ \\hline\n2019 & Sabzailan et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Traditional + Deep\\\\ Learning Features\\end{tabular} \\\\ \\hline\n2019 & Zhu et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Optical Flow as inputs\\\\ to Multiple-Instance\\\\ Learning\\end{tabular} \\\\ \\hline\n2019 & Zhou et. al. & Reconstruction & \\begin{tabular}[c]{@{}l@{}}AnomalyNet: a unified\\\\ approach\\end{tabular} \\\\ \\hline\n2019 & Gong et. al. & Reconstruction & \\begin{tabular}[c]{@{}l@{}}Autoencoder + memory\\\\ module + attention-based \\\\ addressing\\end{tabular} \\\\ \\hline\n2019 & Lin. et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Multiple-Instance \\\\ Learning + Social Force \\\\ Maps\\end{tabular} \\\\ \\hline\n2019 & Santos et. al. & Classification & \\begin{tabular}[c]{@{}l@{}}Transfer Learning + \\\\ Transfer Component\\\\ Analysis\\end{tabular} \\\\ \\hline\n2019 & Luo et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Sparse Coding-inspired\\\\ Deep Neural Network\\end{tabular} \\\\ \\hline\n2019 & Ionescu et. al. & Classification & \\begin{tabular}[c]{@{}l@{}}Object-Centric \\\\ Convolutional \\\\ Autoencoders\\end{tabular} \\\\ \\hline\n2019 & Xu et. al. & Classification & \\begin{tabular}[c]{@{}l@{}}Adaptive Intra-Frame\\\\ Classification Network\\end{tabular} \\\\ \\hline\n2020 & Fan et. al. & Scoring & \\begin{tabular}[c]{@{}l@{}}Gaussian Mixture\\\\ Fully Convolutional\\\\ Variational Autoencoders\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\end{table}\nOn the other hand,  proposed a method that makes use of local and global descriptors whose aim is utilize both spatial and temporal domains. For local features, they made use of an image similarity metric on the video cubic patches to represent the temporal and spatial features. Meanwhile, the global features are represented by the latent vector of the trained autoencoders. After creating both local and global features, it is then fed to an autoencoder which selects important features that are discriminative enough for anomaly detection. Finally, these features are fed into Gaussian classifiers separately for local and global descriptors and then combined to detect anomalies.\nMost of the above mentioned methods, even those in the previous sections, make use of Convolutional Neural Networks. However,  has mentioned problems with regard to using these networks, one of which is that these networks are too inefficient for patch-based methods. Examples of approaches that made use of patches are as follows: . For this reason, they have proposed a possible solution to the problem which makes use of the discriminative power of a pre-trained model without having to tweak it . More specifically, they use the intermediate layer to generate the features that will be fed to a Gaussian Classifier. In the event that a low confidence is generated by the classifier, it is sent to another convolutional layer on top of the best intermediate layer for further probing. \nSimilar to , the proposed approach of  took advantage of the available pre-trained models. They have investigated the generalization of feature spaces of Convolutional Neural Networks without requiring additional labels. In their experiments, they used transfer component analysis  which attempts to learn a certain subspace that is shared by different domains. They have concluded that generalization through different domains.\nMost of the methods mentioned previously make use of extracting either global or local features without taking the objects of interest into account. The approach of  makes use of a single-shot detector (SSD)  on each frame of the video. After isolating the objects, a convolutional autoencoder is used to learn deep unsupervised features thereby allowing the algorithm to focus on the objects in the scene. Furthermore, they have instead casted the problem of anomaly detection as a multi-class classification problem rather than an unbalanced binary classification problem or a one-class problem. To generate the artificial classes, they have used clustering on the set of features generated by the convolutional autoencoder where each cluster represents a different type of normality. A one-versus-rest classifier is trained which discriminates between the clusters. If the highest classification score is negative, meaning the sample does not belong to any cluster, it is tagged as anomalous. \nSimilar to ,  also framed the problem as a multi-class classification problem as opposed to either a one-class or a binary classification problem. In line with this, they also took note of the fact that most of the previous approaches were able to effectively identify subregions representations of anomalies. However, for most of the approaches, there is a wide array of inputs and outputs such as optical flows, patches, or gradients. This inspired the approach of  which tries to unify all of these approach by creating a network called the adaptive intraframe classification network that takes the raw inputs, computes for motion and appearance features, and determines whether or not the sample is anomalous.", "cites": [4111, 802, 6980], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methods that use classifiers for video anomaly detection and includes a table summarizing various works. It makes minimal synthesis by briefly mentioning similarities (e.g., use of autoencoders and Gaussian classifiers), but lacks a deeper integration of ideas across the cited works. There is little critical analysis of the methods' strengths or weaknesses, and the abstraction level remains low, focusing mainly on specific techniques rather than broader trends or theoretical implications."}}
{"id": "81774b96-e764-47cf-8717-22146791a3cf", "title": "Using Scoring Methods", "level": "subsection", "subsections": [], "parent_id": "b30ed58d-7830-46da-8d93-2d05362712c9", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Deep Learning in Anomaly Detection for Videos"], ["subsection", "Using Scoring Methods"]], "content": "Some researchers have instead, framed the problem as a regression problem wherein the goal is to provide an anomaly score which will then be used as a means to determine whether or not a video segment or a frame is anomalous . The scoring methods can be viewed as a function $k$ such that it takes a video segment or frame $x$ as its input. It outputs a real number $z$ representing the anomaly score as seen in Eqn \\ref{eq_scoring}.\n\\begin{equation}\\label{eq_scoring}\n    z = k(x), z \\in \\mathbb{R}\n\\end{equation}\nThe proposed approach of , makes use of their novel sum squared derivative to score the features generated by their approach. This basically determines if the sequence of frames is anomalous. Prior to their scoring method, they combined both deep learning and slow feature analysis  in order to learn semantic-level representations given raw video frames. It is also worth noting that their approach has an online variant, thereby making their approach adaptive.\nThe approach of  made use of a multiple instance learning to identify anomalies in video segments based on weakly-labelled videos (labels are on a video-level and not frame-level). Their approach uses C3D, a 3D Convolutional Neural Network that learns spatiotemporal features by exposing the model to large-scale video datasets . These spatiotemporal features are then fed to fully connected layers for generating the anomaly score. The backpropagation of the error is guided by the principle of multiple instance learning, allowing the model to learn anomalous segments despite having weak labels. This idea was taken up by  where, instead of using C3D, they made use of computing for the optical flows which are then fed to a temporal augmented network. Their proposed approach also makes use of an attention mechanism  that allows the network to identify which features are important to look at. Similarly,  also built upon this idea where they proposed a dual-branch network that incorporates motion into the initial network introduced by . The approach of  adapts the same network of  as the first branch with a modification wherein an attention module  was added after the feature extraction layer. The second branch is similar in structure as the first branch except that it takes as an input social force maps  computed from the raw images to represent motion. \nMeanwhile, the approach of  makes full use of the effectiveness of traditional and deep learning features for anomaly detection. Their proposed approach starts by identifying the foreground of the video by using optical flows. Once the regions of interest have been identified, a pre-trained Convolutional Neural Network is used to extract features alongside computing for traditional features like histogram of gradients and histogram of optical flows. These three features are combined by making use of an iteratively weighted nonnegative matrix factorization method . Afterwards, the features are clustered and the discrimination of whether or not the sample is an anomaly will be done via a voting system.\nAside from framing the problem as a regression problem,  proposed to make use of locality when computing for the anomaly score. The approach is similar to that of  except that their approach extracts a tube from the video which in a way localizes and adjusts the level of granularity when extracting features. From their experiments, they have shown that locality or, more specifically, zoning in on one region where the anomalous event takes place actually helps the method to accurately compute anomaly scores. \nSparse coding for anomaly detection is an approach that learns a dictionary which attempts to encodes all normal events . By revisiting sparse coding,  proposed temporally-coherent sparse coding to model the coherence between neighboring events for normal frames. These temporal features are then combined with spatial features learn from pre-trained networks across different scales for a normality score. Note that the features extracted pass through a Stacked Recurrent Neural Network autoencoder to generate the final features for scoring.\nPast works demonstrated the effectiveness of autoencoders and that normal samples can be associated with at least one Gaussian Mixture Model. Because of this,  proposed an end-to-end neural network called the Gaussian Mixture Fully Convolutional Variational Autoencoder to model anomalies and to predict them. Their model is trained on image and dynamic flow patches wherein both of them are separately fed into different networks. This basically captures separate motion and appearance features. Afterwards, joint probabilities are used to detect both appearance and motion anomalies via a sample energy-based method.", "cites": [4111, 784, 38, 4112, 4110], "cite_extract_rate": 0.38461538461538464, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of various scoring methods used in video anomaly detection, integrating different approaches in terms of their general framework. However, it lacks deeper synthesis or a unifying narrative that connects the methods more cohesively. There is minimal critical evaluation or identification of broader trends, making it largely descriptive with limited analytical depth."}}
{"id": "8740d493-64d2-4190-8620-34032654ac3b", "title": "The Subway Dataset", "level": "subsection", "subsections": [], "parent_id": "fc0df811-22c8-46cf-83ea-2bb566eb7619", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Existing Benchmark Datasets"], ["subsection", "The Subway Dataset"]], "content": "The Subway Dataset\\footnote{http://vision.eecs.yorku.ca/research/anomalous-behaviour-data/. This link only contains the Subway Exit}  contains two types of videos namely the \"exit gate\" and \"entrace gate\" videos. All in all, the videos are around two (2) hours long with a resolution of 512 $\\times$ 384. \n\\begin{figure}[h]\\centering\n    \\subfloat{\\includegraphics[scale=0.3]{subway_entrance.png}}\n    \\subfloat{\\includegraphics[scale=0.3]{subway_exit.png}}\n    \\caption{Subway Dataset}\n    \\label{example_subway}\n\\end{figure}\nThe exit gate video has 136,524 frames while the entrance gate video has 72,401 frames . In both scenarios, abnormality may include avoiding payment or walking in the wrong direction as the crowd. Comparing it to other datasets, the anomalies present in this dataset are relatively low .", "cites": [6980], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Subway Dataset, including its video types, length, resolution, and frame count. It mentions the types of anomalies but lacks synthesis of ideas from the cited paper and fails to integrate broader themes or insights from other works. There is minimal critical analysis or abstraction beyond the dataset's characteristics."}}
{"id": "4b6c8364-5af6-412e-8305-a1d0a28bbc7c", "title": "The ShanghaiTech Campus Dataset", "level": "subsection", "subsections": [], "parent_id": "fc0df811-22c8-46cf-83ea-2bb566eb7619", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Existing Benchmark Datasets"], ["subsection", "The ShanghaiTech Campus Dataset"]], "content": "The ShanghaiTech Campus\\footnote{https://svip-lab.github.io/dataset/campus\\_dataset.html} dataset  was proposed due to the lack of scene diversity from pre-existing benchmark datasets. Compared to previous datasets, the ShanghaiTech dataset has a larger number of videos having 330 training videos and 107 testing videos which consists of 13 different scenes and a large amount of varying anomaly types. The resolution of the videos in this dataset is at 856 $\\times$ 480.\n\\begin{figure}[h]\\centering\n    \\subfloat{\\includegraphics[scale=0.2]{shanghai_normal.png}}\n    \\subfloat{\\includegraphics[scale=0.2]{shanghai_anomaly.png}}\n    \\caption{ShanghaiTech Campus}\n    \\label{example_shanghai}\n\\end{figure}\nAn example is shown in Fig. \\ref{example_shanghai} where the left image is the normal image with students walking while the right image contains the anomaly where there is a biker. Furthermore, there are also anomalies which are cause by sudden motion such as chasing and brawling. These types of anomalies are not included in datasets such was UCSD Pedestrian, CUHK Avenue, UMN Dataset, and Subway Dataset.", "cites": [6980], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides basic factual information about the ShanghaiTech Campus dataset, such as its size, scenes, and anomaly types. It includes a comparison to other datasets but does so in a superficial way without deeper synthesis or evaluation of the cited work. The only cited paper is not directly analyzed or connected to the dataset description, limiting the insight quality."}}
{"id": "7d21f9fb-e029-4a96-afa1-1cb79e0e3f6e", "title": "The UCF-Crime Dataset", "level": "subsection", "subsections": [], "parent_id": "fc0df811-22c8-46cf-83ea-2bb566eb7619", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Existing Benchmark Datasets"], ["subsection", "The UCF-Crime Dataset"]], "content": "Due to the previous datasets being relatively small in size, the UCF-Crime Dataset\\footnote{https://webpages.uncc.edu/cchen62/dataset.html} was created by . This dataset contains 13 real-world anomalies namely accidents, burglary, explosion, fighting, robbery, shooting, stealing, shoplifting, and vandalism. Compared with previous datasets which were manually collected, this dataset was taken from Youtube\\footnote{www.youtube.com} and LiveLeak\\footnote{www.liveleak.com} using relevant text queries. These text queries are not limited to English, other languages (using Google Translate) were also used for searching. Overall, there are 950 untrimmed real-world surveillance videos and 950 normal videos garnering a total of 1,900 videos in the dataset. Note that the entire dataset has around 128 hours worth of data having a resolution of 240 $\\times$ 320.\n\\begin{figure}[h]\\centering\n    \\subfloat{\\includegraphics[scale=0.4]{ucfcrime_ex1.png}}\n    \\subfloat{\\includegraphics[scale=0.4]{ucfcrime_ex2.png}}\n    \\caption{UCF-Crime Dataset}\n    \\label{example_ucf}\n\\end{figure}\nThe dataset is already divided into training and test sets for uniformity. The training set consists of 810 anomalous videos while having 800 normal videos while the testing set has 150 normal and 140 anomalous videos. Despite being split into different datasets, all 13 anomalies are present in both sets lying at various locations in the video.", "cites": [4110], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the UCF-Crime Dataset, including its source, content, and structure, but does not effectively synthesize or integrate the cited paper's contributions. It lacks critical evaluation of the methodology used (e.g., the use of text queries and weak labeling), and fails to abstract broader implications or trends in dataset creation for video anomaly detection."}}
{"id": "a99d0377-0967-4872-97ec-a2502dbed884", "title": "The Street Scene Dataset", "level": "subsection", "subsections": [], "parent_id": "fc0df811-22c8-46cf-83ea-2bb566eb7619", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Existing Benchmark Datasets"], ["subsection", "The Street Scene Dataset"]], "content": "One of the recently published datasets, the Street Scene dataset\\footnote{http://www.merl.com/demos/video-anomaly-detection}  was created to solve the existing problems that the older datasets were facing which is to have more realistic anomalies and to have a greater variety with respect to the types of anomalies that are present. In Street Scene, there are 46 training video sequences and 35 testing sequences. These videos are taken from a stationary USB camera which views a two-lane street that has pedestrian sidewalks and bike lanes.\n\\begin{figure}[h]\\centering\n    \\subfloat{\\includegraphics[scale=0.3]{street_anomaly.png}}\n    \\subfloat{\\includegraphics[scale=0.3]{street_normal.png}}\n    \\caption{Street Scene}\n    \\label{example_street}\n\\end{figure}\nExample normal and anomaly in the Street Scene dataset are shown in Fig. \\ref{example_street}. The left-hand side of the figure shows a person jaywalking which is an anommaly in the dataset while the right figure shows a normal scene. There are a total of 17 different anomaly types in the dataset namely jaywalking, biker outside lane, loitering, dog on sidewalk, car outside lane, biker on sidewalk, pedestrian reverses direction, and so on.", "cites": [8733], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Street Scene dataset, including its purpose, composition, and examples of anomalies and normal scenes. It cites one paper and integrates minimal context from it, such as the motivation for creating the dataset. However, there is little synthesis with other works, no critical evaluation of the datasetâ€™s strengths or limitations, and no abstraction to broader trends or principles in video anomaly detection datasets."}}
{"id": "79767fa1-b1a4-4110-93ea-2e18800796ca", "title": "Evaluation Metrics", "level": "section", "subsections": [], "parent_id": "8b8ae080-427e-4f7b-a004-922aff6933a4", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Evaluation Metrics"]], "content": "This section briefly discusses the mostly used evaluation metrics by the papers that have been presented in this paper. Most of the works have followed the metrics introduced by  where there are two (2) different criteria. The first one is a \\textit{frame-level} criterion where a frame is considered anomalous if \\textit{at least one of its pixels} are tagged as anomalous. To evaluate using the frame-level criterion, the temporal labels are used to determine metrics true positives and false positives. The second one is a \\textit{pixel-level} criterion where if at least 40\\% of the anomalous pixels are detected, the frame is considered to be anomalous. For both criterion, the area under the curve (AUC) of the receiver operating characteristic curve (ROC) is computed to measure the final performance of the models. Given a classification model having different thresholds, the receiver operating characteristic curve (ROC) illustrates the performance of the model. The true positive rate and false positive rates defined in Equations \\ref{eq_tpr} and \\ref{eq_fpr} are the parameters of the said curve . \n\\begin{equation}\\label{eq_tpr}\n    \\mbox{True Positive Rate} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives} + \\mbox{False Negatives}}\n\\end{equation}\n\\begin{equation}\\label{eq_fpr}\n    \\mbox{False Positive Rate} = \\frac{\\mbox{False Positives}}{\\mbox{False Positives} + \\mbox{True Negatives}}\n\\end{equation}\nBasically, the ROC is a plot such that the true positive rate is on the y-axis and the false positive rate is on the x-axis. The values for each point in the plot is taken from different classification thresholds. The area under curve (AUC) of the ROC is used as a measure to determine how the good the model is performing. A higher value for the AUC of the ROC signifies that the model is performing well. The strengths of this metric include threshold-invariance and scale-invariance. It is scale-invariant because it does not look at the absolute values of the predictions and looks at how well the predictions are ranked. Meanwhile, it is also threshold-invariant since it measures the performance without considering the threshold chosen for classification. However, its strengths are also its weaknesses such as the scale-invariance of the metric might not be suited if well-calibrated probabilities are desired. Moreover, it is not suited for optimizing on metrics such as false positives in specific use cases since it expresses them as an aggregated value. Additionally, an equal error rate (EER) is computed alongside the receiver operating characteristic curve. The equal error rate computes for the percentage of misclassified frames when the false positive rate is equal to the miss rate. More specifically, it is when the $\\mbox{False Positive Rate} = 1 - \\mbox{True Positive Rate}$ for the frame-level criterion while it is $1 - \\mbox{EER}$ for the pixel-level criterion .\nThere are problems in both of these metrics as mentioned in the work of . They have pointed out that in the frame-level criterion, an algorithm could still be considered correct even if the anomalous pixel doesn't necessarily overlap with the spatial region as to where the event is happening. Additionally, the pixel-level criterion does not take into account predictions that do not overlap with the ground truth. This prompted  to propose new evaluation metrics alongside their recently published dataset. They have proposed to use track-based detection criterion and region-based detection criterion which they claim is similar to object tracking and object detection metrics. The track-based detection criterion measures the false positive regions per frame against the track-based detection rate (TBDR) which is defined in Equations \\ref{eq_tbdr} and \\ref{eq_fpr2}.\n\\begin{equation}\\label{eq_tbdr}\n    \\mbox{TBDR} = \\frac{\\mbox{number of anomalous tracks detected}}{\\mbox{total number of anomalous tracks}}\n\\end{equation}\n\\begin{equation}\\label{eq_fpr2}\n    \\mbox{FPR} = \\frac{\\mbox{total false positive regions}}{\\mbox{total frames}}\n\\end{equation}\nMeanwhile, the region-based detection criterion measures the false positive regions per frame against the region-based detection rate (RBDR) across all testing frames. Correctly detected anomalous regions in frames are identified similar to the track-based detection criterion. The definition of RBDR is shown in Equation \\ref{eq_rbdr}\n\\begin{equation}\\label{eq_rbdr}\n\\mbox{RBDR} = \\frac{\\mbox{number of anomalous regions detected}}{\\mbox{total number of anomalous regions}}\n\\end{equation}\nNote that anomalous tracks are correctly identified if the ground truth has an intersection over union (IoU) above a threshold $\\alpha$ with the detections. Similarly, anomalous regions in the frame is considered correctly identified if the ground truth has an IoU of above a threshold $\\beta$ with the corresponding detected regions.", "cites": [8733], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section integrates the evaluation metrics from the cited paper and extends the discussion by comparing the strengths and weaknesses of AUC and EER. It introduces newer criteria proposed in the literature and explains how they address the limitations of traditional metrics, demonstrating both synthesis and critical evaluation. The use of concepts like IoU and the distinction between track- and region-based detection adds a level of abstraction."}}
{"id": "89ad711f-9a42-41d3-bc53-37f8ab578c01", "title": "Discussion", "level": "section", "subsections": [], "parent_id": "8b8ae080-427e-4f7b-a004-922aff6933a4", "prefix_titles": [["title", "A Survey on Deep Learning Techniques for Video Anomaly Detection"], ["section", "Discussion"]], "content": "Based on the different methodologies discussed in this paper, it is evident that anomaly detection is indeed a hard task. Several deep learning methods ranging from simple architectures to complex unified approaches have been proposed by different researchers. By categorizing the different approaches together into groups such as reconstruction error, future frame prediction, using classifiers, and scoring, a paradigm has been introduced on how to view anomaly detection approaches. Moreover, the variety of the type of approaches present also goes to show that researchers have been exploring different ways and thinking out of the box to determine anomalous events mainly because of its difficulty.\nOne common theme from all of the papers is that most of them still are careful about taking into account several aspects of human action such as appearance and motion. Representations may differ such as the work of  which uses social force maps while  uses optical flows but the main idea remains the same. This points the research community to a direction that appearance and motion play a big part in detecting anomalies. More so, that even in deep learning approaches (which is supposed to automatically learn discriminative features), researchers still make use of these features or concepts to guide the network and make it look properly at these specific variables.\nRecent papers have started to think of creating end-to-end deep learning solutions and unified architectures rather than making use of separate components in a traditional pipeline. This is important as well because end-to-end deep learning solutions are easily deployable in real-life, making the research more accessible and more usable than it is now. However, end-to-end deep learning solutions require a lot of data which might be a problem for older datasets such as the UCSD Pedestrian or UMN Dataset but large scale datasets have been proposed by  to help solve this problem. Yet, an important issue to also consider as well is that video data is very laborious to annotate and collect which one of the main reasons why there haven't been as much large scale datasets published yet despite having tons of data publicly available in video sharing sites. This stresses the importance of making use of unsupervised or weakly-supervised approaches in tackling this problem.\nWith regards to evaluation, as presented by , the current evaluation metrics using the frame-level criterion and pixel-level criterion might not be representative of the performance of the model due to the reasons stated in their work. Hence, there might be a need to have more robust evaluation metrics which would be more effective irrespective of the type of new datasets that might be published in the future. Future evaluation metrics must consider providing better ways to assess spatial aspects of future methodologies since it is important to know which part of the frames cause the anomalies. This in turn, allows faster and better inference to what is happening should the approaches be deployed in real life.\nLooking from a different perspective, results have become better over time because methods by various researchers, have successfully managed to incorporate spatial and temporal information to their models, thereby achieving excellent results. Yet, for real-life anomalous events, it is more than spatial and temporal information, there also needs to be context added to make the models more robust. As seen from the different definitions of different authors, the very definition of what an anomaly is also vary from one context to another. One possible way to achieve this is to slowly pivot the research area towards larger datasets and datasets captured from real-life videos and real-life scenarios. Furthermore, borrowing concepts such as attention or transformers from different fields might also be helpful to achieve this goal.", "cites": [4110, 6980, 8733], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from the cited papers by connecting different methodologies and highlighting their shared goals and challenges. It provides critical analysis by pointing out limitations in current evaluation metrics and the scarcity of annotated datasets. The discussion abstracts beyond specific papers to identify broader trends such as the shift toward end-to-end architectures, the importance of spatial-temporal modeling, and the need for contextual understanding in anomaly detection."}}
